# Mean and Varaince Assumptions

So far we have not made any probabilistic assumptions about the different 
elements in our model. So the model errors are unknown but are not random.

Now we will assume that the random errors are random variables. However, we will
not specify the complete distribution of the errors and will limit ourselves to
make assumptions about the mean and variance of the errors.

## Mean Assumptions

We will begin by making the assumptions about the mean of the errors. 
Specifically, we will assume that:

$$ \dE[e_i] = 0 \in \dR \quad \forall i\in\{1,\ldots,n\}$$

This can be expressed in vector form as follows:

$$ \dE[\be] = \bzero \in \dR^n$$

Note that $\bX$ is a known constant and $\bgb$ is an unknown constant (an unknown 
parameter). This implies that \bX \bgb + $\by$ is a random vector. And therefor, any function
of $\by$ will be a random varaible. In particular, our estimates:

$$ \hat{\bgb} = (\bX'\bX)^{-1}\bX' \by$$
$$ \hat{\by} = \bX(\bX'\bX)^{-1}\bX' \by = \bH \by$$
$$ \hat{\be} = \by - \hat{\by} = (\bI - \bH)\by$$

are random vectors. Then we can try to compute the mean of these values. This 
should be possible since all 3 estimates are linear combinations of $\by$ and
we can compute the mean of $\by$.

### Expectation of $\by$:

\begin{align*}
\dE[\by] 
  &=  \dE[\bX \bgb + \be] \\
  &= \bX \bgb + \dE[\be] \\
  &= \bX \bgb
\end{align*}

### Expectation of $\hat{\bgb}$

\begin{align*}
\dE[\hat{\bgb}] 
  &= \dE[(\bX'\bX)^{-1}\bX' \by] \\
  &= (\bX'\bX)^{-1}\bX'\dE[\by] \\
  &= (\bX'\bX)^{-1}\bX' \bX \bgb \\
  &= \bgb \\
\end{align*}

So, $\hat{\bgb}$ is an unbiased estimator of $\bgb$.

### Expectation of $\hat{\by}$

\begin{align*}
\dE[\hat{\by}] 
  &= \dE[\bX \hat{\bgb}] \\
  &= \bX \dE[\hat{\bgb}] \\
  &= \bX \bgb \\
  &= \dE[\by] \\
\end{align*}

So $\by$ and $\hat{\by}$ have the same mean.

### Expectation of $\hat{\be}$

\begin{align*}
\dE[\hat{\be}] 
  &= \dE[\by - \hat{\by}] \\
  &= \dE[\by] - \dE[\hat{\by}] \\
  &= \bzero
\end{align*}

So $\by$ and $\hat{\by}$ have the same mean.

Without any further assumptions, we can get more additional results. Next, we
move to assumptions on the variance of the errors.

## Variance Assumptions

While the assumptions on the mean are assumptions about the first moment of the 
errors, now we will make assumptions about the second moment of the errors. In
particular, we will assume that all the errors have the same (finite) variance
and are uncorrelated. That is:

$$ \dV[e_i] = \sigma^2 < \infty \quad \forall i \in \{1,\ldots,n\}, \quad \text{and} \quad \dC[e_i, e_j] = 0 \quad \forall i \neq j$$

We can express this assumption in vector form as:

$$ \dV[\be] = \sigma^2 \bI \quad \sigma^2 < \infty $$

As we did before, we can try to compute the variance of all the random quantities
we have.

### Variance of $\by$

\begin{align*}
\dV[\by] 
  &= \dV[\bX + \be] \\
  &= \dV[\be]       \\
  &= \sigma^2 \bI
\end{align*}

So, the observations $\by$ and the errors $\be$ have the same variance. This makes
sense since they only differ by a non-random element $\bX \bgb$.

### Variance of $\hat{\bgb}$

\begin{align*}
\dV[\hat{\bgb}] 
  &= \dV[(\bX'\bX)^{-1}\bX' \by]                         \\
  &= (\bX'\bX)^{-1}\bX' \dV[\by] \bX(\bX'\bX)^{-1}       \\
  &= (\bX'\bX)^{-1}\bX' (\sigma^2 \bI) \bX(\bX'\bX)^{-1} \\
  &= \sigma^2 (\bX'\bX)^{-1}\bX'\bX(\bX'\bX)^{-1}        \\
  &= \sigma^2 (\bX'\bX)^{-1}                             \\
\end{align*}

### Variance of $\hat{\by}$

\begin{align*}
\dV[\hat{\by}] 
  &= \dV[\bH \by]                                              \\
  &= \bH \dV[\by] \bH'                                         \\
  &= \bH (\sigma^2 \bI) \bH & \text{since $\bH$ is symmetric}  \\
  &= \sigma^2 \bH\bH                                           \\
  &= \sigma^2 \bH           & \text{since $\bH$ is idempotent} \\
\end{align*}

So, while $\by$ and $\hat{\by}$ have the same mean, they do not have the same
variance. We will see that the variance of $\hat{\by}$ is "smaller" than the 
variance of $\by$. I say "smaller" since for matrices it is not clear what does 
it mean to be smaller or bigger.

Not for now, that the diagonal elements of these matrices satisfy the following:

$$[\dV[\by]]_{ii} = \sigma^2 \leq \sigma^2 h_{ii} = [\dV[\hat{\by}]]_{ii}$$
since the leverages $h_{ii}$ satisfy $0 \leq h_{ii} \leq 1$.

### Variance of $\hat{\be}$

\begin{align*}
\dV[\hat{\be}] 
  &= \dV[(\bI - \bH) \by]                                                             \\
  &= (\bI - \bH) \dV[\by] (\bI - \bH)'                                                \\
  &= (\bI - \bH) (\sigma^2 \bI) (\bI - \bH) & \text{since $(\bI - \bH)$ is symmetric} \\
  &= \sigma^2 (\bI - \bH)(\bI - \bH)                                                  \\
  &= \sigma^2 (\bI - \bH)           & \text{since $(\bI - \bH)$ is idempotent}        \\
\end{align*}

Then we can see that $\by$, $\hat{\by}$ and $\hat{\be}$ have variances that are
idempotent matrices multiplied by the scalar $\sigma^2$.

## Cross-Covariances

When we introduce the assumption of the variance, we can check the 
cross-covariances of our estimators. This will help us later in the course.

We can check several cross-covariances but for now I will only check 2.

### Cross-covaraince of $\hat{y}$ and $\hat{e}$

\begin{align*}
\dC[\hat{\by}, \hat{\be}]
  &=\dC[(\bI - \bH) \by, \bH \by]                                                           \\
  &=(\bI - \bH)\dC[\by,\by] \bH'                                                            \\
  &=(\bI - \bH)\dV[\by] \bH & \text{since $\bH$ is symmetric and $\dC[\by,\by] = \dV[\by]$} \\
  &=(\bI - \bH)(\sigma^2 \bI) \bH & \text{since $\dV[\by] = \sigma^2 \bI$}                  \\
  &=\sigma^2 (\bI - \bH) \bH                                                                \\
  &=\sigma^2 (\bH - \bH \bH)                                                                \\
  &=\sigma^2 (\bH - \bH) & \text{since $\bH$ is idempotent}                                 \\
  &=\bzero \in \dR^{n \times n}                                                             \\
\end{align*}

Then, the residuals $\hat{\be}$ and the estimates of the observations 
$\hat{\by}$ are uncorrelated. We had a similar result before, that however is not
the same (we couldn't have even talked before expectation and covariance since we
didn't have random variables). That result was:

$$ \hat{\be}'\hat{\by} = 0 \in \dR $$

Notice that, the dimension of the zero's.


### Cross-covaraince of $\hat{y}$ and $\hat{\bgb}$

\begin{align*}
\dC[\hat{\be}, \hat{\bgb}]
  &= \dC[(\bI - \bH) \by, (\bX' \bX)^{-1} \bX' \by]                \\
  &= (\bI - \bH) \dC[\by, \by] \bX (\bX' \bX)^{-1}                 \\
  &= (\bI - \bH) \dV[\by] \bX (\bX' \bX)^{-1}                      \\
  &= (\bI - \bH) (\sigma^2 \bI) \bX (\bX' \bX)^{-1}                \\
  &= \sigma^2 (\bI - \bH) \bX (\bX' \bX)^{-1}                      \\
  &= \sigma^2 (\bI - \bX (\bX' \bX)^{-1} \bX') \bX (\bX' \bX)^{-1} \\
  &= \sigma^2 (\bX - \bX (\bX' \bX)^{-1} \bX'\bX)(\bX' \bX)^{-1}   \\
  &= \sigma^2 (\bX - \bX)(\bX' \bX)^{-1}                           \\
  &= \bzero \in \dR^{n \times p}                                   \\
\end{align*}

So the residuals and the estimate of $\bgb$ are uncorrelated. We will use this
result in the next chapter.

## Gauss-Markov Theorem

This theorem justifies the use of the Ordinary Least Squares (OLS) estimator,
since it is the "best" estimator in a way.

### Assumptions

The theorem assumes that:

- The errors have the same finite variance.
- The errors are uncorrelated.

This assumption is equivalent to our assumption of:

$$ \dV[\be] = \sigma^2 \bI \quad \sigma^2 < \infty $$

### Statement

In the linear regression model, the OLS estimator $\hat{\bgb} = (\bX' \bX)^{-1} \bX' \by$
has the smallest variance among all unbiased linear estimators.

That is, if $\tilde{\bgb}$ is a linear estimator, then:

$$ \dV[\tilde{\bgb}] - \dV[\hat{\bgb}] $$

is semi-positive definite.

Because of this, the OLS estimator is called the Best Linear Unbiased Estimator
(BLUE), where best means smaller variance.

### Proof

Let $\tilde{\bgb}$ be an unbiased linear estimator of $\bgb$. Then, we can write
$\tilde{\bgb}$ as follows:

$$\tilde{\bgb} = \bA \by$$

where $\bA$ is a constant matrix of the appropriate dimensions. Then we can write
$\tilde{\bgb}$ as follows:

\begin{align*}
\tilde{\bgb}
  &=(\bA - (\bX' \bX)^{-1} \bX' + (\bX' \bX)^{-1} \bX') \by             \\
  &=(\bA - (\bX' \bX)^{-1} \bX')\by + (\bX' \bX)^{-1} \bX' \by          \\
  &=(\bD) + \hat{\bgb} & \text{with $\bD = \bA - (\bX' \bX)^{-1} \bX'$} \\
\end{align*}

Now, since $\tilde{\bgb}$ is unbiased, we have that:

$$\dE[\tilde{\bgb}] = \bgb \quad \forall \bgb \in \dR^p $$

And

\begin{align*}
\dE[\tilde{\bgb}]
  &= \dE[\bD \by + \hat{\bgb}] \quad \forall \bgb \in \dR^p      \\
  &= \bD \dE[\by] + \dE[\hat{\bgb}] \quad \forall \bgb \in \dR^p \\
  &= \bD \bX \bgb + \bgb \quad \forall \bgb \in \dR^p & \text{since $\dE[\by] = \bX \bgb$ and $\dE[\hat{\bgb}] = \bgb$} \\
\end{align*}

Then

\begin{align*}
  \dE[\tilde{\bgb}] = \bgb \quad \forall \bgb \in \dR^p           \\
  &\implies \bD \bX \bgb + \bgb = \bgb \quad \forall \bgb \in \dR^p  \\
  &\implies \bD \bX \bgb = \bzero \quad \forall \bgb \in \dR^p       \\
  &\implies \bD \bX = \bzero                                         \\
\end{align*}

Now, we can analyze the variance of $\tilde{\bgb}$.

\begin{align*}
  \dV[\tilde{\bgb}]
    &= \dV[\bD \by + \hat{\bgb}]                                                                                            \\
    &= \dV[\bD \by + (\bX' \bX)^{-1} \bX' \by] && \text{since $\hat{\bgb} = (\bX' \bX)^{-1} \bX' \by$}                      \\
    &= \dV[(\bD + (\bX' \bX)^{-1} \bX') \by]                                                                                \\
    &= (\bD + (\bX' \bX)^{-1} \bX') \dV[\by] (\bD + (\bX' \bX)^{-1} \bX')'                                                  \\
    &= (\bD + (\bX' \bX)^{-1} \bX') \dV[\by] (\bD' + \bX (\bX' \bX)^{-1})                                                   \\
    &= (\bD + (\bX' \bX)^{-1} \bX') (\sigma^2 \bI) (\bD' + \bX (\bX' \bX)^{-1}) && \text{since $\dV[\by] = \sigma^2 \bI$}   \\
    &= \sigma^2 (\bD + (\bX' \bX)^{-1} \bX') (\bD' + \bX (\bX' \bX)^{-1})                                                   \\
    &= \sigma^2 (\bD \bD' + (\bX' \bX)^{-1} \bX' \bD'                                                                       \\
    &\quad + \bD \bX (\bX' \bX)^{-1} + (\bX' \bX)^{-1} \bX' \bX (\bX' \bX)^{-1})                                            \\
    &= \sigma^2 (\bD \bD' + (\bX' \bX)^{-1}) && \text{since $\bD \bX = \bzero$}                                             \\
    &= \sigma^2 (\bX' \bX)^{-1} + \sigma^2 (\bD \bD')                                                                       \\
    &= \dV[\hat{\bgb}] + \sigma^2 (\bD \bD') && \text{since $\dV[\hat{\bgb}] = \sigma^2 (\bX' \bX)^{-1}$}                   \\
\end{align*}

Since $\bD \bD'$ is semi-positive definite, we have that:

$$ \dV[\tilde{\bgb}] - \dV[\hat{\bgb}] = \sigma^2 (\bD \bD') $$

is semi-positive definite. This concludes the end of the proof.

So, as it turns out, the OLS estimator that came out of minimizing the least
squares, and made no assumptions about the expectation and variances is a very
good estimator, since it is:

- Unbiased
- Among the unbiased and linear estimators, it has the smallest variance.

Another common measure of performance is the Mean Square Error (MSE) of an estimator
with respect to a parameter. Using the Gauss-Markov Theorem we can conclude that,
among the unbiased and linear estimators, the OLS estimator has the smallest MSE.

Since, both estimators are unbiased, we have that:

$$ \dM(\hat{\mathbf{\bgb}}) = \text{tr}(\text{Var}(\hat{\mathbf{\bgb}})) + \|\text{Bias}(\hat{\mathbf{\bgb}})\|^2 = \text{tr}(\text{Var}(\hat{\mathbf{\bgb}})) $$
$$ \dM(\tilde{\mathbf{\bgb}}) = \text{tr}(\text{Var}(\tilde{\mathbf{\bgb}})) + \|\text{Bias}(\tilde{\mathbf{\bgb}})\|^2 = \text{tr}(\text{Var}(\tilde{\mathbf{\bgb}})) $$

Then, by the Gauss-Markov theorem:

\begin{align*}
  \dM[\tilde{\bgb}] - \dM[\hat{\bgb}]
    &= \text{tr}(\text{Var}(\tilde{\mathbf{\bgb}})) - \text{tr}(\text{Var}(\hat{\mathbf{\bgb}}))                                     \\
    &= \text{tr}(\text{Var}(\tilde{\mathbf{\bgb}})) - \text{Var}(\hat{\mathbf{\bgb}})) && \text{since the trace operator is linear.} \\
    &\geq 0          \\
\end{align*}

Where we use the fact that $\text{Var}(\tilde{\mathbf{\bgb}})) - \text{Var}(\hat{\mathbf{\bgb}})$
is positive semi-definite by the Gauss-Markov theorem and the trace of a positive
semi-definite matrix is non-negative.

## Estimate of $\sigma^2$

Since we have introduced a new parameter $\sigma^2$, we might be interested in 
estimate it. We will propose an estimate of $\sigma^2$ of the following shape:

$$\hat{\sigma}^2 = a \hat{\be}'\hat{\be}$$

where we will choose the scalar $a$ later.

In particular, we want to take the expectation of this estimate. To do so, we will
take a roundabout way. First we will show that:

$$(\by - \dE[\by])'(\by - \dE[\by]) = \hat{\be}'\hat{\be} + (\hat{\by} - \dE[\hat{\by}])'(\hat{\by} - \dE[\hat{\by}])$$

and we will use this fact to compute the expectation of $\hat{\be}'\hat{\be}$.

\begin{align*}
  (\by - \dE[\by])'& (\by - \dE[\by])                                                                                           \\
    &= (\by - \bX \bgb)'(\by - \bX \bgb) && \text{since $\dE[\by] = \bX \bgb$}                                                  \\
    &= (\by - \bX \hat{\bgb} + \bX \hat{\bgb} - \bX \bgb)'(\by - \bX \hat{\bgb} + \bX \hat{\bgb} - \bX \bgb)                    \\
    &= (\by - \bX \hat{\bgb})'(\by - \bX \hat{\bgb}) + 2(\by - \bX \hat{\bgb})'(\bX \hat{\bgb} - \bX \bgb)                      \\
    &\quad + (\bX \hat{\bgb} - \bX \bgb)'(\bX \hat{\bgb} - \bX \bgb)                                                            \\
    &= (\by - \hat{\by})'(\by - \hat{\by}) + 2(\by - \hat{\by})'(\bX \hat{\bgb} - \bX \bgb)                                     \\
    &\quad + (\hat{\by} - \dE[\hat{\by}])'(\hat{\by} - \dE[\hat{\by}])                                                          \\
    &= \hat{\be}'\hat{\be} + 2\hat{\be}'(\bX \hat{\bgb} - \bX \bgb) + (\hat{\by} - \dE[\hat{\by}])'(\hat{\by} - \dE[\hat{\by}]) \\
    &= \hat{\be}'\hat{\be} + 2\hat{\be}'\bX (\hat{\bgb} - \bgb) + (\hat{\by} - \dE[\hat{\by}])'(\hat{\by} - \dE[\hat{\by}])     \\
    &= \hat{\be}'\hat{\be} + (\hat{\by} - \dE[\hat{\by}])'(\hat{\by} - \dE[\hat{\by}]) && \text{since $\hat{\be}'\bX=\bzero$}   \\
\end{align*}

Since

$$\dE[(\by - \dE[\by])'(\by - \dE[\by])] = \text{tr}(\dV[\by]) = \text{tr}(\sigma^2 \bI) = \sigma^2 \text{tr}(\bI) = \sigma^2 n$$
$$\dE[(\hat{\by} - \dE[\hat{\by}])'(\hat{\by} - \dE[\hat{\by}])] = \text{tr}(\dV[\hat{\by}]) = \text{tr}(\sigma^2 \bH) = \sigma^2 \text{tr}(\bH) = \sigma^2 p$$
where we use the fact that the trace of an idempotent matrix is equal to the rank of the matrix.  Then we get that:

\begin{align*}
  (\by - \dE[\by])'& (\by - \dE[\by]) = \hat{\be}'\hat{\be} + (\hat{\by} - \dE[\hat{\by}])'(\hat{\by} - \dE[\hat{\by}])                          \\
    &\implies \dE[(\by - \dE[\by])'(\by - \dE[\by])] = \dE[\hat{\be}'\hat{\be}] + \dE[(\hat{\by} - \dE[\hat{\by}])'(\hat{\by} - \dE[\hat{\by}])] \\
    &\implies \text{tr}(\dV[\by]) = \dE[\hat{\be}'\hat{\be}] + \text{tr}(\dV[\hat{\by}])                                                         \\
    &\implies \sigma^2 n = \dE[\hat{\be}'\hat{\be}] + \sigma^2 p                                                                                 \\
    &\implies \dE[\hat{\be}'\hat{\be}] = \sigma^2 n - \sigma^2 p                                                                                 \\
    &\implies \dE[\hat{\be}'\hat{\be}] = \sigma^2 (n - p)                                                                                        \\
\end{align*}

Then, if we set $a=(\frac{1}{n-p})$ we have that our proposed estimator $\hat{\sigma}^2$
is unbiased. Since:

\begin{align*}
  \dE[a \hat{\sigma}^2] 
    &= \dE[a \hat{\be}'\hat{\be}]   \\
    &= a \dE[\hat{\be}'\hat{\be}]   \\
    &= a \sigma^2 (n-p)             \\
    &= \frac{1}{n-p} \sigma^2 (n-p) \\
    &= \sigma^2                     \\
\end{align*}

We can choose or obtain other values for $a$, however the estimator will not be
unbiased.

















