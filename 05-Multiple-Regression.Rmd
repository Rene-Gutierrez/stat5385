# Multiple Linear Regression

## Introduction

Multiple regression is a statistical technique used to model the relationship between a dependent variable and two or more independent variables. It extends simple linear regression by allowing for a more complex analysis of how various factors impact an outcome. The general form of the multiple regression equation is:

$$
y_i = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \ldots + \beta_p x_{p,i} + e_i \quad i=\{1,\ldots,n\}
$$

Where:
- $y$ is the dependent variable.
- $\beta_0$ is the intercept.
- $\beta_1, \beta_2, \ldots, \beta_p$ are the coefficients of the independent variables $X_1, X_2, \ldots, X_p$.
- $e represents the error term.

This technique is widely used across various fields, including economics, social sciences, and healthcare, to control for multiple factors and enhance prediction accuracy. However, it requires careful attention to assumptions. Beyond linearity, now we have to consider issues like independence, multicollinearity, and variable selection which can affect the results.

We already have seen an example of Multiple linear regression when we worked with
Polynomial regression. However, multiple linear regression is more general.

## Example

Consider, for example, the task of explaining a country's GDP using other economic variables such as inflation, unemployment, reference interest rate, government spending (as a percentage of GDP), and exports (as a percentage of GDP).

In this case, visualization is not as straightforward, and visually inspecting these relationships is much less practical. Nevertheless, with this number of variables, we can visually explore the relationships between them as follows:

```{r paris-plot-gdp}
# Reads Data
dat <- read.csv(file = "Gdp Data.csv")

# Plot the scatterplots for each pair of variables
pairs(dat)
```

Here we can see, that some independent variables are more related to `GDP` and
some independent variables are more related between themselves. This is valuable information that will help us
to develop the right linear model with this variables.

We can also observe the correlation between these variables as follows:

```{r gdp-cor}
# Computes the correlation between variables
cor(dat)
```
We can also fit simple linear regression with each one of the independent
variables.

**Inflation Rate**

```{r gdp-inf-fit}
# Fits with Inflation
outRegInf <- lm(gdp ~ inf, data = dat)
varVal    <- dat$inf
out       <- outRegInf
varNam    <- "Inflation Rate"
# Plots Regression Line and Scatterplot and residuals plot
par(mfrow = c(1, 2))
plot(x    = varVal,
     y    = dat$gd,
     xlab = varNam,
     ylab = "GDP")
abline(a   = out$coefficients[1],
       b   = out$coefficients[2],
       col = 'red',
       lwd = 2)
plot(x    = varVal,
     y    = out$residuals,
     xlab = varNam,
     ylab = "Residuals")
abline(h   = 0,
       lwd = 2)
```
**Unemployment Rate**

```{r gdp-unp-fit}
# Fits with Inflation
outRegUne <- lm(gdp ~ une, data = dat)
varVal    <- dat$une
out       <- outRegUne
varNam    <- "Unemplyment Rate"
# Plots Regression Line and Scatterplot and residuals plot
par(mfrow = c(1, 2))
plot(x    = varVal,
     y    = dat$gd,
     xlab = varNam,
     ylab = "GDP")
abline(a   = out$coefficients[1],
       b   = out$coefficients[2],
       col = 'red',
       lwd = 2)
plot(x    = varVal,
     y    = out$residuals,
     xlab = varNam,
     ylab = "Residuals")
abline(h   = 0,
       lwd = 2)
```
**Interest Rate**

```{r gdp-int-fit}
# Fits with Inflation
outRegInt <- lm(gdp ~ int, data = dat)
varVal    <- dat$int
out       <- outRegInt
varNam    <- "Interest Rate"
# Plots Regression Line and Scatterplot and residuals plot
par(mfrow = c(1, 2))
plot(x    = varVal,
     y    = dat$gd,
     xlab = varNam,
     ylab = "GDP")
abline(a   = out$coefficients[1],
       b   = out$coefficients[2],
       col = 'red',
       lwd = 2)
plot(x    = varVal,
     y    = out$residuals,
     xlab = varNam,
     ylab = "Residuals")
abline(h   = 0,
       lwd = 2)
```

**Goverment Spending**

```{r gdp-gov-fit}
# Fits with Inflation
outRegGov <- lm(gdp ~ gov, data = dat)
varVal    <- dat$gov
out       <- outRegGov
varNam    <- "Goverment Spending"
# Plots Regression Line and Scatterplot and residuals plot
par(mfrow = c(1, 2))
plot(x    = varVal,
     y    = dat$gd,
     xlab = varNam,
     ylab = "GDP")
abline(a   = out$coefficients[1],
       b   = out$coefficients[2],
       col = 'red',
       lwd = 2)
plot(x    = varVal,
     y    = out$residuals,
     xlab = varNam,
     ylab = "Residuals")
abline(h   = 0,
       lwd = 2)
```

*Exports*

```{r gdp-exp-fit}
# Fits with Inflation
outRegExp <- lm(gdp ~ exp, data = dat)
varVal    <- dat$exp
out       <- outRegExp
varNam    <- "Exports"
# Plots Regression Line and Scatterplot and residuals plot
par(mfrow = c(1, 2))
plot(x    = varVal,
     y    = dat$gd,
     xlab = varNam,
     ylab = "GDP")
abline(a   = out$coefficients[1],
       b   = out$coefficients[2],
       col = 'red',
       lwd = 2)
plot(x    = varVal,
     y    = out$residuals,
     xlab = varNam,
     ylab = "Residuals")
abline(h   = 0,
       lwd = 2)
```

All of them seem like good candidates for a linear relationship with the GDP,
however when we use them all together, a more careful analysis should be made.

We can see the summary reports for the individual regressions and the regression
with all independent variables as follows:

```{r gdp-all-ind-com}
outRegAll <- lm(gdp ~ inf + une + int + gov + exp, data = dat)

# Summary All
print("All Independent Variables")
summary(outRegAll)
print("Only Inflation Rate")
summary(outRegInf)
print("Only Unemployment Rate")
summary(outRegUne)
print("Only Interest Rate")
summary(outRegInt)
print("Only Government Spending")
summary(outRegGov)
print("Only Exports")
summary(outRegExp)
```

As we can see, the values for the coefficients can change when doing simple linear
regression and multiple linear regression. If the changes are very dramatic (like change
in the sign of the coefficient) further inspection is necessary for that variable.

## Least Squares Estimation

For least squares estimation, we need to solve the problem:

$$
\min_\bgb Q(\bgb) = \sum_{i=1}^n (y_i - \hy(\bgb))^2 = (\by - \hat{\by})'(\by - \hat{\by}) = (\by - \bX\bgb)'(\by - \bX\bgb) 
$$
The representation in matrix notation of the problem, allows us to use the same
expression to solve this problem as with simple linear regression. The solution
is obtained in the exact same way, and is given by:

$$
\hat{\bgb} = (\bX' \bX)^{-1}\bX'\by
$$
however in this case:

$$
\hat{\bgb} = \left(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2,\ldots,\hat{\beta}_p\right)'
$$
this is the reason, working in matrix form is very useful.

## Properties of the Estimates

As with simple linear regression, we can consider several estimates:

* $\hat{\by} = \bX \bgb$ the estimates of the observations,
* $\hat{\be} = \by - \hat{\by} = \by - \bX \hat{\bgb}$ the estimates of the errors.

We also note that:

$$
\hat{\by} = \bX (\bX'\bX)^{-1}\bX' \by = \bH y
$$
where $\bH$ is called the hat matrix, because it transforms $\by$ into $\hat{\by}$,
or the projection matrix.

We will see that:

* $\hat{\bgb}$ is a linear combination of $y$.
* The sum of the estimated errors is equal to zero, $\sum_{i=1}^n \hat{e_i} = 0$.
* $\hat{\be}$ and $\hat{\bx_j}$ are orthogonal for $j=\{1,\ldots,p\}$.
* $\hat{\be}$ and $\hat{\by}$ are orthogonal.
* $\bar{y} = \hat{\bar{y}}$.

To see that $\hat{\bgb}$ is a linear combination of $y$, we need to express $\hat{\bgb}$
as follows:

$$
\hat{\bgb} = \bA \by
$$

for some matrix $\bA$. This is very easy to do, we just let $\bA = (\bX'\bX)^{-1}\bX'$, so:

$$
\hat{\bgb} = (\bX'\bX)^{-1}\bX'\by = \bA \by
$$
Now to see that the sum of the estimated errors is equal to zero, $\sum_{i=1}^n \hat{e_i} = 0$, we
notice that we need to show that:

$$
\hat{\be}' \bones = 0
$$

To do so we notice that:

\begin{align*}
\hat{\bgb} = (\bX'\bX)^{-1}\bX'\by 
  &\implies (\bX'\bX)\hat{\bgb} = \bX'\by          \\
  &\implies \bX'\by - \bX'\bX \hat{\bgb} = \bzero \\
  &\implies \bX'\left(\by - \hat{\by}\right) = \bzero \\
  &\implies \bX'\hat{\be} = \bzero 
\end{align*}

Now focusing on the product $\bX'\hat{\be}$ we have that:

$$
\bX'\hat{\be} =
  \left[\begin{matrix}
    \bones' \\
    \bx_1   \\
    \bx_2   \\
    \vdots  \\
    \bx_p
  \end{matrix}\right] \hat{\be} =
  \left[\begin{matrix}
    \bones' \hat{\be} \\
    \bx_1   \hat{\be} \\
    \bx_2   \hat{\be} \\
    \vdots            \\
    \bx_p   \hat{\be}
  \end{matrix}\right]
$$
So we have that:

$$
\left[\begin{matrix}
    \bones' \hat{\be} \\
    \bx_1   \hat{\be} \\
    \bx_2   \hat{\be} \\
    \vdots            \\
    \bx_p   \hat{\be}
  \end{matrix}\right] = 
  \left[\begin{matrix}
    0 \\
    0 \\
    0 \\
    \vdots            \\
    0
  \end{matrix}\right]
$$
So from the first line of this result, we have that:

$$
\bones' \hat{\be} = 0
$$
which is the result we wanted to proof.

Now, to show that $\hat{\be}$ and $\hat{\bx_j}$ are orthogonal for $j=\{1,\ldots,p\}$,
we use again on:

$$
\left[\begin{matrix}
    \bones' \hat{\be} \\
    \bx_1   \hat{\be} \\
    \bx_2   \hat{\be} \\
    \vdots            \\
    \bx_p   \hat{\be}
  \end{matrix}\right] = 
  \left[\begin{matrix}
    0 \\
    0 \\
    0 \\
    \vdots            \\
    0
  \end{matrix}\right]
$$

And notice that lines 2 to $p+1$ proof this results, that is

$$
\bx_i ' \hat{\be} = 0 \quad i=\{1,\ldots,p\}
$$
Now to show that $\hat{\be}$ and $\hat{\by}$ are orthogonal, we show that:

$$
\hat{\be}'\hat{\by} = 0
$$
Now

\begin{align*}
\hat{\be}'\hat{\by} 
  &= (\by - \hat{\by})'\hat{\by} \\
  &= \left(\by - \bX \hat{\bgb}\right)'\bX \hat{\bgb} \\
  &= \left(\by - \bX (\bX'\bX)^{-1}\bX'\by \right)'\bX (\bX'\bX)^{-1}\bX'\by \\
  &= \by' \bX (\bX'\bX)^{-1}\bX'\by - \by' \bX (\bX'\bX)^{-1}\bX' \bX (\bX'\bX)^{-1}\bX' \by \\
  &= \by' \bX (\bX'\bX)^{-1}\bX'\by - \by' \bX (\bX'\bX)^{-1}\bX' \by \\
  &= 0
\end{align*}

Finally, to show that $\bar{y} = \hat{\bar{y}}$, we use:

$$
\hat{\be}'\bones = (\by - \hat{\by})'\bones = \by'\bones - \hat{\by}'\bones = \sum_{i=1}^ny_i - \sum_{i=1}^n\hat{y}_i = n\bar{y} - n\hat{\bar{y}} 
$$
since $\hat{\be}'\bones = 0$, then we have that

$$
n\bar{y} - n\hat{\bar{y}} = 0 \implies n\bar{y} = n\hat{\bar{y}} \implies \bar{y} = \hat{\bar{y}}  
$$


## Multiple $R^2$

As with simple linear regression we can explain the total variability, by decomposing the
variability in two parts, the regression variability and the error variability.

First, we define this concepts:

1. **Total Sum of Squares $SS_{tot}$**:  
   The total sum of squares measures the total variability in $\mathbf{y}$:
   
   $$
   SS_{tot} = (\mathbf{y} - \bar{y} \mathbf{1})' (\mathbf{y} - \bar{y} \mathbf{1})
   $$

2. **Residual Sum of Squares $SS_{res}$**:  
   The residual sum of squares measures the unexplained variability in the regression model:
   
   $$
   SS_{res} = (\mathbf{y} - \hat{\mathbf{y}})' (\mathbf{y} - \hat{\mathbf{y}})
   $$

3. **Explained Sum of Squares $SS_{reg}$**

The explained sum of squares measures how much of the total variability is explained by the regression model. It is the difference between the predicted values and the mean of $\mathbf{y}$:

$$
SS_{reg} = (\hat{\mathbf{y}} - \bar{y} \mathbf{1})' (\hat{\mathbf{y}} - \bar{y} \mathbf{1})
$$
As with simple linear regression, it can be shown that:

$$
SS_{tot} = SS_{reg} + SS_{res}
$$
To see this, we start form $SS_{tot}$, and do the adding and subtracting trick:

\begin{align*}
SS_{tot} 
  &= (\mathbf{y} - \bar{y} \mathbf{1})' (\mathbf{y} - \bar{y} \mathbf{1}) \\
  &= (\mathbf{y} - \hat{\by} + \hat{\by} - \bar{y} \mathbf{1})' (\mathbf{y} - \hat{\by} + \hat{\by} - \bar{y} \mathbf{1}) \\
  &= (\mathbf{y} - \hat{\by})' (\mathbf{y} - \hat{\by}) + (\mathbf{y} - \hat{\by})' (\hat{\by} - \bar{y} \mathbf{1}) + (\hat{\by} - \bar{y} \mathbf{1})' (\mathbf{y} - \hat{\by}\mathbf{1}) + (\hat{\by} - \bar{y} \mathbf{1})' (\hat{\by} - \bar{y} \mathbf{1})
\end{align*}

Now, notice that:

$$
(\mathbf{y} - \hat{\by})' (\hat{\by} - \bar{y} \mathbf{1}) = \hat{\be}' (\hat{\by} - \bar{y} \mathbf{1}) = \hat{\be}'\hat{\by} - \bar{y}\hat{\be}' \mathbf{1} = 0 - \bar{y}0 = 0
$$
And similarly for $(\hat{\by} - \bar{y} \mathbf{1})' (\mathbf{y} - \hat{\by}\mathbf{1}) = 0$, then:

$$
SS_{tot} = (\mathbf{y} - \hat{\by})' (\mathbf{y} - \hat{\by}) + (\hat{\by} - \bar{y} \mathbf{1})' (\hat{\by} - \bar{y} \mathbf{1}) = SS_{reg} + SS_{res}
$$
The multiple $R^2$ is the variability explained by the regression with respect to the total variability
and can be expressed as:

$$
R^2 = \frac{SS_{reg}}{SS_{tot}}
$$
or using the previous expression

$$
1 = \frac{SS_{tot}}{SS_{tot}} = \frac{SS_{reg}}{SS_{tot}} + \frac{SS_{res}}{SS_{tot}} = R^2 + \frac{SS_{res}}{SS_{tot}} \implies R^2 = 1 - \frac{SS_{res}}{SS_{tot}}  
$$

Finally, we work on the expressions of $SS_{res}$ and $SS_{tot}$, to express them
in terms of projection matrices.


First note that:

$$
\by - \hat{\by} = \by - \bX\hat{\bgb} = \by - \bH \by = (\bI - \bH)\by
$$
and also notice that $(\bI - \bH)$ is symmetric and:

$$
(\bI - \bH)(\bI - \bH) = \bI -\bH - \bH + \bH \bH
$$
and

$$
\bH \bH = \bX (\bX'\bX)^{-1}\bX' \bX (\bX'\bX)^{-1}\bX' = \bX (\bX'\bX)^{-1}\bX' = \bH
$$
this means $\bH$ is idempotent. In fact, all projection matrices are idempotent.

Then, we have that:

$$
(\bI - \bH)(\bI - \bH) = \bI -\bH - \bH + \bH = \bI -\bH - \bH
$$
which makes $\bI - \bH$ also idempotent. Therefore:

$$
SS_{res} = (\by - \hat{\by})'(\by - \hat{\by}) = ((\bI - \bH)\by)'((\bI - \bH)\by) = \by'(\bI - \bH)'(\bI - \bH)\by = \by'(\bI - \bH)\by 
$$
And we can do a similar trick for the $SS_{tot}$ by writing $\bar{y} \bones$ as a result
of projecting $\by$ with a design matrix $\bones$:

$$
\bar{y} \bones = \bones \bar{y} = \bones \frac{1}{n} \sum_{i=1}^n y_i = \bones \frac{1}{n}\bones' \by = \bones (\bones'\bones)^{-1}\bones' \by  
$$
where we use the fact that $\bones'\bones = n$.

We call $\bH_0 = \bones (\bones'\bones)^{-1}\bones'$, since $\bones (\bones'\bones)^{-1}\bones'$
is a projection matrix. And since it is a projection matrix it is idempotent (it is also not difficult to check this manually)
and $\bI - \bH_0$ is also idempotent.

So we can do:

$$
\by - \hat{y}\bones = \by - \bH_0 \by = (\bI-\bH_0)\by
$$

$$
SS_{tot} = (\by - \bar{y}\bones)'(\by - \bar{y}\bones) = ((\bI - \bH_0)\by)'((\bI - \bH_0)\by) = \by'(\bI - \bH_0)'(\bI - \bH_0)\by = \by'(\bI - \bH_0)\by 
$$

so the $R^2$ can be expressed as follows:

$$
R^2 = 1 - \frac{\by'(\bI - \bH)\by}{\by'(\bI - \bH_0)\by}
$$
When written like this, it is easy to see that:

$$
\by'(\bI - \bH)\by = \min_\bgb (\by - \bX \bgb)'(\by - \bX \bgb)
$$
the solution to this minimization problem, since we are using the optimal value
$\hat{\bgb}$. And

$$
\by'(\bI - \bH_0)\by = \min_{\beta_0} (\by - \bX_0 \beta_0)'(\by - \bX_0 \beta_0)
$$

where $\bX_0$ is just a matrix with one column $\bones$.

Now, we also have that:

$$
\min_\bgb (\by - \bX \bgb)'(\by - \bX \bgb) \leq \min_{\beta_0} (\by - \bX_0 \beta_0)'(\by - \bX_0 \beta_0)
$$

therefore

$$
\by'(\bI - \bH)\by \leq \by'(\bI - \bH_0)\by
$$
and since both of them are quadratic forms, we have that:

$\by'(\bI - \bH_0)\by, \by'(\bI - \bH)\by \geq 0$

then:

$$0 \leq \frac{\by'(\bI - \bH)\by}{\by'(\bI - \bH_0)\by} \leq 0$$
then:

$$0 \leq R^2 \leq 0$$.

Where we use the fact that all symmetric idempotent matrices are symmetric positive semi-definite.

Another interpretation of $R^2$ is the percentage of the variability explained 
by multiple regression of a "*poor man's regression*" in which you don't have
independent variables (that is you are independent variable poor). In this way,
we can define

$$
\bar{y} \bones = \hat{\by}_0
$$
the "*poor man's prediction*", of which $\bH_0$ is it's projection matrix (or hat matrix).

## Geometric Interpretation of Multiple Linear Regression

Multiple linear regression can be thought as projecting $\by$ in the column 
space of the design matrix $\bX$. The following diagram pictures multiple linear regression.

```{r geo-int, echo=FALSE}
par(mar = c(0,0,0,0))
plot(NULL,
     xlim = c(0,100),
     ylim = c(0,100),
     xaxt = "n",
     yaxt = "n",
     xlab = "",
     ylab = "")
xPad <- 20
yPad <- 50
polygon(x = c(0, xPad, 100, 100 - xPad, 0),
        y = c(0, yPad, yPad,         0, 0),
        col = rgb(0, 0, 0, 0.2),
        border = NA)
segments(x0  = xPad,
         x1  = 100 - xPad - 10,
         y0  = yPad,
         y1  = 100 - 10,
         lwd = 2)
segments(x0  = 100 - xPad - 10,
         x1  = 100 - xPad - 10,
         y0  = 100 - 10,
         y1  = yPad - 20,
         lwd = 2,
         col = 'red')
segments(x0  = xPad,
         x1  = 100 - xPad - 10,
         y0  = yPad,
         y1  = yPad - 20,
         lwd = 2)
segments(x0  = xPad,
         x1  = 100 - xPad - 30,
         y0  = yPad,
         y1  = yPad - 40,
         lwd = 2)
segments(x0  = 100 - xPad - 10,
         x1  = 100 - xPad - 30,
         y0  = 100 - 10,
         y1  = yPad - 40,
         lwd = 2,
         col = 'red')
segments(x0  = 100 - xPad - 10,
         x1  = 100 - xPad - 30,
         y0  = yPad - 20,
         y1  = yPad - 40,
         lwd = 2,
         col = 'red')
segments(x0  = 100 - xPad - 13,
         x1  = 100 - xPad - 13,
         y0  = yPad - 23,
         y1  = yPad - 18,
         lwd = 2,
         col = 'blue')
segments(x0  = 100 - xPad - 13,
         x1  = 100 - xPad - 10,
         y0  = yPad - 18,
         y1  = yPad - 15,
         lwd = 2,
         col = 'blue')
segments(x0  = 100 - xPad - 10,
         x1  = 100 - xPad - 15,
         y0  = yPad - 15,
         y1  = yPad - 13,
         lwd = 2,
         col = 'blue')
segments(x0  = 100 - xPad - 15,
         x1  = 100 - xPad - 15,
         y0  = yPad - 18,
         y1  = yPad - 13,
         lwd = 2,
         col = 'blue')
text(x = 35,
     y = 70,
     labels = expression(y))
text(x = 35,
     y = 25,
     labels = expression(hat(y)[0]))
text(x = 35,
     y = 50,
     labels = expression(hat(y)))
text(x = 35,
     y = 50,
     labels = expression(hat(y)))
text(x = 35,
     y = 50,
     labels = expression(hat(y)))
text(x      = 64,
     y      = 20,
     labels = expression(hat(y) - hat(y)[0]),
     srt    = 40)
text(x      = 73,
     y      = 60,
     labels = expression(y - hat(y)),
     srt    = 90)
text(x      = 55,
     y      = 45,
     labels = expression(y - hat(y)[0]),
     srt    = 70)
text(x = 20,
     y =  5,
     labels = expression(italic("Column Space X")))
text(x = 90,
     y = 90,
     labels = expression(bold(R)^n))
```

Here we can see several components:

* $\by$ is the vector of observations. Is a vector in $\mathbb{R}^n$.
* The grey hyper-plane is the column space generated by $\bX$, a sub-space of $\mathbb{R}^n$.
* The multiple regression prediction $\hat{\by}$ of $\by$ is the projection of $\by$
  on the space generated by the column of $\bX$.
* The poor man's prediction $\hat{\by}_0$, in the column space of $\bX$ (since,
one of the columns is $\bones$), but in most cases it is different to $\hat{\by}$ (the closest vector in
the column space of $\bX$ to $\by$).
* We notice that the differences:
  - $\by - \hat{\by}$.
  - $\by - \hat{\by}_0$
  - $\hat{\by} - \hat{\by}_0$
  
  form a right triangle, then it must be that:
  $$
  ||\by - \hat{\by}_0||^2 = ||\by - \hat{\by}||^2 + ||\hat{\by} - \hat{\by}_0||^2
  $$
  which is the same as
  $$
  (\by - \hat{\by})'(\by - \hat{\by}_0) = (\by - \hat{\by})'(\by - \hat{\by}) + (\hat{\by} - \hat{\by}_0)'(\hat{\by} - \hat{\by}_0)
  $$
  that can be expressed as:
  $$
  SS_{tot} = SS_{res} + SS_{reg}
  $$

## Centered and Standarized Variables

### Centered Variables

Like with simple linear regression we can center and standardize our variables.

For this section, let us use the notation:

$$\bX = \left[\bx_1, \bx_2, \ldots \bx_p \right]$$
a matrix with $p$ variables in which each column is a variable $\bx_i$. In the 
context of linear regression you can this is similar to the design matrix except
that it doesn't have the column of ones. In this way, we will rename the design 
matrix as

$$\bX_{*} = \left[\bones \bX \right] $$
we introduce this notation, since we don't want to center or standardize the 
column of ones.

To center matrix $\bX$ we need to remove the mean of every column. Notice that the
vector of means is given by:

$$\bar{\bx} = 
  \left[\begin{matrix}
    \bar{\bx}_1 \\
    \bar{\bx}_2 \\
    \vdots      \\
    \bar{\bx}_p
  \end{matrix}\right] =
  \left[\begin{matrix}
    \frac{1}{n} \bx_1'\bones \\
    \frac{1}{n} \bx_2'\bones \\
    \vdots                   \\
    \frac{1}{n} \bx_p'\bones
  \end{matrix}\right] = \frac{1}{n}
  \left[\begin{matrix}
    \bx_1'\bones \\
    \bx_2'\bones \\
    \vdots                   \\
    \bx_p'\bones
  \end{matrix}\right] = \frac{1}{n} \bX' \bones $$
  
then the centered data $\bX_c$ is given by:

$$\bX_c = \bX - \left[\begin{matrix}
    \bar{\bx}_1 & \bar{\bx}_2 & \dots  & \bar{\bx}_p\\
    \bar{\bx}_1 & \bar{\bx}_2 & \dots  & \bar{\bx}_p\\
    \vdots      & \vdots      & \ddots & \vdots     \\
    \bar{\bx}_1 & \bar{\bx}_2 & \dots  & \bar{\bx}_p
  \end{matrix}\right] = \bX - \bones \bar{\bx}' = \bX - \bones \left(\frac{1}{n} \bX' \bones\right)' = \bX - \frac{1}{n}\bones \bones' \bX = \left(\bI - \frac{1}{n}\bones \bones' \right)\bX $$

We call $$\bC =  \left(\bI - \frac{1}{n}\bones \bones' \right) = \bI - \bH_0 $$ the centering
matrix, since it centers the variables of matrix $\bX$. Note also, that $\bC$ also
centers any matrix with $n$ rows, in particular a vector of size $n$ is also centered
by $\bC$. So we can center $y$ the dependent variable, the same way:

$$\by_c = \bC \by $$

### Sample Covariance

Having defined, the centered matrix $\bX_c$ we can define the sample covariance
of $\bX$, $\bS_{XX} \in \mathbb{R}^{p \times p}$ as follows:

$$ \bS_{XX} = \frac{1}{n-1} \left(\bX - \bones \bar{\bx}'\right)'\left(\bX - \bones \bar{\bx}'\right) = \frac{1}{n-1} \bX_c'\bX_c = \frac{1}{n-1} \bX'\bC' \bC \bX $$
Now, since $\bC = \bI - \bH_0$ is idempotent and symmetric we have that:

$$ \bS_{XX} = \frac{1}{n-1} \bX'\bC' \bC \bX = \frac{1}{n-1} \bX' \bC \bX = \frac{1}{n-1} \bX_c'\bX = \frac{1}{n-1} \bX'\bX_c $$
So the sample covariance, is the same for the original variables and the centered variables.

We can also define the covariance vector between variables $\bx_1, \bx_2, \ldots \bx_p$ and variable $\by$, 
$\bS_{Xy} \in \dR^{p \times 1}$, as follows:

$$ \bS_{Xy} = \frac{1}{n-1} \left(\bX - \bones \bar{\bx}'\right)'\left(\by - \bones \bar{y}\right) = \frac{1}{n-1} \bX_c'\by_c = \frac{1}{n-1} \bX'\bC' \bC \by$$

and, in the same way than before, we have that:

$$\bS_{Xy} = \frac{1}{n-1} \bX' \bC' \bC \by = \frac{1}{n-1} \bX' \bC \by = \frac{1}{n-1} \bX_c'\by = \frac{1}{n-1} \bX'\by_c$$

so, you don't need to center both variables. As long as you center one of them
the result will be the same.

With this measures, we can focus on splitting the vector of estimated coefficients $\hat{\bgb}$, into the estimate for
the intercept and the estimates for the independent variables, as follows:

$$\hat{\bgb} = 
  \left[\begin{matrix}
    \hat{\beta}_0 \\
    \hat{\bgb}_{-0} 
  \end{matrix}\right]$$
  
with $\hat{\beta}_0$ the estimate of the intercept and $\hat{\bgb}_{-0}$ the coefficients
for all independent variables. That is:

$$\hat{\bgb}_{-0} = 
  \left[\begin{matrix}
    \hat{\beta}_1 \\
    \hat{\beta}_2 \\
    \vdots        \\
    \hat{\beta}_p  
  \end{matrix}\right] \in \dR^{p \times 1}$$
  
Under the new notation, $\bX_{*}$ for the design matrix, we have that:

$$ \hat{\bgb} = \left(\bX_{*}'\bX_{*}\right)^{-1}\bX_{*}'\by $$
so:

$$ \hat{\bgb} = \left[\begin{matrix}
    \hat{\beta}_0 \\
    \hat{\bgb}_{-0} 
  \end{matrix}\right] = 
  \left( \left[\bones \bX \right]' \left[\bones \bX \right] \right)^{-1} \left[\bones \bX \right]' \by $$

so we need to compute $\left( \left[\bones \bX \right]' \left[\bones \bX \right] \right)^{-1}$.

We start by computing:

$$\left[\bones \bX \right]' \left[\bones \bX \right] = 
  \left[\begin{matrix}
    \bones' \\
    \bX'  
  \end{matrix}\right] \left[\bones \bX \right] =
  \left[\begin{matrix}
    \bones' \bones & \bones' \bX \\
    \bX'\bones     & \bX' \bX'  
  \end{matrix}\right] =
  \left[\begin{matrix}
    n          & n\bar{\bx}' \\
    n\bar{\bx} & \bX' \bX'  
  \end{matrix}\right]
$$
Now, we need to invert a 2 by 2 block matrix (luckily there is a formula for this).
The formula is in the prerequisites section, however note that in this case
one of the blocks is of height 1, since the first value we are looking for $\hat{\beta}_0$
is a scalar. After applying the formula, we have:

\begin{align*}
\left( \left[\bones \bX \right]' \left[\bones \bX \right] \right)^{-1} 
  &= \left[\begin{matrix}
    n^{-1} + n^{-1} n\bar{\bx}' \left(\bX' \bX' -  n\bar{\bx} n^{-1} n\bar{\bx}' \right)^{-1}n\bar{\bx} n^{-1} & -n^{-1} n\bar{\bx}' \left(\bX' \bX' -  n\bar{\bx} n^{-1} n\bar{\bx}'     \right)^{-1} \\
    -\left(\bX' \bX' -  n\bar{\bx} n^{-1} n\bar{\bx}' \right)^{-1}n\bar{\bx} n^{-1} & \left(\bX' \bX' -  n\bar{\bx} n^{-1} n\bar{\bx}' \right)^{-1}  
  \end{matrix}\right] \\
  &= \left[\begin{matrix}
    n^{-1} + \bar{\bx}' \left(\bX' \bX -  n\bar{\bx}\bar{\bx}' \right)^{-1}\bar{\bx} & \bar{\bx}' \left(\bX' \bX -  n\bar{\bx} \bar{\bx}' \right)^{-1} \\
    -\left(\bX' \bX -  n\bar{\bx} \bar{\bx}' \right)^{-1}\bar{\bx}                   & \left(\bX' \bX -  n\bar{\bx} \bar{\bx}' \right)^{-1}  
  \end{matrix}\right]  
\end{align*}

Now, notice that:

\begin{align*}
\bX' \bX -  n\bar{\bx} \bar{\bx}' 
  &= \bX' \bX -  n\left(\frac{\bX'\bones}{n}\right)\left(\frac{\bX'\bones}{n}\right)' \\
  &= \bX' \bX -  \frac{1}{n}\bX'\bones \bones' \bX                                    \\
  &= \bX'\left(\bI - \frac{1}{n}\bones \bones'\right) \bX                             \\
  &= \bX'\bC\bX                                                                       \\
  &= (n-1)\bS_{XX}
\end{align*}

then:

\begin{align*}
\left( \left[\bones \bX \right]' \left[\bones \bX \right] \right)^{-1}  
  &= 
  \left[\begin{matrix}
    n^{-1} + \bar{\bx}' \left((n-1)\bS_{XX} \right)^{-1}\bar{\bx} & -\bar{\bx}' \left((n-1)\bS_{XX} \right)^{-1} \\
    -\left((n-1)\bS_{XX} \right)^{-1}\bar{\bx}                    & \left((n-1)\bS_{XX} \right)^{-1}   
  \end{matrix}\right] \\
  &=
  \left[\begin{matrix}
    n^{-1} + \frac{1}{n-1}\bar{\bx}' \bS_{XX}^{-1} \bar{\bx} & -\frac{1}{n-1}\bar{\bx}' \bS_{XX}^{-1} \\
    -\frac{1}{n-1}\bS_{XX}^{-1} \bar{\bx}                    & \frac{1}{n-1}\bS_{XX}^{-1}   
  \end{matrix}\right]
\end{align*}


In a similar way we can easily compute:

$$\left[\bones \bX \right]' \by = 
  \left[\begin{matrix}
    \bones' \by\\
    \bX'\by  
  \end{matrix}\right] =
  \left[\begin{matrix}
    n\bar{y} \\
    \bX'\by  
  \end{matrix}\right]$$

Then, we have that:

\begin{align*}
\hat{\bgb} 
  &= \left[\begin{matrix} \\
    \hat{\beta}_0 \\
    \hat{\bgb}_{-0} 
  \end{matrix}\right] \\
  &=
  \left[\begin{matrix}
    n^{-1} + \frac{1}{n-1}\bar{\bx}' \bS_{XX}^{-1} \bar{\bx} & -\frac{1}{n-1}\bar{\bx}' \bS_{XX}^{-1} \\
    -\frac{1}{n-1}\bS_{XX}^{-1} \bar{\bx}                    & \frac{1}{n-1}\bS_{XX}^{-1}   
  \end{matrix}\right]
  \left[\begin{matrix}
    n\bar{y} \\
    \bX'\by  
  \end{matrix}\right] \\ 
  &= 
  \left[\begin{matrix}
    n^{-1}n\bar{y} + \frac{n\bar{y}}{n-1}\bar{\bx}' \bS_{XX}^{-1} \bar{\bx} - \frac{1}{n-1}\bar{\bx}' \bS_{XX}^{-1}\bX'\by \\
    -\frac{n\bar{y}}{n-1}\bS_{XX}^{-1} \bar{\bx} + \frac{1}{n-1}\bS_{XX}^{-1}\bX'\by  
  \end{matrix}\right]
\end{align*}

Then, working first with the second row-block:

\begin{align*}
\hat{\bgb}_{-0} 
  &=  -\frac{n\bar{y}}{n-1}\bS_{XX}^{-1} \bar{\bx} + \frac{1}{n-1}\bS_{XX}^{-1}\bX'y \\
  &= \frac{1}{n-1}\bS_{XX}^{-1}\bX'y -\frac{1}{n-1}\bS_{XX}^{-1} \bar{\bx}n\bar{y}   \\
  &= \frac{1}{n-1}\bS_{XX}^{-1}\left(\bX'y - n\bar{\bx}\bar{y}\right)
\end{align*}

Now, we note that:

\begin{align*}
\bX'\by - n\bar{\bx}\bar{y} 
  &= \bX'\by - n\bar{\bx}\bar{y}                                                   \\
  &= \bX'\by - n\left(\frac{\bX'\bones}{n}\right)\left(\frac{\bones'\by}{n}\right) \\
  &= \bX'\by - \frac{1}{n}\bX'\bones \bones'\by                                    \\
  &= \bX' \left(\bI - \frac{1}{n} \bones \bones' \right) \by                       \\
  &= \bX' \bC \by                                                                  \\
  &= (n-1)\bS_{Xy}
\end{align*}

Then, we have that:

$$\hat{\bgb}_{-0} = \frac{1}{n-1}\bS_{XX}^{-1}(n-1)\bS_{Xy} = \bS_{XX}^{-1}\bS_{Xy}$$
An equivalent result to that of simple linear regression, both using the covariance
matrix of the independent variables and the covariance vector of the independent
variables and the dependent variable.

This way of writing the coefficients shows the influence of each component:

* $\bS_{XX}^{-1}$: The relationship between the independent variables.
* $\bS_{Xy}$: The relationship between the independent variables and the dependent variables.

Also notice that, since centralizing doesn't change the values of $\bS_{X_cX_c}^{-1}$ and $\bS_{X_cy_c}$, centralizing
the independent variables or the dependent variable (or both), doesn't change the 
value of the coefficients of the independent variables.

Now, we can work more easily with the intercept estimate:

\begin{align*}
\hat{\beta}_0 
  &= n^{-1}n\bar{y} + \frac{n\bar{y}}{n-1}\bar{\bx}' \bS_{XX}^{-1} \bar{\bx} - \frac{1}{n-1}\bar{\bx}' \bS_{XX}^{-1}\bX'\by \\
  &= \bar{y} + \frac{1}{n-1}\bar{\bx}'\bS_{XX}^{-1} n\bar{\bx}\bar{y} - \frac{1}{n-1}\bar{\bx}'\bS_{XX}^{-1}\bX'\by         \\
  &= \bar{y} + \frac{1}{n-1}\bar{\bx}'\bS_{XX}^{-1} \left(n\bar{\bx}\bar{y}  - \bX'\by \right)                              \\
  &= \bar{y} - \frac{1}{n-1}\bar{\bx}'\bS_{XX}^{-1} \left(\bX'\by - n\bar{\bx}\bar{y} \right)                               \\
  &= \bar{y} - \frac{1}{n-1}\bar{\bx}'\bS_{XX}^{-1} (n-1)\bS_{Xy}                                                           \\
  &= \bar{y} - \bar{\bx}'\bS_{XX}^{-1} \bS_{Xy}                                                                             \\
  &= \bar{y} - \bar{\bx}' \hat{\bgb}_{-0}
\end{align*}

Again, an equivalent result to that of simple linear regression. Like in simple
linear regression, centering the independent variables does affect the intercept
estimate, since $\bar{\bx}=\bzero$, we have that the coefficient after centering
the independent variables is $\bar{y}$ the mean of the dependent variable. And
if we also center the dependent variable, then $\bar{y}=0$ so the estimate of the
intercept is $0$ also. Therefore, if you are centering all variables, it is not
necessary to add the column of ones in the design matrix, since the estimate of the
intercept is $0$.

### Satandard Variables

In the same way we worked with centered variables, we can work with standard variables
to define the sample correlations.

The standardization of $\bX$, $\bX_s$, is given by:

\begin{align*}
\bX_s 
  &=
  \left[\begin{matrix}
    \frac{x_{11} - \bar{\bx}_1}{S_{x_1x_1}^{1/2}} & \frac{x_{12} - \bar{\bx}_2}{S_{x_2x_2}^{1/2}} & \dots  & \frac{x_{1p} - \bar{\bx}_p}{S_{x_px_p}^{1/2}} \\
    \frac{x_{21} - \bar{\bx}_1}{S_{x_1x_1}^{1/2}} & \frac{x_{22} - \bar{\bx}_2}{S_{x_2x_2}^{1/2}} & \dots  & \frac{x_{2p} - \bar{\bx}_p}{S_{x_px_p}^{1/2}} \\
    \vdots                                        & \vdots                                        & \ddots & \vdots                                        \\
    \frac{x_{n1} - \bar{\bx}_1}{S_{x_1x_1}^{1/2}} & \frac{x_{n2} - \bar{\bx}_2}{S_{x_2x_2}^{1/2}} & \dots  & \frac{x_{np} - \bar{\bx}_p}{S_{x_px_p}^{1/2}}
  \end{matrix}\right] \\
  &=
  \left[\begin{matrix}
    x_{11} - \bar{\bx}_1 & x_{12} - \bar{\bx}_2 & \dots  & x_{1p} - \bar{\bx}_p \\
    x_{21} - \bar{\bx}_1 & x_{22} - \bar{\bx}_2 & \dots  & x_{2p} - \bar{\bx}_p \\
    \vdots               & \vdots               & \ddots & \vdots               \\
    x_{n1} - \bar{\bx}_1 & x_{n2} - \bar{\bx}_2 & \dots  & x_{np} - \bar{\bx}_p
  \end{matrix}\right]
  \left[\begin{matrix}
    \frac{1}{S_{x_1x_1}^{1/2}} & 0                                             & \dots  & 0 \\
    0                          & \frac{x_{22} - \bar{\bx}_2}{S_{x_2x_2}^{1/2}} & \dots  & 0 \\
    \vdots                     & \vdots                                        & \ddots & \vdots                                        \\
    0                          & 0                                             & \dots  & \frac{x_{np} - \bar{\bx}_p}{S_{x_px_p}^{1/2}}
  \end{matrix}\right]
  &= \bX_c \bD_X
  &= \bC \bX \bD_X
\end{align*}

where

$$ \bD_X =  \left[\begin{matrix}
    \frac{1}{S_{x_1x_1}^{1/2}} & 0                                             & \dots  & 0 \\
    0                          & \frac{x_{22} - \bar{\bx}_2}{S_{x_2x_2}^{1/2}} & \dots  & 0 \\
    \vdots                     & \vdots                                        & \ddots & \vdots                                        \\
    0                          & 0                                             & \dots  & \frac{x_{np} - \bar{\bx}_p}{S_{x_px_p}^{1/2}}
  \end{matrix}\right]$$

is the matrix that standardizes $\bX$. Notice that unlike $\bC$, that centers
any matrix with the appropriate number of rows, $\bD_X$ only standardizes $\bX$.

### Sample Correlation Matrix

We define the sample correlation matrix as:

$$ r_{XX} = \frac{\bX_s'\bX_s}{n-1} $$
where we break a little our notation convention of using bold capital letters for 
matrices. Entry at row $i$ and column $j$ of $r_{XX}$ is given by:

$$ \left[r_{XX}\right]_{ij} = \frac{S_{x_i x_j}}{S_{x_ix_i}^{1/2}S_{x_jx_j}^{1/2}} $$

We can also define the sample correlation between $\bX$ and $\by$ as follows:

$$r_{Xy} = \frac{\bX_s' \by_s}{n-1} $$

where $\by_s$ is the standardization of $\by$.

With this definitions in hand, we can see how the coefficients look like with 
standardized variables. Since, standardized variables are also centered, it is not
necessary to include the column of ones in the design matrix, as the intercept
estimate is always 0. Then the estimate of the coefficients of the independent 
variables is given by:

$$\hat{\bgb_s} = \left(\bX_s' \bX_s \right)^{-1} \bX_s' \by_s = \frac{1}{n-1}r_{XX}^{-1}(n-1)r_{Xy}=r_{XX}^{-1}r_{Xy}$$
Notice that the coefficients estimates do change for standardized variables, since
in general $r_{XX}\neq\bS_{XX}$ and $r_{Xy}\neq\bS_{Xy}$. In the case of standardized
variables the coefficient estimates depend in part from the correlation between
independent variables $r_{XX}$ and the correlations between independent and
dependent variables $r_{Xy}$. 

Working with standardized variables is useful, since standardized variables are unit-less, 
so the estimated coefficients magnitudes are comparable.

We can see these results in practice with our GDP data:

```{r gdp-centering}
# Read Data
dat <- read.csv("Gdp data.csv")

# Design Matrix Independent Variables
X <- as.matrix(dat[, -1])
# Dependent Variable
y <- dat$gdp
# NUmber of Observations
n <- nrow(X)
# Design Matrix with the column of Ones
Z <- cbind(rep(1, n), X)

# Centering
# Vector of Ones
v1 <- rep(1, n)
# Centering Matrix
C  <- diag(n) - (1/n) * v1 %*% t(v1)
# Independent Variables Centered
Xc <- C %*% X
# Dependent Variable Centered
yc <- C %*% y
# Design Matrix with Independent Variables Centered
Zc <- cbind(rep(1, n), Xc)

# Checks that the Variables are actually centered
print(round(colMeans(Xc), 8))
print(round(mean(yc), 8))

# Shows the Mean of y
print(round(mean(y), 8))

# Compute the Estimates of the Coefficients with Original Variables
b   <- solve(t(Z) %*% Z, t(Z) %*% y)
# Compute the Estimates of the Coefficients with Centered Independent Variables Only
bs1 <- solve(t(Zc) %*% Zc, t(Zc) %*% y)
# Compute the Estimates of the Coefficients with All Variables Centered
bs2 <- solve(t(Zc) %*% Zc, t(Zc) %*% yc)
# Compute the Estimates of the Coefficients with All variables Centered and no column of ones
bs3 <- solve(t(Xc) %*% Xc, t(Xc) %*% yc)

# Shows the Estimated Coefficients Side-by-Side
print(round(cbind(b, bs1, bs2, c(0, bs3)), 8))
```
Here we can appreciate that the estimated coefficients for the independent variables
do not change, however the estimate for the intercept changes depending on if the
independent variables are centered or the dependent variable is centered of both. Also, notice that
this are the that we had using the `lm` function of `R`.

We can also check that computing the sample covariance matrix using our formula
results in the same quantities that using the `cov` function in `R`.

```{r gdp-sample-covariance}
# Sample Covariance of X
SXX <- t(Xc) %*% Xc / (n-1)
# Sample Covariance between X and y
SXy <- t(Xc) %*% yc / (n-1)

# Shows the comparison in covariance matrices
print(SXX)
print(cov(X))

# Shows the comparison in covariance vectors
print(SXy)
print(cov(X, y))
```

Finally, we can test our new formulas for the estimates:

```{r gdp-sample-covariance-estimation}
# Computes the estimates of the coefficients using the covariance matrices
b1 <- solve(SXX, SXy)
b0 <- mean(y) - t(colMeans(X)) %*% b1

# Shows the Estimates Side by Side
print(round(cbind(b, c(b0, b1)), 8))
```

In the same way, we can work with the sample correlations

```{r gdp-sample-correlation}
# Standardizing matrix of X
DX <- diag(1/sqrt(diag(SXX)))
# Standardize X
Xs <- Xc %*% D
# Shows that Xs is indeed standardize
print(round(colMeans(Xs), 8))
print(apply(X = Xs, MARGIN = 2, FUN = sd))

# Standardizes y
ys <- yc / sd(y)
# Shows that ys is standardized
print(round(mean(ys), 8))
print(round(sd(ys), 8))

# Sample Correlation of X
rXX <- t(Xs) %*% Xs / (n-1)
# Sample Correlation between X and y
rXy <- t(Xs) %*% ys / (n-1)

# Shows the comparison in correlation matrices
print(rXX)
print(cor(X))

# Shows the comparison in correlation vectors
print(rXy)
print(cor(X, y))
```
and can test that:

$$\hat{\bgb_s} = r_{XX}^{-1}r_{Xy}$$

```{r gdp-sample-correlation-estimation}
# Computes the estimates of the coefficients using the covariance matrices
bs1 <- solve(t(Xs) %*% Xs, t(Xs) %*% ys)
bs2 <- solve(rXX, rXy)

# Shows the Estimates Side by Side
print(round(cbind(bs1, bs2), 8))
```

We can also contrast the effects of the standarization on the coefficients

```{r gdp-sample-standardization-comparisson}
# Computes the estimates of the coefficients using the covariance matrices
bs1 <- lm(y ~ X)$coefficients
bs2 <- lm(ys ~ Xs)$coefficients

# Shows the Estimates Side by Side
print(round(cbind(bs1, bs2), 8))
```

First, we can observe the 0 intercept estimate on the standardize values, so it
is not necessary to estimate it, we could have done so by using `lm(ys ~ Xs - 1)` 
instead of `lm(ys ~ Xs)`. Next we observe, the change in magnitudes for the 
estimated coefficients of the independent variables.

## Variable Cross-Effects

For this sub-section we will work with standardized values so there is no need to 
estimate the intercept. Since all variables are standardized, we will not use
the $\bX_s$ notation, but instead only $\bX$, to make notation less confusing.

The same goes with $\hat{\bgb}_s$, it will be only $\hat{\bgb}$.

The idea is to analyze the estimated coefficients when you divide the independent 
variables in to groups $1$ and $2$. So we can divide the design matrix in two:

$$\bX = [\bX_1 \bX_2]$$
Then, we can compute the coefficient estimates:

$$\hat{\bgb} = 
  \left[\begin{matrix}
    \hat{\bgb}_1 \\
    \hat{\bgb}_2  
  \end{matrix}\right] =
  \left([\bX_1 \bX_2]'[\bX_1 \bX_2]\right)^{-1}[\bX_1 \bX_2]' \by$$

So, we can work with these computations in the same way we did before:

\begin{align*}
[\bX_1 \bX_2]'[\bX_1 \bX_2] 
  &= 
  \left[\begin{matrix}
    \bX_1' \\
    \bX_2'  
  \end{matrix}\right] [\bX_1 \bX_2] \\
  &= 
  \left[\begin{matrix}
    \bX_1'\bX_1 & \bX_1'\bX_2 \\
    \bX_2'\bX_1 & \bX_2'\bX_2  
  \end{matrix}\right] \\
  &= \frac{1}{n-1} 
  \left[\begin{matrix}
    \frac{\bX_1'\bX_1}{n-1} & \frac{\bX_1'\bX_2}{n-1} \\
    \frac{\bX_2'\bX_1}{n-1} & \frac{\bX_2'\bX_2}{n-1}  
  \end{matrix}\right] \\
  &= \frac{1}{n-1} 
  \left[\begin{matrix}
    r_{X_1X_1} & r_{X_1X_2} \\
    r_{X_2X_1} & r_{X_2X_2}  
  \end{matrix}\right]
\end{align*}

In the same way, we have that:

\begin{align*}
[\bX_1 \bX_2]'\by 
  &= 
  \left[\begin{matrix}
    \bX_1' \\
    \bX_2'  
  \end{matrix}\right] \by \\
  &= 
  \left[\begin{matrix}
    \bX_1'\by \\
    \bX_2'\by  
  \end{matrix}\right] \\
  &= \frac{1}{n-1} 
  \left[\begin{matrix}
    \frac{\bX_1'\by}{n-1} \\
    \frac{\bX_2'\by}{n-1}   
  \end{matrix}\right] \\
  &= \frac{1}{n-1} 
  \left[\begin{matrix}
    r_{X_1y} \\
    r_{X_2y}  
  \end{matrix}\right]
\end{align*}

Then we have that:

$$
\hat{\bgb} = 
  \left[\begin{matrix}
    \hat{\bgb}_1 \\
    \hat{\bgb}_2  
  \end{matrix}\right] =
  \left(\frac{1}{n-1} 
  \left[\begin{matrix}
    r_{X_1X_1} & r_{X_1X_2} \\
    r_{X_2X_1} & r_{X_2X_2}  
  \end{matrix}\right]\right)^{-1} \frac{1}{n-1} 
  \left[\begin{matrix}
    r_{Xy,1} \\
    r_{Xy,2}  
  \end{matrix}\right] =
  \left[\begin{matrix}
    r_{X_1X_1} & r_{X_1X_2} \\
    r_{X_2X_1} & r_{X_2X_2}  
  \end{matrix}\right]^{-1}  
  \left[\begin{matrix}
    r_{X_1y} \\
    r_{X_2y}  
  \end{matrix}\right]
$$

Now we can compute the inverse of the 2 by 2 block matrix as before with, but 
first we define:

$$r_{X_1|X_2} = r_{X_1X_1} - r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2X_1}$$
this is also, the Schur component. This can also be seen as the sample correlation
matrix of $X_1$ after accounting by the relationships with $X_2$.

$$
\left[\begin{matrix}
    r_{XX,11} & r_{XX,12} \\
    r_{XX,12} & r_{XX,22}  
  \end{matrix}\right]^{-1} =
\left[\begin{matrix}
                              r_{X_1|X_2}^{-1} &                         -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1} \\
    -r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1} & r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}  \end{matrix}\right]
$$
Then, we have that:

\begin{align*}
\hat{\bgb} 
  &= 
  \left[\begin{matrix}
    \hat{\bgb}_1 \\
    \hat{\bgb}_2  
  \end{matrix}\right] \\
  &=
  \left[\begin{matrix}
                              r_{X_1|X_2}^{-1} &                         -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1} \\
    -r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1} & r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}
  \end{matrix}\right] 
  \left[\begin{matrix}
    r_{X_1y} \\
    r_{X_2y}  
  \end{matrix}\right] \\
  &= 
  \left[\begin{matrix}
    r_{X_1|X_2}^{-1}r_{X_1y} -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y} \\
    -r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1y} + r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y}
  \end{matrix}\right]
\end{align*}

Since the variables in the groupings 1 and 2 can be switched and have no special 
characteristics, we only need to analyze the structure of $\hat{\bgb}_1$ in relation
to the variables of group 2, the results would be analogous for $\hat{\bgb}_1$.  
  
Then we have that:

$$
\hat{\bgb}_1 
  = r_{X_1|X_2}^{-1}r_{X_1y} -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y}
  = r_{X_1|X_2}^{-1}\left(r_{X_1y} -r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y}\right)
$$
Now, suppose that we want to fit the following linear models:

$$\by = \bX_1 \bgb_1 + \be \quad \text{and} \quad \by = \bX_2 \bgb_2 + \be$$
that is a linear model of $\by$ using only the variables in group 1 for one model
and only variables from the group 2 in the second model. Then, 
the coefficient estimates will be:

$$\tilde{\bgb}_1 = r_{X_1X_1}^{-1}r_{X_1y} \quad \text{and} \quad \tilde{\bgb}_2 = r_{X_2X_2}^{-1}r_{X_2y}$$
Note that, in general, this estimates will be different to the estimates using
all the variables, that is:

$$\tilde{\bgb}_1 \neq \hat{\bgb}_1 \quad \text{and} \quad \tilde{\bgb}_2 \neq \hat{\bgb}_2$$
Then, we can re-write, our coefficient estimate for $\bgb_1$ as follows:

$$
\hat{\bgb}_1 
  = r_{X_1|X_2}^{-1}\left(r_{X_1y} -r_{X_1X_2}\tilde{\bgb}_2\right)
$$
Now, if $r_{X_1X_2} = \bzero$, that is the variables in group 1 and group 2 are 
uncorrelated, then

$$r_{X_1|X_2} = r_{X_1X_1} - r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2X_1} = r_{X_1X_1}$$
so:
$$\hat{\bgb}_1 
  = r_{X_1|X_2}^{-1}\left(r_{X_1y} -r_{X_1X_2}\tilde{\bgb}_2\right)
  = r_{X_1X_1}^{-1}r_{X_1y}
  = \tilde{\bgb}_1$$
So, when the variables in 1 group are uncorrelated with the other the coefficient
estimates are the same for the full and partial model.

We can also deduce that even when the sample correlation between
$X_1$ and $y$ is $\bzero$, that is $r_{X_1y} = \bzero$, the estimated coefficients
will not be $\bzero$ in general. In fact, we have that:

$$
\hat{\bgb}_1 
  = -r_{X_1|X_2}^{-1}r_{X_1X_2}\tilde{\bgb}_2
$$

### Single Variable Cross-Effects

In the special case where group 1 consists of only 1 variable, we have that:

$\hat{\beta}_1, \quad r_{X_1X_1} = r_{x_1x_1}=1, \quad r_{X_1|X_2}=r_{x_1|X_2}, \quad r_{X_1y}=r_{x_1y}=\tilde{\beta}_1$

are scalars, while

$$r_{X_1X_2} = r_{x_1X_2}=r_{X_2X_1}' = r_{X_2x_1}', \quad r_{X_2y}, \quad \tilde{\bgb}_2$$
are vectors of size $p-1$.

Now, consider the following linear model:

$$\bx_1 = \bX_2 \bga_2 + \be$$

that is, fitting the single variable in group 1, as the dependent variable, and
the variables in group 2, as the independent variables. Then we have that the
estimated coefficients are:

$$ \hat{\bga}_2 = r_{X_2X_2}^{-1}r_{X_2x_1}$$
then, we notice the following

$$r_{x_1|X_2} 
  = r_{X_1X_1} - r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2X_1} 
  = r_{x_1X_1} - r_{x_1X_2}r_{X_2X_2}^{-1}r_{X_2x_1}
  = 1 - r_{x_1X_2}\hat{\bga}_2
  = 1 - r_{x_1X_2}r_{X_2X_2}^{-1}r_{X_2X_2}\hat{\bga}_2
  = 1 - \hat{\bga}_2'r_{X_2X_2}\hat{\bga}_2
  = 1 - R^2_1
$$

where, $R^2_1$ is the multiple coefficient of determination for model:

$$\bx_1 = \bX_2 \bga_2 + \be$$

that is, how much of $\bx_1$ is explained by $\bX_2$.

Then, we have that:

$$\hat{\beta}_1 = r_{1_1|X_2}^{-1}\left(r_{x_1y} -r_{x_1X_2}\tilde{\bgb}_2\right) = \frac{1}{1-R^2_1} \left(\tilde{\beta}_1 -r_{x_1X_2}\tilde{\bgb}_2\right)$$
We name $\frac{1}{1 - R^2_1}$ as:

$$VIF_1 = \frac{1}{1 - R^2_1} $$
the bigger, the more distorted is the value of $\hat{\beta}_1$. This number increases
as $R^2_1$ in creases.

If $R^2_1 = 0$, that is $\bX_2$ explains nothing of $\bx_1$, then

$$ VIF_1 = 1 $$
that represents no distortion. If $R^2_1 \to 1$, then:

$$ VIF_1 \to 1 $$
which represents infinite distortion.

The name $VIF$ stands for variance inflation factor. We will see that later, how
it relates to the variance of the estimate $\hat{\beta}_1$.

Of course, variable 1, didn't have any particular property, so we can generalize
this to any variable $k$.

On the other hand, we also have an effect $r_{x_1X_2}\tilde{\bgb}_2$ on the
coefficient

#### Single Variable Cross-effects example

We can see the effects the other variables have in changing the coefficients in
our GDP data set, by computing $R^2_k$ for each variable.

```{r gdp-cross-effects}
# Re-names Standardized Variables
y <- ys
X <- Xs

# Coefficients for the Full Model
b <- lm(y ~ X -1)$coefficients

# Individual Coefficients
# Number of Variables
P <- ncol(X)
# Vector to Save Individual Coefficients
bi <- numeric(length = P)
for(k in 1:P){
  bi[k] <- lm(y ~ X[, k] - 1)$coefficients
}

# Coefficients without variable k
bt <- matrix(data = NA, nrow = P-1, ncol = P)
for(k in 1:P){
  bt[, k] <- lm(y ~ X[, -k] - 1)$coefficients
}

# Coefficient of Determination of X_k ~ X_{-k} models and coefficients
R2k <- numeric(length = P)
for(k in 1:P){
  outReg <- lm(X[, k] ~ X[, -k] - 1)
  R2k[k] <- summary(outReg)$r.squared
}

# Cross product of correlation and coefficients
rbt <- numeric(length = P)
rXX <- cor(X)
for(k in 1:P){
  rbt[k] <- rXX[k, -k] %*% bt[, k]
}

# Formats and Show Info Table
tab           <- cbind(b, bi, R2k, 1/(1-R2k), rbt)
colnames(tab) <- c("b_full", "b_ind", "R2k", "VIF", "rb_ind")
rownames(tab) <- colnames(dat)[2:6]
knitr::kable(tab)
```

## Outliers and Leverage

It is harder to identify outliers visually in multiple linear regression, specially
if the number of variables is big. The effect of an observation $i$ on the 
linear regression fit will depend on 2 things:

* The size of the residual for observation $i$, that is $\hat{e}_i^2$.
* How "close" is observation $i$ to other observations.

**Note**: For this section, we are going to use $X_i\in\dR^{1 \times p}$ as the $i-th$ row of the
design matrix. That is the $i$-th observation of the independent varaibles.

### Leverage

In multiple linear regression, **leverage** refers to the influence that a 
particular data point has on the estimation of the regression model. 
Specifically, it measures how far an observation's values for the independent 
variables deviate from the mean of those variables. Leverage identifies data 
points that could potentially have a significant effect on the regression line,
particularly those that are outliers in terms of the input variables.

Leverage values are calculated from the **hat matrix** $\bH$, which is derived
from the design matrix $\bX$ used in the regression. The diagonal elements of 
the hat matrix, $h_{ii}$, represent the leverage of each data point. 
These values range from 0 to 1, where:

- A leverage value close to 0 indicates that the point is not influential in
determining the regression line.
- A leverage value closer to 1 suggests that the point has a higher influence on
the fitted model.

High-leverage points are those whose independent variable values are far from 
the average, and they may disproportionately affect the slope and intercept of 
the regression line. These points can distort the regression model, leading to 
biased estimates if not handled properly.

A high-leverage point may not necessarily be an outlier in the response variable
(dependent variable), but it could have a significant impact on the overall 
model fit, making leverage a critical diagnostic in regression analysis.

Lets analyze the structure of the hat matrix.

First let us see that the diagonal elements are between 0 and 1.

To see this, consider leverage $i$ ($h_{ii}$), and the basis vectors $v_j \in \dR^n$ such
that are full of zeros except for entry $j$. Then:

$$h_{ij} = v_i '\bH v_j$$
in particular, leverage $i$ is given by:

$$h_{ii} = v_i '\bH v_i$$
Now, consider $\bH v_i$ and notice that:

\begin{align*}
||v_i - \bH v_i||_2^2 
  &= (v_i - \bH v_i)'(v_i - \bH v_i) \\
  &= v_i'v_i - v_i'\bH v_i - v_i'\bH v_i + v_i' \bH \bH v_i \\
  &= v_i'v_i - v_i'\bH\bH v_i - v_i'\bH\bH v_i + v_i' \bH \bH v_i \\
  &= v_i'v_i - v_i'\bH\bH v_i \\
  &= ||v_i||_2^2 - ||\bH v_i||_2^2 \\
  &= 1 - ||\bH v_i||_2^2
\end{align*}

Then

\begin{align*}
||v_i - \bH v_i||_2^2 = 1 - ||\bH v_i||_2^2
  & \implies ||v_i - \bH v_i||_2^2 + ||\bH v_i||_2^2 = 1 \\
  & \implies ||\bH v_i||_2^2 \leq 1 \\
  & \implies v_i' \bH \bH v_i  \leq 1 \\
  & \implies v_i' \bH v_i  \leq 1 \\
  & \implies h_{ii}  \leq 1 \\
\end{align*}

Since $h_{ii}=||\bH v_i||_2^2$ is a norm then $h_{ii} \geq 0$, then:

$$ 0 \leq h_{ii} \leq 1 $$
So, leverages are bounded.

Now, recall that the hat matrix is given by:

$$\bH = \bX(\bX'\bX)^{-1}\bX' $$
and we can express the design matrix as follows:

$$ \bX = \left[\begin{matrix}
    X_1    \\
    X_2    \\
    \vdots \\
    X_n
  \end{matrix}\right] $$
  
and

$$ \bX' = [X_1' X_2' \ldots X_n'] $$

Then, the hat matrix is given by:

\begin{align*}
\bH 
  &= \bX(\bX'\bX)^{-1}\bX'                                                                 \\
  &= \bX(\bX'\bX)^{-1} [X_1' X_2' \ldots X_n']                                             \\
  &= \bX [(\bX'\bX)^{-1}X_1' (\bX'\bX)^{-1}X_2' \ldots (\bX'\bX)^{-1}X_n']                 \\
  &= \left[\begin{matrix}
      X_1    \\
      X_2    \\
      \vdots \\
      X_n
     \end{matrix}\right] [(\bX'\bX)^{-1}X_1' (\bX'\bX)^{-1}X_2' \ldots (\bX'\bX)^{-1}X_n'] \\
  &= \left[\begin{matrix}
      X_1 (\bX'\bX)^{-1}X_1' & X_1 (\bX'\bX)^{-1}X_2' & \ldots & X_1 (\bX'\bX)^{-1}X_n'    \\
      X_2 (\bX'\bX)^{-1}X_1' & X_2 (\bX'\bX)^{-1}X_2' & \ldots & X_2 (\bX'\bX)^{-1}X_n'    \\
      \vdots                 & \vdots                 & \vdots                             \\
      X_n (\bX'\bX)^{-1}X_1' & X_n (\bX'\bX)^{-1}X_2' & \ldots & X_n (\bX'\bX)^{-1}X_n'    \\
     \end{matrix}\right]
\end{align*}

we are going to use this expression latter, but we can also notice that:

$$h_{ij} = X_i (\bX'\bX)^{-1}X_j'$$

### Outliers identification

Identifying outliers in multiple regression is hard. One way to measure the influence
of an observation is to see how different are the predicted values if an observation
is removed. We will call:

* $\bX_{(i)}$ the design matrix, when the $i$-th observation is removed.
* $\by_{(i)}$ the dependent variables without the observation $i$.
* $\hat{\by}_{(i)}$ the predicted values when the $i$-th observation is removed.
* $\bH_{(i)}$ the hat matrix when the $i$-th observation is removed.

Then a measure of influence can be:

$$||\by - \hat{\by}_{(i)}||^2_2 = (\by - \hat{\by}_{(i)})'(\by - \hat{\by}_{(i)}) = \sum_{j=1}^n (\by_j - \hat{\by}_{(i),j})$$
Notice that:

$$\hat{\by}_i = \bH \by$$
and, similarly

$$\hat{\by}_{(i)} = \bH_{(i)} \by_{(i)}$$

Now, let us compute:

\begin{align*}
\hat{\by} 
  &= \bH \by \\
  &= \left[\begin{matrix}
      X_1 (\bX'\bX)^{-1}X_1' & X_1 (\bX'\bX)^{-1}X_2' & \ldots & X_1 (\bX'\bX)^{-1}X_n'    \\
      X_2 (\bX'\bX)^{-1}X_1' & X_2 (\bX'\bX)^{-1}X_2' & \ldots & X_2 (\bX'\bX)^{-1}X_n'    \\
      \vdots                 & \vdots                 & \vdots                             \\
      X_n (\bX'\bX)^{-1}X_1' & X_n (\bX'\bX)^{-1}X_2' & \ldots & X_n (\bX'\bX)^{-1}X_n'    \\
     \end{matrix}\right] \left[\begin{matrix}
      y_1    \\
      y_2    \\
      \vdots \\
      y_n
      \end{matrix}\right] \\
  &=
      \left[\begin{matrix}
        \sum_{k=1}^n X_1 (\bX'\bX)^{-1}X_k'y_k  \\
        \sum_{k=1}^n X_2 (\bX'\bX)^{-1}X_k'y_k    \\
        \vdots \\
        \sum_{k=1}^n X_n (\bX'\bX)^{-1}X_k'y_k
      \end{matrix}\right]
\end{align*}

$$
$$

Then

$$\hat{y}_j = \sum_{k=1}^n X_j (\bX'\bX)^{-1}X_k'y_k = X_j (\bX'\bX)^{-1} \sum_{k=1}^n X_k'y_k$$
in a similar way, observation :

$$\hat{y}_{(i)j} = \sum_{k=1}^n X_j (\bX_{(i)}'\bX_{(i)})^{-1}X_k'y_k = X_j (\bX_{(i)}'\bX_{(i)})^{-1} \sum_{k=1}^n X_k'y_k$$
Notice that it is not necessary to use the subscript $(i)$ when we are dealing
with single observations.

Now, we will relate $(\bX'\bX)^{-1}$ to $(\bX_{(i)}'\bX_{(i)})^{-1}$ using the 
[Sherman–Morrison formula](#sherman-morrison-formula). First notice that:

$$\bX' \bX = \bX_{(i)}'\bX_{(i)} + X_i'X_i$$

then

$$\bX_{(i)}'\bX_{(i)} = \bX' \bX - X_i'X_i$$

Now we apply the [Sherman–Morrison formula](#sherman-morrison-formula).

\begin{align*}
\left(\bX_{(i)}'\bX_{(i)}\right)^{-1} = \left(\bX' \bX - X_i'X_i\right)^{-1}
  &= \left(\bX' \bX + (-X_i)'X_i\right)^{-1} \\
  &= \left(\bX' \bX\right)^{-1} - \frac{\left(\bX' \bX\right)^{-1}(-X_i)'X_i\left(\bX' \bX\right)^{-1}}{1 + (-X_i)'\left(\bX' \bX\right)^{-1}X_i} \\
  &= \left(\bX' \bX\right)^{-1} + \frac{\left(\bX' \bX\right)^{-1}X_i'X_i\left(\bX' \bX\right)^{-1}}{1 - X_i'\left(\bX' \bX\right)^{-1}X_i} \\
  &= \left(\bX' \bX\right)^{-1} + \frac{\left(\bX' \bX\right)^{-1}X_i'X_i\left(\bX' \bX\right)^{-1}}{1 - h_{ii}}
\end{align*}

Then,

$$\hat{y}_{(i)j} = X_j \left(\bX_{(i)}'\bX_{(i)}\right)^{-1} \sum_{k \neq i}X_k'y_k = X_j\left(\bX' \bX\right)^{-1}\sum_{k \neq i}X_k'y_k + \frac{X_j\left(\bX' \bX\right)^{-1}X_i'X_i\left(\bX' \bX\right)^{-1}\sum_{k \neq i}X_k'y_k}{1 - h_{ii}}$$
Now

\begin{align*}
X_j\left(\bX' \bX\right)^{-1}\sum_{k \neq i}X_k'y_k 
  &= X_j\left(\bX' \bX\right)^{-1}\left(\sum_{k \neq i}X_k'y_k + X_i'y_i - X_i'y_i\right) \\
  &= X_j\left(\bX' \bX\right)^{-1}\left(\sum_{k = 1}^nX_k'y_k - X_i'y_i\right) \\
  &= X_j\left(\bX' \bX\right)^{-1}\sum_{k = 1}^nX_k'y_k - X_j\left(\bX' \bX\right)^{-1}X_i'y_i \\
  &= \hat{y}_j - h_{ij}y_i
\end{align*}

and

\begin{align*}
X_j\left(\bX' \bX\right)^{-1}X_i'X_i\left(\bX' \bX\right)^{-1}\sum_{k \neq i}X_k'y_k 
  &= h_{ij}X_i\left(\bX' \bX\right)^{-1}\sum_{k \neq i}X_k'y_k \\
  &= h_{ij}\sum_{k \neq i}X_i\left(\bX' \bX\right)^{-1}X_k'y_k \\
  &= h_{ij}\sum_{k \neq i}h_{ik}y_k
\end{align*}

Then

\begin{align*}
\hat{y}_{(i)j} = \hat{y}_j - h_{ij}y_i + \frac{h_{ij}\sum_{k \neq i}h_{ik}y_k}{1 - h_{ii}}
  &= \hat{y}_j - h_{ij}\left(y_i  - \frac{\sum_{k \neq i}h_{ik}y_k}{1 - h_{ii}} \right) \\
  &= \hat{y}_j - h_{ij}\left(\frac{y_i - y_ih_{ii}}{1 - h_{ii}} - \frac{\sum_{k \neq i}h_{ik}y_k}{1 - h_{ii}} \right) \\
  &= \hat{y}_j - h_{ij}\left(\frac{y_i - y_ih_{ii} - \sum_{k \neq i}h_{ik}y_k}{1 - h_{ii}} \right) \\
  &= \hat{y}_j - h_{ij}\left(\frac{y_i - \sum_{k=1}^n h_{ik}y_k}{1 - h_{ii}} \right) \\
  &= \hat{y}_j - h_{ij}\left(\frac{y_i - \hat{y}_i}{1 - h_{ii}} \right) \\
  &= \hat{y}_j - h_{ij}\left(\frac{\hat{e}_i}{1 - h_{ii}} \right)
\end{align*}

Then

\begin{align*}
\hat{y}_{(i)j} - \hat{y}_j = - h_{ij}\left(\frac{\hat{e}_i}{1 - h_{ii}} \right)
  &\implies \left(\hat{y}_{(i)j} - \hat{y}_j \right)^2= \left(h_{ij}\left(\frac{\hat{e}_i}{1 - h_{ii}} \right) \right)^2 = \frac{\hat{e}_i^2}{(1 - h_{ii})^2}h_{ij}^2 \\
  &\implies \left(\hat{\by}_{(i)} - \hat{\by} \right)'\left(\hat{\by}_{(i)} - \hat{\by} \right) = \sum_{j=1}^n \frac{\hat{e}_i^2}{(1 - h_{ii})^2}h_{ij}^2 \\
  &\implies ||\hat{\by}_{(i)} - \hat{\by}||_2^2 = \frac{\hat{e}_i^2}{(1 - h_{ii})^2} \sum_{j=1}^n h_{ij}^2 
\end{align*}

Now for any symmetric and idempotent matrix $\bM \in \dR^{n \times n}$ we have that:

$$m_{ii} = \sum_{j = 1}^n h_{ij}^2$$
Then

$$||\hat{\by}_{(i)} - \hat{\by}||_2^2 = \frac{\hat{e}_i^2}{(1 - h_{ii})^2} h_{ii}^2 = \frac{h_{ii}^2}{(1 - h_{ii})^2}\hat{e}_i^2 $$
That is the change in the predictions is the result of a function of the leverages
and a function of the error of observation $i$.
  
We can see this with our GDP dataset. We generate new observations, in which

```{r gdp-leverage-outliers}
# Reads Data
dat <- read.csv(file = "Gdp data.csv")

# Extracts the Data
y <- as.numeric(dat[,  1])
X <- as.matrix(dat[, -1])
n <- nrow(X)
X <- cbind(rep(1, n), X)

# Obtains the Estimates
b <- solve(t(X) %*% X, t(X) %*% y)

# Predicted Values
yh <- X %*% b

# Errors
eh <- abs(y - yh)

# Obtains the Minimum, Median and Maximum Errors
err <- quantile(eh, probs = c(0, 0.5, 1))

# Generates New Observations
# An observation at one extreme of the Observed Independent Variables
X1 <- apply(X = X, MARGIN = 2, FUN = max)
# An observation at the center of the Observed Independent Variables
X2 <- apply(X = X, MARGIN = 2, FUN = mean)
# Dependent Variables with different Error levels
y1 <- c(t(X1) %*% b) + c(0, err, err[3] * 2)
y2 <- c(t(X2) %*% b) + c(0, err, err[3] * 2)

# Outlier Level
m <- length(y1)

# Predictions without the Observations
yhi <- yh
bi  <- b
yi  <- y
Xi  <- X

# Table with Changes
tab <- matrix(data = NA, nrow = 0, ncol = 4)

for(i in 1:m){
  # Complete Data Set
  X <- rbind(Xi, X2)
  y <- c(yi, y2[i])
  
  # Coefficients
  b <- solve(t(X) %*% X, t(X) %*% y)
  
  # Predictions
  yh  <- c(X %*% b)
  yhi <- c(rbind(Xi, X2) %*% bi)
  
  # Hat matrix
  H <- X %*% solve(t(X) %*% X, t(X))
  
  # Leverage New Observation
  h <- H[n+1, n+1]
  
  # Change in Predictions
  preCha <- sum((yh - yhi)^2)
  
  # Leverage Component
  levCom <- h / ((1 - h)^2)
  
  # Error Component
  errCom <- (y[n+1] - yh[n+1])^2
  
  # Saves Values
  tab <- rbind(tab, c(preCha, levCom, errCom, levCom * errCom))
}

for(i in 1:m){
  # Complete Data Set
  X <- rbind(Xi, X1)
  y <- c(yi, y1[i])
  
  # Coefficients
  b <- solve(t(X) %*% X, t(X) %*% y)
  
  # Predictions
  yh  <- c(X %*% b)
  yhi <- c(rbind(Xi, X1) %*% bi)
  
  # Hat matrix
  H <- X %*% solve(t(X) %*% X, t(X))
  
  # Leverage New Observation
  h <- H[n+1, n+1]
  
  # Change in Predictions
  preCha <- sum((yh - yhi)^2)
  
  # Leverage Component
  levCom <- h / ((1 - h)^2)
  
  # Error Component
  errCom <- (y[n+1] - yh[n+1])^2
  
  # Saves Values
  tab <- rbind(tab, c(preCha, levCom, errCom, levCom * errCom))
}

  tab <- data.frame(tab)
  tab <- cbind(c(rep("Low", 5), rep("High", 5)), tab)
  tab <- cbind(rep(c("Minimum", "Low", "Medium", "High", "Very High"), 2), tab)
  colnames(tab) <- c("Error Level", "Leverage Level", "Sum of Sq. Diff.", "Lev. Comp.", "Err. Comp.", "Using Formula")
  knitr::kable(tab, digits = 5)
```
  
  
  
  
  
  
  
  
  
  
  
  
  
  