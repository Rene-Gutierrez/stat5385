\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\hy}{\hat{y}}
\newcommand{\he}{\hat{e}}
\newcommand{\hgb}{\hat{\beta}}
\newcommand{\hby}{\hat{\mathbf{y}}}
\newcommand{\hbe}{\hat{\mathbf{e}}}

# Simple Linear Regression

Simple linear regression (**SLR**) is a linear regression model with a single explanatory variable. It focuses on the linear relationship between one independent variable and one dependent variable, making it the most basic form of linear regression analysis.

## Model

The model for simple linear regression is as follows:

$$y_i = \beta_0 + \beta_1 x_i + e_i, \quad i\in\{1,\ldots,n\}$$

where:

* $y_i$ represents the $i$-th observation of the dependent variable.
* $x_i$ represents the $i$-th observation of the independent variable.
* $e_i$ represents the $i$-th observation of the error term.
* $\beta_0$ is the intercept of the linear model, or regression line.
* $\beta_1$ is the slope of the linear model, or regression line.
* $n$ is the number of observations for both variables.

Note that we are not making any assumptions about the error terms.

In the case of the [wine example](#wine-example), we generated the data based on the following linear model:

$$y_i = 75 + 1.5 x_i + e_i $$


```{r slr-parts}
dat <- read.csv(file = "Wine Data.csv")
plot(x    = dat$Glasses,
     y    = dat$Years,
     xlab = "Avg. Glasses of Wine per Week",
     ylab = "Life Expectancy (Years)")
abline(a   = 75,
       b   = 1.5,
       col = 'red',
       lwd = 2)
abline(v   = 0,
       lwd = 2)
text(x = 0.25, y = 76, expression(beta[0] ~ "=75"))
text(x = 3.25, y = 79, expression(beta[1] ~ "=1.5"))
segments(x0 = c(2, 3),
         x1 = c(3, 3),
         y0 = c(78, 78),
         y1 = c(78, 79.5),
         lwd = 2,
         col = 'blue')
```

In this case, the intercept $\beta_0$ is meaningful, as it represents the expected number of years a person would live if they didnâ€™t drink wine at all. However, depending on the data, the intercept may or may not have a meaningful interpretation. The slope $\beta_1$ indicates that for each additional glass of wine consumed per week, our model predicts an increase of 1.5 years in life expectancy.

In practice, we rarely know the true regression line. Instead, it must be estimated from the data. The goal is to find the "best" line that fits the data, where "best" means the line that minimizes the sum of squared errors (SSE) between the observed values and the values predicted by the model.

## Least Squares Estimation

As explained before, we want to minimize the SSE, we can create a function of 
$\beta_0$ and $\beta_1$ with this sum as follows:

$$Q(\beta_0, \beta_1) = \sum_{i=1}^n (e_i(\beta_0, \beta_1))^2 
= \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2  $$

and we can find the minimum of this function easily since it is a differentiable function. We can find the both components of the gradient and equal them to zero to find the critical points. We start with $\beta_0$:

\begin{align*}
\frac{\partial Q}{\partial \beta_0} 
  &= \frac{\partial}{\partial \beta_0} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 \\
  &= \frac{\partial}{\partial \beta_0} \sum_{i=1}^n (y_i^2 + \beta_0^2 + \beta_1^2 x_i^2 - 2 \beta_0 y_i - 2 \beta_1 x_i y_i + 2 \beta_0 \beta_1 x_i) \\
  &= \sum_{i = 1}^n (2 \beta_0 - 2 y_i + 2 \beta_1 x_i) \\
  &= -2 \left( n \beta_0 - n \bar{y} + n\beta_1 \bar{x} \right)
\end{align*}

where we have adopted the notation: $\bar{x} = \frac{1}{n}\sum_{i}^n x_i$ and $\bar{y} = \frac{1}{n}\sum_{i}^n y_i$.

\begin{align}
\frac{\partial Q}{\partial \beta_0} = 0 
  &\iff -2 \left( n \beta_0 - n \bar{y} + n\beta_1 \bar{x} \right) = 0 \notag \\
  &\iff \beta_0 = \bar{y} - \beta_1 \bar{x} \tag{1}
\end{align}

And we can do a similar thing for $\beta_1$:

\begin{align*}
\frac{\partial Q}{\partial \beta_1} 
  &= \frac{\partial}{\partial \beta_0} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 \\
  &= \sum_{i = 1}^n 2(y_i -\beta_0 - \beta_1 x_i)(-x_i) \\
  &= -2\sum_{i = 1}^n y_i x_i + 2 \beta_0 \sum_{i = 1}^n x_i + 2 \beta_1 \sum_{i = 1}^n x_i^2 \\
  &= -2\sum_{i = 1}^n y_i x_i + 2 n \beta_0 \bar{x} + 2 \beta_1 \sum_{i = 1}^n x_i^2
\end{align*}

then:

\begin{align}
\frac{\partial Q}{\partial \beta_1} = 0 
  &\iff -2\sum_{i = 1}^n y_i x_i + 2 n \beta_0 \bar{x} + 2 \beta_1 \sum_{i = 1}^n x_i^2 = 0 \notag \\
  &\iff \sum_{i = 1}^n y_i x_i = n \beta_0 \bar{x} + \beta_1  \sum_{i = 1}^n x_i^2 \tag{2}
\end{align}

Now, substituting (1) into (2) we have that

\begin{align*}
\sum_{i = 1}^n y_i x_i 
  &= n (\bar{y} - \beta_1 \bar{x}) \bar{x}  + \beta_1  \sum_{i = 1}^n x_i^2 \\
  &= n \bar{y} \bar{x} - n \beta_1 \bar{x}^2 + \beta_1  \sum_{i = 1}^n x_i^2 \\
  &= n \bar{y} \bar{x} + \beta_1 \left( \sum_{i = 1}^n x_i^2 - n \bar{x}^2 \right)
\end{align*}

Then, 

$$ \beta_1 = \frac{\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i = 1}^n x_i^2 - n \bar{x}^2} $$

so, the only critical point for $Q(\beta_0,\beta_1)$ is when:

$$ \hat{\beta}_1 = \frac{\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i = 1}^n x_i^2 - n \bar{x}^2} $$
$$ \hat{\beta_0} = \bar{y} - \hat{\beta}_1 \bar{x}$$

where we use $\hat{}$, to denote the specific critical point. It remains to see
if this is indeed a minimum. One can check the second order conditions.

Now, if we introduce the notation for sample variance and covariance:

$$ S^2_{xx} = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 $$
$$ S_{xy}   = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) $$

and note the following:

\begin{align*}
\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
  &= \frac{1}{n-1} \sum_{i=1}^n (x_i^2 - 2\bar{x}x_i + \bar{x}^2) \\
  &= \frac{1}{n-1} \sum_{i=1}^n x_i^2 - 2\bar{x}\sum_{i=1}^n x_i + \sum_{i=1}^n \bar{x}^2 \\
  &= \frac{1}{n-1} \sum_{i=1}^n x_i^2 - 2\bar{x}(n\bar{x}) + n \bar{x}^2 \\
  &= \frac{1}{n-1} \sum_{i=1}^n x_i^2 - 2n\bar{x}^2 + n \bar{x}^2 \\
  &= \frac{1}{n-1} \sum_{i=1}^n x_i^2 - n\bar{x}^2
\end{align*}

and

\begin{align*}
\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})
  &= \frac{1}{n-1} \sum_{i=1}^n (x_iy_i - \bar{x}y_i - \bar{y}x_i + \bar{x}\bar{y}) \\
  &= \frac{1}{n-1} \sum_{i=1}^n x_iy_i - \bar{x} \sum_{i=1}^ny_i - \bar{y} \sum_{i=1}^n x_i + \sum_{i=1}^n \bar{x}\bar{y} \\
  &= \frac{1}{n-1} \sum_{i=1}^n x_iy_i - n\bar{x} \bar{y} - n\bar{y} \bar{x} + n \bar{x}\bar{y} \\
  &= \frac{1}{n-1} \sum_{i=1}^n x_iy_i - n\bar{x} \bar{y} \\
\end{align*}

then we can express $\hat{\beta}_1$ as:

$$\hat{\beta}_1 = \frac{(n-1)S_{xy}}{(n-1)S_{xx}^2}=\frac{S_{xy}}{S_{xx}^2} $$

Now notice that in order to find the Least Squares estimates you don't require
the complete data set, but only require the following quantities:

* $\bar{y}$.
* $\bar{x}$.
* $S_{xx}^2$.
* $S_{xy}$.

### Other estimated quantites

If we use the Least squares estimates in the regression equation, we can derive
other estimated quantities:

The estimated value for observation $i$:

$$ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i $$

and the estimated error:

$$ \hat{e}_i = y_i - \hat{y}_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i $$

And we can also compare our estimated regression line (blue) with the real 
regression line (red) in the following as follows:

```{r slr-real-vs-est}
outReg <- lm(Years ~ Glasses, data = dat)
plot(x    = dat$Glasses,
     y    = dat$Years,
     xlab = "Avg. Glasses of Wine per Week",
     ylab = "Life Expectancy (Years)")
abline(a   = 75,
       b   = 1.5,
       col = 'red',
       lwd = 2)
abline(a   = outReg$coefficients[1],
       b   = outReg$coefficients[2],
       col = 'blue',
       lwd = 2)
```

## Properties of the Estimates

The estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ are linear combinations of 
$\mathbf{y} = (y_1,\ldots,y_n)'$. To see this, notice the following:

\begin{align*}
\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
  &= \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} \\
  &= \sum_{i=1}^n x_i y_i - \bar{x} \sum_{i=1}^n y_i \\
  &= \sum_{i=1}^n x_i y_i - \sum_{i=1}^n \bar{x} y_i \\
  &= \sum_{i=1}^n (x_i y_i - \bar{x} y_i) \\
  &= \sum_{i=1}^n (x_i - \bar{x}) y_i \\
\end{align*}

Then

$$ \hat{\beta}_1 =  \frac{\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{\sum_{i=1}^n (x_i - \bar{x}) y_i}{\sum_{i=1}^n (x_i - \bar{x})^2} = \sum_{i=1}^n\frac{(x_i - \bar{x}) }{\sum_{i=1}^n (x_i - \bar{x})^2}y_i $$

and similarly:

$$ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} = \sum_{i=1}^n \frac{y_i}{n} - \sum_{i=1}^n\frac{(x_i - \bar{x}) }{\sum_{j = 1}^n x_j^2 - n \bar{x}^2}y_i \bar{x} = \sum_{i=1}^n \left( \frac{1}{n} - \frac{(x_i - \bar{x}) }{\sum_{j = 1}^n x_j^2 - n \bar{x}^2} \bar{x} \right)y_i $$

Also, notice that the sum of the errors is $0$.

\begin{align*}
\sum_{i=1}^n \hat{e}_i 
  &= \sum_{i=1}^n(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \\
  &= \sum_{i=1}^n y_i - \sum_{i=1}^n \hat{\beta}_0 - \hat{\beta}_1 \sum_{i=1}^n x_i \\
  &= n\bar{y} - n \hat{\beta}_0 - n \hat{\beta}_1 \bar{x} \\
  &= n\bar{y} - n (\bar{y} - \hat{\beta}_1 \bar{x}) - n \hat{\beta}_1 \bar{x} \\
  &= n\bar{y} - n \bar{y} + n \hat{\beta}_1 \bar{x} - n \hat{\beta}_1 \bar{x} \\
  &= 0
\end{align*}

If we let $\hat{\mathbf{e}} = (\hat{e}_i,\ldots,\hat{e}_n)'$ and $\mathbf{x}=(x_1,\ldots,x_n)'$,
two vectors of size $n$, then we have that $\hat{\mathbf{e}}$ and $\mathbf{x}$ are orthogonal.
That is:


\begin{align*}
\langle \hbe, \bx \rangle 
  &= \sum_{i=1}^{n} \he_i x_i \\
  &= \sum_{i=1}^{n}  (y_i - \hgb_0 - \hgb_1 x_i)x_i \\
  &= \sum_{i=1}^{n}  (y_i x_i - \hgb_0x_i - \hgb_1 x_i x_i) \\
  &= \sum_{i=1}^{n} y_i x_i - \sum_{i=1}^{n} \hgb_0x_i - \sum_{i=1}^{n} \hgb_1 x_i x_i \\
  &= \sum_{i=1}^{n} y_i - n \hgb_0 \bar{x} - \hgb_1 \sum_{i=1}^{n} x_i^2 \\
  &= \sum_{i=1}^{n} y_i - n (\bar{y} - \hat{\beta}_1 \bar{x}) \bar{x} - \hgb_1 \sum_{i=1}^{n} x_i^2 \\
  &= \sum_{i=1}^{n} y_i - n \bar{y} \bar{x} + n\hat{\beta}_1 \bar{x}^2 - \hgb_1 \sum_{i=1}^{n} x_i^2 \\
  &= \sum_{i=1}^{n} y_i - n \bar{y} \bar{x} - \hat{\beta}_1 (\sum_{i=1}^{n} x_i^2 - n\bar{x}^2)  \\
  &= \sum_{i=1}^{n} y_i - n \bar{y} \bar{x} -  \frac{\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i = 1}^n x_i^2 - n \bar{x}^2}(\sum_{i=1}^{n} x_i^2 - n\bar{x}^2)  \\
  &= \sum_{i=1}^{n} y_i - n \bar{y} \bar{x} -  (\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x})  \\
  &=0
\end{align*}

The same applies to $\hby = (\hy_1,\ldots,\hby_n)'$ and $\hbe$, as we can see:

\begin{align*}
\langle \hbe, \hby \rangle 
  &= \sum_{i=1}^{n} \he_i \hy_i \\
  &= \sum_{i=1}^{n} \he_i(\hgb_0 + \hgb_1 x_i) \\
  &= \sum_{i=1}^{n} (\he_i \hgb_0 + he_i \hgb_1 x_i) \\
  &= \hgb_0 \sum_{i=1}^{n} \he_i + \hgb_1 \sum_{i=1}^{n} he_i x_i \\
  &= \hgb_1 \langle \hbe, \bx \rangle  \\
  &= 0
\end{align*}

Finally, the average of $\hby$ and $\by$ are the same, to see this notice:

\begin{align*}
\frac{1}{n} \sum_{i=1}^n \hy_i 
  &= \frac{1}{n} \sum_{i=1}^n (\hgb_0 + \hgb_1 x_i) \\
  &= \frac{1}{n} (n \hgb_0 + \hgb_1 \sum_{i=1}^n x_i) \\
  &= \frac{1}{n} (n \hgb_0 + n \hgb_1 \bx) \\
  &= \hgb_0 + \hgb_1 \bx \\
  &= \bar{y} - \hgb_1 \bx  + \hgb_1 \bx \\
  &= \bar{y} \\
\end{align*}

## Centering and Standarizing the Data

Some transformations of the data can help the regression analysis 
or make it more intuitive. There are 2 main transformations of the data: 
**centering** and **standardization**.

Consider observations $x_1,\ldots,x_n$, then the centered version of observation
$i$ is given by:

$$x_i' = x_i - \bar{x}$$

The new observations $x_1',\ldots,x_n'$ are centered and their mean is $0$.

$$\bar{x}' = \frac{1}{n} \sum_{i=1}^n x_i' = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x}) = \frac{1}{n} \left(\sum_{i=1}^n x_i - \sum_{i=1}^n \bar{x} \right) = \frac{1}{n} \left(n\bar{x} - n \bar{x} \right) = 0$$
Also, let us see that the variance of the standardized variables is the same as 
the variance of the original observations.

$$ S_{xx}' = \frac{1}{n-1} \sum_{i=1}^n (x_i' - \bar{x}') = \frac{1}{n-1} \sum_{i=1}^n (x_i') = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x}) = S_{xx} $$
So the variance of the observations is not affected by the centering.

If we center another set of observations $y_1,\ldots,y_n$, and compute the covariance,
we have that it also doesn't change.

$$S_{xy}' = \frac{1}{n-1} \sum_{i=1}^n (x_i' - \bar{x})(y_i' - \bar{x}) = \frac{1}{n-1} \sum_{i=1}^n (x_i')(y_i') = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{x}) = S_{xy}$$

The standardized version of observation $i$ is given by:

$$x_i'' = \frac{x_i - \bar{x}}{\sqrt{S_{xx}}} = \frac{x_i'}{\sqrt{S_{xx}}}$$



The standardized observations have mean of 0 and variance 1. Let's see first that
the sample mean is 

$$ \bar{x}_i'' =  \frac{1}{n} \sum_{i=1}^n x_i'' =  \frac{1}{n} \sum_{i=1}^n \frac{x_i'}{\sqrt{S_{xx}}} = \frac{1}{\sqrt{S_{xx}}}\frac{1}{n} \sum_{i=1}^n x_i' = \frac{1}{\sqrt{S_{xx}}} \bar{x}' = 0$$

Now let us see that the variance of the standardized  observations is 1.

\begin{align*}
S_{xx}'' 
  &= \frac{1}{n-1} \sum_{i=1}^n (x_i'' - \bar{x}'')^2 \\
  &= \frac{1}{n-1} \sum_{i=1}^n (x_i'')^2 \\
  &= \frac{1}{n-1} \sum_{i=1}^n \left(\frac{x_i - \bar{x}}{\sqrt{S_{xx}}}\right)^2 \\
  &= \frac{1}{n-1} \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{S_{xx}} \\
  &= \frac{1}{S_{xx}} \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \\
  &= \frac{1}{S_{xx}}S_{xx} \\
  &= 1
\end{align*}

Now, let us introduce the sample correlation as:

$$ r_{xy} = \frac{S_{xy}}{\sqrt{S_{xx}{S_{yy}}}} $$
If we standardize two sets of observations, then the covaraince of the standardized version
is the correlation of the standardized version. Let us see it:

\begin{align*}
S_{xy}'' 
  &= \frac{1}{n-1} \sum_{i=1} (x_i'' - \bar{x}'')(y_i'' - \bar{x}'') \\
  &= \frac{1}{n-1} \sum_{i=1} (x_i'')(y_i'') \\
  &= \frac{1}{n-1} \sum_{i=1} \left(\frac{x_i - \bar{x}}{\sqrt{S_{xx}}}\right) \left(\frac{y_i - \bar{y}}{\sqrt{S_{yy}}}\right) \\
  &= \frac{1}{\sqrt{S_{yy}}\sqrt{S_{xx}}}\frac{1}{n-1} \sum_{i=1} (x_i - \bar{x}) (y_i - \bar{y}) \\
  &= \frac{1}{\sqrt{S_{yy}}\sqrt{S_{xx}}} S_{xy} \\
  &= r_{xy}
\end{align*}

With this results, we can analyze the effects of following 3 scenarios on the
estimated coefficients:

* Independent variable centered.
* Both, Independent and dependent variable centered.
* Both, Independent and dependent variable standardized.

### Independent variable centered

Lets compute the value for $\beta_1$ when the data is centered.

$$\hgb_1' = \frac{S_{xy}'}{S_{xx}'} = \frac{S_{xy}}{S_{xx}} = \hgb_1 $$
So centering the data doesn't change the value of the estimated slope.

$$ \hgb_0' = \bar{y} - \hgb_1' \bar{x}' = \bar{y} - \hgb_1' 0 = \bar{y} $$
So centering the data, makes the estimated intercept to coincide with the mean of
the independent variable.

We can see this in one of our example data sets, looking at the ad spending data
we can perform linear regression on the original data and the centered data:

```{r center-x}
# Read Data
dat <- read.csv("Ad spending Data.csv")
# Assign data
x <- dat$Ad.Spending
y <- dat$Revenue
# Centers x
xCen <- x - mean(x)
# Linear regression on the original data
outRegOri <- lm(y ~ x)
# Linear regression on the centered independent variable data
outRegCen <- lm(y ~ xCen)
# Plots
## Two plots in the same image
par(mfrow = c(1, 2))
## Original data
# Plots the points
plot(x    = x,
     y    = y,
     xlab = "Ad spending",
     ylab = "Revenue")
# Plots the regression line
abline(a   = outRegOri$coefficients[1],
       b   = outRegOri$coefficients[2],
       col = 'red',
       lwd = 2)
## Independent Variable centered data
# Plots the points
plot(x    = xCen,
     y    = y,
     xlab = "Ad spending (Centered)",
     ylab = "Revenue")
# Plots the regression line
abline(a   = outRegCen$coefficients[1],
       b   = outRegCen$coefficients[2],
       col = 'red',
       lwd = 2)
abline(v   = 0,
       lwd = 2)
```

So we can appreciate that centering the independent variable just shifts the data
horizontally so the mean will be at zero.

### Both Variables centered

Now lets see the effects when both variables are centered. Here, we will denote
the estimates again with one prime, that is $\hgb'$.

Again, the estimate of the slope doesn't change:

$$\hgb_1' = \frac{S_{xy}'}{S_{xx}'} = \frac{S_{xy}}{S_{xx}} = \hgb_1 $$
while the estimate of the intercept becomes zero (the new mean of the centered
dependent variable)

$$ \hgb_0' = \bar{y}' - \hgb_1' \bar{x}' = \bar{y}' = 0 $$
The effect of this transformation can be observed, here:

```{r center-xy}
# Read Data
dat <- read.csv("Ad spending Data.csv")
# Assign data
x <- dat$Ad.Spending
y <- dat$Revenue
# Centers x and y
xCen <- x - mean(x)
yCen <- y - mean(y)
# Linear regression on the original data
outRegOri <- lm(y ~ x)
# Linear regression on the centered data
outRegCen <- lm(yCen ~ xCen)
# Plots
## Two plots in the same image
par(mfrow = c(1, 2))
## Original data
# Plots the points
plot(x    = x,
     y    = y,
     xlab = "Ad spending",
     ylab = "Revenue")
# Plots the regression line
abline(a   = outRegOri$coefficients[1],
       b   = outRegOri$coefficients[2],
       col = 'red',
       lwd = 2)
## Centered data
# Plots the points
plot(x    = xCen,
     y    = yCen,
     xlab = "Ad spending (Centered)",
     ylab = "Revenue")
# Plots the regression line
abline(a   = outRegCen$coefficients[1],
       b   = outRegCen$coefficients[2],
       col = 'red',
       lwd = 2)
abline(v   = 0,
       lwd = 2)
abline(h   = 0,
       lwd = 2)
```

### Independent and dependent variable standardized

Again we start we the slope estimate:

$$\hgb_1'' = \frac{S_{xy}''}{S_{xx}''} = \frac{r_{xy}}{1} =r_{xy} $$
so, the estimate of the slope is the sample correlation of the original observations.

Again, we can see this graphically:

```{r slr-standarize-xy}
# Read Data
dat <- read.csv("Ad spending Data.csv")
# Assign data
x <- dat$Ad.Spending
y <- dat$Revenue
# Standardizes x and y
xSta <- (x - mean(x))/sqrt(var(x))
ySta <- (y - mean(y))/sqrt(var(y))
# Linear regression on the original data
outRegOri <- lm(y ~ x)
# Linear regression on the standard data
outRegSta <- lm(ySta ~ xSta)
# Plots
## Two plots in the same image
par(mfrow = c(1, 2))
## Original data
# Plots the points
plot(x    = x,
     y    = y,
     xlab = "Ad spending",
     ylab = "Revenue")
# Plots the regression line
abline(a   = outRegOri$coefficients[1],
       b   = outRegOri$coefficients[2],
       col = 'red',
       lwd = 2)
## Standard data
# Plots the points
plot(x    = xSta,
     y    = ySta,
     xlab = "Ad spending (Centered)",
     ylab = "Revenue")
# Plots the regression line
abline(a   = outRegSta$coefficients[1],
       b   = outRegSta$coefficients[2],
       col = 'red',
       lwd = 2)
abline(v   = 0,
       lwd = 2)
abline(h   = 0,
       lwd = 2)
```

Now, let us see that the correlation is always in the interval $(-1,1)$. To see
this, notice the following:

\begin{align*}
\frac{1}{n-1} \sum_{i=1}^n (x_i'' + y_i'')^2 
  &= \frac{1}{n-1} \sum_{i=1}^n \left((x_i'')^2 + 2x_i'' y_i'' + (y_i'')^2 \right) \\
  &= \frac{\sum_{i=1}^n (x_i'')^2}{n-1}  + 2\frac{\sum_{i=1}^n x_i''y_i''}{n-1}  + \frac{\sum_{i=1}^n (y_i'')^2}{n-1} \\
  &= \frac{\sum_{i=1}^n (x_i'' - \bar{x}'')^2}{n-1}  + 2\frac{\sum_{i=1}^n (x_i'' - \bar{x}'')(y_i'' - \bar{y}'')}{n-1}  + \frac{\sum_{i=1}^n (y_i'' - \bar{y}'')^2}{n-1} \\
  &= S_{xx}'' + 2 S_{xy}'' + S_{yy}'' \\
  &= 1 + 2r_{xy} + 1 \\
  &= 2(1 + r_{xy})
\end{align*}

In a similar way it can be shown that:

$$\frac{1}{n-1} \sum_{i=1}^n (x_i'' + y_i'')^2 = 2(1 - r_{xy})$$

Now since, $$\frac{1}{n-1} \sum_{i=1}^n (x_i'' + y_i'')^2 \geq 0$$
then we have that

\begin{align*}
\frac{1}{n-1} \sum_{i=1}^n (x_i'' + y_i'')^2 \geq 0
  &\implies 2(1 + r_{xy}) \geq 0 \\
  &\implies 1 + r_{xy} \geq 0 \\
  &\implies r_{xy} \geq -1 \\
\end{align*}

Similarly, since

$$\frac{1}{n-1} \sum_{i=1}^n (x_i'' - y_i'')^2 \geq 0$$

implies

$$r_{xy} \leq 1 $$
then, we have that:
$$ -1 \leq r_{xy} \leq 1$$
which implies that the slope of the regression analysis after standarizing both
variables is going to be in the interval $(-1, 1)$.

## Residual Analysis





















