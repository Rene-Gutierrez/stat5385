<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Simple Linear Regression | _main.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Simple Linear Regression | _main.knit" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Simple Linear Regression | _main.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="polynomial-regression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Stat 5385/6385</a></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#general-math"><i class="fa fa-check"></i><b>2.1</b> General Math</a></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.2.3</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.2.4</b> Positive Definite Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.3</b> Probability</a></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.4</b> Statistics</a></li>
<li class="chapter" data-level="2.5" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.5</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.5.1</b> Gradient</a></li>
<li class="chapter" data-level="2.5.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.5.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.5.3</b> Applications:</a></li>
<li class="chapter" data-level="2.5.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.5.4</b> Matrix Calculus</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#examples"><i class="fa fa-check"></i><b>3.1</b> Examples</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction.html"><a href="introduction.html#ad-spending"><i class="fa fa-check"></i><b>3.1.1</b> Ad Spending</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction.html"><a href="introduction.html#winw-example"><i class="fa fa-check"></i><b>3.1.2</b> Wine and Life Expectancy</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction.html"><a href="introduction.html#burger-demand"><i class="fa fa-check"></i><b>3.1.3</b> Burger Demand</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model"><i class="fa fa-check"></i><b>4.1</b> Model</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>4.2</b> Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#other-estimated-quantites"><i class="fa fa-check"></i><b>4.2.1</b> Other estimated quantites</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#properties-of-the-estimates"><i class="fa fa-check"></i><b>4.3</b> Properties of the Estimates</a></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-and-standarizing-the-data"><i class="fa fa-check"></i><b>4.4</b> Centering and Standarizing the Data</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independent-variable-centered"><i class="fa fa-check"></i><b>4.4.1</b> Independent variable centered</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#both-variables-centered"><i class="fa fa-check"></i><b>4.4.2</b> Both Variables centered</a></li>
<li class="chapter" data-level="4.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independent-and-dependent-variable-standardized"><i class="fa fa-check"></i><b>4.4.3</b> Independent and dependent variable standardized</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>4.5</b> Coefficient of Determination</a></li>
<li class="chapter" data-level="4.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residual-analysis"><i class="fa fa-check"></i><b>4.6</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#non-linear-regression-function"><i class="fa fa-check"></i><b>4.6.1</b> Non-linear regression function</a></li>
<li class="chapter" data-level="4.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#heteroscedasticity"><i class="fa fa-check"></i><b>4.6.2</b> Heteroscedasticity</a></li>
<li class="chapter" data-level="4.6.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outliers"><i class="fa fa-check"></i><b>4.6.3</b> Outliers</a></li>
<li class="chapter" data-level="4.6.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#variables-ommited"><i class="fa fa-check"></i><b>4.6.4</b> Variables Ommited</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#cross-validation"><i class="fa fa-check"></i><b>4.7</b> Cross-Validation</a></li>
<li class="chapter" data-level="4.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>4.8</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="4.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-in-matrix-form"><i class="fa fa-check"></i><b>4.9</b> Model in Matrix Form</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares-in-matrix-form"><i class="fa fa-check"></i><b>4.9.1</b> Weighted Least Squares in Matrix Form</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="polynomial-regression.html"><a href="polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Polynomial Regression</a></li>
<li class="chapter" data-level="6" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="multiple-regression.html"><a href="multiple-regression.html#example-5"><i class="fa fa-check"></i><b>6.2</b> Example</a></li>
<li class="chapter" data-level="6.3" data-path="multiple-regression.html"><a href="multiple-regression.html#least-squares-estimation-1"><i class="fa fa-check"></i><b>6.3</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.4" data-path="multiple-regression.html"><a href="multiple-regression.html#properties-of-the-estimates-1"><i class="fa fa-check"></i><b>6.4</b> Properties of the Estimates</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple-linear-regression" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Simple Linear Regression<a href="simple-linear-regression.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Simple linear regression (<strong>SLR</strong>) is a linear regression model with a single explanatory variable. It focuses on the linear relationship between one independent variable and one dependent variable, making it the most basic form of linear regression analysis.</p>
<div id="model" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Model<a href="simple-linear-regression.html#model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The model for simple linear regression is as follows:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_i + e_i, \quad i\in\{1,\ldots,n\}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y_i\)</span> represents the <span class="math inline">\(i\)</span>-th observation of the dependent variable.</li>
<li><span class="math inline">\(x_i\)</span> represents the <span class="math inline">\(i\)</span>-th observation of the independent variable.</li>
<li><span class="math inline">\(e_i\)</span> represents the <span class="math inline">\(i\)</span>-th observation of the error term.</li>
<li><span class="math inline">\(\beta_0\)</span> is the intercept of the linear model, or regression line.</li>
<li><span class="math inline">\(\beta_1\)</span> is the slope of the linear model, or regression line.</li>
<li><span class="math inline">\(n\)</span> is the number of observations for both variables.</li>
</ul>
<p>Note that we are not making any assumptions about the error terms.</p>
<p>In the case of the <a href="#wine-example">wine example</a>, we generated the data based on the following linear model:</p>
<p><span class="math display">\[y_i = 75 + 1.5 x_i + e_i \]</span></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="simple-linear-regression.html#cb16-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">&quot;Wine Data.csv&quot;</span>)</span>
<span id="cb16-2"><a href="simple-linear-regression.html#cb16-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> dat<span class="sc">$</span>Glasses,</span>
<span id="cb16-3"><a href="simple-linear-regression.html#cb16-3" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>Years,</span>
<span id="cb16-4"><a href="simple-linear-regression.html#cb16-4" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Avg. Glasses of Wine per Week&quot;</span>,</span>
<span id="cb16-5"><a href="simple-linear-regression.html#cb16-5" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Life Expectancy (Years)&quot;</span>)</span>
<span id="cb16-6"><a href="simple-linear-regression.html#cb16-6" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> <span class="dv">75</span>,</span>
<span id="cb16-7"><a href="simple-linear-regression.html#cb16-7" tabindex="-1"></a>       <span class="at">b   =</span> <span class="fl">1.5</span>,</span>
<span id="cb16-8"><a href="simple-linear-regression.html#cb16-8" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb16-9"><a href="simple-linear-regression.html#cb16-9" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb16-10"><a href="simple-linear-regression.html#cb16-10" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v   =</span> <span class="dv">0</span>,</span>
<span id="cb16-11"><a href="simple-linear-regression.html#cb16-11" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb16-12"><a href="simple-linear-regression.html#cb16-12" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x =</span> <span class="fl">0.25</span>, <span class="at">y =</span> <span class="dv">76</span>, <span class="fu">expression</span>(beta[<span class="dv">0</span>] <span class="sc">~</span> <span class="st">&quot;=75&quot;</span>))</span>
<span id="cb16-13"><a href="simple-linear-regression.html#cb16-13" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x =</span> <span class="fl">3.25</span>, <span class="at">y =</span> <span class="dv">79</span>, <span class="fu">expression</span>(beta[<span class="dv">1</span>] <span class="sc">~</span> <span class="st">&quot;=1.5&quot;</span>))</span>
<span id="cb16-14"><a href="simple-linear-regression.html#cb16-14" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>),</span>
<span id="cb16-15"><a href="simple-linear-regression.html#cb16-15" tabindex="-1"></a>         <span class="at">x1 =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb16-16"><a href="simple-linear-regression.html#cb16-16" tabindex="-1"></a>         <span class="at">y0 =</span> <span class="fu">c</span>(<span class="dv">78</span>, <span class="dv">78</span>),</span>
<span id="cb16-17"><a href="simple-linear-regression.html#cb16-17" tabindex="-1"></a>         <span class="at">y1 =</span> <span class="fu">c</span>(<span class="dv">78</span>, <span class="fl">79.5</span>),</span>
<span id="cb16-18"><a href="simple-linear-regression.html#cb16-18" tabindex="-1"></a>         <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb16-19"><a href="simple-linear-regression.html#cb16-19" tabindex="-1"></a>         <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/slr-parts-1.png" width="672" /></p>
<p>In this case, the intercept <span class="math inline">\(\beta_0\)</span> is meaningful, as it represents the expected number of years a person would live if they didn’t drink wine at all. However, depending on the data, the intercept may or may not have a meaningful interpretation. The slope <span class="math inline">\(\beta_1\)</span> indicates that for each additional glass of wine consumed per week, our model predicts an increase of 1.5 years in life expectancy.</p>
<p>In practice, we rarely know the true regression line. Instead, it must be estimated from the data. The goal is to find the “best” line that fits the data, where “best” means the line that minimizes the sum of squared errors (SSE) between the observed values and the values predicted by the model.</p>
</div>
<div id="least-squares-estimation" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Least Squares Estimation<a href="simple-linear-regression.html#least-squares-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As explained before, we want to minimize the SSE, we can create a function of
<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> with this sum as follows:</p>
<p><span class="math display">\[Q(\beta_0, \beta_1) = \sum_{i=1}^n (e_i(\beta_0, \beta_1))^2
= \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2  \]</span></p>
<p>and we can find the minimum of this function easily since it is a differentiable function. We can find the both components of the gradient and equal them to zero to find the critical points. We start with <span class="math inline">\(\beta_0\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial Q}{\partial \beta_0}
  &amp;= \frac{\partial}{\partial \beta_0} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 \\
  &amp;= \frac{\partial}{\partial \beta_0} \sum_{i=1}^n (y_i^2 + \beta_0^2 + \beta_1^2 x_i^2 - 2 \beta_0 y_i - 2 \beta_1 x_i y_i + 2 \beta_0 \beta_1 x_i) \\
  &amp;= \sum_{i = 1}^n (2 \beta_0 - 2 y_i + 2 \beta_1 x_i) \\
  &amp;= -2 \left( n \beta_0 - n \bar{y} + n\beta_1 \bar{x} \right)
\end{align*}\]</span></p>
<p>where we have adopted the notation: <span class="math inline">\(\bar{x} = \frac{1}{n}\sum_{i}^n x_i\)</span> and <span class="math inline">\(\bar{y} = \frac{1}{n}\sum_{i}^n y_i\)</span>.</p>
<p><span class="math display">\[\begin{align}
\frac{\partial Q}{\partial \beta_0} = 0
  &amp;\iff -2 \left( n \beta_0 - n \bar{y} + n\beta_1 \bar{x} \right) = 0 \notag \\
  &amp;\iff \beta_0 = \bar{y} - \beta_1 \bar{x} \tag{1}
\end{align}\]</span></p>
<p>And we can do a similar thing for <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial Q}{\partial \beta_1}
  &amp;= \frac{\partial}{\partial \beta_0} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 \\
  &amp;= \sum_{i = 1}^n 2(y_i -\beta_0 - \beta_1 x_i)(-x_i) \\
  &amp;= -2\sum_{i = 1}^n y_i x_i + 2 \beta_0 \sum_{i = 1}^n x_i + 2 \beta_1 \sum_{i = 1}^n x_i^2 \\
  &amp;= -2\sum_{i = 1}^n y_i x_i + 2 n \beta_0 \bar{x} + 2 \beta_1 \sum_{i = 1}^n x_i^2
\end{align*}\]</span></p>
<p>then:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial Q}{\partial \beta_1} = 0
  &amp;\iff -2\sum_{i = 1}^n y_i x_i + 2 n \beta_0 \bar{x} + 2 \beta_1 \sum_{i = 1}^n x_i^2 = 0 \notag \\
  &amp;\iff \sum_{i = 1}^n y_i x_i = n \beta_0 \bar{x} + \beta_1  \sum_{i = 1}^n x_i^2 \tag{2}
\end{align}\]</span></p>
<p>Now, substituting (1) into (2) we have that</p>
<p><span class="math display">\[\begin{align*}
\sum_{i = 1}^n y_i x_i
  &amp;= n (\bar{y} - \beta_1 \bar{x}) \bar{x}  + \beta_1  \sum_{i = 1}^n x_i^2 \\
  &amp;= n \bar{y} \bar{x} - n \beta_1 \bar{x}^2 + \beta_1  \sum_{i = 1}^n x_i^2 \\
  &amp;= n \bar{y} \bar{x} + \beta_1 \left( \sum_{i = 1}^n x_i^2 - n \bar{x}^2 \right)
\end{align*}\]</span></p>
<p>Then,</p>
<p><span class="math display">\[ \beta_1 = \frac{\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i = 1}^n x_i^2 - n \bar{x}^2} \]</span></p>
<p>so, the only critical point for <span class="math inline">\(Q(\beta_0,\beta_1)\)</span> is when:</p>
<p><span class="math display">\[ \hat{\beta}_1 = \frac{\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i = 1}^n x_i^2 - n \bar{x}^2} \]</span>
<span class="math display">\[ \hat{\beta_0} = \bar{y} - \hat{\beta}_1 \bar{x}\]</span></p>
<p>where we use <span class="math inline">\(\hat{}\)</span>, to denote the specific critical point. It remains to see
if this is indeed a minimum. One can check the second order conditions.</p>
<p>Now, if we introduce the notation for sample variance and covariance:</p>
<p><span class="math display">\[ S^2_{xx} = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \]</span>
<span class="math display">\[ S_{xy}   = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \]</span></p>
<p>and note the following:</p>
<p><span class="math display">\[\begin{align*}
\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i^2 - 2\bar{x}x_i + \bar{x}^2) \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n x_i^2 - 2\bar{x}\sum_{i=1}^n x_i + \sum_{i=1}^n \bar{x}^2 \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n x_i^2 - 2\bar{x}(n\bar{x}) + n \bar{x}^2 \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n x_i^2 - 2n\bar{x}^2 + n \bar{x}^2 \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n x_i^2 - n\bar{x}^2
\end{align*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align*}
\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_iy_i - \bar{x}y_i - \bar{y}x_i + \bar{x}\bar{y}) \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n x_iy_i - \bar{x} \sum_{i=1}^ny_i - \bar{y} \sum_{i=1}^n x_i + \sum_{i=1}^n \bar{x}\bar{y} \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n x_iy_i - n\bar{x} \bar{y} - n\bar{y} \bar{x} + n \bar{x}\bar{y} \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n x_iy_i - n\bar{x} \bar{y} \\
\end{align*}\]</span></p>
<p>then we can express <span class="math inline">\(\hat{\beta}_1\)</span> as:</p>
<p><span class="math display">\[\hat{\beta}_1 = \frac{(n-1)S_{xy}}{(n-1)S_{xx}^2}=\frac{S_{xy}}{S_{xx}^2} \]</span></p>
<p>Now notice that in order to find the Least Squares estimates you don’t require
the complete data set, but only require the following quantities:</p>
<ul>
<li><span class="math inline">\(\bar{y}\)</span>.</li>
<li><span class="math inline">\(\bar{x}\)</span>.</li>
<li><span class="math inline">\(S_{xx}^2\)</span>.</li>
<li><span class="math inline">\(S_{xy}\)</span>.</li>
</ul>
<div id="other-estimated-quantites" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Other estimated quantites<a href="simple-linear-regression.html#other-estimated-quantites" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If we use the Least squares estimates in the regression equation, we can derive
other estimated quantities:</p>
<p>The estimated value for observation <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i \]</span></p>
<p>and the estimated error:</p>
<p><span class="math display">\[ \hat{e}_i = y_i - \hat{y}_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \]</span></p>
<p>And we can also compare our estimated regression line (blue) with the real
regression line (red) in the following as follows:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="simple-linear-regression.html#cb17-1" tabindex="-1"></a>outReg <span class="ot">&lt;-</span> <span class="fu">lm</span>(Years <span class="sc">~</span> Glasses, <span class="at">data =</span> dat)</span>
<span id="cb17-2"><a href="simple-linear-regression.html#cb17-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> dat<span class="sc">$</span>Glasses,</span>
<span id="cb17-3"><a href="simple-linear-regression.html#cb17-3" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>Years,</span>
<span id="cb17-4"><a href="simple-linear-regression.html#cb17-4" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Avg. Glasses of Wine per Week&quot;</span>,</span>
<span id="cb17-5"><a href="simple-linear-regression.html#cb17-5" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Life Expectancy (Years)&quot;</span>)</span>
<span id="cb17-6"><a href="simple-linear-regression.html#cb17-6" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> <span class="dv">75</span>,</span>
<span id="cb17-7"><a href="simple-linear-regression.html#cb17-7" tabindex="-1"></a>       <span class="at">b   =</span> <span class="fl">1.5</span>,</span>
<span id="cb17-8"><a href="simple-linear-regression.html#cb17-8" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb17-9"><a href="simple-linear-regression.html#cb17-9" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb17-10"><a href="simple-linear-regression.html#cb17-10" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outReg<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb17-11"><a href="simple-linear-regression.html#cb17-11" tabindex="-1"></a>       <span class="at">b   =</span> outReg<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb17-12"><a href="simple-linear-regression.html#cb17-12" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>,</span>
<span id="cb17-13"><a href="simple-linear-regression.html#cb17-13" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/slr-real-vs-est-1.png" width="672" /></p>
</div>
</div>
<div id="properties-of-the-estimates" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Properties of the Estimates<a href="simple-linear-regression.html#properties-of-the-estimates" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The estimates <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are linear combinations of
<span class="math inline">\(\mathbf{y} = (y_1,\ldots,y_n)&#39;\)</span>. To see this, notice the following:</p>
<p><span class="math display">\[\begin{align*}
\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
  &amp;= \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} \\
  &amp;= \sum_{i=1}^n x_i y_i - \bar{x} \sum_{i=1}^n y_i \\
  &amp;= \sum_{i=1}^n x_i y_i - \sum_{i=1}^n \bar{x} y_i \\
  &amp;= \sum_{i=1}^n (x_i y_i - \bar{x} y_i) \\
  &amp;= \sum_{i=1}^n (x_i - \bar{x}) y_i \\
\end{align*}\]</span></p>
<p>Then</p>
<p><span class="math display">\[ \hat{\beta}_1 =  \frac{\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{\sum_{i=1}^n (x_i - \bar{x}) y_i}{\sum_{i=1}^n (x_i - \bar{x})^2} = \sum_{i=1}^n\frac{(x_i - \bar{x}) }{\sum_{i=1}^n (x_i - \bar{x})^2}y_i \]</span></p>
<p>and similarly:</p>
<p><span class="math display">\[ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} = \sum_{i=1}^n \frac{y_i}{n} - \sum_{i=1}^n\frac{(x_i - \bar{x}) }{\sum_{j = 1}^n x_j^2 - n \bar{x}^2}y_i \bar{x} = \sum_{i=1}^n \left( \frac{1}{n} - \frac{(x_i - \bar{x}) }{\sum_{j = 1}^n x_j^2 - n \bar{x}^2} \bar{x} \right)y_i \]</span></p>
<p>Also, notice that the sum of the errors is <span class="math inline">\(0\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\sum_{i=1}^n \hat{e}_i
  &amp;= \sum_{i=1}^n(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \\
  &amp;= \sum_{i=1}^n y_i - \sum_{i=1}^n \hat{\beta}_0 - \hat{\beta}_1 \sum_{i=1}^n x_i \\
  &amp;= n\bar{y} - n \hat{\beta}_0 - n \hat{\beta}_1 \bar{x} \\
  &amp;= n\bar{y} - n (\bar{y} - \hat{\beta}_1 \bar{x}) - n \hat{\beta}_1 \bar{x} \\
  &amp;= n\bar{y} - n \bar{y} + n \hat{\beta}_1 \bar{x} - n \hat{\beta}_1 \bar{x} \\
  &amp;= 0
\end{align*}\]</span></p>
<p>If we let <span class="math inline">\(\hat{\mathbf{e}} = (\hat{e}_i,\ldots,\hat{e}_n)&#39;\)</span> and <span class="math inline">\(\mathbf{x}=(x_1,\ldots,x_n)&#39;\)</span>,
two vectors of size <span class="math inline">\(n\)</span>, then we have that <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> are orthogonal.
That is:</p>
<p><span class="math display">\[\begin{align*}
\langle \hat{\mathbf{e}}, \mathbf{x}\rangle
  &amp;= \sum_{i=1}^{n} \hat{e}_i x_i \\
  &amp;= \sum_{i=1}^{n}  (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)x_i \\
  &amp;= \sum_{i=1}^{n}  (y_i x_i - \hat{\beta}_0x_i - \hat{\beta}_1 x_i x_i) \\
  &amp;= \sum_{i=1}^{n} y_i x_i - \sum_{i=1}^{n} \hat{\beta}_0x_i - \sum_{i=1}^{n} \hat{\beta}_1 x_i x_i \\
  &amp;= \sum_{i=1}^{n} y_i - n \hat{\beta}_0 \bar{x} - \hat{\beta}_1 \sum_{i=1}^{n} x_i^2 \\
  &amp;= \sum_{i=1}^{n} y_i - n (\bar{y} - \hat{\beta}_1 \bar{x}) \bar{x} - \hat{\beta}_1 \sum_{i=1}^{n} x_i^2 \\
  &amp;= \sum_{i=1}^{n} y_i - n \bar{y} \bar{x} + n\hat{\beta}_1 \bar{x}^2 - \hat{\beta}_1 \sum_{i=1}^{n} x_i^2 \\
  &amp;= \sum_{i=1}^{n} y_i - n \bar{y} \bar{x} - \hat{\beta}_1 (\sum_{i=1}^{n} x_i^2 - n\bar{x}^2)  \\
  &amp;= \sum_{i=1}^{n} y_i - n \bar{y} \bar{x} -  \frac{\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i = 1}^n x_i^2 - n \bar{x}^2}(\sum_{i=1}^{n} x_i^2 - n\bar{x}^2)  \\
  &amp;= \sum_{i=1}^{n} y_i - n \bar{y} \bar{x} -  (\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x})  \\
  &amp;=0
\end{align*}\]</span></p>
<p>The same applies to <span class="math inline">\(\hat{\mathbf{y}}= (\hat{y}_1,\ldots,\hat{\mathbf{y}}_n)&#39;\)</span> and <span class="math inline">\(\hat{\mathbf{e}}\)</span>, as we can see:</p>
<p><span class="math display">\[\begin{align*}
\langle \hat{\mathbf{e}}, \hat{\mathbf{y}}\rangle
  &amp;= \sum_{i=1}^{n} \hat{e}_i \hat{y}_i \\
  &amp;= \sum_{i=1}^{n} \hat{e}_i(\hat{\beta}_0 + \hat{\beta}_1 x_i) \\
  &amp;= \sum_{i=1}^{n} (\hat{e}_i \hat{\beta}_0 + he_i \hat{\beta}_1 x_i) \\
  &amp;= \hat{\beta}_0 \sum_{i=1}^{n} \hat{e}_i + \hat{\beta}_1 \sum_{i=1}^{n} he_i x_i \\
  &amp;= \hat{\beta}_1 \langle \hat{\mathbf{e}}, \mathbf{x}\rangle  \\
  &amp;= 0
\end{align*}\]</span></p>
<p>Finally, the average of <span class="math inline">\(\hat{\mathbf{y}}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are the same, to see this notice:</p>
<p><span class="math display">\[\begin{align*}
\frac{1}{n} \sum_{i=1}^n \hat{y}_i
  &amp;= \frac{1}{n} \sum_{i=1}^n (\hat{\beta}_0 + \hat{\beta}_1 x_i) \\
  &amp;= \frac{1}{n} (n \hat{\beta}_0 + \hat{\beta}_1 \sum_{i=1}^n x_i) \\
  &amp;= \frac{1}{n} (n \hat{\beta}_0 + n \hat{\beta}_1 \mathbf{x}) \\
  &amp;= \hat{\beta}_0 + \hat{\beta}_1 \mathbf{x}\\
  &amp;= \bar{y} - \hat{\beta}_1 \mathbf{x}+ \hat{\beta}_1 \mathbf{x}\\
  &amp;= \bar{y} \\
\end{align*}\]</span></p>
</div>
<div id="centering-and-standarizing-the-data" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Centering and Standarizing the Data<a href="simple-linear-regression.html#centering-and-standarizing-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Some transformations of the data can help the regression analysis
or make it more intuitive. There are 2 main transformations of the data:
<strong>centering</strong> and <strong>standardization</strong>.</p>
<p>Consider observations <span class="math inline">\(x_1,\ldots,x_n\)</span>, then the centered version of observation
<span class="math inline">\(i\)</span> is given by:</p>
<p><span class="math display">\[x_i&#39; = x_i - \bar{x}\]</span></p>
<p>The new observations <span class="math inline">\(x_1&#39;,\ldots,x_n&#39;\)</span> are centered and their mean is <span class="math inline">\(0\)</span>.</p>
<p><span class="math display">\[\bar{x}&#39; = \frac{1}{n} \sum_{i=1}^n x_i&#39; = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x}) = \frac{1}{n} \left(\sum_{i=1}^n x_i - \sum_{i=1}^n \bar{x} \right) = \frac{1}{n} \left(n\bar{x} - n \bar{x} \right) = 0\]</span>
Also, let us see that the variance of the standardized variables is the same as
the variance of the original observations.</p>
<p><span class="math display">\[ S_{xx}&#39; = \frac{1}{n-1} \sum_{i=1}^n (x_i&#39; - \bar{x}&#39;) = \frac{1}{n-1} \sum_{i=1}^n (x_i&#39;) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x}) = S_{xx} \]</span>
So the variance of the observations is not affected by the centering.</p>
<p>If we center another set of observations <span class="math inline">\(y_1,\ldots,y_n\)</span>, and compute the covariance,
we have that it also doesn’t change.</p>
<p><span class="math display">\[S_{xy}&#39; = \frac{1}{n-1} \sum_{i=1}^n (x_i&#39; - \bar{x})(y_i&#39; - \bar{x}) = \frac{1}{n-1} \sum_{i=1}^n (x_i&#39;)(y_i&#39;) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{x}) = S_{xy}\]</span></p>
<p>The standardized version of observation <span class="math inline">\(i\)</span> is given by:</p>
<p><span class="math display">\[x_i&#39;&#39; = \frac{x_i - \bar{x}}{\sqrt{S_{xx}}} = \frac{x_i&#39;}{\sqrt{S_{xx}}}\]</span></p>
<p>The standardized observations have mean of 0 and variance 1. Let’s see first that
the sample mean is</p>
<p><span class="math display">\[ \bar{x}_i&#39;&#39; =  \frac{1}{n} \sum_{i=1}^n x_i&#39;&#39; =  \frac{1}{n} \sum_{i=1}^n \frac{x_i&#39;}{\sqrt{S_{xx}}} = \frac{1}{\sqrt{S_{xx}}}\frac{1}{n} \sum_{i=1}^n x_i&#39; = \frac{1}{\sqrt{S_{xx}}} \bar{x}&#39; = 0\]</span></p>
<p>Now let us see that the variance of the standardized observations is 1.</p>
<p><span class="math display">\[\begin{align*}
S_{xx}&#39;&#39;
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i&#39;&#39; - \bar{x}&#39;&#39;)^2 \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i&#39;&#39;)^2 \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n \left(\frac{x_i - \bar{x}}{\sqrt{S_{xx}}}\right)^2 \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{S_{xx}} \\
  &amp;= \frac{1}{S_{xx}} \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \\
  &amp;= \frac{1}{S_{xx}}S_{xx} \\
  &amp;= 1
\end{align*}\]</span></p>
<p>Now, let us introduce the sample correlation as:</p>
<p><span class="math display">\[ r_{xy} = \frac{S_{xy}}{\sqrt{S_{xx}{S_{yy}}}} \]</span>
If we standardize two sets of observations, then the covaraince of the standardized version
is the correlation of the standardized version. Let us see it:</p>
<p><span class="math display">\[\begin{align*}
S_{xy}&#39;&#39;
  &amp;= \frac{1}{n-1} \sum_{i=1} (x_i&#39;&#39; - \bar{x}&#39;&#39;)(y_i&#39;&#39; - \bar{x}&#39;&#39;) \\
  &amp;= \frac{1}{n-1} \sum_{i=1} (x_i&#39;&#39;)(y_i&#39;&#39;) \\
  &amp;= \frac{1}{n-1} \sum_{i=1} \left(\frac{x_i - \bar{x}}{\sqrt{S_{xx}}}\right) \left(\frac{y_i - \bar{y}}{\sqrt{S_{yy}}}\right) \\
  &amp;= \frac{1}{\sqrt{S_{yy}}\sqrt{S_{xx}}}\frac{1}{n-1} \sum_{i=1} (x_i - \bar{x}) (y_i - \bar{y}) \\
  &amp;= \frac{1}{\sqrt{S_{yy}}\sqrt{S_{xx}}} S_{xy} \\
  &amp;= r_{xy}
\end{align*}\]</span></p>
<p>With this results, we can analyze the effects of following 3 scenarios on the
estimated coefficients:</p>
<ul>
<li>Independent variable centered.</li>
<li>Both, Independent and dependent variable centered.</li>
<li>Both, Independent and dependent variable standardized.</li>
</ul>
<div id="independent-variable-centered" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Independent variable centered<a href="simple-linear-regression.html#independent-variable-centered" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Lets compute the value for <span class="math inline">\(\beta_1\)</span> when the data is centered.</p>
<p><span class="math display">\[\hat{\beta}_1&#39; = \frac{S_{xy}&#39;}{S_{xx}&#39;} = \frac{S_{xy}}{S_{xx}} = \hat{\beta}_1 \]</span>
So centering the data doesn’t change the value of the estimated slope.</p>
<p><span class="math display">\[ \hat{\beta}_0&#39; = \bar{y} - \hat{\beta}_1&#39; \bar{x}&#39; = \bar{y} - \hat{\beta}_1&#39; 0 = \bar{y} \]</span>
So centering the data, makes the estimated intercept to coincide with the mean of
the independent variable.</p>
<p>We can see this in one of our example data sets, looking at the ad spending data
we can perform linear regression on the original data and the centered data:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="simple-linear-regression.html#cb18-1" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb18-2"><a href="simple-linear-regression.html#cb18-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Ad spending Data.csv&quot;</span>)</span>
<span id="cb18-3"><a href="simple-linear-regression.html#cb18-3" tabindex="-1"></a><span class="co"># Assign data</span></span>
<span id="cb18-4"><a href="simple-linear-regression.html#cb18-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Ad.Spending</span>
<span id="cb18-5"><a href="simple-linear-regression.html#cb18-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Revenue</span>
<span id="cb18-6"><a href="simple-linear-regression.html#cb18-6" tabindex="-1"></a><span class="co"># Centers x</span></span>
<span id="cb18-7"><a href="simple-linear-regression.html#cb18-7" tabindex="-1"></a>xCen <span class="ot">&lt;-</span> x <span class="sc">-</span> <span class="fu">mean</span>(x)</span>
<span id="cb18-8"><a href="simple-linear-regression.html#cb18-8" tabindex="-1"></a><span class="co"># Linear regression on the original data</span></span>
<span id="cb18-9"><a href="simple-linear-regression.html#cb18-9" tabindex="-1"></a>outRegOri <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb18-10"><a href="simple-linear-regression.html#cb18-10" tabindex="-1"></a><span class="co"># Linear regression on the centered independent variable data</span></span>
<span id="cb18-11"><a href="simple-linear-regression.html#cb18-11" tabindex="-1"></a>outRegCen <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> xCen)</span>
<span id="cb18-12"><a href="simple-linear-regression.html#cb18-12" tabindex="-1"></a><span class="co"># Plots</span></span>
<span id="cb18-13"><a href="simple-linear-regression.html#cb18-13" tabindex="-1"></a><span class="do">## Two plots in the same image</span></span>
<span id="cb18-14"><a href="simple-linear-regression.html#cb18-14" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb18-15"><a href="simple-linear-regression.html#cb18-15" tabindex="-1"></a><span class="do">## Original data</span></span>
<span id="cb18-16"><a href="simple-linear-regression.html#cb18-16" tabindex="-1"></a><span class="co"># Plots the points</span></span>
<span id="cb18-17"><a href="simple-linear-regression.html#cb18-17" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb18-18"><a href="simple-linear-regression.html#cb18-18" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb18-19"><a href="simple-linear-regression.html#cb18-19" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb18-20"><a href="simple-linear-regression.html#cb18-20" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>)</span>
<span id="cb18-21"><a href="simple-linear-regression.html#cb18-21" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb18-22"><a href="simple-linear-regression.html#cb18-22" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb18-23"><a href="simple-linear-regression.html#cb18-23" tabindex="-1"></a>       <span class="at">b   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb18-24"><a href="simple-linear-regression.html#cb18-24" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb18-25"><a href="simple-linear-regression.html#cb18-25" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb18-26"><a href="simple-linear-regression.html#cb18-26" tabindex="-1"></a><span class="do">## Independent Variable centered data</span></span>
<span id="cb18-27"><a href="simple-linear-regression.html#cb18-27" tabindex="-1"></a><span class="co"># Plots the points</span></span>
<span id="cb18-28"><a href="simple-linear-regression.html#cb18-28" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> xCen,</span>
<span id="cb18-29"><a href="simple-linear-regression.html#cb18-29" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb18-30"><a href="simple-linear-regression.html#cb18-30" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending (Centered)&quot;</span>,</span>
<span id="cb18-31"><a href="simple-linear-regression.html#cb18-31" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>)</span>
<span id="cb18-32"><a href="simple-linear-regression.html#cb18-32" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb18-33"><a href="simple-linear-regression.html#cb18-33" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegCen<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb18-34"><a href="simple-linear-regression.html#cb18-34" tabindex="-1"></a>       <span class="at">b   =</span> outRegCen<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb18-35"><a href="simple-linear-regression.html#cb18-35" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb18-36"><a href="simple-linear-regression.html#cb18-36" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb18-37"><a href="simple-linear-regression.html#cb18-37" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v   =</span> <span class="dv">0</span>,</span>
<span id="cb18-38"><a href="simple-linear-regression.html#cb18-38" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/center-x-1.png" width="672" /></p>
<p>So we can appreciate that centering the independent variable just shifts the data
horizontally so the mean will be at zero.</p>
</div>
<div id="both-variables-centered" class="section level3 hasAnchor" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Both Variables centered<a href="simple-linear-regression.html#both-variables-centered" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now lets see the effects when both variables are centered. Here, we will denote
the estimates again with one prime, that is <span class="math inline">\(\hat{\beta}&#39;\)</span>.</p>
<p>Again, the estimate of the slope doesn’t change:</p>
<p><span class="math display">\[\hat{\beta}_1&#39; = \frac{S_{xy}&#39;}{S_{xx}&#39;} = \frac{S_{xy}}{S_{xx}} = \hat{\beta}_1 \]</span>
while the estimate of the intercept becomes zero (the new mean of the centered
dependent variable)</p>
<p><span class="math display">\[ \hat{\beta}_0&#39; = \bar{y}&#39; - \hat{\beta}_1&#39; \bar{x}&#39; = \bar{y}&#39; = 0 \]</span>
The effect of this transformation can be observed, here:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="simple-linear-regression.html#cb19-1" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb19-2"><a href="simple-linear-regression.html#cb19-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Ad spending Data.csv&quot;</span>)</span>
<span id="cb19-3"><a href="simple-linear-regression.html#cb19-3" tabindex="-1"></a><span class="co"># Assign data</span></span>
<span id="cb19-4"><a href="simple-linear-regression.html#cb19-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Ad.Spending</span>
<span id="cb19-5"><a href="simple-linear-regression.html#cb19-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Revenue</span>
<span id="cb19-6"><a href="simple-linear-regression.html#cb19-6" tabindex="-1"></a><span class="co"># Centers x and y</span></span>
<span id="cb19-7"><a href="simple-linear-regression.html#cb19-7" tabindex="-1"></a>xCen <span class="ot">&lt;-</span> x <span class="sc">-</span> <span class="fu">mean</span>(x)</span>
<span id="cb19-8"><a href="simple-linear-regression.html#cb19-8" tabindex="-1"></a>yCen <span class="ot">&lt;-</span> y <span class="sc">-</span> <span class="fu">mean</span>(y)</span>
<span id="cb19-9"><a href="simple-linear-regression.html#cb19-9" tabindex="-1"></a><span class="co"># Linear regression on the original data</span></span>
<span id="cb19-10"><a href="simple-linear-regression.html#cb19-10" tabindex="-1"></a>outRegOri <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb19-11"><a href="simple-linear-regression.html#cb19-11" tabindex="-1"></a><span class="co"># Linear regression on the centered data</span></span>
<span id="cb19-12"><a href="simple-linear-regression.html#cb19-12" tabindex="-1"></a>outRegCen <span class="ot">&lt;-</span> <span class="fu">lm</span>(yCen <span class="sc">~</span> xCen)</span>
<span id="cb19-13"><a href="simple-linear-regression.html#cb19-13" tabindex="-1"></a><span class="co"># Plots</span></span>
<span id="cb19-14"><a href="simple-linear-regression.html#cb19-14" tabindex="-1"></a><span class="do">## Two plots in the same image</span></span>
<span id="cb19-15"><a href="simple-linear-regression.html#cb19-15" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb19-16"><a href="simple-linear-regression.html#cb19-16" tabindex="-1"></a><span class="do">## Original data</span></span>
<span id="cb19-17"><a href="simple-linear-regression.html#cb19-17" tabindex="-1"></a><span class="co"># Plots the points</span></span>
<span id="cb19-18"><a href="simple-linear-regression.html#cb19-18" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb19-19"><a href="simple-linear-regression.html#cb19-19" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb19-20"><a href="simple-linear-regression.html#cb19-20" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb19-21"><a href="simple-linear-regression.html#cb19-21" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>)</span>
<span id="cb19-22"><a href="simple-linear-regression.html#cb19-22" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb19-23"><a href="simple-linear-regression.html#cb19-23" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb19-24"><a href="simple-linear-regression.html#cb19-24" tabindex="-1"></a>       <span class="at">b   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb19-25"><a href="simple-linear-regression.html#cb19-25" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb19-26"><a href="simple-linear-regression.html#cb19-26" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb19-27"><a href="simple-linear-regression.html#cb19-27" tabindex="-1"></a><span class="do">## Centered data</span></span>
<span id="cb19-28"><a href="simple-linear-regression.html#cb19-28" tabindex="-1"></a><span class="co"># Plots the points</span></span>
<span id="cb19-29"><a href="simple-linear-regression.html#cb19-29" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> xCen,</span>
<span id="cb19-30"><a href="simple-linear-regression.html#cb19-30" tabindex="-1"></a>     <span class="at">y    =</span> yCen,</span>
<span id="cb19-31"><a href="simple-linear-regression.html#cb19-31" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending (Centered)&quot;</span>,</span>
<span id="cb19-32"><a href="simple-linear-regression.html#cb19-32" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>)</span>
<span id="cb19-33"><a href="simple-linear-regression.html#cb19-33" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb19-34"><a href="simple-linear-regression.html#cb19-34" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegCen<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb19-35"><a href="simple-linear-regression.html#cb19-35" tabindex="-1"></a>       <span class="at">b   =</span> outRegCen<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb19-36"><a href="simple-linear-regression.html#cb19-36" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb19-37"><a href="simple-linear-regression.html#cb19-37" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb19-38"><a href="simple-linear-regression.html#cb19-38" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v   =</span> <span class="dv">0</span>,</span>
<span id="cb19-39"><a href="simple-linear-regression.html#cb19-39" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb19-40"><a href="simple-linear-regression.html#cb19-40" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb19-41"><a href="simple-linear-regression.html#cb19-41" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/center-xy-1.png" width="672" /></p>
</div>
<div id="independent-and-dependent-variable-standardized" class="section level3 hasAnchor" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> Independent and dependent variable standardized<a href="simple-linear-regression.html#independent-and-dependent-variable-standardized" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Again we start we the slope estimate:</p>
<p><span class="math display">\[\hat{\beta}_1&#39;&#39; = \frac{S_{xy}&#39;&#39;}{S_{xx}&#39;&#39;} = \frac{r_{xy}}{1} =r_{xy} \]</span>
so, the estimate of the slope is the sample correlation of the original observations.</p>
<p>Again, we can see this graphically:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="simple-linear-regression.html#cb20-1" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb20-2"><a href="simple-linear-regression.html#cb20-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Ad spending Data.csv&quot;</span>)</span>
<span id="cb20-3"><a href="simple-linear-regression.html#cb20-3" tabindex="-1"></a><span class="co"># Assign data</span></span>
<span id="cb20-4"><a href="simple-linear-regression.html#cb20-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Ad.Spending</span>
<span id="cb20-5"><a href="simple-linear-regression.html#cb20-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Revenue</span>
<span id="cb20-6"><a href="simple-linear-regression.html#cb20-6" tabindex="-1"></a><span class="co"># Standardizes x and y</span></span>
<span id="cb20-7"><a href="simple-linear-regression.html#cb20-7" tabindex="-1"></a>xSta <span class="ot">&lt;-</span> (x <span class="sc">-</span> <span class="fu">mean</span>(x))<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">var</span>(x))</span>
<span id="cb20-8"><a href="simple-linear-regression.html#cb20-8" tabindex="-1"></a>ySta <span class="ot">&lt;-</span> (y <span class="sc">-</span> <span class="fu">mean</span>(y))<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">var</span>(y))</span>
<span id="cb20-9"><a href="simple-linear-regression.html#cb20-9" tabindex="-1"></a><span class="co"># Linear regression on the original data</span></span>
<span id="cb20-10"><a href="simple-linear-regression.html#cb20-10" tabindex="-1"></a>outRegOri <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb20-11"><a href="simple-linear-regression.html#cb20-11" tabindex="-1"></a><span class="co"># Linear regression on the standard data</span></span>
<span id="cb20-12"><a href="simple-linear-regression.html#cb20-12" tabindex="-1"></a>outRegSta <span class="ot">&lt;-</span> <span class="fu">lm</span>(ySta <span class="sc">~</span> xSta)</span>
<span id="cb20-13"><a href="simple-linear-regression.html#cb20-13" tabindex="-1"></a><span class="co"># Plots</span></span>
<span id="cb20-14"><a href="simple-linear-regression.html#cb20-14" tabindex="-1"></a><span class="do">## Two plots in the same image</span></span>
<span id="cb20-15"><a href="simple-linear-regression.html#cb20-15" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb20-16"><a href="simple-linear-regression.html#cb20-16" tabindex="-1"></a><span class="do">## Original data</span></span>
<span id="cb20-17"><a href="simple-linear-regression.html#cb20-17" tabindex="-1"></a><span class="co"># Plots the points</span></span>
<span id="cb20-18"><a href="simple-linear-regression.html#cb20-18" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb20-19"><a href="simple-linear-regression.html#cb20-19" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb20-20"><a href="simple-linear-regression.html#cb20-20" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb20-21"><a href="simple-linear-regression.html#cb20-21" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>)</span>
<span id="cb20-22"><a href="simple-linear-regression.html#cb20-22" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb20-23"><a href="simple-linear-regression.html#cb20-23" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb20-24"><a href="simple-linear-regression.html#cb20-24" tabindex="-1"></a>       <span class="at">b   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb20-25"><a href="simple-linear-regression.html#cb20-25" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb20-26"><a href="simple-linear-regression.html#cb20-26" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb20-27"><a href="simple-linear-regression.html#cb20-27" tabindex="-1"></a><span class="do">## Standard data</span></span>
<span id="cb20-28"><a href="simple-linear-regression.html#cb20-28" tabindex="-1"></a><span class="co"># Plots the points</span></span>
<span id="cb20-29"><a href="simple-linear-regression.html#cb20-29" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> xSta,</span>
<span id="cb20-30"><a href="simple-linear-regression.html#cb20-30" tabindex="-1"></a>     <span class="at">y    =</span> ySta,</span>
<span id="cb20-31"><a href="simple-linear-regression.html#cb20-31" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending (Centered)&quot;</span>,</span>
<span id="cb20-32"><a href="simple-linear-regression.html#cb20-32" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>)</span>
<span id="cb20-33"><a href="simple-linear-regression.html#cb20-33" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb20-34"><a href="simple-linear-regression.html#cb20-34" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegSta<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb20-35"><a href="simple-linear-regression.html#cb20-35" tabindex="-1"></a>       <span class="at">b   =</span> outRegSta<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb20-36"><a href="simple-linear-regression.html#cb20-36" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb20-37"><a href="simple-linear-regression.html#cb20-37" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb20-38"><a href="simple-linear-regression.html#cb20-38" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v   =</span> <span class="dv">0</span>,</span>
<span id="cb20-39"><a href="simple-linear-regression.html#cb20-39" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb20-40"><a href="simple-linear-regression.html#cb20-40" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb20-41"><a href="simple-linear-regression.html#cb20-41" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/slr-standarize-xy-1.png" width="672" /></p>
<p>Now, let us see that the correlation is always in the interval <span class="math inline">\((-1,1)\)</span>. To see
this, notice the following:</p>
<p><span class="math display">\[\begin{align*}
\frac{1}{n-1} \sum_{i=1}^n (x_i&#39;&#39; + y_i&#39;&#39;)^2
  &amp;= \frac{1}{n-1} \sum_{i=1}^n \left((x_i&#39;&#39;)^2 + 2x_i&#39;&#39; y_i&#39;&#39; + (y_i&#39;&#39;)^2 \right) \\
  &amp;= \frac{\sum_{i=1}^n (x_i&#39;&#39;)^2}{n-1}  + 2\frac{\sum_{i=1}^n x_i&#39;&#39;y_i&#39;&#39;}{n-1}  + \frac{\sum_{i=1}^n (y_i&#39;&#39;)^2}{n-1} \\
  &amp;= \frac{\sum_{i=1}^n (x_i&#39;&#39; - \bar{x}&#39;&#39;)^2}{n-1}  + 2\frac{\sum_{i=1}^n (x_i&#39;&#39; - \bar{x}&#39;&#39;)(y_i&#39;&#39; - \bar{y}&#39;&#39;)}{n-1}  + \frac{\sum_{i=1}^n (y_i&#39;&#39; - \bar{y}&#39;&#39;)^2}{n-1} \\
  &amp;= S_{xx}&#39;&#39; + 2 S_{xy}&#39;&#39; + S_{yy}&#39;&#39; \\
  &amp;= 1 + 2r_{xy} + 1 \\
  &amp;= 2(1 + r_{xy})
\end{align*}\]</span></p>
<p>In a similar way it can be shown that:</p>
<p><span class="math display">\[\frac{1}{n-1} \sum_{i=1}^n (x_i&#39;&#39; + y_i&#39;&#39;)^2 = 2(1 - r_{xy})\]</span></p>
<p>Now since, <span class="math display">\[\frac{1}{n-1} \sum_{i=1}^n (x_i&#39;&#39; + y_i&#39;&#39;)^2 \geq 0\]</span>
then we have that</p>
<p><span class="math display">\[\begin{align*}
\frac{1}{n-1} \sum_{i=1}^n (x_i&#39;&#39; + y_i&#39;&#39;)^2 \geq 0
  &amp;\implies 2(1 + r_{xy}) \geq 0 \\
  &amp;\implies 1 + r_{xy} \geq 0 \\
  &amp;\implies r_{xy} \geq -1 \\
\end{align*}\]</span></p>
<p>Similarly, since</p>
<p><span class="math display">\[\frac{1}{n-1} \sum_{i=1}^n (x_i&#39;&#39; - y_i&#39;&#39;)^2 \geq 0\]</span></p>
<p>implies</p>
<p><span class="math display">\[r_{xy} \leq 1 \]</span>
then, we have that:
<span class="math display">\[ -1 \leq r_{xy} \leq 1\]</span>
which implies that the slope of the regression analysis after standarizing both
variables is going to be in the interval <span class="math inline">\((-1, 1)\)</span>.</p>
</div>
</div>
<div id="coefficient-of-determination" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Coefficient of Determination<a href="simple-linear-regression.html#coefficient-of-determination" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have been concerned on findin the “best” line, that is estimating the
coefficients that minimize the sum of squared errors. We have find some derivated
estimated values and some properties of these values. However, we hanven’t analized
how good is our estimation. To see how well we are doing we will look at the
coeficient of determination. To do so we will first introduce some quantities:</p>
<p><strong>Total Sum of Squares</strong></p>
<p><span class="math display">\[SS_{tot} = \sum_{i=1}^n (y_i - \bar{y})\]</span></p>
<p>Is a measure of total variability of the dependent
variable. You can think of it in many ways:</p>
<ul>
<li>As a proxy for uncertainty. The bigger the more uncertainty in the dependent
varaible.</li>
<li>It is proportional to the sample varaince.</li>
<li>It is what you get if when you solve the following minimization problem:</li>
</ul>
<p><span class="math display">\[ \min_{a} \sum_{i=1}^n (y_i - \beta_0)^2 \]</span>
that is, the best you can do to minimize the sum of squares without having access
to the independent variables <span class="math inline">\(x_1,\ldots,x_n\)</span>.</p>
<p><strong>Residual Sum of Squares</strong></p>
<p><span class="math display">\[ SS_{res} = \sum_{i=1}^n(\hat{e}_i)^2 = \sum_{i=1}^n(y_i - \hat{y}_i)^2\]</span></p>
<p>The residual Sum of Squares is the minimum value of our optimization problem. It
is the the amount of variability that no matter what we do we will have remaing
even after finding the “best” line.</p>
<p><strong>Explained Sum of Squares</strong></p>
<p><span class="math display">\[ SS_{reg} = \sum_{i=1}^n(\hat{y}_i - \bar{y}) = \sum_{i=1}^n(\hat{y}_i - \hat{\bar{y}}) \]</span></p>
<p>The variability of the fitted value. This is the variability we can explain with
our regression model.</p>
<p>These 3 quantities are related by:</p>
<p><span class="math display">\[ SS_{tot} = SS_{reg} + SS_{res} \]</span>
That is the total variability si the sum of the variability that is explained by
the regression model and the variability we can’t explain with the regression model.</p>
<p><span class="math display">\[\begin{align*}
SS_{tot}
  &amp;= \sum_{i=1}^n(y_i - \bar{y})^2 = \sum_{i=1}^n(y_i - \hat{y}_i + \hat{y}_i - \bar{y})^2 \\
  &amp;= \sum_{i=1}^n\left((y_i - \hat{y}_i)^2 + 2(y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) + (\hat{y}_i - \bar{y})^2\right) \\
  &amp;= \sum_{i=1}^n(y_i - \hat{y}_i)^2 + 2 \sum_{i=1}^n(y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) + \sum_{i=1}^n(\hat{y}_i - \bar{y})^2 \\
  &amp;= SS_{res} + 2 \sum_{i=1}^n(y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) + SS_{reg} \\
  &amp;= SS_{res} + 2 \sum_{i=1}^n\hat{e}_i(\hat{y}_i - \bar{y}) + SS_{reg} \\
  &amp;= SS_{res} + 2 \sum_{i=1}^n(\hat{e}_i\hat{y}_i - \hat{e}_i\bar{y}) + SS_{reg} \\
  &amp;= SS_{res} + 2 \sum_{i=1}^n\hat{e}_i\hat{y}_i - 2 \sum_{i=1}^n\hat{e}_i\bar{y} + SS_{reg} \\
  &amp;= SS_{res} + 2(0) - 2  \bar{y} \sum_{i=1}^n\hat{e}_i + SS_{reg} \\
  &amp;= SS_{res} - 2 \bar{y} (0) + SS_{reg} \\
  &amp;= SS_{res} + SS_{reg} \\
\end{align*}\]</span></p>
<p><strong>Coefficient of Determination</strong></p>
<p><span class="math display">\[ R^2 = \frac{SS_{reg}}{SS_{tot}} = 1 - \frac{SS_{res}}{SS_{tot}} \]</span>
The coefficient of determination then can be explained as the percentage of the
total varaibility that can be explained with linear regression.</p>
<p>As a percentage, it has to be a number between 0 and 1. To see this let us show
that:</p>
<p><span class="math display">\[ R^2 = r_{xy}^2 \]</span>
To see this, first let us express <span class="math inline">\(SS_{reg}\)</span> in a more convinient way:</p>
<p><span class="math display">\[\begin{align*}
SS_{reg}
  &amp;= \sum_{i=1}^n(\hat{y}_i - \bar{y})^2 \\
  &amp;= \sum_{i=1}^n(\hat{\beta}_0 + \hat{\beta}_1 x_i - \bar{y})^2 \\
  &amp;= \sum_{i=1}^n(\bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 x_i - \bar{y})^2 \\
  &amp;= \sum_{i=1}^n(\hat{\beta}_1 x_i - \hat{\beta}_1 \bar{x})^2 \\
  &amp;= \sum_{i=1}^n\hat{\beta}_1^2(x_i - \bar{x})^2 \\
  &amp;= \hat{\beta}_1^2 \sum_{i=1}^n(x_i - \bar{x})^2 \\
  &amp;= \hat{\beta}_1^2 S_{xx} (n-1) \\
  &amp;= \left( \frac{S_{xy}}{S_{xx}} \right)^2 S_{xx} (n-1) \\
  &amp;= \frac{S_{xy}^2}{S_{xx}^2} S_{xx} (n-1) \\
  &amp;= \frac{S_{xy}^2}{S_{xx}} (n-1) \\
\end{align*}\]</span></p>
<p>Then, we can see that:</p>
<p><span class="math display">\[\begin{align*}
R^2
  &amp;= \frac{SS_{reg}}{SS_{tot}} \\
  &amp;= \frac{\frac{S_{xy}^2}{S_{xx}} (n-1)}{\sum_{i=1}^n(y_i - \bar{y})^2} \\
  &amp;= \frac{\frac{S_{xy}^2}{S_{xx}} (n-1)}{S_{yy}(n-1)} \\
  &amp;= \frac{S_{xy}^2}{S_{xx}S_{yy}} \\
  &amp;= \left( \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}} \right)^2 \\
  &amp;= r_{xy}^2
\end{align*}\]</span></p>
<p>The bigger the <span class="math inline">\(R^2\)</span>, the better is the fit of our linear model. The <span class="math inline">\(R^2\)</span> can
be low for 2 reasons:</p>
<ul>
<li>The first one is if our data is not linear, then a linear
model will explain little about the relationship (some times a linear model can
be a good approximation of a non-linear model).</li>
<li>The second reason, is when the
data is noisy. This can reduce the <span class="math inline">\(R^2\)</span> even when we know the relationship
between the variables is linear.</li>
</ul>
<p>As an example of noisy data, recall the Ad spending data. I actually generated
the data under a linear model, so the relationship between the variables is linear.
You can verify this be looking at the code where the data is generated in the introduction.
Next I show the effects of adding additional noise to the data, at 3 levels:</p>
<ul>
<li>Level1: Small Noise.</li>
<li>Level2: Medium Noise.</li>
<li>Level3: High noise.</li>
</ul>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="simple-linear-regression.html#cb21-1" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb21-2"><a href="simple-linear-regression.html#cb21-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Ad spending Data.csv&quot;</span>)</span>
<span id="cb21-3"><a href="simple-linear-regression.html#cb21-3" tabindex="-1"></a><span class="co"># Assign data</span></span>
<span id="cb21-4"><a href="simple-linear-regression.html#cb21-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Ad.Spending</span>
<span id="cb21-5"><a href="simple-linear-regression.html#cb21-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Revenue</span>
<span id="cb21-6"><a href="simple-linear-regression.html#cb21-6" tabindex="-1"></a></span>
<span id="cb21-7"><a href="simple-linear-regression.html#cb21-7" tabindex="-1"></a><span class="co"># Adds Noise</span></span>
<span id="cb21-8"><a href="simple-linear-regression.html#cb21-8" tabindex="-1"></a>yNoiLe1 <span class="ot">&lt;-</span> y <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">sd =</span>  <span class="dv">50</span>)</span>
<span id="cb21-9"><a href="simple-linear-regression.html#cb21-9" tabindex="-1"></a>yNoiLe2 <span class="ot">&lt;-</span> y <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">200</span>)</span>
<span id="cb21-10"><a href="simple-linear-regression.html#cb21-10" tabindex="-1"></a>yNoiLe3 <span class="ot">&lt;-</span> y <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">500</span>)</span>
<span id="cb21-11"><a href="simple-linear-regression.html#cb21-11" tabindex="-1"></a></span>
<span id="cb21-12"><a href="simple-linear-regression.html#cb21-12" tabindex="-1"></a><span class="co"># Auxiliary Variables</span></span>
<span id="cb21-13"><a href="simple-linear-regression.html#cb21-13" tabindex="-1"></a>ymax <span class="ot">&lt;-</span> <span class="fu">max</span>(y, yNoiLe1, yNoiLe2, yNoiLe3)</span>
<span id="cb21-14"><a href="simple-linear-regression.html#cb21-14" tabindex="-1"></a>ymin <span class="ot">&lt;-</span> <span class="fu">min</span>(y, yNoiLe1, yNoiLe2, yNoiLe3)</span>
<span id="cb21-15"><a href="simple-linear-regression.html#cb21-15" tabindex="-1"></a>xmax <span class="ot">=</span> <span class="fu">max</span>(x)</span>
<span id="cb21-16"><a href="simple-linear-regression.html#cb21-16" tabindex="-1"></a>xmin <span class="ot">=</span> <span class="fu">min</span>(x)</span>
<span id="cb21-17"><a href="simple-linear-regression.html#cb21-17" tabindex="-1"></a></span>
<span id="cb21-18"><a href="simple-linear-regression.html#cb21-18" tabindex="-1"></a><span class="co"># Performs Linear Regression</span></span>
<span id="cb21-19"><a href="simple-linear-regression.html#cb21-19" tabindex="-1"></a>outRegOri    <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb21-20"><a href="simple-linear-regression.html#cb21-20" tabindex="-1"></a>outRegNoiLe1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(yNoiLe1 <span class="sc">~</span> x)</span>
<span id="cb21-21"><a href="simple-linear-regression.html#cb21-21" tabindex="-1"></a>outRegNoiLe2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(yNoiLe2 <span class="sc">~</span> x)</span>
<span id="cb21-22"><a href="simple-linear-regression.html#cb21-22" tabindex="-1"></a>outRegNoiLe3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(yNoiLe3 <span class="sc">~</span> x)</span>
<span id="cb21-23"><a href="simple-linear-regression.html#cb21-23" tabindex="-1"></a></span>
<span id="cb21-24"><a href="simple-linear-regression.html#cb21-24" tabindex="-1"></a><span class="co"># Plots</span></span>
<span id="cb21-25"><a href="simple-linear-regression.html#cb21-25" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb21-26"><a href="simple-linear-regression.html#cb21-26" tabindex="-1"></a></span>
<span id="cb21-27"><a href="simple-linear-regression.html#cb21-27" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb21-28"><a href="simple-linear-regression.html#cb21-28" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb21-29"><a href="simple-linear-regression.html#cb21-29" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb21-30"><a href="simple-linear-regression.html#cb21-30" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb21-31"><a href="simple-linear-regression.html#cb21-31" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Original Data&quot;</span>,</span>
<span id="cb21-32"><a href="simple-linear-regression.html#cb21-32" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb21-33"><a href="simple-linear-regression.html#cb21-33" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax))</span>
<span id="cb21-34"><a href="simple-linear-regression.html#cb21-34" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb21-35"><a href="simple-linear-regression.html#cb21-35" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb21-36"><a href="simple-linear-regression.html#cb21-36" tabindex="-1"></a>       <span class="at">b   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb21-37"><a href="simple-linear-regression.html#cb21-37" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb21-38"><a href="simple-linear-regression.html#cb21-38" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb21-39"><a href="simple-linear-regression.html#cb21-39" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb21-40"><a href="simple-linear-regression.html#cb21-40" tabindex="-1"></a>     <span class="at">y    =</span> yNoiLe1,</span>
<span id="cb21-41"><a href="simple-linear-regression.html#cb21-41" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb21-42"><a href="simple-linear-regression.html#cb21-42" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb21-43"><a href="simple-linear-regression.html#cb21-43" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Small Noise Added&quot;</span>,</span>
<span id="cb21-44"><a href="simple-linear-regression.html#cb21-44" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb21-45"><a href="simple-linear-regression.html#cb21-45" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax))</span>
<span id="cb21-46"><a href="simple-linear-regression.html#cb21-46" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb21-47"><a href="simple-linear-regression.html#cb21-47" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegNoiLe1<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb21-48"><a href="simple-linear-regression.html#cb21-48" tabindex="-1"></a>       <span class="at">b   =</span> outRegNoiLe1<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb21-49"><a href="simple-linear-regression.html#cb21-49" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb21-50"><a href="simple-linear-regression.html#cb21-50" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb21-51"><a href="simple-linear-regression.html#cb21-51" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb21-52"><a href="simple-linear-regression.html#cb21-52" tabindex="-1"></a>     <span class="at">y    =</span> yNoiLe2,</span>
<span id="cb21-53"><a href="simple-linear-regression.html#cb21-53" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb21-54"><a href="simple-linear-regression.html#cb21-54" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb21-55"><a href="simple-linear-regression.html#cb21-55" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Medium Noise Added&quot;</span>,</span>
<span id="cb21-56"><a href="simple-linear-regression.html#cb21-56" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb21-57"><a href="simple-linear-regression.html#cb21-57" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax))</span>
<span id="cb21-58"><a href="simple-linear-regression.html#cb21-58" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb21-59"><a href="simple-linear-regression.html#cb21-59" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegNoiLe2<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb21-60"><a href="simple-linear-regression.html#cb21-60" tabindex="-1"></a>       <span class="at">b   =</span> outRegNoiLe2<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb21-61"><a href="simple-linear-regression.html#cb21-61" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb21-62"><a href="simple-linear-regression.html#cb21-62" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb21-63"><a href="simple-linear-regression.html#cb21-63" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb21-64"><a href="simple-linear-regression.html#cb21-64" tabindex="-1"></a>     <span class="at">y    =</span> yNoiLe3,</span>
<span id="cb21-65"><a href="simple-linear-regression.html#cb21-65" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb21-66"><a href="simple-linear-regression.html#cb21-66" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb21-67"><a href="simple-linear-regression.html#cb21-67" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;High Noise Added&quot;</span>,</span>
<span id="cb21-68"><a href="simple-linear-regression.html#cb21-68" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb21-69"><a href="simple-linear-regression.html#cb21-69" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax))</span>
<span id="cb21-70"><a href="simple-linear-regression.html#cb21-70" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb21-71"><a href="simple-linear-regression.html#cb21-71" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegNoiLe3<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb21-72"><a href="simple-linear-regression.html#cb21-72" tabindex="-1"></a>       <span class="at">b   =</span> outRegNoiLe3<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb21-73"><a href="simple-linear-regression.html#cb21-73" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb21-74"><a href="simple-linear-regression.html#cb21-74" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/r2-noise-1.png" width="672" /></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="simple-linear-regression.html#cb22-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Original Data LM summary&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Original Data LM summary&quot;</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="simple-linear-regression.html#cb24-1" tabindex="-1"></a><span class="fu">summary</span>(outRegOri)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -248.394  -58.805    3.782   63.577  196.745 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 997.3894    28.8185   34.61   &lt;2e-16 ***
## x             5.0247     0.3818   13.16   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 89.01 on 98 degrees of freedom
## Multiple R-squared:  0.6386, Adjusted R-squared:  0.6349 
## F-statistic: 173.2 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="simple-linear-regression.html#cb26-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Level 1 LM summary&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Level 1 LM summary&quot;</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="simple-linear-regression.html#cb28-1" tabindex="-1"></a><span class="fu">summary</span>(outRegNoiLe1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = yNoiLe1 ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -225.09  -62.78  -11.97   73.73  254.06 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1004.0513    30.9057   32.49   &lt;2e-16 ***
## x              4.9766     0.4095   12.15   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 95.45 on 98 degrees of freedom
## Multiple R-squared:  0.6011, Adjusted R-squared:  0.5971 
## F-statistic: 147.7 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="simple-linear-regression.html#cb30-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Level 2 LM summary&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Level 2 LM summary&quot;</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="simple-linear-regression.html#cb32-1" tabindex="-1"></a><span class="fu">summary</span>(outRegNoiLe2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = yNoiLe2 ~ x)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -588.4 -122.2    8.4  136.1  449.7 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1007.1088    64.7010  15.566  &lt; 2e-16 ***
## x              4.9009     0.8573   5.717 1.17e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 199.8 on 98 degrees of freedom
## Multiple R-squared:  0.2501, Adjusted R-squared:  0.2424 
## F-statistic: 32.68 on 1 and 98 DF,  p-value: 1.173e-07</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="simple-linear-regression.html#cb34-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Level 3 LM summary&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Level 3 LM summary&quot;</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="simple-linear-regression.html#cb36-1" tabindex="-1"></a><span class="fu">summary</span>(outRegNoiLe3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = yNoiLe3 ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1157.74  -264.68   -27.29   239.51   894.85 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  998.139    134.510   7.421 4.28e-11 ***
## x              4.948      1.782   2.776  0.00659 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 415.4 on 98 degrees of freedom
## Multiple R-squared:  0.0729, Adjusted R-squared:  0.06344 
## F-statistic: 7.706 on 1 and 98 DF,  p-value: 0.006592</code></pre>
<p>where we observe that the point cloud is more dispersed and looks less than a line
the more noise is added, but the estimated regression line changes only a little bit.
We can also see how the <span class="math inline">\(R^2\)</span> becomes smaller as more noise is added.</p>
</div>
<div id="residual-analysis" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Residual Analysis<a href="simple-linear-regression.html#residual-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>While the coefficient of determination can tell us how good is the fit of the
data, it can’t tell us why is it good or bad. The easiest way to check for any
problems is to check the residuals or estimated errors. Right now, we will focus
on 4 problems:</p>
<ul>
<li>The regression function is not linear.</li>
<li>The variance of the error terms is not constant.</li>
<li>There are outliers.</li>
<li>Important variables are ommited.</li>
</ul>
<div id="non-linear-regression-function" class="section level3 hasAnchor" number="4.6.1">
<h3><span class="header-section-number">4.6.1</span> Non-linear regression function<a href="simple-linear-regression.html#non-linear-regression-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sometimes the relationship between the two variables that we are analyzing is
not linear. Looking at the example of the relationship between burger price and
burgers sold we can look at the fitted regression line and the residuals plot.</p>
<p><img src="_main_files/figure-html/burgers-residuals-1.png" width="672" />
Here we can clearly appreciate that there is something wrong. The residuals clearly
indicate that a non linear relationship is present in the data. One can solve these
problem by transforming one or both of the variables and then applying linear
regression. Common transformations functions <span class="math inline">\(g\)</span> are (but are not limited to):</p>
<ul>
<li><span class="math inline">\(g(x) = x^2\)</span></li>
<li><span class="math inline">\(g(x) = \sqrt{x}\)</span></li>
<li><span class="math inline">\(g(x) = log(x)\)</span></li>
</ul>
<p>This transformations can be applied to the independent variable, to the dependent
variable of both.</p>
<p>In the next example we work with <span class="math inline">\(log(\text{Burgers Sold})\)</span> instead of directly
working with “Burgers Sold”.</p>
<p><img src="_main_files/figure-html/burgers-residuals-tr2-1.png" width="672" /></p>
<p>In this case there seems to be a much better fit. We can compare this to the
following transformation <span class="math inline">\(\log{(\text{Price})}\)</span>:</p>
<p><img src="_main_files/figure-html/burgers-residuals-tr1-1.png" width="672" /></p>
<p>It is this these transformations improve the fit of the model, however which one
is the best one?</p>
<p>One can check the <span class="math inline">\(R^2\)</span> of the different transformations:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="simple-linear-regression.html#cb38-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Burger Data.csv&quot;</span>)</span>
<span id="cb38-2"><a href="simple-linear-regression.html#cb38-2" tabindex="-1"></a></span>
<span id="cb38-3"><a href="simple-linear-regression.html#cb38-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Price</span>
<span id="cb38-4"><a href="simple-linear-regression.html#cb38-4" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Burgers</span>
<span id="cb38-5"><a href="simple-linear-regression.html#cb38-5" tabindex="-1"></a></span>
<span id="cb38-6"><a href="simple-linear-regression.html#cb38-6" tabindex="-1"></a>outRegOri <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb38-7"><a href="simple-linear-regression.html#cb38-7" tabindex="-1"></a>outRegTr1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(y) <span class="sc">~</span> x)</span>
<span id="cb38-8"><a href="simple-linear-regression.html#cb38-8" tabindex="-1"></a>outRegTr2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">log</span>(x))</span>
<span id="cb38-9"><a href="simple-linear-regression.html#cb38-9" tabindex="-1"></a></span>
<span id="cb38-10"><a href="simple-linear-regression.html#cb38-10" tabindex="-1"></a><span class="fu">summary</span>(outRegOri)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -17.310  -5.637  -0.553   2.899  47.182 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  76.9130     2.0098   38.27   &lt;2e-16 ***
## x            -4.3417     0.2194  -19.79   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.522 on 98 degrees of freedom
## Multiple R-squared:  0.7999, Adjusted R-squared:  0.7978 
## F-statistic: 391.7 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="simple-linear-regression.html#cb40-1" tabindex="-1"></a><span class="fu">summary</span>(outRegTr1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(y) ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.39409 -0.08669  0.00065  0.09325  0.37302 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  4.518050   0.034072  132.60   &lt;2e-16 ***
## x           -0.109240   0.003719  -29.37   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1445 on 98 degrees of freedom
## Multiple R-squared:  0.898,  Adjusted R-squared:  0.897 
## F-statistic: 862.8 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="simple-linear-regression.html#cb42-1" tabindex="-1"></a><span class="fu">summary</span>(outRegTr2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ log(x))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -12.4312  -2.8468   0.1724   3.1830   9.8330 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  99.6184     1.4927   66.74   &lt;2e-16 ***
## log(x)      -29.8274     0.7237  -41.22   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.449 on 98 degrees of freedom
## Multiple R-squared:  0.9455, Adjusted R-squared:  0.9449 
## F-statistic:  1699 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We will see other ways to choose the transformation later.</p>
</div>
<div id="heteroscedasticity" class="section level3 hasAnchor" number="4.6.2">
<h3><span class="header-section-number">4.6.2</span> Heteroscedasticity<a href="simple-linear-regression.html#heteroscedasticity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sometimes, errors are expected to increase depending on the explanatory variable.
For example, it could be expected that errors for bigger values of the explanatory
variable will be also bigger.</p>
<p>As an example, consider a new data set with the Height of several children. One
could expect there is a linear relationship between the age of a child and the
height. However, one also would expect that height difference are bigger for
older children than for younger children. We can see this in the following
simulated data:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="simple-linear-regression.html#cb44-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Height Data.csv&quot;</span>)</span>
<span id="cb44-2"><a href="simple-linear-regression.html#cb44-2" tabindex="-1"></a></span>
<span id="cb44-3"><a href="simple-linear-regression.html#cb44-3" tabindex="-1"></a>outReg <span class="ot">&lt;-</span> <span class="fu">lm</span>(Height <span class="sc">~</span> Age, <span class="at">data =</span> dat)</span>
<span id="cb44-4"><a href="simple-linear-regression.html#cb44-4" tabindex="-1"></a></span>
<span id="cb44-5"><a href="simple-linear-regression.html#cb44-5" tabindex="-1"></a><span class="co"># Original Data</span></span>
<span id="cb44-6"><a href="simple-linear-regression.html#cb44-6" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb44-7"><a href="simple-linear-regression.html#cb44-7" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> dat<span class="sc">$</span>Age,</span>
<span id="cb44-8"><a href="simple-linear-regression.html#cb44-8" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>Height,</span>
<span id="cb44-9"><a href="simple-linear-regression.html#cb44-9" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Age (years)&quot;</span>,</span>
<span id="cb44-10"><a href="simple-linear-regression.html#cb44-10" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Height (in)&quot;</span>,</span>
<span id="cb44-11"><a href="simple-linear-regression.html#cb44-11" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Original Data&quot;</span>)</span>
<span id="cb44-12"><a href="simple-linear-regression.html#cb44-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outReg<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb44-13"><a href="simple-linear-regression.html#cb44-13" tabindex="-1"></a>       <span class="at">b   =</span> outReg<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb44-14"><a href="simple-linear-regression.html#cb44-14" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb44-15"><a href="simple-linear-regression.html#cb44-15" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb44-16"><a href="simple-linear-regression.html#cb44-16" tabindex="-1"></a><span class="co"># Residuals</span></span>
<span id="cb44-17"><a href="simple-linear-regression.html#cb44-17" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> dat<span class="sc">$</span>Age,</span>
<span id="cb44-18"><a href="simple-linear-regression.html#cb44-18" tabindex="-1"></a>     <span class="at">y =</span> outReg<span class="sc">$</span>residuals,</span>
<span id="cb44-19"><a href="simple-linear-regression.html#cb44-19" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Age (years)&quot;</span>,</span>
<span id="cb44-20"><a href="simple-linear-regression.html#cb44-20" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals (in)&quot;</span>,</span>
<span id="cb44-21"><a href="simple-linear-regression.html#cb44-21" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb44-22"><a href="simple-linear-regression.html#cb44-22" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb44-23"><a href="simple-linear-regression.html#cb44-23" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/height-data-residuals-1.png" width="672" /></p>
<p>While one can infer this error behavior in the original scatter plot, the residual
plot makes this pattern much more clearly. In the residual plot, it is pretty clear
that the errors increase with the age of the children. While this is not a problem
in itself when doing least squares estimation, it might be better to consider
other alternatives that do not penalize the same way the errors at young age than
the errors at a later age. We will see this with weighted least squares.</p>
</div>
<div id="outliers" class="section level3 hasAnchor" number="4.6.3">
<h3><span class="header-section-number">4.6.3</span> Outliers<a href="simple-linear-regression.html#outliers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sometimes the data follows a linear regression for most of the observations, but
there might be some observations that do not follow the linear relationship.</p>
<p>In the next example, we work with the Wine data set and add outliers, to see the
effect this might have on the estimation.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="simple-linear-regression.html#cb45-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Wine Data.csv&quot;</span>)</span>
<span id="cb45-2"><a href="simple-linear-regression.html#cb45-2" tabindex="-1"></a></span>
<span id="cb45-3"><a href="simple-linear-regression.html#cb45-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Glasses</span>
<span id="cb45-4"><a href="simple-linear-regression.html#cb45-4" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Years</span>
<span id="cb45-5"><a href="simple-linear-regression.html#cb45-5" tabindex="-1"></a></span>
<span id="cb45-6"><a href="simple-linear-regression.html#cb45-6" tabindex="-1"></a>ymin <span class="ot">&lt;-</span> <span class="fu">min</span>(y, <span class="fu">max</span>(y) <span class="sc">-</span> <span class="dv">22</span>)</span>
<span id="cb45-7"><a href="simple-linear-regression.html#cb45-7" tabindex="-1"></a>ymax <span class="ot">&lt;-</span> <span class="fu">max</span>(y, <span class="fu">min</span>(y) <span class="sc">+</span> <span class="dv">22</span>)</span>
<span id="cb45-8"><a href="simple-linear-regression.html#cb45-8" tabindex="-1"></a>xmin <span class="ot">&lt;-</span> <span class="fu">min</span>(x)</span>
<span id="cb45-9"><a href="simple-linear-regression.html#cb45-9" tabindex="-1"></a>xmax <span class="ot">&lt;-</span> <span class="fu">max</span>(x)</span>
<span id="cb45-10"><a href="simple-linear-regression.html#cb45-10" tabindex="-1"></a></span>
<span id="cb45-11"><a href="simple-linear-regression.html#cb45-11" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb45-12"><a href="simple-linear-regression.html#cb45-12" tabindex="-1"></a><span class="co"># No Outliers</span></span>
<span id="cb45-13"><a href="simple-linear-regression.html#cb45-13" tabindex="-1"></a>outRegNoo <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb45-14"><a href="simple-linear-regression.html#cb45-14" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb45-15"><a href="simple-linear-regression.html#cb45-15" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb45-16"><a href="simple-linear-regression.html#cb45-16" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb45-17"><a href="simple-linear-regression.html#cb45-17" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb45-18"><a href="simple-linear-regression.html#cb45-18" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb45-19"><a href="simple-linear-regression.html#cb45-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Life Expectancy&quot;</span>,</span>
<span id="cb45-20"><a href="simple-linear-regression.html#cb45-20" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;No Outliers&quot;</span>)</span>
<span id="cb45-21"><a href="simple-linear-regression.html#cb45-21" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegNoo<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb45-22"><a href="simple-linear-regression.html#cb45-22" tabindex="-1"></a>       <span class="at">b   =</span> outRegNoo<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb45-23"><a href="simple-linear-regression.html#cb45-23" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb45-24"><a href="simple-linear-regression.html#cb45-24" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb45-25"><a href="simple-linear-regression.html#cb45-25" tabindex="-1"></a></span>
<span id="cb45-26"><a href="simple-linear-regression.html#cb45-26" tabindex="-1"></a><span class="co"># Outlier on the left side</span></span>
<span id="cb45-27"><a href="simple-linear-regression.html#cb45-27" tabindex="-1"></a><span class="co"># Adds Observation</span></span>
<span id="cb45-28"><a href="simple-linear-regression.html#cb45-28" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(x, <span class="dv">0</span>)</span>
<span id="cb45-29"><a href="simple-linear-regression.html#cb45-29" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(y, <span class="dv">90</span>)</span>
<span id="cb45-30"><a href="simple-linear-regression.html#cb45-30" tabindex="-1"></a>outRegLef <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb45-31"><a href="simple-linear-regression.html#cb45-31" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb45-32"><a href="simple-linear-regression.html#cb45-32" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb45-33"><a href="simple-linear-regression.html#cb45-33" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb45-34"><a href="simple-linear-regression.html#cb45-34" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb45-35"><a href="simple-linear-regression.html#cb45-35" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb45-36"><a href="simple-linear-regression.html#cb45-36" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Life Expectancy&quot;</span>,</span>
<span id="cb45-37"><a href="simple-linear-regression.html#cb45-37" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Outlier Left&quot;</span>)</span>
<span id="cb45-38"><a href="simple-linear-regression.html#cb45-38" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x   =</span> x[<span class="dv">21</span>],</span>
<span id="cb45-39"><a href="simple-linear-regression.html#cb45-39" tabindex="-1"></a>       <span class="at">y   =</span> y[<span class="dv">21</span>],</span>
<span id="cb45-40"><a href="simple-linear-regression.html#cb45-40" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">16</span>,</span>
<span id="cb45-41"><a href="simple-linear-regression.html#cb45-41" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb45-42"><a href="simple-linear-regression.html#cb45-42" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegLef<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb45-43"><a href="simple-linear-regression.html#cb45-43" tabindex="-1"></a>       <span class="at">b   =</span> outRegLef<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb45-44"><a href="simple-linear-regression.html#cb45-44" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb45-45"><a href="simple-linear-regression.html#cb45-45" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb45-46"><a href="simple-linear-regression.html#cb45-46" tabindex="-1"></a><span class="co"># Outlier on the right side</span></span>
<span id="cb45-47"><a href="simple-linear-regression.html#cb45-47" tabindex="-1"></a><span class="co"># Adds Observation</span></span>
<span id="cb45-48"><a href="simple-linear-regression.html#cb45-48" tabindex="-1"></a>x[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb45-49"><a href="simple-linear-regression.html#cb45-49" tabindex="-1"></a>y[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">68</span></span>
<span id="cb45-50"><a href="simple-linear-regression.html#cb45-50" tabindex="-1"></a>outRegRig <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb45-51"><a href="simple-linear-regression.html#cb45-51" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb45-52"><a href="simple-linear-regression.html#cb45-52" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb45-53"><a href="simple-linear-regression.html#cb45-53" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb45-54"><a href="simple-linear-regression.html#cb45-54" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb45-55"><a href="simple-linear-regression.html#cb45-55" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb45-56"><a href="simple-linear-regression.html#cb45-56" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Life Expectancy&quot;</span>,</span>
<span id="cb45-57"><a href="simple-linear-regression.html#cb45-57" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Right Outliers&quot;</span>)</span>
<span id="cb45-58"><a href="simple-linear-regression.html#cb45-58" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x   =</span> x[<span class="dv">21</span>],</span>
<span id="cb45-59"><a href="simple-linear-regression.html#cb45-59" tabindex="-1"></a>       <span class="at">y   =</span> y[<span class="dv">21</span>],</span>
<span id="cb45-60"><a href="simple-linear-regression.html#cb45-60" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">16</span>,</span>
<span id="cb45-61"><a href="simple-linear-regression.html#cb45-61" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb45-62"><a href="simple-linear-regression.html#cb45-62" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegRig<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb45-63"><a href="simple-linear-regression.html#cb45-63" tabindex="-1"></a>       <span class="at">b   =</span> outRegRig<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb45-64"><a href="simple-linear-regression.html#cb45-64" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb45-65"><a href="simple-linear-regression.html#cb45-65" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb45-66"><a href="simple-linear-regression.html#cb45-66" tabindex="-1"></a><span class="co"># Outlier on the right side</span></span>
<span id="cb45-67"><a href="simple-linear-regression.html#cb45-67" tabindex="-1"></a><span class="co"># Adds Observation</span></span>
<span id="cb45-68"><a href="simple-linear-regression.html#cb45-68" tabindex="-1"></a>x[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb45-69"><a href="simple-linear-regression.html#cb45-69" tabindex="-1"></a>y[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">95</span></span>
<span id="cb45-70"><a href="simple-linear-regression.html#cb45-70" tabindex="-1"></a>outRegCen <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb45-71"><a href="simple-linear-regression.html#cb45-71" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb45-72"><a href="simple-linear-regression.html#cb45-72" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb45-73"><a href="simple-linear-regression.html#cb45-73" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb45-74"><a href="simple-linear-regression.html#cb45-74" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb45-75"><a href="simple-linear-regression.html#cb45-75" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb45-76"><a href="simple-linear-regression.html#cb45-76" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Life Expectancy&quot;</span>,</span>
<span id="cb45-77"><a href="simple-linear-regression.html#cb45-77" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Center Outliers&quot;</span>)</span>
<span id="cb45-78"><a href="simple-linear-regression.html#cb45-78" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x   =</span> x[<span class="dv">21</span>],</span>
<span id="cb45-79"><a href="simple-linear-regression.html#cb45-79" tabindex="-1"></a>       <span class="at">y   =</span> y[<span class="dv">21</span>],</span>
<span id="cb45-80"><a href="simple-linear-regression.html#cb45-80" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">16</span>,</span>
<span id="cb45-81"><a href="simple-linear-regression.html#cb45-81" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb45-82"><a href="simple-linear-regression.html#cb45-82" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegCen<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb45-83"><a href="simple-linear-regression.html#cb45-83" tabindex="-1"></a>       <span class="at">b   =</span> outRegCen<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb45-84"><a href="simple-linear-regression.html#cb45-84" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb45-85"><a href="simple-linear-regression.html#cb45-85" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/wine-outliers-1.png" width="672" />
The presence of outliers, will also be more clear when looking at the residuals.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="simple-linear-regression.html#cb46-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Glasses</span>
<span id="cb46-2"><a href="simple-linear-regression.html#cb46-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Years</span>
<span id="cb46-3"><a href="simple-linear-regression.html#cb46-3" tabindex="-1"></a></span>
<span id="cb46-4"><a href="simple-linear-regression.html#cb46-4" tabindex="-1"></a>ymin <span class="ot">&lt;-</span> <span class="fu">min</span>(outRegNoo<span class="sc">$</span>residuals, outRegLef<span class="sc">$</span>residuals, outRegRig<span class="sc">$</span>residuals, outRegCen<span class="sc">$</span>residuals)</span>
<span id="cb46-5"><a href="simple-linear-regression.html#cb46-5" tabindex="-1"></a>ymax <span class="ot">&lt;-</span> <span class="fu">max</span>(outRegNoo<span class="sc">$</span>residuals, outRegLef<span class="sc">$</span>residuals, outRegRig<span class="sc">$</span>residuals, outRegCen<span class="sc">$</span>residuals)</span>
<span id="cb46-6"><a href="simple-linear-regression.html#cb46-6" tabindex="-1"></a>xmin <span class="ot">&lt;-</span> <span class="fu">min</span>(x)</span>
<span id="cb46-7"><a href="simple-linear-regression.html#cb46-7" tabindex="-1"></a>xmax <span class="ot">&lt;-</span> <span class="fu">max</span>(x)</span>
<span id="cb46-8"><a href="simple-linear-regression.html#cb46-8" tabindex="-1"></a></span>
<span id="cb46-9"><a href="simple-linear-regression.html#cb46-9" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb46-10"><a href="simple-linear-regression.html#cb46-10" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> dat<span class="sc">$</span>Glasses,</span>
<span id="cb46-11"><a href="simple-linear-regression.html#cb46-11" tabindex="-1"></a>     <span class="at">y    =</span> outRegNoo<span class="sc">$</span>residuals,</span>
<span id="cb46-12"><a href="simple-linear-regression.html#cb46-12" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb46-13"><a href="simple-linear-regression.html#cb46-13" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb46-14"><a href="simple-linear-regression.html#cb46-14" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb46-15"><a href="simple-linear-regression.html#cb46-15" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb46-16"><a href="simple-linear-regression.html#cb46-16" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;No Outliers&quot;</span>)</span>
<span id="cb46-17"><a href="simple-linear-regression.html#cb46-17" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb46-18"><a href="simple-linear-regression.html#cb46-18" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb46-19"><a href="simple-linear-regression.html#cb46-19" tabindex="-1"></a></span>
<span id="cb46-20"><a href="simple-linear-regression.html#cb46-20" tabindex="-1"></a><span class="co"># Outlier on the left side</span></span>
<span id="cb46-21"><a href="simple-linear-regression.html#cb46-21" tabindex="-1"></a>x[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb46-22"><a href="simple-linear-regression.html#cb46-22" tabindex="-1"></a>y[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">90</span></span>
<span id="cb46-23"><a href="simple-linear-regression.html#cb46-23" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb46-24"><a href="simple-linear-regression.html#cb46-24" tabindex="-1"></a>     <span class="at">y    =</span> outRegLef<span class="sc">$</span>residuals,</span>
<span id="cb46-25"><a href="simple-linear-regression.html#cb46-25" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb46-26"><a href="simple-linear-regression.html#cb46-26" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb46-27"><a href="simple-linear-regression.html#cb46-27" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb46-28"><a href="simple-linear-regression.html#cb46-28" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb46-29"><a href="simple-linear-regression.html#cb46-29" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Left Outlier&quot;</span>)</span>
<span id="cb46-30"><a href="simple-linear-regression.html#cb46-30" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb46-31"><a href="simple-linear-regression.html#cb46-31" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb46-32"><a href="simple-linear-regression.html#cb46-32" tabindex="-1"></a><span class="co"># Outlier on the right side</span></span>
<span id="cb46-33"><a href="simple-linear-regression.html#cb46-33" tabindex="-1"></a><span class="co"># Adds Observation</span></span>
<span id="cb46-34"><a href="simple-linear-regression.html#cb46-34" tabindex="-1"></a>x[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb46-35"><a href="simple-linear-regression.html#cb46-35" tabindex="-1"></a>y[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">68</span></span>
<span id="cb46-36"><a href="simple-linear-regression.html#cb46-36" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb46-37"><a href="simple-linear-regression.html#cb46-37" tabindex="-1"></a>     <span class="at">y    =</span> outRegRig<span class="sc">$</span>residuals,</span>
<span id="cb46-38"><a href="simple-linear-regression.html#cb46-38" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb46-39"><a href="simple-linear-regression.html#cb46-39" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb46-40"><a href="simple-linear-regression.html#cb46-40" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb46-41"><a href="simple-linear-regression.html#cb46-41" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb46-42"><a href="simple-linear-regression.html#cb46-42" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Left Outlier&quot;</span>)</span>
<span id="cb46-43"><a href="simple-linear-regression.html#cb46-43" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb46-44"><a href="simple-linear-regression.html#cb46-44" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb46-45"><a href="simple-linear-regression.html#cb46-45" tabindex="-1"></a><span class="co"># Outlier on the right side</span></span>
<span id="cb46-46"><a href="simple-linear-regression.html#cb46-46" tabindex="-1"></a><span class="co"># Adds Observation</span></span>
<span id="cb46-47"><a href="simple-linear-regression.html#cb46-47" tabindex="-1"></a>x[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb46-48"><a href="simple-linear-regression.html#cb46-48" tabindex="-1"></a>y[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">95</span></span>
<span id="cb46-49"><a href="simple-linear-regression.html#cb46-49" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb46-50"><a href="simple-linear-regression.html#cb46-50" tabindex="-1"></a>     <span class="at">y    =</span> outRegCen<span class="sc">$</span>residuals,</span>
<span id="cb46-51"><a href="simple-linear-regression.html#cb46-51" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb46-52"><a href="simple-linear-regression.html#cb46-52" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb46-53"><a href="simple-linear-regression.html#cb46-53" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb46-54"><a href="simple-linear-regression.html#cb46-54" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb46-55"><a href="simple-linear-regression.html#cb46-55" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Left Outlier&quot;</span>)</span>
<span id="cb46-56"><a href="simple-linear-regression.html#cb46-56" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb46-57"><a href="simple-linear-regression.html#cb46-57" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/wine-res-1.png" width="672" />
And we can also see how the <span class="math inline">\(R^2\)</span> changes:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="simple-linear-regression.html#cb47-1" tabindex="-1"></a><span class="fu">summary</span>(outRegNoo)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.6443 -1.4398 -0.3390  0.9071  4.8057 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  73.7154     0.8674  84.988  &lt; 2e-16 ***
## x             2.4686     0.4364   5.656 2.29e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.162 on 18 degrees of freedom
## Multiple R-squared:   0.64,  Adjusted R-squared:   0.62 
## F-statistic:    32 on 1 and 18 DF,  p-value: 2.295e-05</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="simple-linear-regression.html#cb49-1" tabindex="-1"></a><span class="fu">summary</span>(outRegLef)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.550 -2.296 -1.413  1.440 14.028 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  75.9724     1.5101   50.31   &lt;2e-16 ***
## x             1.5258     0.7786    1.96   0.0649 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.056 on 19 degrees of freedom
## Multiple R-squared:  0.1682, Adjusted R-squared:  0.1244 
## F-statistic: 3.841 on 1 and 19 DF,  p-value: 0.06486</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="simple-linear-regression.html#cb51-1" tabindex="-1"></a><span class="fu">summary</span>(outRegRig)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.228  -1.428  -0.211   2.283   5.654 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  75.0353     1.4815  50.648   &lt;2e-16 ***
## x             1.2981     0.6965   1.864   0.0779 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.803 on 19 degrees of freedom
## Multiple R-squared:  0.1545, Adjusted R-squared:   0.11 
## F-statistic: 3.473 on 1 and 19 DF,  p-value: 0.0779</code></pre>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="simple-linear-regression.html#cb53-1" tabindex="-1"></a><span class="fu">summary</span>(outRegCen)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4964 -2.2377 -0.9105  0.9596 15.4953 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  74.1257     1.6870  43.939  &lt; 2e-16 ***
## x             2.6895     0.8486   3.169  0.00505 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.214 on 19 degrees of freedom
## Multiple R-squared:  0.3459, Adjusted R-squared:  0.3114 
## F-statistic: 10.05 on 1 and 19 DF,  p-value: 0.005048</code></pre>
</div>
<div id="variables-ommited" class="section level3 hasAnchor" number="4.6.4">
<h3><span class="header-section-number">4.6.4</span> Variables Ommited<a href="simple-linear-regression.html#variables-ommited" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When looking at the residuals, there should be no aparent pattern. Sometimes,
the pattern is only noticeable when considering other variables. We can see this
with the Height data set. The data includes the sex of the children. If you fit
least squares without taking this into consideration, a pattern is appears in the
residuals when you look at the residuals for each sex.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="simple-linear-regression.html#cb55-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Height Data.csv&quot;</span>)</span>
<span id="cb55-2"><a href="simple-linear-regression.html#cb55-2" tabindex="-1"></a></span>
<span id="cb55-3"><a href="simple-linear-regression.html#cb55-3" tabindex="-1"></a><span class="co"># Fits linear Model</span></span>
<span id="cb55-4"><a href="simple-linear-regression.html#cb55-4" tabindex="-1"></a>outReg <span class="ot">&lt;-</span> <span class="fu">lm</span>(dat<span class="sc">$</span>Height <span class="sc">~</span> dat<span class="sc">$</span>Age)</span>
<span id="cb55-5"><a href="simple-linear-regression.html#cb55-5" tabindex="-1"></a></span>
<span id="cb55-6"><a href="simple-linear-regression.html#cb55-6" tabindex="-1"></a><span class="co"># Plots residuals for each sex</span></span>
<span id="cb55-7"><a href="simple-linear-regression.html#cb55-7" tabindex="-1"></a>ymin <span class="ot">&lt;-</span> <span class="fu">min</span>(outReg<span class="sc">$</span>residuals)</span>
<span id="cb55-8"><a href="simple-linear-regression.html#cb55-8" tabindex="-1"></a>ymax <span class="ot">&lt;-</span> <span class="fu">max</span>(outReg<span class="sc">$</span>residuals)</span>
<span id="cb55-9"><a href="simple-linear-regression.html#cb55-9" tabindex="-1"></a></span>
<span id="cb55-10"><a href="simple-linear-regression.html#cb55-10" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb55-11"><a href="simple-linear-regression.html#cb55-11" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> dat<span class="sc">$</span>Age[dat<span class="sc">$</span>Sex <span class="sc">==</span> <span class="dv">1</span>],</span>
<span id="cb55-12"><a href="simple-linear-regression.html#cb55-12" tabindex="-1"></a>     <span class="at">y =</span> outReg<span class="sc">$</span>residuals[dat<span class="sc">$</span>Sex <span class="sc">==</span> <span class="dv">1</span>],</span>
<span id="cb55-13"><a href="simple-linear-regression.html#cb55-13" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb55-14"><a href="simple-linear-regression.html#cb55-14" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Age&quot;</span>,</span>
<span id="cb55-15"><a href="simple-linear-regression.html#cb55-15" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb55-16"><a href="simple-linear-regression.html#cb55-16" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Males&quot;</span>)</span>
<span id="cb55-17"><a href="simple-linear-regression.html#cb55-17" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb55-18"><a href="simple-linear-regression.html#cb55-18" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb55-19"><a href="simple-linear-regression.html#cb55-19" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> dat<span class="sc">$</span>Age[dat<span class="sc">$</span>Sex <span class="sc">==</span> <span class="dv">0</span>],</span>
<span id="cb55-20"><a href="simple-linear-regression.html#cb55-20" tabindex="-1"></a>     <span class="at">y =</span> outReg<span class="sc">$</span>residuals[dat<span class="sc">$</span>Sex <span class="sc">==</span> <span class="dv">0</span>],</span>
<span id="cb55-21"><a href="simple-linear-regression.html#cb55-21" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb55-22"><a href="simple-linear-regression.html#cb55-22" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Age&quot;</span>,</span>
<span id="cb55-23"><a href="simple-linear-regression.html#cb55-23" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb55-24"><a href="simple-linear-regression.html#cb55-24" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Females&quot;</span>)</span>
<span id="cb55-25"><a href="simple-linear-regression.html#cb55-25" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb55-26"><a href="simple-linear-regression.html#cb55-26" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/height-other-var-1.png" width="672" /></p>
</div>
</div>
<div id="cross-validation" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Cross-Validation<a href="simple-linear-regression.html#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When testing different models, a good idea to evaluate the performance of each
model beyond <span class="math inline">\(R^2\)</span>, is to separate your data set into a training set and a
validation or test set. Doing this is called cross-validation. There are several
alternatives to doing cross-validation, here are a few of the most relevant ones:</p>
<ol style="list-style-type: decimal">
<li><strong>Holdout Method (Train/Test Split)</strong>
<ul>
<li><strong>Description</strong>: The data is split into two (or three) sets: training and test (and sometimes validation). The model is trained on the training set and evaluated on the test set.</li>
<li><strong>Use Case</strong>:
<ul>
<li>Simple to implement, but has high variance. The performance may depend on the specific split.</li>
</ul></li>
</ul></li>
<li><strong>K-Fold Cross-Validation</strong>
<ul>
<li><strong>Description</strong>: The data is divided into <span class="math inline">\(k\)</span> equal-sized folds (subsets). The model is trained on <span class="math inline">\(k-1\)</span> folds and tested on the remaining fold. This process is repeated <span class="math inline">\(k\)</span> times, with each fold used as the test set once.</li>
<li><strong>Use Case</strong>:
<ul>
<li>Works well for most applications and balances the bias-variance tradeoff.</li>
<li>Common choices for <span class="math inline">\(k\)</span>: 5, 10.</li>
</ul></li>
</ul></li>
<li><strong>Leave-One-Out Cross-Validation (LOOCV)</strong>
<ul>
<li><strong>Description</strong>: Each data point is used once as a test set, and the rest of the data is used as the training set. This results in <span class="math inline">\(n\)</span> iterations, where <span class="math inline">\(n\)</span> is the number of samples.</li>
<li><strong>Use Case</strong>:
<ul>
<li>Good when the dataset is small, but can be computationally expensive for large datasets.</li>
</ul></li>
</ul></li>
<li><strong>Leave-P-Out Cross-Validation (LPOCV)</strong>
<ul>
<li><strong>Description</strong>: Similar to LOOCV, but instead of leaving out one data point, <span class="math inline">\(p\)</span> data points are left out. This creates <span class="math inline">\(\binom{n}{p}\)</span> different training/testing splits.</li>
<li><strong>Use Case</strong>:
<ul>
<li>Rarely used due to its high computational cost for large datasets but might be useful in specific scenarios.</li>
</ul></li>
</ul></li>
<li><strong>Stratified K-Fold Cross-Validation</strong>
<ul>
<li><strong>Description</strong>: Similar to K-fold cross-validation but ensures that each fold has the same proportion of each class in classification tasks (i.e., balanced classes in each fold).</li>
<li><strong>Use Case</strong>:
<ul>
<li>Useful for imbalanced datasets in classification problems.</li>
</ul></li>
</ul></li>
<li><strong>Repeated K-Fold Cross-Validation</strong>
<ul>
<li><strong>Description</strong>: A variation of K-Fold Cross-Validation where the process is repeated multiple times with different random splits.</li>
<li><strong>Use Case</strong>:
<ul>
<li>Provides more robust estimates of model performance, particularly when the dataset is small.</li>
</ul></li>
</ul></li>
</ol>
<p>Here is an example of <span class="math inline">\(k\)</span>-fold Cross-Validation using the “Burger Data” set.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="simple-linear-regression.html#cb56-1" tabindex="-1"></a><span class="do">### Cross Validation Example</span></span>
<span id="cb56-2"><a href="simple-linear-regression.html#cb56-2" tabindex="-1"></a></span>
<span id="cb56-3"><a href="simple-linear-regression.html#cb56-3" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb56-4"><a href="simple-linear-regression.html#cb56-4" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">&quot;Burger Data.csv&quot;</span>)</span>
<span id="cb56-5"><a href="simple-linear-regression.html#cb56-5" tabindex="-1"></a></span>
<span id="cb56-6"><a href="simple-linear-regression.html#cb56-6" tabindex="-1"></a><span class="co"># Saves Variables</span></span>
<span id="cb56-7"><a href="simple-linear-regression.html#cb56-7" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Price</span>
<span id="cb56-8"><a href="simple-linear-regression.html#cb56-8" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Burgers</span>
<span id="cb56-9"><a href="simple-linear-regression.html#cb56-9" tabindex="-1"></a></span>
<span id="cb56-10"><a href="simple-linear-regression.html#cb56-10" tabindex="-1"></a><span class="co"># Number of Observations</span></span>
<span id="cb56-11"><a href="simple-linear-regression.html#cb56-11" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb56-12"><a href="simple-linear-regression.html#cb56-12" tabindex="-1"></a></span>
<span id="cb56-13"><a href="simple-linear-regression.html#cb56-13" tabindex="-1"></a><span class="co"># Number of Folds</span></span>
<span id="cb56-14"><a href="simple-linear-regression.html#cb56-14" tabindex="-1"></a>numFol <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb56-15"><a href="simple-linear-regression.html#cb56-15" tabindex="-1"></a><span class="co"># Fold Size</span></span>
<span id="cb56-16"><a href="simple-linear-regression.html#cb56-16" tabindex="-1"></a>sizFol <span class="ot">&lt;-</span> <span class="fu">round</span>(n <span class="sc">/</span> numFol)</span>
<span id="cb56-17"><a href="simple-linear-regression.html#cb56-17" tabindex="-1"></a></span>
<span id="cb56-18"><a href="simple-linear-regression.html#cb56-18" tabindex="-1"></a><span class="co"># List Containing the Fold indices</span></span>
<span id="cb56-19"><a href="simple-linear-regression.html#cb56-19" tabindex="-1"></a>fol <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb56-20"><a href="simple-linear-regression.html#cb56-20" tabindex="-1"></a><span class="co"># Select Fold Indeces</span></span>
<span id="cb56-21"><a href="simple-linear-regression.html#cb56-21" tabindex="-1"></a><span class="co"># Initialization</span></span>
<span id="cb56-22"><a href="simple-linear-regression.html#cb56-22" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n</span>
<span id="cb56-23"><a href="simple-linear-regression.html#cb56-23" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>numFol){</span>
<span id="cb56-24"><a href="simple-linear-regression.html#cb56-24" tabindex="-1"></a>  <span class="co"># Computes the remaining number of indices</span></span>
<span id="cb56-25"><a href="simple-linear-regression.html#cb56-25" tabindex="-1"></a>  numInd   <span class="ot">&lt;-</span> <span class="fu">length</span>(ind)</span>
<span id="cb56-26"><a href="simple-linear-regression.html#cb56-26" tabindex="-1"></a>  <span class="co"># Randomly selects indices from the remaining indeces</span></span>
<span id="cb56-27"><a href="simple-linear-regression.html#cb56-27" tabindex="-1"></a>  indFol   <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(ind), <span class="at">size =</span> sizFol, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb56-28"><a href="simple-linear-regression.html#cb56-28" tabindex="-1"></a>  <span class="co"># Saves the inidices to the list of Fold Indeces</span></span>
<span id="cb56-29"><a href="simple-linear-regression.html#cb56-29" tabindex="-1"></a>  fol[[i]] <span class="ot">&lt;-</span> ind[indFol]</span>
<span id="cb56-30"><a href="simple-linear-regression.html#cb56-30" tabindex="-1"></a>  <span class="co"># Removes the indices from the indeces vector</span></span>
<span id="cb56-31"><a href="simple-linear-regression.html#cb56-31" tabindex="-1"></a>  ind      <span class="ot">&lt;-</span> ind[<span class="sc">-</span> indFol]</span>
<span id="cb56-32"><a href="simple-linear-regression.html#cb56-32" tabindex="-1"></a>}</span>
<span id="cb56-33"><a href="simple-linear-regression.html#cb56-33" tabindex="-1"></a></span>
<span id="cb56-34"><a href="simple-linear-regression.html#cb56-34" tabindex="-1"></a><span class="co"># Models to try</span></span>
<span id="cb56-35"><a href="simple-linear-regression.html#cb56-35" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(x, <span class="fu">log</span>(x),      x, <span class="fu">log</span>(x))</span>
<span id="cb56-36"><a href="simple-linear-regression.html#cb56-36" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">cbind</span>(y,      y, <span class="fu">log</span>(y), <span class="fu">log</span>(y))</span>
<span id="cb56-37"><a href="simple-linear-regression.html#cb56-37" tabindex="-1"></a></span>
<span id="cb56-38"><a href="simple-linear-regression.html#cb56-38" tabindex="-1"></a><span class="co"># Number of Models to compare</span></span>
<span id="cb56-39"><a href="simple-linear-regression.html#cb56-39" tabindex="-1"></a>numMod <span class="ot">&lt;-</span> <span class="fu">dim</span>(Y)[<span class="dv">2</span>]</span>
<span id="cb56-40"><a href="simple-linear-regression.html#cb56-40" tabindex="-1"></a><span class="co"># Saves the validation metric for each model and each fold </span></span>
<span id="cb56-41"><a href="simple-linear-regression.html#cb56-41" tabindex="-1"></a>matMet <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="cn">NA</span>, <span class="at">nrow =</span> numFol, <span class="at">ncol =</span> numMod)</span>
<span id="cb56-42"><a href="simple-linear-regression.html#cb56-42" tabindex="-1"></a></span>
<span id="cb56-43"><a href="simple-linear-regression.html#cb56-43" tabindex="-1"></a></span>
<span id="cb56-44"><a href="simple-linear-regression.html#cb56-44" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb56-45"><a href="simple-linear-regression.html#cb56-45" tabindex="-1"></a><span class="co"># Loops through the models</span></span>
<span id="cb56-46"><a href="simple-linear-regression.html#cb56-46" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>numMod){</span>
<span id="cb56-47"><a href="simple-linear-regression.html#cb56-47" tabindex="-1"></a>  yMod <span class="ot">&lt;-</span> Y[, k]</span>
<span id="cb56-48"><a href="simple-linear-regression.html#cb56-48" tabindex="-1"></a>  xMod <span class="ot">&lt;-</span> X[, k]</span>
<span id="cb56-49"><a href="simple-linear-regression.html#cb56-49" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="at">x =</span> xMod,</span>
<span id="cb56-50"><a href="simple-linear-regression.html#cb56-50" tabindex="-1"></a>       <span class="at">y =</span> yMod,</span>
<span id="cb56-51"><a href="simple-linear-regression.html#cb56-51" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Price ($)&quot;</span>,</span>
<span id="cb56-52"><a href="simple-linear-regression.html#cb56-52" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">&quot;Burgers&quot;</span>,</span>
<span id="cb56-53"><a href="simple-linear-regression.html#cb56-53" tabindex="-1"></a>       <span class="at">main =</span> <span class="fu">paste0</span>(<span class="st">&quot;Model &quot;</span>, k))</span>
<span id="cb56-54"><a href="simple-linear-regression.html#cb56-54" tabindex="-1"></a>  metFol <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="at">length =</span> numFol)</span>
<span id="cb56-55"><a href="simple-linear-regression.html#cb56-55" tabindex="-1"></a>  <span class="co"># Loops through the folds</span></span>
<span id="cb56-56"><a href="simple-linear-regression.html#cb56-56" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>numFol){</span>
<span id="cb56-57"><a href="simple-linear-regression.html#cb56-57" tabindex="-1"></a>    <span class="co"># At Each Fold do Linear Regression without the test set</span></span>
<span id="cb56-58"><a href="simple-linear-regression.html#cb56-58" tabindex="-1"></a>    outReg   <span class="ot">&lt;-</span> <span class="fu">lm</span>(yMod[<span class="sc">-</span>fol[[i]]] <span class="sc">~</span> xMod[<span class="sc">-</span>fol[[i]]])</span>
<span id="cb56-59"><a href="simple-linear-regression.html#cb56-59" tabindex="-1"></a>    <span class="co"># Evaluate Out of Sample</span></span>
<span id="cb56-60"><a href="simple-linear-regression.html#cb56-60" tabindex="-1"></a>    yHatOut   <span class="ot">&lt;-</span> outReg<span class="sc">$</span>coefficients[<span class="dv">1</span>] <span class="sc">+</span> outReg<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">*</span> xMod[fol[[i]]] </span>
<span id="cb56-61"><a href="simple-linear-regression.html#cb56-61" tabindex="-1"></a>    <span class="cf">if</span>(k <span class="sc">&gt;=</span> <span class="dv">3</span> ){</span>
<span id="cb56-62"><a href="simple-linear-regression.html#cb56-62" tabindex="-1"></a>      metEva    <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">exp</span>(yMod[fol[[i]]]) <span class="sc">-</span> <span class="fu">exp</span>(yHatOut))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb56-63"><a href="simple-linear-regression.html#cb56-63" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb56-64"><a href="simple-linear-regression.html#cb56-64" tabindex="-1"></a>      metEva    <span class="ot">&lt;-</span> <span class="fu">mean</span>((yMod[fol[[i]]] <span class="sc">-</span> yHatOut)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb56-65"><a href="simple-linear-regression.html#cb56-65" tabindex="-1"></a>    }</span>
<span id="cb56-66"><a href="simple-linear-regression.html#cb56-66" tabindex="-1"></a>    metFol[i] <span class="ot">&lt;-</span> metEva</span>
<span id="cb56-67"><a href="simple-linear-regression.html#cb56-67" tabindex="-1"></a>  }</span>
<span id="cb56-68"><a href="simple-linear-regression.html#cb56-68" tabindex="-1"></a>  matMet[, k] <span class="ot">&lt;-</span> metFol</span>
<span id="cb56-69"><a href="simple-linear-regression.html#cb56-69" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="_main_files/figure-html/cross-validation-1.png" width="672" /></p>
</div>
<div id="weighted-least-squares" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> Weighted Least Squares<a href="simple-linear-regression.html#weighted-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Weighted Least Squares (WLS) is an extension of ordinary least squares (OLS) used when the assumption of constant variance (homoscedasticity) is violated. In cases of heteroscedasticity (unequal variances), OLS produces inefficient estimates.</p>
<p>WLS solves this by assigning weights to each observation, giving more importance to data points with lower variance. This leads to more reliable estimates in the presence of heteroscedasticity, making WLS valuable for improving regression models when error variances vary.</p>
<ul>
<li><strong>Lower variance</strong> → <strong>Higher weight</strong></li>
<li><strong>Higher variance</strong> → <strong>Lower weight</strong></li>
</ul>
<p>In a simple linear regression model with one independent variable, OLS minimizes the sum of squared residuals:</p>
<p><span class="math display">\[ \min \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \min \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2
\]</span></p>
<p>Weighted Least Squares (WLS) corrects for heteroscedasticity by minimizing the weighted sum of squared residuals:
<span class="math display">\[
\min \sum_{i=1}^{n} w_i (y_i - \hat{y}_i)^2 = \min \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i)^2
\]</span></p>
<p>Step 1: Define the weighted residual sum of squares</p>
<p>The objective function is:</p>
<p><span class="math display">\[Q_w(\beta_0, \beta_1) = \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i)^2\]</span></p>
<p>We aim to minimize <span class="math inline">\(Q(\beta_0, \beta_1)\)</span> with respect to <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p>Step 2: Take the partial derivatives with respect to $ _0 $ and $ _1 $:</p>
<p><span class="math display">\[\frac{\partial Q_w}{\partial \beta_0} = -2 \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i)\]</span></p>
<p>Setting the derivative equal to zero:</p>
<p><span class="math display">\[\sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i) = 0 \]</span>
<span class="math display">\[\sum_{i=1}^{n} w_i y_i = \sum_{i=1}^{n} w_i (\beta_0 + \beta_1 x_i)\]</span>
<span class="math display">\[\beta_0 \sum_{i=1}^{n} w_i + \beta_1 \sum_{i=1}^{n} w_i x_i = \sum_{i=1}^{n} w_i y_i \tag{3}\]</span></p>
<p>Now with respect to <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[\frac{\partial Q_w}{\partial \beta_1} = -2 \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i) x_i\]</span></p>
<p>Setting the derivative equal to zero:</p>
<p><span class="math display">\[-2 \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i) x_i = 0\]</span>
<span class="math display">\[\sum_{i=1}^{n} w_i y_i x_i = \sum_{i=1}^{n} w_i (\beta_0 + \beta_1 x_i) x_i\]</span></p>
<p><span class="math display">\[\beta_0 \sum_{i=1}^{n} w_i x_i + \beta_1 \sum_{i=1}^{n} w_i x_i^2 = \sum_{i=1}^{n} w_i y_i x_i \tag{4}\]</span></p>
<p>Now we solve the system of two linear equations:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\beta_0 \sum_{i=1}^{n} w_i + \beta_1 \sum_{i=1}^{n} w_i x_i = \sum_{i=1}^{n} w_i y_i\)</span></li>
<li><span class="math inline">\(\beta_0 \sum_{i=1}^{n} w_i x_i + \beta_1 \sum_{i=1}^{n} w_i x_i^2 = \sum_{i=1}^{n} w_i y_i x_i\)</span></li>
</ol>
<p>Solve for <span class="math inline">\(\beta_1\)</span></p>
<p>Multiply equation (3) by <span class="math inline">\(\frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^n w_i}\)</span> and subtract it from equation (4) to eliminate <span class="math inline">\(\beta_0\)</span>:</p>
<p><span class="math display">\[\beta_1 \left( \sum_{i=1}^{n} w_i x_i^2 - \frac{\left( \sum_{i=1}^{n} w_i x_i \right)^2}{\sum_{i=1}^{n} w_i} \right) = \sum_{i=1}^{n} w_i y_i x_i - \frac{\sum_{i=1}^{n} w_i x_i \sum_{i=1}^{n} w_i y_i}{\sum_{i=1}^{n} w_i}\]</span></p>
<p>Thus, <span class="math inline">\(\beta_1\)</span> is:</p>
<p><span class="math display">\[\beta_1 = \frac{\sum_{i=1}^{n} w_i x_i y_i - \frac{\sum_{i=1}^{n} w_i x_i \sum_{i=1}^{n} w_i y_i}{\sum_{i=1}^{n} w_i}}{\sum_{i=1}^{n} w_i x_i^2 - \frac{\left( \sum_{i=1}^{n} w_i x_i \right)^2}{\sum_{i=1}^{n} w_i}}\]</span></p>
<p>Then the weighted estimators for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are:</p>
<p><span class="math display">\[\hat{\beta}_{w,1} = \frac{\sum_{i=1}^{n} w_i x_i y_i - \frac{\sum_{i=1}^{n} w_i x_i \sum_{i=1}^{n} w_i y_i}{\sum_{i=1}^{n} w_i}}{\sum_{i=1}^{n} w_i x_i^2 - \frac{\left( \sum_{i=1}^{n} w_i x_i \right)^2}{\sum_{i=1}^{n} w_i}},\]</span></p>
<p><span class="math display">\[\hat{\beta}_{w,0} = \frac{\sum_{i=1}^{n} w_i y_i - \beta_1 \sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^{n} w_i}.\]</span></p>
</div>
<div id="model-in-matrix-form" class="section level2 hasAnchor" number="4.9">
<h2><span class="header-section-number">4.9</span> Model in Matrix Form<a href="simple-linear-regression.html#model-in-matrix-form" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can specify the same model as in <a href="simple-linear-regression.html#model">Model</a>, in matrix form as follows:</p>
<p><span class="math display">\[\mathbf{y}= \mathbf{X}\boldsymbol{\beta}+ \mathbf{e}\]</span>
where:</p>
<ul>
<li><span class="math inline">\(\mathbf{y}={y_1,\ldots,y_n}&#39;\)</span></li>
<li><span class="math inline">\(\mathbf{e}={e_1,\ldots,e_n}&#39;\)</span></li>
<li><span class="math inline">\(\boldsymbol{\beta}={\beta_0,\beta_1}&#39;\)</span></li>
<li><span class="math inline">\(\mathbf{X}=[\mathbb{1}_n \mathbf{x}]\)</span></li>
<li><span class="math inline">\(\mathbb{1}_n={1,\ldots,1}&#39;\)</span> (1 <span class="math inline">\(n\)</span>-times).</li>
</ul>
<p>In this way we can re-write our minimization problem as follows:</p>
<p><span class="math display">\[ \min \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \min \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2
= \min \sum_{i=1}^{n} (\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})\]</span>
equivalent to:
<span class="math display">\[\min \mathbf{y}&#39;\mathbf{y}- \mathbf{y}&#39;\mathbf{X}\boldsymbol{\beta}- \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{y}+ \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{X}\boldsymbol{\beta}\]</span></p>
<p>The procedure to find the estimators is very similar, working with the gradient
instead of the partial derivatives:</p>
<p><span class="math display">\[\nabla_\beta Q = \mathbf{0} \]</span>
then:
<span class="math display">\[-\mathbf{X}&#39;\mathbf{y}-\mathbf{X}&#39;\mathbf{y}+ 2 \mathbf{X}&#39; \mathbf{X}\boldsymbol{\beta}= \mathbf{0}\]</span>
<span class="math display">\[2 \mathbf{X}&#39; \mathbf{X}\boldsymbol{\beta}= 2 \mathbf{X}&#39;\mathbf{y}\]</span>
<span class="math display">\[\mathbf{X}&#39; \mathbf{X}\boldsymbol{\beta}= \mathbf{X}&#39;\mathbf{y}\]</span>
<span class="math display">\[\boldsymbol{\beta}= \left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}\mathbf{X}&#39;\mathbf{y}\]</span></p>
<p>Notice how we find the estimates much easier. It is also easy to show that a
minimum is attained by computing the <strong>Hessian Matrix</strong> of <span class="math inline">\(Q\)</span></p>
<p><span class="math display">\[H_Q = 2 \mathbf{X}&#39; \mathbf{X}\]</span></p>
<p>and noticing that:</p>
<p><span class="math display">\[\mathbf{X}&#39; \mathbf{X}\]</span></p>
<p>is <a href="prerequisites.html#positive-definite-matrix">positive-definite</a> if <span class="math inline">\(\mathbf{x}\)</span> is not a constant vector.</p>
<p>We can verify that indeed the estimate <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the same as before by
manually doing the computations for this case:</p>
<p>We have:</p>
<p><span class="math display">\[
   \mathbf{X} = \begin{bmatrix}
   1 &amp; x_1 \\
   1 &amp; x_2 \\
   \vdots &amp; \vdots \\
   1 &amp; x_n
   \end{bmatrix}
   \]</span></p>
<p><span class="math display">\[
   \mathbf{X}&#39; = \begin{bmatrix}
   1 &amp; 1 &amp; \cdots &amp; 1 \\
   x_1 &amp; x_2 &amp; \cdots &amp; x_n
   \end{bmatrix}
   \]</span></p>
<p>Multiplying <span class="math inline">\(\mathbf{X}&#39;\)</span> by <span class="math inline">\(\mathbf{X}\)</span>:</p>
<p><span class="math display">\[
   \mathbf{X}&#39; \mathbf{X} = \begin{bmatrix}
   n &amp; \sum_{i=1}^n x_i \\
   \sum_{i=1}^n x_i &amp; \sum_{i=1}^n x_i^2
   \end{bmatrix}
   \]</span>
Similarly, we have:</p>
<p><span class="math display">\[
   \mathbf{y} = \begin{bmatrix}
   y_1 \\
   y_2 \\
   \vdots \\
   y_n
   \end{bmatrix}
   \]</span></p>
<p>Multiplying <span class="math inline">\(\mathbf{X}&#39;\)</span> by <span class="math inline">\(\mathbf{y}\)</span>:</p>
<p><span class="math display">\[
   \mathbf{X}&#39; \mathbf{y} = \begin{bmatrix}
   \sum_{i=1}^n y_i \\
   \sum_{i=1}^n x_i y_i
   \end{bmatrix}
   \]</span></p>
<p>To compute the inverse of <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span> we have that the determinant is:</p>
<p><span class="math display">\[
   |\mathbf{X}&#39; \mathbf{X}|=\text{det}(\mathbf{X}^\top \mathbf{X}) = n \sum_{i=1}^n x_i^2 - \left( \sum_{i=1}^n x_i \right)^2
   \]</span></p>
<p>Then, the inverse is:</p>
<p><span class="math display">\[
   (\mathbf{X}&#39; \mathbf{X})^{-1} = \frac{1}{n \sum_{i=1}^n x_i^2 - \left( \sum_{i=1}^n x_i \right)^2}
   \begin{bmatrix}
   \sum_{i=1}^n x_i^2 &amp; - \sum_{i=1}^n x_i \\
   - \sum_{i=1}^n x_i &amp; n
   \end{bmatrix}
   \]</span></p>
<p>Then, we have that:</p>
<p><span class="math display">\[\begin{align*}
   (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
      &amp;= \frac{1}{n \sum_{i=1}^n x_i^2 - \left( \sum_{i=1}^n x_i \right)^2}
   \begin{bmatrix}
   \sum_{i=1}^n x_i^2 &amp; - \sum_{i=1}^n x_i \\
   - \sum_{i=1}^n x_i &amp; n
   \end{bmatrix}
   \begin{bmatrix}
   \sum_{i=1}^n y_i \\
   \sum_{i=1}^n x_i y_i
   \end{bmatrix} \\
      &amp;= \begin{bmatrix}
   \frac{1}{n \sum_{i=1}^n x_i^2 - \left( \sum_{i=1}^n x_i \right)^2} \left( \sum_{i=1}^n x_i^2 \sum_{i=1}^n y_i - \sum_{i=1}^n x_i \sum_{i=1}^n x_i y_i \right) \\
   \frac{1}{n \sum_{i=1}^n x_i^2 - \left( \sum_{i=1}^n x_i \right)^2} \left( - \sum_{i=1}^n x_i \sum_{i=1}^n y_i + n \sum_{i=1}^n x_i y_i \right)
   \end{bmatrix} \\
      &amp;= \begin{bmatrix}
   \frac{1}{n \sum_{i=1}^n x_i^2 - (n \bar{x})^2} \left( n \bar{y} \sum_{i=1}^n x_i^2 - n \bar{x} \sum_{i=1}^n x_i y_i \right) \\
   \frac{1}{n \sum_{i=1}^n x_i^2 - (n \bar{x})^2} \left( - n \bar{x}n \bar{y} + n \sum_{i=1}^n x_i y_i \right)
   \end{bmatrix} \\

\end{align*}\]</span></p>
<p>Now simplifying, the first row, we have that:</p>
<p><span class="math display">\[\begin{align*}
\frac{n \bar{y} \sum_{i=1}^n x_i^2 - n \bar{x} \sum_{i=1}^n x_i y_i}{n \sum_{i=1}^n x_i^2 - (n \bar{x})^2}
  &amp;= \frac{1}{\sum_{i=1}^n x_i^2 - n\bar{x}^2} \left( \bar{y} \sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i y_i \right) \\
  &amp;= \frac{1}{\sum_{i=1}^n x_i^2 - n\bar{x}^2} \left( \bar{y} \sum_{i=1}^n x_i^2 - \bar{y} n\bar{x}^2 + \bar{y} n\bar{x}^2   - \bar{x} \sum_{i=1}^n x_i y_i \right) \\
  &amp;= \frac{1}{\sum_{i=1}^n x_i^2 - n\bar{x}^2} \left( \bar{y} \sum_{i=1}^n x_i^2 - \bar{y} n\bar{x}^2 + \bar{y} n\bar{x}^2   - \bar{x} \sum_{i=1}^n x_i y_i \right) \\
  &amp;= \frac{1}{\sum_{i=1}^n x_i^2 - n\bar{x}^2} \left( \bar{y} \left(\sum_{i=1}^n x_i^2 - n\bar{x}^2 \right) + \bar{x} \left(n\bar{y}\bar{x}   - \sum_{i=1}^n x_i y_i \right) \right) \\
  &amp;= \bar{y} + \bar{x} \frac{n\bar{y}\bar{x} - \sum_{i=1}^n x_i y_i}{\sum_{i=1}^n x_i^2 - n\bar{x}^2} \\
  &amp;= \bar{y} + \bar{x} \frac{-(n-1)S_{xy}}{(n-1)S_{xx}} \\
  &amp;= \bar{y} - \beta_1 \bar{x}
\end{align*}\]</span></p>
<p>And simplifying the second row, we have:</p>
<p><span class="math display">\[\frac{1}{n \sum_{i=1}^n x_i^2 - (n \bar{x})^2} \left( - n \bar{x}n \bar{y} + n \sum_{i=1}^n x_i y_i \right) = \frac{1}{\sum_{i=1}^n x_i^2 - n \bar{x}^2} \left( - n\bar{x}\bar{y} + \sum_{i=1}^n x_i y_i \right) = \frac{S_{xy}}{S_{xx}} = \beta_1\]</span></p>
<p>So both expressions are equivalent.</p>
<div id="weighted-least-squares-in-matrix-form" class="section level3 hasAnchor" number="4.9.1">
<h3><span class="header-section-number">4.9.1</span> Weighted Least Squares in Matrix Form<a href="simple-linear-regression.html#weighted-least-squares-in-matrix-form" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In a similar way we can solve the weighted least squares problem in matrix form
as follows:</p>
<p><span class="math display">\[Q_w = \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i)^2 = (\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;\mathbf{W}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) = \mathbf{y}&#39;\mathbf{W}\mathbf{y}-\mathbf{y}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}- \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{W}\mathbf{y}+ \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}\]</span></p>
<p>where, <span class="math inline">\(\mathbf{W}\)</span> is a diagonal matrix with diagonal entries <span class="math inline">\(w_1,\ldots,w_n\)</span>.</p>
<p>then the gradient is:</p>
<p><span class="math display">\[ \nabla_\beta Q_w = -2 \mathbf{X}&#39;\mathbf{W}\mathbf{y}+ 2\mathbf{X}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}\]</span></p>
<p>then, making it equal to zero, we have that:</p>
<p><span class="math display">\[ 2\mathbf{X}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}= 2 \mathbf{X}&#39;\mathbf{W}\mathbf{y}\]</span>
<span class="math display">\[\mathbf{X}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}= \mathbf{X}&#39;\mathbf{W}\mathbf{y}\]</span>
<span class="math display">\[\boldsymbol{\beta}= \left(\mathbf{X}&#39;\mathbf{W}\mathbf{X}\right)^{-1}\mathbf{X}&#39;\mathbf{W}\mathbf{y}\]</span></p>
<p>And the Hessian Matrix is given by:</p>
<p><span class="math display">\[H_{Q_w} = 2 \mathbf{X}&#39;\mathbf{W}\mathbf{X}\]</span></p>
<p>which is positive-definite under the same conditions that before and if the entries
of <span class="math inline">\(\mathbf{W}\)</span> are positive.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="polynomial-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
