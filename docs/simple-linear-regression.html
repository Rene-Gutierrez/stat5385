<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Simple Linear Regression | _main.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Simple Linear Regression | _main.knit" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Simple Linear Regression | _main.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="polynomial-regression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Stat 5385/6385 Fall 2025</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#calendar"><i class="fa fa-check"></i><b>1.1</b> Calendar</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#important-dates"><i class="fa fa-check"></i><b>1.1.1</b> Important Dates</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#class-schedule"><i class="fa fa-check"></i><b>1.1.2</b> Class Schedule</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#course-overview"><i class="fa fa-check"></i><b>1.2</b> Course Overview</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#chapter-2-mathematical-prerequisites"><i class="fa fa-check"></i><b>1.2.1</b> Chapter 2 — Mathematical Prerequisites</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#chapter-3-the-linear-regression-problem"><i class="fa fa-check"></i><b>1.2.2</b> Chapter 3 — The Linear Regression Problem</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#chapters-4-to-6-linear-regression-as-an-optimization-problem"><i class="fa fa-check"></i><b>1.2.3</b> Chapters 4 to 6 — Linear Regression as an Optimization Problem</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#chapter-7-introducing-uncertainty"><i class="fa fa-check"></i><b>1.2.4</b> Chapter 7 — Introducing Uncertainty</a></li>
<li class="chapter" data-level="1.2.5" data-path="index.html"><a href="index.html#chapters-8-and-9-probabilistic-modeling-and-statistical-inference"><i class="fa fa-check"></i><b>1.2.5</b> Chapters 8 and 9 — Probabilistic Modeling and Statistical Inference</a></li>
<li class="chapter" data-level="1.2.6" data-path="index.html"><a href="index.html#summary"><i class="fa fa-check"></i><b>1.2.6</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#general-math"><i class="fa fa-check"></i><b>2.1</b> General Math</a></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#zero-matrix"><i class="fa fa-check"></i><b>2.2.1</b> Zero Matrix</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.2.2</b> Linear Independence</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.2.3</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.2.4</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.2.5" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.2.5</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.2.6" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.2.6</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.2.7" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.2.7</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.2.8" data-path="prerequisites.html"><a href="prerequisites.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.2.8</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="2.2.9" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.2.9</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.2.10" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.2.10</b> Idempotent Matrix</a></li>
<li class="chapter" data-level="2.2.11" data-path="prerequisites.html"><a href="prerequisites.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>2.2.11</b> Determinant of a Matrix</a></li>
<li class="chapter" data-level="2.2.12" data-path="prerequisites.html"><a href="prerequisites.html#trace-of-a-matrix"><i class="fa fa-check"></i><b>2.2.12</b> Trace of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.3</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.3.1</b> Gradient</a></li>
<li class="chapter" data-level="2.3.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.3.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.3.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.3.3</b> Applications:</a></li>
<li class="chapter" data-level="2.3.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.3.4</b> Matrix Calculus</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.4</b> Probability</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="prerequisites.html"><a href="prerequisites.html#expected-value"><i class="fa fa-check"></i><b>2.4.1</b> Expected Value</a></li>
<li class="chapter" data-level="2.4.2" data-path="prerequisites.html"><a href="prerequisites.html#variance"><i class="fa fa-check"></i><b>2.4.2</b> Variance</a></li>
<li class="chapter" data-level="2.4.3" data-path="prerequisites.html"><a href="prerequisites.html#cross-covariance-matrix"><i class="fa fa-check"></i><b>2.4.3</b> Cross-Covariance Matrix</a></li>
<li class="chapter" data-level="2.4.4" data-path="prerequisites.html"><a href="prerequisites.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.4.4</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="2.4.5" data-path="prerequisites.html"><a href="prerequisites.html#chi2-distribution"><i class="fa fa-check"></i><b>2.4.5</b> <span class="math inline">\(\chi^2\)</span> Distribution</a></li>
<li class="chapter" data-level="2.4.6" data-path="prerequisites.html"><a href="prerequisites.html#t-distribution"><i class="fa fa-check"></i><b>2.4.6</b> <span class="math inline">\(t\)</span> Distribution</a></li>
<li class="chapter" data-level="2.4.7" data-path="prerequisites.html"><a href="prerequisites.html#f-distribution"><i class="fa fa-check"></i><b>2.4.7</b> <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.5</b> Statistics</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="prerequisites.html"><a href="prerequisites.html#bias-of-an-estimator"><i class="fa fa-check"></i><b>2.5.1</b> Bias of an Estimator</a></li>
<li class="chapter" data-level="2.5.2" data-path="prerequisites.html"><a href="prerequisites.html#unbiased-estimator"><i class="fa fa-check"></i><b>2.5.2</b> Unbiased Estimator</a></li>
<li class="chapter" data-level="2.5.3" data-path="prerequisites.html"><a href="prerequisites.html#mean-square-error-of-an-estimator"><i class="fa fa-check"></i><b>2.5.3</b> Mean Square Error of an Estimator</a></li>
<li class="chapter" data-level="2.5.4" data-path="prerequisites.html"><a href="prerequisites.html#interval-estimation"><i class="fa fa-check"></i><b>2.5.4</b> Interval Estimation</a></li>
<li class="chapter" data-level="2.5.5" data-path="prerequisites.html"><a href="prerequisites.html#hypothesis-testing"><i class="fa fa-check"></i><b>2.5.5</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="2.5.6" data-path="prerequisites.html"><a href="prerequisites.html#power-analysis"><i class="fa fa-check"></i><b>2.5.6</b> Power Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#objectives-of-linear-regression"><i class="fa fa-check"></i><b>3.1</b> Objectives of Linear Regression</a></li>
<li class="chapter" data-level="3.2" data-path="introduction.html"><a href="introduction.html#examples-1"><i class="fa fa-check"></i><b>3.2</b> Examples</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="introduction.html"><a href="introduction.html#ad-spending"><i class="fa fa-check"></i><b>3.2.1</b> Ad Spending</a></li>
<li class="chapter" data-level="3.2.2" data-path="introduction.html"><a href="introduction.html#winw-example"><i class="fa fa-check"></i><b>3.2.2</b> Wine and Life Expectancy</a></li>
<li class="chapter" data-level="3.2.3" data-path="introduction.html"><a href="introduction.html#burger-demand"><i class="fa fa-check"></i><b>3.2.3</b> Burger Demand</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#intro-to-slr"><i class="fa fa-check"></i><b>4.1</b> Intro to SLR</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#a-path-not-taken"><i class="fa fa-check"></i><b>4.1.1</b> A path not Taken</a></li>
<li class="chapter" data-level="4.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#slr-model"><i class="fa fa-check"></i><b>4.1.2</b> SLR Model</a></li>
<li class="chapter" data-level="4.1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#possible-optimization-problems"><i class="fa fa-check"></i><b>4.1.3</b> Possible Optimization Problems</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>4.2</b> Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#other-estimated-quantites"><i class="fa fa-check"></i><b>4.2.1</b> Other estimated quantites</a></li>
<li class="chapter" data-level="4.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#properties-of-the-slr-problem"><i class="fa fa-check"></i><b>4.2.2</b> Properties of the SLR problem</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#properties-of-the-estimates"><i class="fa fa-check"></i><b>4.3</b> Properties of the Estimates</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hatbeta_0-and-hatbeta_1-are-linear-combinations-of"><i class="fa fa-check"></i><b>4.3.1</b> <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are linear combinations of</a></li>
<li class="chapter" data-level="4.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-sum-of-the-residuals-is-0"><i class="fa fa-check"></i><b>4.3.2</b> The sum of the residuals is <span class="math inline">\(0\)</span></a></li>
<li class="chapter" data-level="4.3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hatmathbfe-and-mathbfx-are-orthogonal"><i class="fa fa-check"></i><b>4.3.3</b> <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> are orthogonal</a></li>
<li class="chapter" data-level="4.3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hatmathbfy-and-hatmathbfe-are-orthogonal"><i class="fa fa-check"></i><b>4.3.4</b> <span class="math inline">\(\hat{\mathbf{y}}\)</span> and <span class="math inline">\(\hat{\mathbf{e}}\)</span> are orthogonal</a></li>
<li class="chapter" data-level="4.3.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-average-of-hatmathbfy-and-mathbfy-are-the-same"><i class="fa fa-check"></i><b>4.3.5</b> The average of <span class="math inline">\(\hat{\mathbf{y}}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are the same</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-and-standarizing-the-data"><i class="fa fa-check"></i><b>4.4</b> Centering and Standarizing the Data</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#remarks-on-centering-and-standarization"><i class="fa fa-check"></i><b>4.4.1</b> Remarks on Centering and Standarization</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary-centering-and-standarizing"><i class="fa fa-check"></i><b>4.4.2</b> Summary Centering and Standarizing</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-and-standarizing-in-slr"><i class="fa fa-check"></i><b>4.5</b> Centering and Standarizing in SLR</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centered-independent-variable"><i class="fa fa-check"></i><b>4.5.1</b> Centered Independent Variable</a></li>
<li class="chapter" data-level="4.5.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#both-variables-centered"><i class="fa fa-check"></i><b>4.5.2</b> Both Variables Centered</a></li>
<li class="chapter" data-level="4.5.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#both-variables-standarized"><i class="fa fa-check"></i><b>4.5.3</b> Both variables Standarized</a></li>
<li class="chapter" data-level="4.5.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary-of-centering-and-standarizing-in-slr"><i class="fa fa-check"></i><b>4.5.4</b> Summary of Centering and Standarizing in SLR</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>4.6</b> Coefficient of Determination</a></li>
<li class="chapter" data-level="4.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residual-analysis"><i class="fa fa-check"></i><b>4.7</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#non-linear-regression-function"><i class="fa fa-check"></i><b>4.7.1</b> Non-linear regression function</a></li>
<li class="chapter" data-level="4.7.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#heteroscedasticity"><i class="fa fa-check"></i><b>4.7.2</b> Heteroscedasticity</a></li>
<li class="chapter" data-level="4.7.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outliers"><i class="fa fa-check"></i><b>4.7.3</b> Outliers</a></li>
<li class="chapter" data-level="4.7.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#variables-ommited"><i class="fa fa-check"></i><b>4.7.4</b> Variables Ommited</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#cross-validation"><i class="fa fa-check"></i><b>4.8</b> Cross-Validation</a></li>
<li class="chapter" data-level="4.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>4.9</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="4.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-in-matrix-form"><i class="fa fa-check"></i><b>4.10</b> Model in Matrix Form</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares-in-matrix-form"><i class="fa fa-check"></i><b>4.10.1</b> Weighted Least Squares in Matrix Form</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="polynomial-regression.html"><a href="polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Polynomial Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="polynomial-regression.html"><a href="polynomial-regression.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="polynomial-regression.html"><a href="polynomial-regression.html#taylor-polynomials-and-polynomial-regression"><i class="fa fa-check"></i><b>5.1.1</b> Taylor polynomials and polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="polynomial-regression.html"><a href="polynomial-regression.html#why-use-the-sample-mean-as-the-expansion-center-point"><i class="fa fa-check"></i><b>5.2</b> Why use the sample mean as the expansion (center) point?</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="polynomial-regression.html"><a href="polynomial-regression.html#examples-2"><i class="fa fa-check"></i><b>5.2.1</b> Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#the-general-model"><i class="fa fa-check"></i><b>6.1.1</b> The General Model</a></li>
<li class="chapter" data-level="6.1.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#interpretation-and-objectives"><i class="fa fa-check"></i><b>6.1.2</b> Interpretation and Objectives</a></li>
<li class="chapter" data-level="6.1.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#applications-and-context"><i class="fa fa-check"></i><b>6.1.3</b> Applications and Context</a></li>
<li class="chapter" data-level="6.1.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#connection-to-polynomial-regression"><i class="fa fa-check"></i><b>6.1.4</b> Connection to Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#example-modeling-gdp"><i class="fa fa-check"></i><b>6.2</b> Example: Modeling GDP</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#simple-regressions"><i class="fa fa-check"></i><b>6.2.1</b> Simple Regressions</a></li>
<li class="chapter" data-level="6.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-regression"><i class="fa fa-check"></i><b>6.2.2</b> Multiple Regression</a></li>
<li class="chapter" data-level="6.2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#summary-2"><i class="fa fa-check"></i><b>6.2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#least-squares-estimation-1"><i class="fa fa-check"></i><b>6.3</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#properties-of-the-estimates-1"><i class="fa fa-check"></i><b>6.4</b> Properties of the Estimates</a></li>
<li class="chapter" data-level="6.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-r2"><i class="fa fa-check"></i><b>6.5</b> Multiple <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="6.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#geometric-interpretation-of-multiple-linear-regression"><i class="fa fa-check"></i><b>6.6</b> Geometric Interpretation of Multiple Linear Regression</a></li>
<li class="chapter" data-level="6.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#centered-and-standarized-variables"><i class="fa fa-check"></i><b>6.7</b> Centered and Standarized Variables</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#centered-variables"><i class="fa fa-check"></i><b>6.7.1</b> Centered Variables</a></li>
<li class="chapter" data-level="6.7.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sample-covariance"><i class="fa fa-check"></i><b>6.7.2</b> Sample Covariance</a></li>
<li class="chapter" data-level="6.7.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#satandard-variables"><i class="fa fa-check"></i><b>6.7.3</b> Satandard Variables</a></li>
<li class="chapter" data-level="6.7.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sample-correlation-matrix"><i class="fa fa-check"></i><b>6.7.4</b> Sample Correlation Matrix</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#variable-cross-effects"><i class="fa fa-check"></i><b>6.8</b> Variable Cross-Effects</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#single-variable-cross-effects"><i class="fa fa-check"></i><b>6.8.1</b> Single Variable Cross-Effects</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#outliers-and-leverage"><i class="fa fa-check"></i><b>6.9</b> Outliers and Leverage</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#leverage"><i class="fa fa-check"></i><b>6.9.1</b> Leverage</a></li>
<li class="chapter" data-level="6.9.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#influential-observations"><i class="fa fa-check"></i><b>6.9.2</b> Influential Observations</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#stability-of-the-solution"><i class="fa fa-check"></i><b>6.10</b> Stability of the Solution</a></li>
<li class="chapter" data-level="6.11" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#other-approaches-to-regression"><i class="fa fa-check"></i><b>6.11</b> Other Approaches to Regression</a>
<ul>
<li class="chapter" data-level="6.11.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#least-absolute-error-regression"><i class="fa fa-check"></i><b>6.11.1</b> Least Absolute Error Regression</a></li>
<li class="chapter" data-level="6.11.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#ridge-regression"><i class="fa fa-check"></i><b>6.11.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="6.11.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#lasso-regression"><i class="fa fa-check"></i><b>6.11.3</b> Lasso Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bootstrapping.html"><a href="bootstrapping.html"><i class="fa fa-check"></i><b>7</b> Bootstrapping</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bootstrapping.html"><a href="bootstrapping.html#introduction-2"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="bootstrapping.html"><a href="bootstrapping.html#key-points-6"><i class="fa fa-check"></i><b>7.1.1</b> Key Points</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bootstrapping.html"><a href="bootstrapping.html#bootstrapping-example"><i class="fa fa-check"></i><b>7.2</b> Bootstrapping Example</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="bootstrapping.html"><a href="bootstrapping.html#bootstrapping-predictions"><i class="fa fa-check"></i><b>7.2.1</b> Bootstrapping Predictions</a></li>
<li class="chapter" data-level="7.2.2" data-path="bootstrapping.html"><a href="bootstrapping.html#adding-prediction-intervals"><i class="fa fa-check"></i><b>7.2.2</b> Adding Prediction Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html"><i class="fa fa-check"></i><b>8</b> Mean and Varaince Assumptions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#mean-assumptions"><i class="fa fa-check"></i><b>8.1</b> Mean Assumptions</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-mathbfy"><i class="fa fa-check"></i><b>8.1.1</b> Expectation of <span class="math inline">\(\mathbf{y}\)</span>:</a></li>
<li class="chapter" data-level="8.1.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-hatboldsymbolbeta"><i class="fa fa-check"></i><b>8.1.2</b> Expectation of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
<li class="chapter" data-level="8.1.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-hatmathbfy"><i class="fa fa-check"></i><b>8.1.3</b> Expectation of <span class="math inline">\(\hat{\mathbf{y}}\)</span></a></li>
<li class="chapter" data-level="8.1.4" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-hatmathbfe"><i class="fa fa-check"></i><b>8.1.4</b> Expectation of <span class="math inline">\(\hat{\mathbf{e}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-assumptions"><i class="fa fa-check"></i><b>8.2</b> Variance Assumptions</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-mathbfy"><i class="fa fa-check"></i><b>8.2.1</b> Variance of <span class="math inline">\(\mathbf{y}\)</span></a></li>
<li class="chapter" data-level="8.2.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-hatboldsymbolbeta"><i class="fa fa-check"></i><b>8.2.2</b> Variance of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
<li class="chapter" data-level="8.2.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-hatmathbfy"><i class="fa fa-check"></i><b>8.2.3</b> Variance of <span class="math inline">\(\hat{\mathbf{y}}\)</span></a></li>
<li class="chapter" data-level="8.2.4" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-hatmathbfe"><i class="fa fa-check"></i><b>8.2.4</b> Variance of <span class="math inline">\(\hat{\mathbf{e}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#cross-covariances"><i class="fa fa-check"></i><b>8.3</b> Cross-Covariances</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#cross-covaraince-of-haty-and-hate"><i class="fa fa-check"></i><b>8.3.1</b> Cross-covaraince of <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{e}\)</span></a></li>
<li class="chapter" data-level="8.3.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#cross-covaraince-of-haty-and-hatboldsymbolbeta"><i class="fa fa-check"></i><b>8.3.2</b> Cross-covaraince of <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#gauss-markov-theorem"><i class="fa fa-check"></i><b>8.4</b> Gauss-Markov Theorem</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#assumptions"><i class="fa fa-check"></i><b>8.4.1</b> Assumptions</a></li>
<li class="chapter" data-level="8.4.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#statement"><i class="fa fa-check"></i><b>8.4.2</b> Statement</a></li>
<li class="chapter" data-level="8.4.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#proof"><i class="fa fa-check"></i><b>8.4.3</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#estimate-of-sigma2"><i class="fa fa-check"></i><b>8.5</b> Estimate of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="normality-assumption.html"><a href="normality-assumption.html"><i class="fa fa-check"></i><b>9</b> Normality Assumption</a>
<ul>
<li class="chapter" data-level="9.1" data-path="normality-assumption.html"><a href="normality-assumption.html#introduction-3"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="normality-assumption.html"><a href="normality-assumption.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.2</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="normality-assumption.html"><a href="normality-assumption.html#decomposition-of-the-quadratic-form"><i class="fa fa-check"></i><b>9.2.1</b> Decomposition of the Quadratic Form</a></li>
<li class="chapter" data-level="9.2.2" data-path="normality-assumption.html"><a href="normality-assumption.html#estimation-of-sigma2"><i class="fa fa-check"></i><b>9.2.2</b> Estimation of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="9.2.3" data-path="normality-assumption.html"><a href="normality-assumption.html#remarks-on-bias-and-practical-use"><i class="fa fa-check"></i><b>9.2.3</b> Remarks on Bias and Practical Use</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="normality-assumption.html"><a href="normality-assumption.html#distribution-of-the-estimates"><i class="fa fa-check"></i><b>9.3</b> Distribution of the Estimates</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="normality-assumption.html"><a href="normality-assumption.html#distribution-of-hatboldsymbolbeta-hatmathbfy-and-hatmathbfe"><i class="fa fa-check"></i><b>9.3.1</b> Distribution of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, <span class="math inline">\(\hat{\mathbf{y}}\)</span>, and <span class="math inline">\(\hat{\mathbf{e}}\)</span></a></li>
<li class="chapter" data-level="9.3.2" data-path="normality-assumption.html"><a href="normality-assumption.html#distribution-of-hatsigma2"><i class="fa fa-check"></i><b>9.3.2</b> Distribution of <span class="math inline">\(\hat{\sigma}^2\)</span></a></li>
<li class="chapter" data-level="9.3.3" data-path="normality-assumption.html"><a href="normality-assumption.html#independence-of-hatmathbfe-and-hatmathbfy"><i class="fa fa-check"></i><b>9.3.3</b> Independence of <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="normality-assumption.html"><a href="normality-assumption.html#interval-estimation-1"><i class="fa fa-check"></i><b>9.4</b> Interval Estimation</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="normality-assumption.html"><a href="normality-assumption.html#confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i><b>9.4.1</b> Confidence Intervals for the Coefficients</a></li>
<li class="chapter" data-level="9.4.2" data-path="normality-assumption.html"><a href="normality-assumption.html#confidence-intervals-for-the-expected-mean-of-a-new-observation-mathbfx_new"><i class="fa fa-check"></i><b>9.4.2</b> Confidence Intervals for the Expected Mean of a New Observation <span class="math inline">\(\mathbf{x}_{new}\)</span></a></li>
<li class="chapter" data-level="9.4.3" data-path="normality-assumption.html"><a href="normality-assumption.html#confidence-intervals-for-linear-combinations-of-boldsymbolbeta"><i class="fa fa-check"></i><b>9.4.3</b> Confidence Intervals for Linear Combinations of <span class="math inline">\(\boldsymbol{\beta}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="normality-assumption.html"><a href="normality-assumption.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>9.5</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="normality-assumption.html"><a href="normality-assumption.html#testing-for-the-overall-regression"><i class="fa fa-check"></i><b>9.5.1</b> Testing for the Overall Regression</a></li>
<li class="chapter" data-level="9.5.2" data-path="normality-assumption.html"><a href="normality-assumption.html#testing-if-one-variable-is-not-relevant"><i class="fa fa-check"></i><b>9.5.2</b> Testing if one variable is not relevant</a></li>
<li class="chapter" data-level="9.5.3" data-path="normality-assumption.html"><a href="normality-assumption.html#testing-if-a-subgroup-of-the-variables-is-relevant"><i class="fa fa-check"></i><b>9.5.3</b> Testing if a Subgroup of the Variables is Relevant</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="normality-assumption.html"><a href="normality-assumption.html#power-analysis-1"><i class="fa fa-check"></i><b>9.6</b> Power Analysis</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="normality-assumption.html"><a href="normality-assumption.html#power-for-testing-a-single-coefficient"><i class="fa fa-check"></i><b>9.6.1</b> Power for Testing a Single Coefficient</a></li>
<li class="chapter" data-level="9.6.2" data-path="normality-assumption.html"><a href="normality-assumption.html#power-for-testing-multiple-coefficients-partial-f-tests"><i class="fa fa-check"></i><b>9.6.2</b> Power for Testing Multiple Coefficients (Partial F-Tests)</a></li>
<li class="chapter" data-level="9.6.3" data-path="normality-assumption.html"><a href="normality-assumption.html#the-role-of-the-design-matrix"><i class="fa fa-check"></i><b>9.6.3</b> The Role of the Design Matrix</a></li>
<li class="chapter" data-level="9.6.4" data-path="normality-assumption.html"><a href="normality-assumption.html#practical-applications"><i class="fa fa-check"></i><b>9.6.4</b> Practical Applications</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple-linear-regression" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Simple Linear Regression<a href="simple-linear-regression.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="intro-to-slr" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Intro to SLR<a href="simple-linear-regression.html#intro-to-slr" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Simple linear regression (<strong>SLR</strong>) is a linear regression model with a single explanatory variable. It focuses on the linear relationship between one independent variable and one dependent variable, making it the most basic form of linear regression analysis.</p>
<hr />
<div id="a-path-not-taken" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> A path not Taken<a href="simple-linear-regression.html#a-path-not-taken" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The basic idea behind linear regression is to find the “best line” that describes the data. This idea admits several interpretations. One interpretation is to measure the distance between a candidate line and the data points. For a line <span class="math inline">\(\mathcal{l}\)</span> given by</p>
<p><span class="math display">\[
ax + by + c = 0
\]</span></p>
<p>the distance from a point <span class="math inline">\((x_i,y_i)\)</span> to that line is</p>
<p><span class="math display">\[
d_i = d(\mathcal{l},(x_i,y_i)) = \frac{|ax_i + by_i + c|}{\sqrt{a^2+b^2}}.
\]</span></p>
<p>We could then pose the optimization problem</p>
<p><span class="math display">\[
\min_{a,b,c} \sum_{i=1}^n d_i.
\]</span></p>
<p>However, those familiar with regression will recognize that this is not the standard linear regression formulation. That does not make the distance-minimization problem unreasonable, only that its solution will generally differ from the usual linear regression solution.</p>
<p>Linear regression, as commonly defined, is a particular optimization choice made because of its convenient properties. In the next section we introduce a model that refines the notion of a “best line.”</p>
<hr />
</div>
<div id="slr-model" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> SLR Model<a href="simple-linear-regression.html#slr-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The model for simple linear regression is as follows:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_i + e_i, \quad i\in\{1,\ldots,n\}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y_i\)</span> represents the <span class="math inline">\(i\)</span>-th observation of the dependent variable.</li>
<li><span class="math inline">\(x_i\)</span> represents the <span class="math inline">\(i\)</span>-th observation of the independent variable.</li>
<li><span class="math inline">\(e_i\)</span> represents the <span class="math inline">\(i\)</span>-th observation of the error term.</li>
<li><span class="math inline">\(\beta_0\)</span> is the intercept of the linear model, or regression line.</li>
<li><span class="math inline">\(\beta_1\)</span> is the slope of the linear model, or regression line.</li>
<li><span class="math inline">\(n\)</span> is the number of observations for both variables.</li>
</ul>
<p>Notice that in this framework, the error is not measuring the distance from the point
to the regression line, but the vertical distance from the point to the regression line.
Also, note that we are not making any assumptions about the error terms.</p>
<p>In the case of the <a href="#wine-example">wine example</a>, we generated the data based on the following linear model:</p>
<p><span class="math display">\[y_i = 75 + 1.5 x_i + e_i \]</span></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="simple-linear-regression.html#cb16-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">&quot;Wine Data.csv&quot;</span>)</span>
<span id="cb16-2"><a href="simple-linear-regression.html#cb16-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> dat<span class="sc">$</span>Glasses,</span>
<span id="cb16-3"><a href="simple-linear-regression.html#cb16-3" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>Years,</span>
<span id="cb16-4"><a href="simple-linear-regression.html#cb16-4" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Avg. Glasses of Wine per Week&quot;</span>,</span>
<span id="cb16-5"><a href="simple-linear-regression.html#cb16-5" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Life Expectancy (Years)&quot;</span>)</span>
<span id="cb16-6"><a href="simple-linear-regression.html#cb16-6" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> <span class="dv">75</span>,</span>
<span id="cb16-7"><a href="simple-linear-regression.html#cb16-7" tabindex="-1"></a>       <span class="at">b   =</span> <span class="fl">1.5</span>,</span>
<span id="cb16-8"><a href="simple-linear-regression.html#cb16-8" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb16-9"><a href="simple-linear-regression.html#cb16-9" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb16-10"><a href="simple-linear-regression.html#cb16-10" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v   =</span> <span class="dv">0</span>,</span>
<span id="cb16-11"><a href="simple-linear-regression.html#cb16-11" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb16-12"><a href="simple-linear-regression.html#cb16-12" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x =</span> <span class="fl">0.25</span>, <span class="at">y =</span> <span class="dv">76</span>, <span class="fu">expression</span>(beta[<span class="dv">0</span>] <span class="sc">~</span> <span class="st">&quot;=75&quot;</span>))</span>
<span id="cb16-13"><a href="simple-linear-regression.html#cb16-13" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x =</span> <span class="fl">3.25</span>, <span class="at">y =</span> <span class="dv">79</span>, <span class="fu">expression</span>(beta[<span class="dv">1</span>] <span class="sc">~</span> <span class="st">&quot;=1.5&quot;</span>))</span>
<span id="cb16-14"><a href="simple-linear-regression.html#cb16-14" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>),</span>
<span id="cb16-15"><a href="simple-linear-regression.html#cb16-15" tabindex="-1"></a>         <span class="at">x1 =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb16-16"><a href="simple-linear-regression.html#cb16-16" tabindex="-1"></a>         <span class="at">y0 =</span> <span class="fu">c</span>(<span class="dv">78</span>, <span class="dv">78</span>),</span>
<span id="cb16-17"><a href="simple-linear-regression.html#cb16-17" tabindex="-1"></a>         <span class="at">y1 =</span> <span class="fu">c</span>(<span class="dv">78</span>, <span class="fl">79.5</span>),</span>
<span id="cb16-18"><a href="simple-linear-regression.html#cb16-18" tabindex="-1"></a>         <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb16-19"><a href="simple-linear-regression.html#cb16-19" tabindex="-1"></a>         <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/slr-parts-1.png" width="672" /></p>
<p>In this case, the intercept <span class="math inline">\(\beta_0\)</span> is meaningful, as it represents the expected number of years a person would live if they didn’t drink wine at all. However, depending on the data, the intercept may or may not have a meaningful interpretation. The slope <span class="math inline">\(\beta_1\)</span> indicates that for each additional glass of wine consumed per week, our model predicts an increase of 1.5 years in life expectancy.</p>
<p>In practice, we rarely know the true regression line. Instead, it must be estimated from the data. The goal is to find the “best” line that fits the data, where “best” means the line that minimizes the sum of squared errors (SSE) between the observed values and the values predicted by the model.</p>
<hr />
</div>
<div id="possible-optimization-problems" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Possible Optimization Problems<a href="simple-linear-regression.html#possible-optimization-problems" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Other than minimizing the sum of squared errors (SSE), we can consider the following alternative optimization problems. Here <span class="math inline">\(e_i(\beta_0,\beta_1)=y_i-(\beta_0+\beta_1 x_i)\)</span> denotes the residual.</p>
<div id="minimizing-the-sum-of-absolute-errors" class="section level4 hasAnchor" number="4.1.3.1">
<h4><span class="header-section-number">4.1.3.1</span> Minimizing the Sum of Absolute Errors<a href="simple-linear-regression.html#minimizing-the-sum-of-absolute-errors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
\min_{\beta_0, \beta_1} \sum_{i=1}^n |e_i(\beta_0, \beta_1)|
\]</span></p>
<p>This L1 criterion is common in the machine-learning community and is more robust to outliers than the SSE. Laplace was among the first to investigate this approach when developing early methods for fitting lines to data.</p>
</div>
<div id="minimizing-the-maximum-error" class="section level4 hasAnchor" number="4.1.3.2">
<h4><span class="header-section-number">4.1.3.2</span> Minimizing the Maximum Error<a href="simple-linear-regression.html#minimizing-the-maximum-error" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
\min_{\beta_0, \beta_1} \max_{i=1}^n |e_i(\beta_0, \beta_1)|
\]</span></p>
<p>The minimax approach focuses on reducing the largest error and is appropriate when large errors can be catastrophic. Laplace also explored this formulation in his early work.</p>
</div>
<div id="minimizing-the-sum-of-squared-errors" class="section level4 hasAnchor" number="4.1.3.3">
<h4><span class="header-section-number">4.1.3.3</span> Minimizing the Sum of Squared Errors<a href="simple-linear-regression.html#minimizing-the-sum-of-squared-errors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
\min_{\beta_0, \beta_1} \sum_{i=1}^n e_i(\beta_0, \beta_1)^2
\]</span></p>
<p>This is the standard formulation in simple linear regression (the least squares criterion), introduced by Legendre and Gauss (apparently independently). It remains widely used because of its convenient properties (closed-form solution, computational ease, and optimality under Gaussian error assumptions), which we will analyze later.</p>
<hr />
</div>
</div>
</div>
<div id="least-squares-estimation" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Least Squares Estimation<a href="simple-linear-regression.html#least-squares-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As explained before, we want to minimize the SSE. Define</p>
<p><span class="math display">\[Q(\beta_0, \beta_1) = \sum_{i=1}^n (e_i(\beta_0, \beta_1))^2
= \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2  \]</span></p>
<p>Because Q is differentiable (in fact a convex quadratic), we find its minimum by setting the gradient to zero.</p>
<p>The solution to this minimization problem is given by:</p>
<p><span class="math display">\[ \hat{\beta}_1 = \frac{\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i = 1}^n x_i^2 - n \bar{x}^2} \]</span>
<span class="math display">\[ \hat{\beta_0} = \bar{y} - \hat{\beta}_1 \bar{x}\]</span>
where we use <span class="math inline">\(\hat{}\)</span>, to denote the specific critical point. And we have
adopted the standard notation:
<span class="math display">\[\bar{x} = \frac{1}{n}\sum_{i}^n x_i\]</span>
and
<span class="math display">\[\bar{y} = \frac{1}{n}\sum_{i}^n y_i\]</span>.
It remains to see if this is indeed a minimum. One can check the second order conditions or argue
the quadratic form of the problem only admits a minimum.</p>
<details>
<summary>
<span style="color:green">Derivation</span>
</summary>
<p>We start with <span class="math inline">\(\beta_0\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial Q}{\partial \beta_0}
  &amp;= \frac{\partial}{\partial \beta_0} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2
    &amp; \text{def. } Q \\
  &amp;= \frac{\partial}{\partial \beta_0} \sum_{i=1}^n (y_i^2 + \beta_0^2 + \beta_1^2 x_i^2 - 2 \beta_0 y_i - 2 \beta_1 x_i y_i + 2 \beta_0 \beta_1 x_i)
    &amp; \text{expanding square} \\
  &amp;= \sum_{i=1}^n \frac{\partial}{\partial \beta_0} (y_i^2 + \beta_0^2 + \beta_1^2 x_i^2 - 2 \beta_0 y_i - 2 \beta_1 x_i y_i + 2 \beta_0 \beta_1 x_i)
    &amp; \text{linearity of derivative} \\
  &amp;= \sum_{i = 1}^n (2 \beta_0 - 2 y_i + 2 \beta_1 x_i)
    &amp; \text{applying the derivative} \\
  &amp;=  2 \sum_{i = 1}^n \beta_0 - 2 \sum_{i = 1}^n y_i + 2 \beta_1 \sum_{i = 1}^n x_i
    &amp; \text{linearity sum} \\
  &amp;=  2 \left( n \beta_0 - n \bar{y} + n \beta_1 \bar{x} \right)
    &amp; \text{def. } \bar{y}, \text{def. } \bar{x} \\
\end{align*}\]</span></p>
<p>Then we have that:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial Q}{\partial \beta_0} = 0
  &amp;\iff -2 \left( n \beta_0 - n \bar{y} + n\beta_1 \bar{x} \right) = 0 \notag \\
  &amp;\iff \beta_0 = \bar{y} - \beta_1 \bar{x} \tag{1}
\end{align}\]</span></p>
<p>And we can do a similar thing for <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial Q}{\partial \beta_1}
  &amp;= \frac{\partial}{\partial \beta_1} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2
    &amp; \text{def. } Q \\
  &amp;= \sum_{i=1}^n \frac{\partial}{\partial \beta_1} (y_i - \beta_0 - \beta_1 x_i)^2
    &amp; \text{linwarity of derivative} \\
  &amp;= \sum_{i = 1}^n 2(y_i -\beta_0 - \beta_1 x_i)(-x_i)
    &amp; \text{applying the derivative} \\
  &amp;= -2\sum_{i = 1}^n y_i x_i + 2 \beta_0 \sum_{i = 1}^n x_i + 2 \beta_1 \sum_{i = 1}^n x_i^2
    &amp; \text{linearity sum} \\
  &amp;= -2\sum_{i = 1}^n y_i x_i + 2 n \beta_0 \bar{x} + 2 \beta_1 \sum_{i = 1}^n x_i^2
    &amp; \text{def. } \bar{x} \\
\end{align*}\]</span></p>
<p>then:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial Q}{\partial \beta_1} = 0
  &amp;\iff -2\sum_{i = 1}^n y_i x_i + 2 n \beta_0 \bar{x} + 2 \beta_1 \sum_{i = 1}^n x_i^2 = 0 \notag \\
  &amp;\iff \sum_{i = 1}^n y_i x_i = n \beta_0 \bar{x} + \beta_1  \sum_{i = 1}^n x_i^2 \tag{2}
\end{align}\]</span></p>
<p>Now, substituting (1) into (2) we have that</p>
<p><span class="math display">\[\begin{align*}
\sum_{i = 1}^n y_i x_i
  &amp;= n (\bar{y} - \beta_1 \bar{x}) \bar{x}  + \beta_1  \sum_{i = 1}^n x_i^2 \\
  &amp;= n \bar{y} \bar{x} - n \beta_1 \bar{x}^2 + \beta_1  \sum_{i = 1}^n x_i^2 \\
  &amp;= n \bar{y} \bar{x} + \beta_1 \left( \sum_{i = 1}^n x_i^2 - n \bar{x}^2 \right)
\end{align*}\]</span></p>
<p>Then,</p>
<p><span class="math display">\[ \beta_1 = \frac{\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i = 1}^n x_i^2 - n \bar{x}^2} \]</span></p>
<p>so, the only critical point for <span class="math inline">\(Q(\beta_0,\beta_1)\)</span> is when:</p>
<p><span class="math display">\[ \hat{\beta}_1 = \frac{\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i = 1}^n x_i^2 - n \bar{x}^2} \]</span>
<span class="math display">\[ \hat{\beta_0} = \bar{y} - \hat{\beta}_1 \bar{x}\]</span></p>
</details>
<p>Now, if we introduce the further notation for the sample variance and covariance:</p>
<p><span class="math display">\[ S^2_{xx} = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \]</span>
<span class="math display">\[ S_{xy}   = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \]</span></p>
<p>and note that:</p>
<span class="math display">\[\sum_{i=1}^n (x_i - \bar{x})^2 = \sum_{i=1}^n x_i^2 - n\bar{x}^2 \]</span>
<details>
<summary>
<span style="color:green">Derivation</span>
</summary>
<p><span class="math display">\[\begin{align*}
\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i^2 - 2\bar{x}x_i + \bar{x}^2)
    &amp; \text{exp. square} \\
  &amp;= \frac{1}{n-1} \left[ \sum_{i=1}^n x_i^2 - 2\bar{x}\sum_{i=1}^n x_i + \sum_{i=1}^n \bar{x}^2 \right]
    &amp; \text{linearity sum} \\
  &amp;= \frac{1}{n-1} \left[ \sum_{i=1}^n x_i^2 - 2\bar{x}(n\bar{x}) + n \bar{x}^2 \right]
    &amp; \text{def. } \bar{x} \\
  &amp;= \frac{1}{n-1} \left[ \sum_{i=1}^n x_i^2 - 2n\bar{x}^2 + n \bar{x}^2 \right]
    &amp; \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n x_i^2 - n\bar{x}^2
    &amp; \\
\end{align*}\]</span></p>
</details>
<p>and</p>
<span class="math display">\[ \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) = \sum_{i=1}^n x_iy_i - n\bar{x} \bar{y} \]</span>
<details>
<summary>
<span style="color:green">Derivation</span>
</summary>
<p><span class="math display">\[\begin{align*}
\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_iy_i - \bar{x}y_i - \bar{y}x_i + \bar{x}\bar{y})  
    &amp; \text{exp. prod.} \\
  &amp;= \frac{1}{n-1} \left[ \sum_{i=1}^n x_iy_i - \bar{x} \sum_{i=1}^ny_i - \bar{y} \sum_{i=1}^n x_i + \sum_{i=1}^n \bar{x}\bar{y} \right]
    &amp; \text{lin. sum} \\
  &amp;= \frac{1}{n-1} \left[ \sum_{i=1}^n x_iy_i - n\bar{x} \bar{y} - n\bar{y} \bar{x} + n \bar{x}\bar{y} \right]
    &amp; \text{def. } \bar{y}, \bar{x} \\
  &amp;= \frac{1}{n-1} \left[ \sum_{i=1}^n x_iy_i - n\bar{x} \bar{y} \right]
    &amp;  \\
\end{align*}\]</span></p>
</details>
<p>Then we can express <span class="math inline">\(\hat{\beta}_1\)</span> as:</p>
<p><span class="math display">\[\hat{\beta}_1 = \frac{(n-1)S_{xy}}{(n-1)S_{xx}^2}=\frac{S_{xy}}{S_{xx}} \]</span></p>
<p>Now notice that in order to find the Least Squares estimates you don’t require
the complete data set, but only require the following quantities (Suffient Statistics):</p>
<ul>
<li><span class="math inline">\(\bar{y}\)</span>.</li>
<li><span class="math inline">\(\bar{x}\)</span>.</li>
<li><span class="math inline">\(S_{xx}\)</span>.</li>
<li><span class="math inline">\(S_{xy}\)</span>.</li>
</ul>
<div id="other-estimated-quantites" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Other estimated quantites<a href="simple-linear-regression.html#other-estimated-quantites" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If we substitute the least-squares estimates into the regression model we obtain the fitted (predicted) values and residuals.</p>
<p>The fitted value for observation <span class="math inline">\(i\)</span> is
<span class="math display">\[
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i,
\]</span></p>
<p>and the residual (estimated error) is
<span class="math display">\[
\hat{e}_i = y_i - \hat{y}_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i.
\]</span></p>
<p>And we can also compare our estimated regression line (blue) with the real
regression line (red) in the following as follows:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="simple-linear-regression.html#cb17-1" tabindex="-1"></a>outReg <span class="ot">&lt;-</span> <span class="fu">lm</span>(Years <span class="sc">~</span> Glasses, <span class="at">data =</span> dat)</span>
<span id="cb17-2"><a href="simple-linear-regression.html#cb17-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> dat<span class="sc">$</span>Glasses,</span>
<span id="cb17-3"><a href="simple-linear-regression.html#cb17-3" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>Years,</span>
<span id="cb17-4"><a href="simple-linear-regression.html#cb17-4" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Avg. Glasses of Wine per Week&quot;</span>,</span>
<span id="cb17-5"><a href="simple-linear-regression.html#cb17-5" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Life Expectancy (Years)&quot;</span>)</span>
<span id="cb17-6"><a href="simple-linear-regression.html#cb17-6" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> <span class="dv">75</span>,</span>
<span id="cb17-7"><a href="simple-linear-regression.html#cb17-7" tabindex="-1"></a>       <span class="at">b   =</span> <span class="fl">1.5</span>,</span>
<span id="cb17-8"><a href="simple-linear-regression.html#cb17-8" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb17-9"><a href="simple-linear-regression.html#cb17-9" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb17-10"><a href="simple-linear-regression.html#cb17-10" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outReg<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb17-11"><a href="simple-linear-regression.html#cb17-11" tabindex="-1"></a>       <span class="at">b   =</span> outReg<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb17-12"><a href="simple-linear-regression.html#cb17-12" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>,</span>
<span id="cb17-13"><a href="simple-linear-regression.html#cb17-13" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/slr-real-vs-est-1.png" width="672" /></p>
<hr />
</div>
<div id="properties-of-the-slr-problem" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Properties of the SLR problem<a href="simple-linear-regression.html#properties-of-the-slr-problem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The least-squares solution in simple linear regression (SLR) enjoys several important and useful properties that are not generally shared by minimax or least-absolute-value fits (even for the same linear model). Below is a concise, polished list of these properties with brief explanations.</p>
<ul>
<li><p><strong>Closed-form solution.</strong><br />
The normal equations yield explicit formulas (e.g. <span class="math inline">\(\hat\beta_1 = S_{xy}/S_{xx}\)</span>, <span class="math inline">\(\hat\beta_0=\bar y-\hat\beta_1\bar x\)</span>). Closed-form expressions provide direct insight into how the estimates depend on the data.</p></li>
<li><p><strong>Computationally cheap (O(n)).</strong><br />
Only a few sums (or averages and second moments) are needed, so the estimates can be computed in linear time and with constant memory beyond the data summaries.</p></li>
<li><p><strong>Depends only on a few summary statistics (sufficient-type summaries).</strong><br />
The estimates require only <span class="math inline">\(\bar x,\bar y,S_{xx}^2,S_{xx}\)</span></p></li>
</ul>
<hr />
</div>
</div>
<div id="properties-of-the-estimates" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Properties of the Estimates<a href="simple-linear-regression.html#properties-of-the-estimates" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before analyzing more properties of the least squares problem, let us define the
following variables:</p>
<ul>
<li><span class="math inline">\(\mathbf{y} = (y_1,\ldots,y_n)&#39;\)</span>.</li>
<li><span class="math inline">\(\mathbf{x} = (x_1,\ldots,x_n)&#39;\)</span>.</li>
<li><span class="math inline">\(\mathbf{e} = (e_1,\ldots,e_n)&#39;\)</span>.</li>
<li><span class="math inline">\(\hat{\mathbf{y}} = (\hat{y}_1,\ldots,\hat{y}_n)&#39;\)</span>.</li>
<li><span class="math inline">\(\hat{\mathbf{e}} = (\hat{e}_1,\ldots,\hat{e}_n)&#39;\)</span>.</li>
<li><span class="math inline">\(\mathbb{1} = (1,\ldots,1)&#39;\)</span>.</li>
</ul>
<p>As a bonus, we can write the Linear regression model as follows:</p>
<p><span class="math display">\[ \mathbf{y} = \beta_0 \mathbb{1} + \beta_1 \mathbf{x} + \mathbf{e} \]</span></p>
<hr />
<div id="hatbeta_0-and-hatbeta_1-are-linear-combinations-of" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are linear combinations of<a href="simple-linear-regression.html#hatbeta_0-and-hatbeta_1-are-linear-combinations-of" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math inline">\(\mathbf{y}\)</span></p>
<p>The estimates <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are linear combinations of
<span class="math inline">\(\mathbf{y} = (y_1,\ldots,y_n)&#39;\)</span>.</p>
<p>First note the following:</p>
<span class="math display">\[ \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) =  \sum_{i=1}^n (x_i - \bar{x}) y_i \]</span>
<details>
<summary>
<span style="color:green">Derivation</span>
</summary>
<p><span class="math display">\[\begin{align*}
\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})
  &amp;= \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y}
    &amp; \text{dist. prod.} \\
  &amp;= \sum_{i=1}^n x_i y_i - \bar{x} \sum_{i=1}^n y_i  
    &amp; \text{def. } \bar{y} \\
  &amp;= \sum_{i=1}^n x_i y_i - \sum_{i=1}^n \bar{x} y_i  
    &amp; \text{liniarity of sum} \\
  &amp;= \sum_{i=1}^n (x_i y_i - \bar{x} y_i)   
    &amp; \text{ass. sum} \\
  &amp;= \sum_{i=1}^n (x_i - \bar{x}) y_i    
    &amp; \text{factoring } y_i \\
\end{align*}\]</span></p>
</details>
<p>Then</p>
<p><span class="math display">\[ \hat{\beta}_1 =  \frac{\sum_{i = 1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{\sum_{i=1}^n (x_i - \bar{x}) y_i}{\sum_{i=1}^n (x_i - \bar{x})^2} = \sum_{i=1}^n\frac{(x_i - \bar{x}) }{\sum_{i=1}^n (x_i - \bar{x})^2}y_i \]</span></p>
<p>and similarly:</p>
<p><span class="math display">\[ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} = \sum_{i=1}^n \frac{y_i}{n} - \sum_{i=1}^n\frac{(x_i - \bar{x}) }{\sum_{j = 1}^n x_j^2 - n \bar{x}^2}y_i \bar{x} = \sum_{i=1}^n \left( \frac{1}{n} - \frac{(x_i - \bar{x}) }{\sum_{j = 1}^n x_j^2 - n \bar{x}^2} \bar{x} \right)y_i \]</span></p>
<hr />
</div>
<div id="the-sum-of-the-residuals-is-0" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> The sum of the residuals is <span class="math inline">\(0\)</span><a href="simple-linear-regression.html#the-sum-of-the-residuals-is-0" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>That is:</p>
<p><span class="math display">\[ \sum_{i=1}^n \hat{e}_i = 0 \]</span></p>
<details>
<summary>
<span style="color:green">Derivation</span>
</summary>
<p><span class="math display">\[\begin{align*}
\sum_{i=1}^n \hat{e}_i
  &amp;= \sum_{i=1}^n(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)    
    &amp; \text{def. } \hat{\mathbf{e}}\\
  &amp;= \sum_{i=1}^n y_i - \sum_{i=1}^n \hat{\beta}_0 - \hat{\beta}_1 \sum_{i=1}^n x_i    
    &amp; \text{linearity sum} \\
  &amp;= n\bar{y} - n \hat{\beta}_0 - n \hat{\beta}_1 \bar{x}     
    &amp;  \\
  &amp;= n\bar{y} - n (\bar{y} - \hat{\beta}_1 \bar{x}) - n \hat{\beta}_1 \bar{x}      
    &amp; \text{def. } \hat{\beta}_0 \\
  &amp;= n\bar{y} - n \bar{y} + n \hat{\beta}_1 \bar{x} - n \hat{\beta}_1 \bar{x}      
    &amp; \text{dist.} n \\
  &amp;= 0
\end{align*}\]</span></p>
</details>
<hr />
</div>
<div id="hatmathbfe-and-mathbfx-are-orthogonal" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> are orthogonal<a href="simple-linear-regression.html#hatmathbfe-and-mathbfx-are-orthogonal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We need to show that:</p>
<p><span class="math display">\[ \langle \hat{\mathbf{e}}, \mathbf{x}\rangle = 0 \]</span></p>
<details>
<summary>
<span style="color:green">Derivation</span>
</summary>
<p><span class="math display">\[\begin{align*}
\langle \hat{\mathbf{e}}, \mathbf{x}\rangle
  &amp;= \sum_{i=1}^{n} \hat{e}_i x_i
    &amp; \text{def. dot prod.} \\
  &amp;= \sum_{i=1}^{n}  (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)x_i
    &amp; \text{def. } \hat{e}_i \\
  &amp;= \sum_{i=1}^{n}  (y_i x_i - \hat{\beta}_0x_i - \hat{\beta}_1 x_i x_i)
    &amp; \text{dist. } x_i \\
  &amp;= \sum_{i=1}^{n} y_i x_i - \hat{\beta}_0 \sum_{i=1}^{n} x_i - \hat{\beta}_1 \sum_{i=1}^{n}  x_i x_i  
    &amp; \text{linearity sum} \\
  &amp;= \sum_{i=1}^{n} y_i x_i - n \hat{\beta}_0 \bar{x} - \hat{\beta}_1 \sum_{i=1}^{n} x_i^2
    &amp; \text{def. } \bar{x} \\
  &amp;= \sum_{i=1}^{n} y_i x_i - n (\bar{y} - \hat{\beta}_1 \bar{x}) \bar{x} - \hat{\beta}_1 \sum_{i=1}^{n} x_i^2
    &amp; \text{def. } \hat{\beta}_0 \\
  &amp;= \sum_{i=1}^{n} y_i x_i - n \bar{y} \bar{x} + n\hat{\beta}_1 \bar{x}^2 - \hat{\beta}_1 \sum_{i=1}^{n} x_i^2
    &amp; \text{dist. } \bar{x} \\
  &amp;= \sum_{i=1}^{n} y_i x_i - n \bar{y} \bar{x} - \hat{\beta}_1 (\sum_{i=1}^{n} x_i^2 - n\bar{x}^2)   
    &amp; \text{fact. } \hat{\beta}_1 \\
  &amp;= \sum_{i=1}^{n} y_i x_i - n \bar{y} \bar{x} -  \frac{\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i = 1}^n x_i^2 - n \bar{x}^2}(\sum_{i=1}^{n} x_i^2 - n\bar{x}^2)  
    &amp; \text{def. } \hat{\beta}_1 \\
  &amp;= \sum_{i=1}^{n} y_i x_i - n \bar{y} \bar{x} -  (\sum_{i = 1}^n y_i x_i - n \bar{y} \bar{x})  \\
  &amp;=0
\end{align*}\]</span></p>
</details>
<hr />
</div>
<div id="hatmathbfy-and-hatmathbfe-are-orthogonal" class="section level3 hasAnchor" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> <span class="math inline">\(\hat{\mathbf{y}}\)</span> and <span class="math inline">\(\hat{\mathbf{e}}\)</span> are orthogonal<a href="simple-linear-regression.html#hatmathbfy-and-hatmathbfe-are-orthogonal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We need to show that:</p>
<p><span class="math display">\[ \langle \hat{\mathbf{e}}, \hat{\mathbf{y}}\rangle = 0 \]</span></p>
<details>
<summary>
<span style="color:green">Derivation</span>
</summary>
<p><span class="math display">\[\begin{align*}
\langle \hat{\mathbf{e}}, \hat{\mathbf{y}}\rangle
  &amp;= \sum_{i=1}^{n} \hat{e}_i \hat{y}_i
    &amp; \text{def. dot prod.} \\
  &amp;= \sum_{i=1}^{n} \hat{e}_i(\hat{\beta}_0 + \hat{\beta}_1 x_i)  
    &amp; \text{def. } \hat{y}_i \\
  &amp;= \sum_{i=1}^{n} (\hat{e}_i \hat{\beta}_0 + \hat{e}_i \hat{\beta}_1 x_i)
    &amp; \text{dist. } \hat{e}_i \\
  &amp;= \hat{\beta}_0 \sum_{i=1}^{n} \hat{e}_i + \hat{\beta}_1 \sum_{i=1}^{n} \hat{e}_i x_i
    &amp; \text{lin. sum } \\
  &amp;= \hat{\beta}_1 \sum_{i=1}^{n} \hat{e}_i x_i
    &amp;  \sum_{i=1}^{n} \hat{e}_i = 0\\
  &amp;= \hat{\beta}_1 \langle \hat{\mathbf{e}}, \mathbf{x}\rangle  
    &amp;  \text{def. dot prod.}\\
  &amp;= 0
    &amp;  \langle \hat{\mathbf{e}}, \mathbf{x}\rangle = 0 \\
\end{align*}\]</span></p>
</details>
<hr />
</div>
<div id="the-average-of-hatmathbfy-and-mathbfy-are-the-same" class="section level3 hasAnchor" number="4.3.5">
<h3><span class="header-section-number">4.3.5</span> The average of <span class="math inline">\(\hat{\mathbf{y}}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are the same<a href="simple-linear-regression.html#the-average-of-hatmathbfy-and-mathbfy-are-the-same" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>That is:</p>
<span class="math display">\[ \frac{1}{n} \sum_{i=1}^n \hat{y}_i = \bar{y} \]</span>
<details>
<summary>
<span style="color:green">Derivation</span>
</summary>
<p><span class="math display">\[\begin{align*}
\frac{1}{n} \sum_{i=1}^n \hat{y}_i
  &amp;= \frac{1}{n} \sum_{i=1}^n (\hat{\beta}_0 + \hat{\beta}_1 x_i)
    &amp; \text{def. } \hat{y}_i \\
  &amp;= \frac{1}{n} (n \hat{\beta}_0 + \hat{\beta}_1 \sum_{i=1}^n x_i)
    &amp; \text{dist. sum} \\
  &amp;= \frac{1}{n} (n \hat{\beta}_0 + n \hat{\beta}_1 \mathbf{x})
    &amp; \text{dist. } n \\
  &amp;= \hat{\beta}_0 + \hat{\beta}_1 \mathbf{x}
    &amp; \text{def. } \hat{\beta}_0 \\
  &amp;= \bar{y} - \hat{\beta}_1 \mathbf{x}+ \hat{\beta}_1 \mathbf{x}\\
    &amp; \\
  &amp;= \bar{y} \\
    &amp; \\
\end{align*}\]</span></p>
</details>
<p>All these properties of the estimates, are the result of solving the least squares
problem. If another problem is solve, several of these properties, if not all of them,
will be lost.</p>
<hr />
</div>
</div>
<div id="centering-and-standarizing-the-data" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Centering and Standarizing the Data<a href="simple-linear-regression.html#centering-and-standarizing-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Some transformations of the data can aid regression analysis or make coefficient interpretations more intuitive. Two standard linear transformations are centering and standardization. Before discussing their implications for linear regression, we define each transformation and analyze its effect on the data itself.</p>
<div class="definition">
<p><span id="def:def-centered-variable" class="definition"><strong>Definition 4.1  (Centered variable) </strong></span>Given observations <span class="math inline">\(\{x_i\}_{i=1}^n\)</span>, then the centered version of observation
<span class="math inline">\(i\)</span>, denoted by <span class="math inline">\(x_i^c\)</span> is given by:
<span class="math display">\[x_i^c = x_i - \bar{x}\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:prp-centered-variable-mean-0" class="proposition"><strong>Proposition 4.1  </strong></span>The centered observations <span class="math inline">\(\{x_i^c\}_{i=1}^n\)</span> have mean <span class="math inline">\(0\)</span>.</p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>We need to show that:
<span class="math display">\[ \bar{x}^c = 0 \]</span></p>
<p><span class="math display">\[\begin{align*}
\bar{x}^c
  &amp;= \frac{1}{n} \sum_{i=1}^n x_i^c
    &amp; \text{def. } \bar{x}^c \\
  &amp;= \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})
    &amp; \text{def. } x_i^c \\
  &amp;= \frac{1}{n} \left(\sum_{i=1}^n x_i - \sum_{i=1}^n \bar{x} \right)
    &amp; \text{lin. sum} x_i^c \\
  &amp;= \frac{1}{n} \left(n\bar{x} - n \bar{x} \right)
    &amp; \text{def. } \bar{x} \\
  &amp;= 0 \\
    &amp; \\
\end{align*}\]</span></p>
</div>
</details>
<div class="proposition">
<p><span id="prp:prp-centered-variable-variance-equal" class="proposition"><strong>Proposition 4.2  </strong></span>The centered observations <span class="math inline">\(\{x_i^c\}_{i=1}^n\)</span> have the same sample variance as
the original observations <span class="math inline">\(\{x_i\}_{i=1}^n\)</span>.</p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>We need to show that:
<span class="math display">\[ S_{xx}^c = S_{xx} \]</span></p>
<p><span class="math display">\[\begin{align*}
S_{xx}^c
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i^c - \bar{x}^c)^2
    &amp; \text{def. } S_{xx}^c \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i^c)^2
    &amp; \bar{x}^c = 0 \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
    &amp; \text{def. } x_i^c \\
  &amp;= S_{xx}
    &amp; \text{def. } S_{xx} \\
\end{align*}\]</span></p>
</div>
</details>
<div class="proposition">
<p><span id="prp:prp-centered-variable-covariance-equal" class="proposition"><strong>Proposition 4.3  </strong></span>The centered observations <span class="math inline">\(\{x_i^c\}_{i=1}^n\)</span> and <span class="math inline">\(\{y_i^c\}_{i=1}^n\)</span> have the same sample covariance as
the original observations <span class="math inline">\(\{x_i\}_{i=1}^n\)</span> and <span class="math inline">\(\{y_i\}_{i=1}^n\)</span>.</p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-3" class="proof"><em>Proof</em>. </span>We need to show that:
<span class="math display">\[ S_{xy}^c = S_{xy} \]</span></p>
<p><span class="math display">\[\begin{align*}
S_{xy}^c
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i^c - \bar{x})(y_i^c - \bar{y})
    &amp; \text{def. } S_{xy}^c \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i^c)(y_i^c)
    &amp; \bar{x}^c = 0,\bar{y}^c = 0 \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})
    &amp; \text{def. } x_i^c,y_i^c \\
  &amp;= S_{xy}
    &amp; \text{def. } S_{xy} \\
\end{align*}\]</span></p>
</div>
</details>
<p>Effect on the data:</p>
<ul>
<li>Location: The mean of the centered variable is zero. Centering moves the distribution so that its central point is at the origin.</li>
<li>Spread and shape: Centering does not change the spread (variance or standard deviation) or the shape (skewness/kurtosis) of the distribution; only the location changes.</li>
<li>Units and interpretation: Units remain the same; only the zero point is shifted.</li>
<li>Linear relationships: Pairwise linear relationships are unchanged in magnitude.</li>
<li>Numerical effects: Centering often reduces the magnitude of numerical values and can improve numerical stability in computations.</li>
</ul>
<div class="definition">
<p><span id="def:def-standarized-variable" class="definition"><strong>Definition 4.2  (Standarized variable) </strong></span>Given observations <span class="math inline">\(\{x_i\}_{i=1}^n\)</span>, then the standarized version of observation
<span class="math inline">\(i\)</span>, denoted by <span class="math inline">\(x_i^s\)</span>, is given by:
<span class="math display">\[x_i^s = \frac{x_i - \bar{x}}{\sqrt{S_{xx}}} = \frac{x_i^c}{\sqrt{S_{xx}}}\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:prp-standarized-variable-equal-1" class="proposition"><strong>Proposition 4.4  </strong></span>The standardized observations <span class="math inline">\(\{x_i^s\}_{i=1}^n\)</span> have mean of <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>.</p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>We need to show that:
<span class="math display">\[ \bar{x}^s = 0 \]</span>
and
<span class="math display">\[ S_{xx}^s = 1 \]</span></p>
<p>First, let us show that <span class="math inline">\(\bar{x}^s = 0\)</span>.
<span class="math display">\[\begin{align*}
\bar{x}^s = 0
  &amp;=  \frac{1}{n} \sum_{i=1}^n x_i^c
    &amp; \text{def. } \bar{x}^s \\
  &amp;= \frac{1}{n} \sum_{i=1}^n \frac{x_i^c}{\sqrt{S_{xx}}}
    &amp; \text{def. } x_i^s \\
  &amp;= \frac{1}{\sqrt{S_{xx}}}\frac{1}{n} \sum_{i=1}^n x_i^c
    &amp; \text{lin. sum} \\
  &amp;= \frac{1}{\sqrt{S_{xx}}} \bar{x}^c
    &amp; \text{def. } \bar{x}^c \\
  &amp;= 0
    &amp; \bar{x}^c = 0 \\
\end{align*}\]</span></p>
<p>Now let us see that the variance of the standardized observations is 1.
<span class="math display">\[\begin{align*}
S_{xx}^s
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i^s - \bar{x}^s)^2
    &amp; \text{def. } S_{xx}^s \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i^s)^2
    &amp; \bar{x}^s = 0 \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n \left(\frac{x_i - \bar{x}}{\sqrt{S_{xx}}}\right)^2
    &amp; \text{def. } x_i^s \\
  &amp;= \frac{1}{n-1} \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{S_{xx}}
    &amp; \\
  &amp;= \frac{1}{S_{xx}} \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
    &amp; \text{lin. sum} \\
  &amp;= \frac{1}{S_{xx}}S_{xx}
    &amp; \text{def. } S_{xx} \\
  &amp;= 1
    &amp; \\
\end{align*}\]</span></p>
</div>
</details>
<p>Effect on the data:</p>
<ul>
<li>Location and scale: The standardized variable has mean zero and a typical magnitude of one.</li>
<li>Units: Units are removed; values are expressed in “standard‑deviation units” relative to the original scale.</li>
<li>Shape: The shape of the distribution (skewness, kurtosis) is unchanged; only location and scale are affected.</li>
<li>Comparability: Variables measured on different original scales become directly comparable after standardization, because they share a common scale.</li>
<li>Sensitivity to outliers: The standardization factor is sensitive to extreme values if the sample standard deviation is used; robust alternatives (median and robust spread) can be substituted when outliers are a concern.</li>
</ul>
<div id="remarks-on-centering-and-standarization" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Remarks on Centering and Standarization<a href="simple-linear-regression.html#remarks-on-centering-and-standarization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Standardization implies centering. Every standardized variable is centered; the converse need not hold.</li>
<li>Both operations are linear transformations: they preserve linear relationships among variables and do not change correlation signs, though numeric slope values will change according to scale.</li>
<li>Centering is often sufficient when the goal is to simplify interpretation of an intercept or reduce collinearity with polynomial or interaction terms.</li>
<li>Standardization is recommended when predictors have different units or scales, when comparing effect sizes, or when using algorithms or penalties that depend on predictor scale.</li>
<li>When applying transformations in modeling workflows, compute centering/standardization parameters (means and spread measures) on the training data and apply the same transformations to validation and test sets to avoid data leakage.</li>
<li>Record and report the transformations so coefficients and predictions can be interpreted or converted back to original units when needed.</li>
</ul>
</div>
<div id="summary-centering-and-standarizing" class="section level3 hasAnchor" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Summary Centering and Standarizing<a href="simple-linear-regression.html#summary-centering-and-standarizing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Centering shifts a variable so its mean is zero; it changes location only and leaves spread and shape intact.<br />
</li>
<li>Standardization shifts and rescales so that the variable has mean zero and unit typical magnitude; it removes units and facilitates comparability.<br />
</li>
<li>Both are linear, preserve linear association, and are valuable preprocessing steps; the choice between them depends on interpretive goals, numerical stability needs, and sensitivity to outliers.</li>
</ul>
<p>Now, let us introduce the sample correlation.</p>
<div class="definition">
<p><span id="def:sample-correlation" class="definition"><strong>Definition 4.3  (Sample Correlation) </strong></span>Given observations <span class="math inline">\(\{x_i\}_{i=1}^n\)</span> and <span class="math inline">\(\{y_i\}_{i=1}^n\)</span>, the sample correlation
is given by:
<span class="math display">\[ r_{xy} = \frac{S_{xy}}{\sqrt{S_{xx}{S_{yy}}}} \]</span></p>
</div>
<div class="proposition">
<p><span id="prp:covariance-standarized-equal-correlation" class="proposition"><strong>Proposition 4.5  </strong></span>The sample covariance of the standardized observations <span class="math inline">\(\{x_i^s\}_{i=1}^n\)</span> and
<span class="math inline">\(\{x_i^s\}_{i=1}^n\)</span> is equal to the correlation of the original observations
<span class="math inline">\(\{x_i\}_{i=1}^n\)</span> and <span class="math inline">\(\{x_i\}_{i=1}^n\)</span></p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-5" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
S_{xy}^s
  &amp;= \frac{1}{n-1} \sum_{i=1} (x_i^s - \bar{x}^s)(y_i^s - \bar{x}^s)
    &amp; \text{def. } S_{xx}^s \\
  &amp;= \frac{1}{n-1} \sum_{i=1} (x_i^s)(y_i^s)
    &amp; \bar{x}^s=0,\bar{y}^s=0 \\
  &amp;= \frac{1}{n-1} \sum_{i=1} \left(\frac{x_i - \bar{x}}{\sqrt{S_{xx}}}\right) \left(\frac{y_i - \bar{y}}{\sqrt{S_{yy}}}\right)
    &amp; \text{def. } x^c_i,y^c_i \\
  &amp;= \frac{1}{\sqrt{S_{yy}}\sqrt{S_{xx}}}\frac{1}{n-1} \sum_{i=1} (x_i - \bar{x}) (y_i - \bar{y})
    &amp; \text{lin. sum} \\
  &amp;= \frac{1}{\sqrt{S_{yy}}\sqrt{S_{xx}}} S_{xy}
    &amp; \text{def. } S_{xy} \\
  &amp;= r_{xy}
    &amp; \text{def. } r_{xy} \\
\end{align*}\]</span></p>
</div>
</details>
<p>With this results, we can analyze the effects of the following 3 scenarios on the
estimated coefficients:</p>
<ul>
<li>Independent variable centered.</li>
<li>Both, independent and dependent variable centered.</li>
<li>Both, independent and dependent variable standardized.</li>
</ul>
<hr />
</div>
</div>
<div id="centering-and-standarizing-in-slr" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Centering and Standarizing in SLR<a href="simple-linear-regression.html#centering-and-standarizing-in-slr" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Centering and standardizing are two common preprocessing steps for continuous variables before fitting a regression model. They are simple linear transformations of the data but have important practical and interpretive consequences.</p>
<hr />
<div id="centered-independent-variable" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Centered Independent Variable<a href="simple-linear-regression.html#centered-independent-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="proposition">
<p><span id="prp:prp-centered-independent-variable" class="proposition"><strong>Proposition 4.6  </strong></span>The LS estimators <span class="math inline">\(\hat{\beta}^c_0, \hat{\beta}^c_1\)</span> with centered independent variables <span class="math inline">\(\{x_i^c\}_{i=1}^n\)</span> and original dependent variables <span class="math inline">\(\{y_i\}_{i=1}^n\)</span> can be expressed in terms of the original estimators and data as follows:
<span class="math display">\[\hat{\beta}_1^c = \hat{\beta}_1\]</span>
and
<span class="math display">\[\hat{\beta}_0^c = \bar{y}\]</span></p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-6" class="proof"><em>Proof</em>. </span>First lets compute <span class="math inline">\(\beta_1^c\)</span>
<span class="math display">\[\begin{align*}
\hat{\beta}_1^c
  &amp;= \frac{S_{xy}^c}{S_{xx}^c}
    &amp; \text{def. } \hat{\beta}_1^c \\
  &amp;= \frac{S_{xy}}{S_{xx}}
    &amp; S_{xy}^c=S_{xy},S_{xx}^c=S_{xx}\\
  &amp;= \hat{\beta}_1
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
\hat{\beta}_0^c
  &amp;= \bar{y} - \hat{\beta}_^c \bar{x}^c
    &amp; \text{def. } \hat{\beta}_0^c \\
  &amp;= \bar{y} - \hat{\beta}_1^c 0
    &amp; \bar{x}^c=0 \\
  &amp;= \bar{y}
\end{align*}\]</span></p>
</div>
</details>
<p>So centering the dependent variable doesn’t change the value of the estimated
slope and makes the estimated intercept to coincide with the mean of the independent variable.</p>
<p>We can see this in one of our example data sets, looking at the ad spending data
we can perform linear regression on the original data and the centered data:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="simple-linear-regression.html#cb18-1" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb18-2"><a href="simple-linear-regression.html#cb18-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Ad spending Data.csv&quot;</span>)</span>
<span id="cb18-3"><a href="simple-linear-regression.html#cb18-3" tabindex="-1"></a><span class="co"># Assign data</span></span>
<span id="cb18-4"><a href="simple-linear-regression.html#cb18-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Ad.Spending</span>
<span id="cb18-5"><a href="simple-linear-regression.html#cb18-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Revenue</span>
<span id="cb18-6"><a href="simple-linear-regression.html#cb18-6" tabindex="-1"></a><span class="co"># Centers x</span></span>
<span id="cb18-7"><a href="simple-linear-regression.html#cb18-7" tabindex="-1"></a>xCen <span class="ot">&lt;-</span> x <span class="sc">-</span> <span class="fu">mean</span>(x)</span>
<span id="cb18-8"><a href="simple-linear-regression.html#cb18-8" tabindex="-1"></a><span class="co"># Linear regression on the original data</span></span>
<span id="cb18-9"><a href="simple-linear-regression.html#cb18-9" tabindex="-1"></a>outRegOri <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb18-10"><a href="simple-linear-regression.html#cb18-10" tabindex="-1"></a><span class="co"># Linear regression on the centered data</span></span>
<span id="cb18-11"><a href="simple-linear-regression.html#cb18-11" tabindex="-1"></a>outRegCen <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> xCen)</span>
<span id="cb18-12"><a href="simple-linear-regression.html#cb18-12" tabindex="-1"></a><span class="do">## Two plots in the same image</span></span>
<span id="cb18-13"><a href="simple-linear-regression.html#cb18-13" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb18-14"><a href="simple-linear-regression.html#cb18-14" tabindex="-1"></a><span class="do">## Original data</span></span>
<span id="cb18-15"><a href="simple-linear-regression.html#cb18-15" tabindex="-1"></a><span class="co"># Plots the points</span></span>
<span id="cb18-16"><a href="simple-linear-regression.html#cb18-16" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb18-17"><a href="simple-linear-regression.html#cb18-17" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb18-18"><a href="simple-linear-regression.html#cb18-18" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb18-19"><a href="simple-linear-regression.html#cb18-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb18-20"><a href="simple-linear-regression.html#cb18-20" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb18-21"><a href="simple-linear-regression.html#cb18-21" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb18-22"><a href="simple-linear-regression.html#cb18-22" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb18-23"><a href="simple-linear-regression.html#cb18-23" tabindex="-1"></a>       <span class="at">b   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb18-24"><a href="simple-linear-regression.html#cb18-24" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb18-25"><a href="simple-linear-regression.html#cb18-25" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb18-26"><a href="simple-linear-regression.html#cb18-26" tabindex="-1"></a><span class="do">## Independent Variable centered data</span></span>
<span id="cb18-27"><a href="simple-linear-regression.html#cb18-27" tabindex="-1"></a><span class="co"># Plots the points</span></span>
<span id="cb18-28"><a href="simple-linear-regression.html#cb18-28" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> xCen,</span>
<span id="cb18-29"><a href="simple-linear-regression.html#cb18-29" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb18-30"><a href="simple-linear-regression.html#cb18-30" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending (Centered)&quot;</span>,</span>
<span id="cb18-31"><a href="simple-linear-regression.html#cb18-31" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb18-32"><a href="simple-linear-regression.html#cb18-32" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb18-33"><a href="simple-linear-regression.html#cb18-33" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb18-34"><a href="simple-linear-regression.html#cb18-34" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegCen<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb18-35"><a href="simple-linear-regression.html#cb18-35" tabindex="-1"></a>       <span class="at">b   =</span> outRegCen<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb18-36"><a href="simple-linear-regression.html#cb18-36" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb18-37"><a href="simple-linear-regression.html#cb18-37" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb18-38"><a href="simple-linear-regression.html#cb18-38" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v   =</span> <span class="dv">0</span>,</span>
<span id="cb18-39"><a href="simple-linear-regression.html#cb18-39" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/center-x-1.png" width="672" /></p>
<p>Effect on the fitted model:</p>
<ul>
<li>The estimated slope that measures how the response changes with the predictor is unchanged in magnitude.<br />
</li>
<li>The intercept becomes the predicted response at the typical predictor value (the predictor average), which makes the intercept directly interpretable as a typical outcome.<br />
</li>
<li>Predictions for any observation are unchanged after converting back to the original predictor scale.</li>
</ul>
<p>Statistical and numerical consequences:</p>
<ul>
<li>The sampling variability of the slope estimator is unaffected.<br />
</li>
<li>Centering often improves numerical stability in computation because predictor
values are closer to zero, reducing the chance of poor conditioning or round-off error.</li>
</ul>
<p>Practical considerations:</p>
<ul>
<li>Centering is especially useful when the intercept at an observed or typical
predictor value is of substantive interest.<br />
</li>
<li>It is recommended when polynomial terms or interactions are present, because it reduces collinearity between lower- and higher-order terms.<br />
</li>
<li>Always record the predictor mean used for centering so that future predictions
or reporting can be done on the original scale.</li>
</ul>
<hr />
</div>
<div id="both-variables-centered" class="section level3 hasAnchor" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Both Variables Centered<a href="simple-linear-regression.html#both-variables-centered" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="proposition">
<p><span id="prp:prp-centered-both-variable" class="proposition"><strong>Proposition 4.7  </strong></span>The LS estimators <span class="math inline">\(\hat{\beta}^c_0, \hat{\beta}^c_1\)</span> with centered independent variables <span class="math inline">\(\{x_i^c\}_{i=1}^n\)</span> and centered dependent variables <span class="math inline">\(\{y_i^c\}_{i=1}^n\)</span> can be expressed in terms of the original estimators and data as follows:
<span class="math display">\[\hat{\beta}_1^c = \hat{\beta}_1\]</span>
and
<span class="math display">\[\hat{\beta}_0^c = 0\]</span></p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>First lets compute <span class="math inline">\(\beta_1^c\)</span>
<span class="math display">\[\begin{align*}
\hat{\beta}_1^c
  &amp;= \frac{S_{xy}^c}{S_{xx}^c}
    &amp; \text{def. } \hat{\beta}_1^c \\
  &amp;= \frac{S_{xy}}{S_{xx}}
    &amp; S_{xy}^c=S_{xy},S_{xx}^c=S_{xx}\\
  &amp;= \hat{\beta}_1
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
\hat{\beta}_0^c
  &amp;= \bar{y}^c - \hat{\beta}_1^c \bar{x}^c
    &amp; \text{def. } \hat{\beta}_0^c \\
  &amp;= 0 - \hat{\beta}_1^c 0
    &amp; \bar{x}^c=\bar{y}^c=0 \\
  &amp;= 0
\end{align*}\]</span></p>
</div>
</details>
<p>Again, the slope doesn’t change while the estimate of the intercept becomes zero
(the new mean of the centered dependent variable).</p>
<p>The effect of this transformation can be observed, in the following example:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="simple-linear-regression.html#cb19-1" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb19-2"><a href="simple-linear-regression.html#cb19-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Ad spending Data.csv&quot;</span>)</span>
<span id="cb19-3"><a href="simple-linear-regression.html#cb19-3" tabindex="-1"></a><span class="co"># Assign data</span></span>
<span id="cb19-4"><a href="simple-linear-regression.html#cb19-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Ad.Spending</span>
<span id="cb19-5"><a href="simple-linear-regression.html#cb19-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Revenue</span>
<span id="cb19-6"><a href="simple-linear-regression.html#cb19-6" tabindex="-1"></a><span class="co"># Centers x and y</span></span>
<span id="cb19-7"><a href="simple-linear-regression.html#cb19-7" tabindex="-1"></a>xCen <span class="ot">&lt;-</span> x <span class="sc">-</span> <span class="fu">mean</span>(x)</span>
<span id="cb19-8"><a href="simple-linear-regression.html#cb19-8" tabindex="-1"></a>yCen <span class="ot">&lt;-</span> y <span class="sc">-</span> <span class="fu">mean</span>(y)</span>
<span id="cb19-9"><a href="simple-linear-regression.html#cb19-9" tabindex="-1"></a><span class="co"># Linear regression on the original data</span></span>
<span id="cb19-10"><a href="simple-linear-regression.html#cb19-10" tabindex="-1"></a>outRegOri <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb19-11"><a href="simple-linear-regression.html#cb19-11" tabindex="-1"></a><span class="co"># Linear regression on the centered data</span></span>
<span id="cb19-12"><a href="simple-linear-regression.html#cb19-12" tabindex="-1"></a>outRegCen <span class="ot">&lt;-</span> <span class="fu">lm</span>(yCen <span class="sc">~</span> xCen)</span>
<span id="cb19-13"><a href="simple-linear-regression.html#cb19-13" tabindex="-1"></a><span class="co"># Plots</span></span>
<span id="cb19-14"><a href="simple-linear-regression.html#cb19-14" tabindex="-1"></a><span class="do">## Two plots in the same image</span></span>
<span id="cb19-15"><a href="simple-linear-regression.html#cb19-15" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb19-16"><a href="simple-linear-regression.html#cb19-16" tabindex="-1"></a><span class="do">## Original data</span></span>
<span id="cb19-17"><a href="simple-linear-regression.html#cb19-17" tabindex="-1"></a><span class="co"># Plots the points</span></span>
<span id="cb19-18"><a href="simple-linear-regression.html#cb19-18" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb19-19"><a href="simple-linear-regression.html#cb19-19" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb19-20"><a href="simple-linear-regression.html#cb19-20" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb19-21"><a href="simple-linear-regression.html#cb19-21" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb19-22"><a href="simple-linear-regression.html#cb19-22" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb19-23"><a href="simple-linear-regression.html#cb19-23" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb19-24"><a href="simple-linear-regression.html#cb19-24" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb19-25"><a href="simple-linear-regression.html#cb19-25" tabindex="-1"></a>       <span class="at">b   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb19-26"><a href="simple-linear-regression.html#cb19-26" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb19-27"><a href="simple-linear-regression.html#cb19-27" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb19-28"><a href="simple-linear-regression.html#cb19-28" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb19-29"><a href="simple-linear-regression.html#cb19-29" tabindex="-1"></a><span class="do">## Centered data</span></span>
<span id="cb19-30"><a href="simple-linear-regression.html#cb19-30" tabindex="-1"></a><span class="co"># Plots the points</span></span>
<span id="cb19-31"><a href="simple-linear-regression.html#cb19-31" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> xCen,</span>
<span id="cb19-32"><a href="simple-linear-regression.html#cb19-32" tabindex="-1"></a>     <span class="at">y    =</span> yCen,</span>
<span id="cb19-33"><a href="simple-linear-regression.html#cb19-33" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending (Centered)&quot;</span>,</span>
<span id="cb19-34"><a href="simple-linear-regression.html#cb19-34" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb19-35"><a href="simple-linear-regression.html#cb19-35" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb19-36"><a href="simple-linear-regression.html#cb19-36" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb19-37"><a href="simple-linear-regression.html#cb19-37" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegCen<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb19-38"><a href="simple-linear-regression.html#cb19-38" tabindex="-1"></a>       <span class="at">b   =</span> outRegCen<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb19-39"><a href="simple-linear-regression.html#cb19-39" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb19-40"><a href="simple-linear-regression.html#cb19-40" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb19-41"><a href="simple-linear-regression.html#cb19-41" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v   =</span> <span class="dv">0</span>,</span>
<span id="cb19-42"><a href="simple-linear-regression.html#cb19-42" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb19-43"><a href="simple-linear-regression.html#cb19-43" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb19-44"><a href="simple-linear-regression.html#cb19-44" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/center-xy-1.png" width="672" /></p>
<p>Effect on the fitted model</p>
<ul>
<li>The fitted line passes through the origin in the centered coordinate system, so there is no separate intercept to estimate in the transformed model.<br />
</li>
<li>The estimated slope remains the same numerical quantity as in the original regression fitted to deviations; the relationship described by the slope is identical to the original slope.</li>
</ul>
<p>Inference and numerical consequences</p>
<ul>
<li>There is no intercept estimate to interpret in the centered model, and the slope is naturally interpreted as how deviations in the response correspond to deviations in the predictor.<br />
</li>
<li>Because centering reduces magnitudes, numerical conditioning and stability often improve.</li>
</ul>
<p>Practical considerations:</p>
<ul>
<li>Centering both variables is useful when the research question is framed in terms of deviations from typical values (for example, “how much above or below average does the response change when the predictor is above or below average?”).<br />
</li>
<li>It simplifies algebraic decompositions of variance and can make interpretation of component sums of squares more direct.<br />
</li>
<li>Keep the centering constants so that any transformations of predictions or coefficients back to original units are straightforward.</li>
</ul>
<hr />
</div>
<div id="both-variables-standarized" class="section level3 hasAnchor" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> Both variables Standarized<a href="simple-linear-regression.html#both-variables-standarized" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Finally, we analyze the effect of standardization.</p>
<div class="proposition">
<p><span id="prp:prp-standardized-both-variable" class="proposition"><strong>Proposition 4.8  </strong></span>The LS estimators <span class="math inline">\(\hat{\beta}^s_0, \hat{\beta}^s_1\)</span> with standardized independent variables <span class="math inline">\(\{x_i^c\}_{i=1}^n\)</span> and standardized dependent variables <span class="math inline">\(\{y_i^c\}_{i=1}^n\)</span> can be expressed in terms of the original variables as follows:
<span class="math display">\[\hat{\beta}_1^s = r_{xy}\]</span>
and
<span class="math display">\[\hat{\beta}_0^s = 0\]</span></p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>Proof</em>. </span>First lets compute <span class="math inline">\(\beta_1^c\)</span>
<span class="math display">\[\begin{align*}
\hat{\beta}_1^s
  &amp;= \frac{S_{xy}^s}{S_{xx}^s}
    &amp; \text{def. } \hat{\beta}_1^s \\
  &amp;= \frac{r_{xy}}{1}
    &amp; S_{xy}^s=r_{xy},S_{xx}^s=1\\
  &amp;= r_{xy}
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
\hat{\beta}_0^s
  &amp;= \bar{y}^s - \hat{\beta}_1^s \bar{x}^s
    &amp; \text{def. } \hat{\beta}_0^s \\
  &amp;= 0 - \hat{\beta}_1^s 0
    &amp; \bar{x}^s=\bar{y}^s=0 \\
  &amp;= 0
\end{align*}\]</span></p>
</div>
</details>
<p>So, the estimate of the slope is the sample correlation of the original observations.</p>
<p>Again, we can see this graphically:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="simple-linear-regression.html#cb20-1" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb20-2"><a href="simple-linear-regression.html#cb20-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Ad spending Data.csv&quot;</span>)</span>
<span id="cb20-3"><a href="simple-linear-regression.html#cb20-3" tabindex="-1"></a><span class="co"># Assign data</span></span>
<span id="cb20-4"><a href="simple-linear-regression.html#cb20-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Ad.Spending</span>
<span id="cb20-5"><a href="simple-linear-regression.html#cb20-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Revenue</span>
<span id="cb20-6"><a href="simple-linear-regression.html#cb20-6" tabindex="-1"></a><span class="co"># Standardizes x and y</span></span>
<span id="cb20-7"><a href="simple-linear-regression.html#cb20-7" tabindex="-1"></a>xSta <span class="ot">&lt;-</span> (x <span class="sc">-</span> <span class="fu">mean</span>(x))<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">var</span>(x))</span>
<span id="cb20-8"><a href="simple-linear-regression.html#cb20-8" tabindex="-1"></a>ySta <span class="ot">&lt;-</span> (y <span class="sc">-</span> <span class="fu">mean</span>(y))<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">var</span>(y))</span>
<span id="cb20-9"><a href="simple-linear-regression.html#cb20-9" tabindex="-1"></a><span class="co"># Linear regression on the original data</span></span>
<span id="cb20-10"><a href="simple-linear-regression.html#cb20-10" tabindex="-1"></a>outRegOri <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb20-11"><a href="simple-linear-regression.html#cb20-11" tabindex="-1"></a><span class="co"># Linear regression on the standard data</span></span>
<span id="cb20-12"><a href="simple-linear-regression.html#cb20-12" tabindex="-1"></a>outRegSta <span class="ot">&lt;-</span> <span class="fu">lm</span>(ySta <span class="sc">~</span> xSta)</span>
<span id="cb20-13"><a href="simple-linear-regression.html#cb20-13" tabindex="-1"></a><span class="co"># Plots</span></span>
<span id="cb20-14"><a href="simple-linear-regression.html#cb20-14" tabindex="-1"></a><span class="do">## Two plots in the same image</span></span>
<span id="cb20-15"><a href="simple-linear-regression.html#cb20-15" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb20-16"><a href="simple-linear-regression.html#cb20-16" tabindex="-1"></a><span class="do">## Original data</span></span>
<span id="cb20-17"><a href="simple-linear-regression.html#cb20-17" tabindex="-1"></a><span class="co"># Plots the points</span></span>
<span id="cb20-18"><a href="simple-linear-regression.html#cb20-18" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb20-19"><a href="simple-linear-regression.html#cb20-19" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb20-20"><a href="simple-linear-regression.html#cb20-20" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb20-21"><a href="simple-linear-regression.html#cb20-21" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb20-22"><a href="simple-linear-regression.html#cb20-22" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb20-23"><a href="simple-linear-regression.html#cb20-23" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb20-24"><a href="simple-linear-regression.html#cb20-24" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb20-25"><a href="simple-linear-regression.html#cb20-25" tabindex="-1"></a>       <span class="at">b   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb20-26"><a href="simple-linear-regression.html#cb20-26" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb20-27"><a href="simple-linear-regression.html#cb20-27" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb20-28"><a href="simple-linear-regression.html#cb20-28" tabindex="-1"></a><span class="do">## Standard data</span></span>
<span id="cb20-29"><a href="simple-linear-regression.html#cb20-29" tabindex="-1"></a><span class="co"># Plots the points</span></span>
<span id="cb20-30"><a href="simple-linear-regression.html#cb20-30" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> xSta,</span>
<span id="cb20-31"><a href="simple-linear-regression.html#cb20-31" tabindex="-1"></a>     <span class="at">y    =</span> ySta,</span>
<span id="cb20-32"><a href="simple-linear-regression.html#cb20-32" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending (Centered)&quot;</span>,</span>
<span id="cb20-33"><a href="simple-linear-regression.html#cb20-33" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb20-34"><a href="simple-linear-regression.html#cb20-34" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb20-35"><a href="simple-linear-regression.html#cb20-35" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb20-36"><a href="simple-linear-regression.html#cb20-36" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegSta<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb20-37"><a href="simple-linear-regression.html#cb20-37" tabindex="-1"></a>       <span class="at">b   =</span> outRegSta<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb20-38"><a href="simple-linear-regression.html#cb20-38" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb20-39"><a href="simple-linear-regression.html#cb20-39" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb20-40"><a href="simple-linear-regression.html#cb20-40" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v   =</span> <span class="dv">0</span>,</span>
<span id="cb20-41"><a href="simple-linear-regression.html#cb20-41" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb20-42"><a href="simple-linear-regression.html#cb20-42" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb20-43"><a href="simple-linear-regression.html#cb20-43" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/slr-standarize-xy-1.png" width="672" /></p>
<p>finally, we show that the sample correlation is between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>.</p>
<div class="proposition">
<p><span id="prp:prp-correlation-1-and-1" class="proposition"><strong>Proposition 4.9  </strong></span>The sample correlation <span class="math inline">\(r_{xy}\)</span> lies in <span class="math inline">\([-1,1]\)</span>.</p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-9" class="proof"><em>Proof</em>. </span>First let us show that <span class="math inline">\(\frac{1}{n-1} \sum_{i=1}^n (x_i^s + y_i^s)^2=2(1 + r_{xy})\)</span>
<span class="math display">\[\begin{align*}
\frac{\sum_{i=1}^n (x_i^s + y_i^s)^2}{n-1}
  &amp;= \frac{1}{n-1} \sum_{i=1}^n \left((x_i^s)^2 + 2x_i^s y_i^s + (y_i^s)^2 \right)
    &amp; \\
  &amp;= \frac{\sum_{i=1}^n (x_i^s)^2}{n-1}  + 2\frac{\sum_{i=1}^n x_i^sy_i^s}{n-1}  + \frac{\sum_{i=1}^n (y_i^s)^2}{n-1}
    &amp; \text{lin. sum} \\
  &amp;= \frac{\sum_{i=1}^n (x_i^s - \bar{x}^s)^2}{n-1}  + 2\frac{\sum_{i=1}^n (x_i^s - \bar{x}^s)(y_i^s - \bar{y}^s)}{n-1}  
    &amp; \\
  &amp;\quad + \frac{\sum_{i=1}^n (y_i^s - \bar{y}^s)^2}{n-1}
    &amp;  \bar{x}^s=\bar{y}^s=0 \\
  &amp;= S_{xx}^2 + 2 S_{xy}^s + S_{yy}^s
    &amp;  \text{def. } S_{xx}^2,S_{xy}^s,S_{yy}^s \\
  &amp;= 1 + 2r_{xy} + 1
    &amp;  S_{xy}^s=r_{xy} \\
  &amp;= 2(1 + r_{xy})
    &amp; \\
\end{align*}\]</span>
similarly, it can be shown that <span class="math inline">\(\frac{1}{n-1} \sum_{i=1}^n (x_i^s - y_i^s)^2=2(1 - r_{xy})\)</span>
<span class="math display">\[\begin{align*}
\frac{\sum_{i=1}^n (x_i^s - y_i^s)^2}{n-1}
  &amp;= \frac{1}{n-1} \sum_{i=1}^n \left((x_i^s)^2 - 2x_i^s y_i^s + (y_i^s)^2 \right)
    &amp; \\
  &amp;= \frac{\sum_{i=1}^n (x_i^s)^2}{n-1}  - 2\frac{\sum_{i=1}^n x_i^sy_i^s}{n-1}  + \frac{\sum_{i=1}^n (y_i^s)^2}{n-1}
    &amp; \text{lin. sum} \\
  &amp;= \frac{\sum_{i=1}^n (x_i^s - \bar{x}^s)^2}{n-1}  - 2\frac{\sum_{i=1}^n (x_i^s - \bar{x}^s)(y_i^s - \bar{y}^s)}{n-1}  
    &amp; \\
  &amp;\quad + \frac{\sum_{i=1}^n (y_i^s - \bar{y}^s)^2}{n-1}
    &amp;  \bar{x}^s=\bar{y}^s=0 \\
  &amp;= S_{xx}^2 - 2 S_{xy}^s + S_{yy}^s
    &amp;  \text{def. } S_{xx}^2,S_{xy}^s,S_{yy}^s \\
  &amp;= 1 - 2r_{xy} + 1
    &amp;  S_{xy}^s=r_{xy} \\
  &amp;= 2(1 - r_{xy})
    &amp; \\
\end{align*}\]</span>
Now since,
<span class="math display">\[\frac{1}{n-1} \sum_{i=1}^n (x_i^s + y_i^s)^2, \frac{1}{n-1} \sum_{i=1}^n (x_i^s - y_i^s)^2 \geq 0\]</span>
then
<span class="math display">\[2(1 + r_{xy}) \geq 0 =&gt; r_{xy} \geq -1\]</span>
and
<span class="math display">\[2(1 - r_{xy}) \geq 0 =&gt; r_{xy} \leq  1\]</span>
Then
<span class="math display">\[ -1 \leq r_{xy} \leq 1 \]</span></p>
</div>
</details>
<p>which implies that the slope of the regression analysis after standarizing both
variables is going to be in the interval <span class="math inline">\((-1, 1)\)</span>.</p>
<p>Effect on the fitted model:</p>
<ul>
<li>The model has no intercept in the standardized coordinates; the estimated slope is unitless and expresses change in the response in standard‑deviation units per one standard‑deviation change in the predictor.<br />
</li>
<li>In the simple linear setting, the standardized slope equals the correlation between predictor and response; its squared value equals the proportion of variance explained by the predictor, as we will see in the next section.</li>
</ul>
<p>Numerical consequences:</p>
<ul>
<li>Standardization often improves numerical conditioning and is recommended before applying algorithms or regularization methods that are sensitive to variable scale.</li>
</ul>
<p>Practical considerations:</p>
<ul>
<li>Standard errors and any inferential statements refer to the transformed units; translating results back to original units requires reversing the standardization.</li>
<li>Standardize when you need unitless effect sizes or when preparing inputs for penalized regression or gradient-based optimization.<br />
</li>
<li>Be mindful of outliers: standardization using a classical spread measure can be heavily influenced by extreme values; consider robust scaling if outliers are a concern.<br />
</li>
<li>Save the centering and scaling constants from the training data to apply the same transformation to new observations and to allow conversion of results back to original units for presentation.</li>
</ul>
</div>
<div id="summary-of-centering-and-standarizing-in-slr" class="section level3 hasAnchor" number="4.5.4">
<h3><span class="header-section-number">4.5.4</span> Summary of Centering and Standarizing in SLR<a href="simple-linear-regression.html#summary-of-centering-and-standarizing-in-slr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Center the predictor when you want a meaningful intercept.</li>
<li>Centering is a minimal, low‑risk transformation useful in many settings.<br />
</li>
<li>Center both variables when the focus is on deviations from typical values or when simplifying variance decomposition is helpful.</li>
<li>Standardize both variables when you want unitless, comparable coefficients or when scale sensitivity of algorithms or penalties is a concern.<br />
</li>
<li>In all cases, transformations do not change the underlying linear relationship or predictive performance (once predictions are transformed back), but they change numerical values and interpretation of coefficients. Always document and retain the transformation constants computed from the training sample to ensure valid application to new data and correct interpretation of results.</li>
</ul>
<hr />
</div>
</div>
<div id="coefficient-of-determination" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Coefficient of Determination<a href="simple-linear-regression.html#coefficient-of-determination" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far we have focused on finding the “best” line by estimating the coefficients that minimize the sum of squared errors. We have derived the least‑squares estimates, obtained fitted values and residuals, and discussed some algebraic properties of those estimates. What remains is to evaluate how well the fitted line captures the variability in the response. The principal summary measure for this purpose in linear regression is the coefficient of determination. Before defining it, we introduce a few standard quantities that decompose the total variability of the response.</p>
<div class="definition">
<p><span id="def:total-sum-of-squares" class="definition"><strong>Definition 4.4  (Total Sum of Squares) </strong></span>The Total Sum of Squares is given by:
<span class="math display">\[SS_{tot} = \sum_{i=1}^n (y_i - \bar{y})\]</span></p>
</div>
<p>The Total Sum of Squares is the fundamental measure of the total variability present in the observed responses. It quantifies how much the individual response values deviate, collectively, from their overall average. In other words, it is the baseline amount of variation in the dependent variable before any predictor information is used. You can think of it in several ways:</p>
<ul>
<li>As a proxy for uncertainty. The larger this quantity, the greater the uncertainty in the dependent variable.<br />
</li>
<li>It is proportional to the sample variance.<br />
</li>
<li>It is what you obtain when solving the following minimization problem:</li>
</ul>
<p><span class="math display">\[
\min_{\beta_0} \sum_{i=1}^n (y_i - \beta_0)^2,
\]</span></p>
<p>that is, the best possible sum of squared errors when no information from the independent variables <span class="math inline">\(x_1,\ldots,x_n\)</span> is available.</p>
<div class="definition">
<p><span id="def:residual-sum-of-squares" class="definition"><strong>Definition 4.5  (Residual Sum of Squares) </strong></span>The Residual Sum of Squares is given by:
<span class="math display">\[ SS_{res} = \sum_{i=1}^n(\hat{e}_i)^2 = \sum_{i=1}^n(y_i - \hat{y}_i)^2\]</span></p>
</div>
<p>The Residual Sum of Squares (RSS) is the aggregate measure of the discrepancies between observed responses and the responses predicted by a fitted regression model. Concretely, it sums the squared prediction errors (residuals) across all observations and therefore quantifies the total unexplained variation left after fitting the model.</p>
<p>Interpretation:</p>
<ul>
<li>Unexplained variability: RSS represents the portion of the data’s variability that the model does not capture. A smaller RSS indicates that the model’s predictions are closer to the observed values and that less variability remains unexplained.<br />
</li>
<li>Measure of fit error: RSS is a direct summary of model misspecification and random noise as reflected in the residuals; it expresses how far, in aggregate, observed outcomes stray from the fitted line.</li>
</ul>
<p>Key properties:</p>
<ul>
<li>Nonnegative and additive: RSS cannot be negative, and it increases when residuals grow larger in magnitude.<br />
</li>
<li>Minimization target: In ordinary least squares, the fitted coefficients are chosen to minimize RSS; thus RSS at the fitted coefficients is the smallest possible sum of squared residuals for the chosen model form.<br />
</li>
<li>Basis for variance estimation: RSS underlies the standard estimate of the model’s error variance, after accounting for the loss of degrees of freedom due to parameter estimation.</li>
</ul>
<p>Role in regression analysis:</p>
<ul>
<li>Diagnostic and evaluation metric: RSS is used together with total variability to compute proportions of variance explained and other fit measures. It is central to testing whether predictors provide a meaningful improvement over a simpler model and to constructing confidence intervals and prediction intervals.<br />
</li>
<li>Guide to improvement: Large RSS relative to total variability suggests the model is missing important structure or that noise is substantial; small RSS suggests good in‑sample fit (but not necessarily good out‑of‑sample performance).</li>
</ul>
<p>Practical notes:</p>
<ul>
<li>Dependence on scale and sample size: RSS depends on the units of the response and on the number of observations, so it is most useful in relative comparisons (e.g., between models on the same dataset) rather than as an absolute measure across datasets.<br />
</li>
<li>Use with diagnostics: Interpreting RSS should be accompanied by residual diagnostics and validation checks to ensure a low RSS reflects genuine explanatory power rather than overfitting or violated assumptions.</li>
</ul>
<div class="definition">
<p><span id="def:explained-sum-of-squares" class="definition"><strong>Definition 4.6  (Explained Sum of Squares) </strong></span>The Explained Sum of Squares is given by:
<span class="math display">\[ SS_{reg} = \sum_{i=1}^n(\hat{y}_i - \bar{y}) = \sum_{i=1}^n(\hat{y}_i - \hat{\bar{y}}) \]</span></p>
</div>
<p>The Explained Sum of Squares is the portion of the total variability in the dependent variable that is accounted for by the fitted regression model. It quantifies how much of the variation in the observed responses is captured by the model’s fitted values, as compared with using only a single constant (the sample mean).</p>
<p>Interpretation:</p>
<ul>
<li>It measures the improvement in fit achieved by the regression line relative to the baseline of predicting the mean for every observation.<br />
</li>
<li>A larger explained sum of squares indicates that the model captures more of the systematic variation in the response and thus provides a better summary of the relationship between predictor and response.</li>
</ul>
<p>Key properties:</p>
<ul>
<li>Nonnegative: it cannot be negative, because the fitted model cannot explain less variation than the baseline in a least‑squares fit.<br />
</li>
<li>Bounded above by the total variability: the explained portion cannot exceed the total variability present in the data, we will see this next.</li>
<li>Basis for fit measures: it is the numerator in the fraction that expresses the proportion of variance explained by the model, and it plays a central role in analysis of variance decompositions and in F‑tests comparing nested models.<br />
</li>
<li>Sensitive to model choice: adding predictors or model flexibility can increase the explained sum of squares, which is why measures that penalize complexity are sometimes used when comparing models.</li>
</ul>
<div class="proposition">
<p><span id="prp:prp-sum-of-squares-decomposition" class="proposition"><strong>Proposition 4.10  (Sum of Squares Decomposition) </strong></span>The <span class="math inline">\(SS_{tot}\)</span>, <span class="math inline">\(SS_{res}\)</span> and <span class="math inline">\(SS_{reg}\)</span> are related by the following identity:
<span class="math display">\[ SS_{tot} = SS_{reg} + SS_{res} \]</span></p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-10" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
SS_{tot}
  &amp;= \sum_{i=1}^n(y_i - \bar{y})^2
    &amp; \text{def. } SS_{tot} \\
  &amp;= \sum_{i=1}^n(y_i - \hat{y}_i + \hat{y}_i - \bar{y})^2
    &amp; \text{add } (0=- \hat{y}_i + \hat{y}_i) \\
  &amp;= \sum_{i=1}^n\left((y_i - \hat{y}_i)^2 + 2(y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) + (\hat{y}_i - \bar{y})^2\right)  
    &amp; \\
  &amp;= \sum_{i=1}^n(y_i - \hat{y}_i)^2 + 2 \sum_{i=1}^n(y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) + \sum_{i=1}^n(\hat{y}_i - \bar{y})^2
    &amp; \text{lin. sum} \\
  &amp;= SS_{res} + 2 \sum_{i=1}^n(y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) + SS_{reg}
    &amp; \text{def. } SS_{res}, SS_{reg} \\
  &amp;= SS_{res} + 2 \sum_{i=1}^n\hat{e}_i(\hat{y}_i - \bar{y}) + SS_{reg}  
    &amp; \text{def. } \hat{e}_i \\
  &amp;= SS_{res} + 2 \sum_{i=1}^n(\hat{e}_i\hat{y}_i - \hat{e}_i\bar{y}) + SS_{reg}   
    &amp; \\
  &amp;= SS_{res} + 2 \sum_{i=1}^n\hat{e}_i\hat{y}_i - 2 \sum_{i=1}^n\hat{e}_i\bar{y} + SS_{reg}  
    &amp; \text{lin. sum} \\
  &amp;= SS_{res} + 2 &lt;\hat{\mathbf{e}},\hat{\mathbf{y}}&gt; - 2 \sum_{i=1}^n\hat{e}_i\bar{y} + SS_{reg}  
    &amp; \text{def. cross-prod.} \\
  &amp;= SS_{res} - 2 \sum_{i=1}^n\hat{e}_i\bar{y} + SS_{reg}  
    &amp; &lt;\hat{\mathbf{e}},\hat{\mathbf{y}}&gt;=0 \\
  &amp;= SS_{res} - 2 \bar{y} \sum_{i=1}^n\hat{e}_i + SS_{reg}  
    &amp; \text{lin. sum} \\
  &amp;= SS_{res} + SS_{reg}
    &amp; \sum_{i=1}^n\hat{e}_i = 0 \\
\end{align*}\]</span></p>
</div>
</details>
<div class="definition">
<p><span id="def:coefficient-of-determmination" class="definition"><strong>Definition 4.7  (Coefficient of Determmination) </strong></span>The Coefficient of Determmination is given by:
<span class="math display">\[ R^2 = \frac{SS_{reg}}{SS_{tot}} \]</span></p>
</div>
<div class="proposition">
<p><span id="prp:prp-coefficient-of-determmination" class="proposition"><strong>Proposition 4.11  (Alternate Definition of the Coefficient of Determmination) </strong></span>The Coefficient of Determmination can be defined alternatively as:
<span class="math display">\[ R^2 = 1 - \frac{SS_{res}}{SS_{tot}} \]</span></p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-11" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
R^2
  &amp;= \frac{SS_{reg}}{SS_{tot}}
    &amp; \text{def. } R^2 \\
  &amp;= 1 - \frac{SS_{tot} - SS_{res}}{SS_{tot}}
    &amp; SS_{tot} = SS_{res} + SS_{reg} \\
  &amp;= \frac{SS_{tot}}{SS_{tot}} - \frac{SS_{res}}{SS_{tot}}  
    &amp; \\
  &amp;= 1 - \frac{SS_{res}}{SS_{tot}}  
    &amp; \\
\end{align*}\]</span></p>
</div>
</details>
<div class="proposition">
<p><span id="prp:prp-coefficient-of-determmination-correlation" class="proposition"><strong>Proposition 4.12  </strong></span>The Coefficient of Determmination is related to the sample correlation by
<span class="math display">\[ R^2 = r_{xy}^2 \]</span></p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-12" class="proof"><em>Proof</em>. </span>First let us express <span class="math inline">\(SS_{reg}\)</span> in a more convinient way:</p>
<p><span class="math display">\[\begin{align*}
SS_{reg}
  &amp;= \sum_{i=1}^n(\hat{y}_i - \bar{y})^2
    &amp; \text{def. } SS_{reg} \\
  &amp;= \sum_{i=1}^n(\hat{\beta}_0 + \hat{\beta}_1 x_i - \bar{y})^2  
    &amp; \text{def. } \hat{y}_i \\
  &amp;= \sum_{i=1}^n(\bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 x_i - \bar{y})^2   
    &amp; \text{def. } \hat{\beta}_0 \\
  &amp;= \sum_{i=1}^n(\hat{\beta}_1 x_i - \hat{\beta}_1 \bar{x})^2    
    &amp; \\
  &amp;= \sum_{i=1}^n\hat{\beta}_1^2(x_i - \bar{x})^2   
    &amp; \\
  &amp;= \hat{\beta}_1^2 \sum_{i=1}^n(x_i - \bar{x})^2   
    &amp; \text{lin. sum}\\
  &amp;= \hat{\beta}_1^2 S_{xx} (n-1)    
    &amp; \text{def. } S_{xx} \\
  &amp;= \left( \frac{S_{xy}}{S_{xx}} \right)^2 S_{xx} (n-1)   
    &amp; \text{def. } \hat{\beta}_1 \\
  &amp;= \frac{S_{xy}^2}{S_{xx}^2} S_{xx} (n-1)
    &amp; \\
  &amp;= \frac{S_{xy}^2}{S_{xx}} (n-1)
    &amp; \\
\end{align*}\]</span>
Now, note that:
<span class="math display">\[ SS_{tot} = (n-1)S_{yy} \]</span>
Then
<span class="math display">\[\begin{align*}
R^2
  &amp;= \frac{SS_reg}{SS_{tot}}
    &amp; \text{def. } R^2 \\
  &amp;= \frac{\frac{S_{xy}^2}{S_{xx}} (n-1)}{S_{yy} (n-1)}  
    &amp; \text{prev. res.} \\
  &amp;= \frac{S_{xy}^2}{S_{xx}S_{yy}}   
    &amp; \\
  &amp;= \left[\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}\right]^2   
    &amp; \\
  &amp;= r_{xy}^2   
    &amp; \text{def. } r_{xy} \\
\end{align*}\]</span></p>
</div>
</details>
<div class="corollary">
<p><span id="cor:prp-coefficient-of-determmination-0-1" class="corollary"><strong>Corollary 4.1  </strong></span>The Coefficient of Determmination is between 0 and 1.</p>
</div>
<p>The coefficient of determination is a summary measure that quantifies the fraction of the total variability in the dependent variable that is accounted for by a regression model. It expresses how much of the observed variation can be attributed to the systematic relationship captured by the model, as opposed to unexplained variation.</p>
<p>Interpretation:</p>
<ul>
<li>Proportion explained: The coefficient gives the proportion of the response variability that the model explains. A larger value indicates that a greater share of the variability is captured by the fitted relationship.<br />
</li>
<li>Measure of fit: It provides a single, unitless index of in‑sample goodness of fit, useful for judging how well the model summarizes the observed data.</li>
</ul>
<p>Key properties:</p>
<ul>
<li>Boundedness: Under the usual regression with an intercept, the coefficient lies between zero and one. Values near one indicate that most variability is explained; values near zero indicate little explanatory power.<br />
</li>
<li>Relation to explained and unexplained variation: The coefficient is obtained by comparing the variation explained by the model with the total variation present in the data; it increases as explained variation grows or unexplained variation shrinks.<br />
</li>
<li>Equivalence in simple linear regression: In the simple linear case, the coefficient equals the square of the sample correlation between predictor and response, linking it directly to linear association strength.</li>
</ul>
<p>Practical considerations and limitations:</p>
<ul>
<li>Not a test of causation: A high coefficient does not imply a causal relationship; it only describes the extent of linear association in the sample.<br />
</li>
<li>Sensitivity to model complexity: Adding predictors will not decrease this measure and can inflate it even for predictors with little true predictive value. Adjusted versions or cross‑validation are often used when comparing models of differing complexity.<br />
</li>
<li>Dependence on context and scale: While unitless, the coefficient’s practical meaning depends on the problem domain; what constitutes a “good” value varies by field and by the intrinsic variability of the outcome.<br />
</li>
<li>Reliance on model assumptions: Interpretation in terms of explained variance assumes the model form is appropriate; violations of assumptions or influential observations can make the coefficient misleading.</li>
</ul>
<p>Use in practice:</p>
<ul>
<li>Primary summary of fit: It is widely used as a convenient summary of how well a regression model captures observed variation.<br />
</li>
<li>Complementary diagnostics: It should be interpreted together with residual analysis, model validation, and, for model selection, complexity‑adjusted criteria or predictive performance measures.</li>
</ul>
<p>Summary<br />
The coefficient of determination quantifies the share of total variability explained by a regression model. It is a useful, unitless index of in‑sample fit, but it must be interpreted with attention to model form, complexity, and the substantive context.</p>
<p>The larger the <span class="math inline">\(R^2\)</span>, the better the fit of the linear model. A low <span class="math inline">\(R^2\)</span> can arise for two reasons:</p>
<ul>
<li>The data are not well described by a linear relationship. In that case a linear model will explain little of the association (although sometimes a linear model can still serve as a useful approximation to a nonlinear relationship).<br />
</li>
<li>The data are noisy. Substantial noise can reduce <span class="math inline">\(R^2\)</span> even when the true relationship between the variables is linear.</li>
</ul>
<p>As an example of noisy data, recall the Ad spending data. The data was generated under a linear model, so the relationship between the variables is linear. You can verify this by looking at the code where the data is generated in the introduction. In the following example the effects of adding additional noise to the data can be analyzed. Three noise leves are considered:</p>
<ul>
<li>Level1: Small Noise.</li>
<li>Level2: Medium Noise.</li>
<li>Level3: High noise.</li>
</ul>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="simple-linear-regression.html#cb21-1" tabindex="-1"></a><span class="co"># Sets seed for noise randomness</span></span>
<span id="cb21-2"><a href="simple-linear-regression.html#cb21-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">9142025</span>)</span>
<span id="cb21-3"><a href="simple-linear-regression.html#cb21-3" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb21-4"><a href="simple-linear-regression.html#cb21-4" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Ad spending Data.csv&quot;</span>)</span>
<span id="cb21-5"><a href="simple-linear-regression.html#cb21-5" tabindex="-1"></a><span class="co"># Assign data</span></span>
<span id="cb21-6"><a href="simple-linear-regression.html#cb21-6" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Ad.Spending</span>
<span id="cb21-7"><a href="simple-linear-regression.html#cb21-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Revenue</span>
<span id="cb21-8"><a href="simple-linear-regression.html#cb21-8" tabindex="-1"></a></span>
<span id="cb21-9"><a href="simple-linear-regression.html#cb21-9" tabindex="-1"></a><span class="co"># Adds Noise</span></span>
<span id="cb21-10"><a href="simple-linear-regression.html#cb21-10" tabindex="-1"></a>yNoiLe1 <span class="ot">&lt;-</span> y <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">sd =</span>  <span class="dv">50</span>)</span>
<span id="cb21-11"><a href="simple-linear-regression.html#cb21-11" tabindex="-1"></a>yNoiLe2 <span class="ot">&lt;-</span> y <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">200</span>)</span>
<span id="cb21-12"><a href="simple-linear-regression.html#cb21-12" tabindex="-1"></a>yNoiLe3 <span class="ot">&lt;-</span> y <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">500</span>)</span>
<span id="cb21-13"><a href="simple-linear-regression.html#cb21-13" tabindex="-1"></a></span>
<span id="cb21-14"><a href="simple-linear-regression.html#cb21-14" tabindex="-1"></a><span class="co"># Auxiliary Variables</span></span>
<span id="cb21-15"><a href="simple-linear-regression.html#cb21-15" tabindex="-1"></a>ymax <span class="ot">&lt;-</span> <span class="fu">max</span>(y, yNoiLe1, yNoiLe2, yNoiLe3)</span>
<span id="cb21-16"><a href="simple-linear-regression.html#cb21-16" tabindex="-1"></a>ymin <span class="ot">&lt;-</span> <span class="fu">min</span>(y, yNoiLe1, yNoiLe2, yNoiLe3)</span>
<span id="cb21-17"><a href="simple-linear-regression.html#cb21-17" tabindex="-1"></a>xmax <span class="ot">=</span> <span class="fu">max</span>(x)</span>
<span id="cb21-18"><a href="simple-linear-regression.html#cb21-18" tabindex="-1"></a>xmin <span class="ot">=</span> <span class="fu">min</span>(x)</span>
<span id="cb21-19"><a href="simple-linear-regression.html#cb21-19" tabindex="-1"></a></span>
<span id="cb21-20"><a href="simple-linear-regression.html#cb21-20" tabindex="-1"></a><span class="co"># Performs Linear Regression</span></span>
<span id="cb21-21"><a href="simple-linear-regression.html#cb21-21" tabindex="-1"></a>outRegOri    <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb21-22"><a href="simple-linear-regression.html#cb21-22" tabindex="-1"></a>outRegNoiLe1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(yNoiLe1 <span class="sc">~</span> x)</span>
<span id="cb21-23"><a href="simple-linear-regression.html#cb21-23" tabindex="-1"></a>outRegNoiLe2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(yNoiLe2 <span class="sc">~</span> x)</span>
<span id="cb21-24"><a href="simple-linear-regression.html#cb21-24" tabindex="-1"></a>outRegNoiLe3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(yNoiLe3 <span class="sc">~</span> x)</span>
<span id="cb21-25"><a href="simple-linear-regression.html#cb21-25" tabindex="-1"></a></span>
<span id="cb21-26"><a href="simple-linear-regression.html#cb21-26" tabindex="-1"></a><span class="co"># Plots</span></span>
<span id="cb21-27"><a href="simple-linear-regression.html#cb21-27" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb21-28"><a href="simple-linear-regression.html#cb21-28" tabindex="-1"></a></span>
<span id="cb21-29"><a href="simple-linear-regression.html#cb21-29" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb21-30"><a href="simple-linear-regression.html#cb21-30" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb21-31"><a href="simple-linear-regression.html#cb21-31" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb21-32"><a href="simple-linear-regression.html#cb21-32" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb21-33"><a href="simple-linear-regression.html#cb21-33" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Original Data&quot;</span>,</span>
<span id="cb21-34"><a href="simple-linear-regression.html#cb21-34" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb21-35"><a href="simple-linear-regression.html#cb21-35" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax))</span>
<span id="cb21-36"><a href="simple-linear-regression.html#cb21-36" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb21-37"><a href="simple-linear-regression.html#cb21-37" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb21-38"><a href="simple-linear-regression.html#cb21-38" tabindex="-1"></a>       <span class="at">b   =</span> outRegOri<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb21-39"><a href="simple-linear-regression.html#cb21-39" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb21-40"><a href="simple-linear-regression.html#cb21-40" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb21-41"><a href="simple-linear-regression.html#cb21-41" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb21-42"><a href="simple-linear-regression.html#cb21-42" tabindex="-1"></a>     <span class="at">y    =</span> yNoiLe1,</span>
<span id="cb21-43"><a href="simple-linear-regression.html#cb21-43" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb21-44"><a href="simple-linear-regression.html#cb21-44" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb21-45"><a href="simple-linear-regression.html#cb21-45" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Small Noise Added&quot;</span>,</span>
<span id="cb21-46"><a href="simple-linear-regression.html#cb21-46" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb21-47"><a href="simple-linear-regression.html#cb21-47" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax))</span>
<span id="cb21-48"><a href="simple-linear-regression.html#cb21-48" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb21-49"><a href="simple-linear-regression.html#cb21-49" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegNoiLe1<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb21-50"><a href="simple-linear-regression.html#cb21-50" tabindex="-1"></a>       <span class="at">b   =</span> outRegNoiLe1<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb21-51"><a href="simple-linear-regression.html#cb21-51" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb21-52"><a href="simple-linear-regression.html#cb21-52" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb21-53"><a href="simple-linear-regression.html#cb21-53" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb21-54"><a href="simple-linear-regression.html#cb21-54" tabindex="-1"></a>     <span class="at">y    =</span> yNoiLe2,</span>
<span id="cb21-55"><a href="simple-linear-regression.html#cb21-55" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb21-56"><a href="simple-linear-regression.html#cb21-56" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb21-57"><a href="simple-linear-regression.html#cb21-57" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Medium Noise Added&quot;</span>,</span>
<span id="cb21-58"><a href="simple-linear-regression.html#cb21-58" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb21-59"><a href="simple-linear-regression.html#cb21-59" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax))</span>
<span id="cb21-60"><a href="simple-linear-regression.html#cb21-60" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb21-61"><a href="simple-linear-regression.html#cb21-61" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegNoiLe2<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb21-62"><a href="simple-linear-regression.html#cb21-62" tabindex="-1"></a>       <span class="at">b   =</span> outRegNoiLe2<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb21-63"><a href="simple-linear-regression.html#cb21-63" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb21-64"><a href="simple-linear-regression.html#cb21-64" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb21-65"><a href="simple-linear-regression.html#cb21-65" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb21-66"><a href="simple-linear-regression.html#cb21-66" tabindex="-1"></a>     <span class="at">y    =</span> yNoiLe3,</span>
<span id="cb21-67"><a href="simple-linear-regression.html#cb21-67" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Ad spending&quot;</span>,</span>
<span id="cb21-68"><a href="simple-linear-regression.html#cb21-68" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Revenue&quot;</span>,</span>
<span id="cb21-69"><a href="simple-linear-regression.html#cb21-69" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;High Noise Added&quot;</span>,</span>
<span id="cb21-70"><a href="simple-linear-regression.html#cb21-70" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb21-71"><a href="simple-linear-regression.html#cb21-71" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax))</span>
<span id="cb21-72"><a href="simple-linear-regression.html#cb21-72" tabindex="-1"></a><span class="co"># Plots the regression line</span></span>
<span id="cb21-73"><a href="simple-linear-regression.html#cb21-73" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegNoiLe3<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb21-74"><a href="simple-linear-regression.html#cb21-74" tabindex="-1"></a>       <span class="at">b   =</span> outRegNoiLe3<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb21-75"><a href="simple-linear-regression.html#cb21-75" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb21-76"><a href="simple-linear-regression.html#cb21-76" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/r2-noise-1.png" width="672" /></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="simple-linear-regression.html#cb22-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Original Data LM summary&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Original Data LM summary&quot;</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="simple-linear-regression.html#cb24-1" tabindex="-1"></a><span class="fu">summary</span>(outRegOri)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -248.394  -58.805    3.782   63.577  196.745 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 997.3894    28.8185   34.61   &lt;2e-16 ***
## x             5.0247     0.3818   13.16   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 89.01 on 98 degrees of freedom
## Multiple R-squared:  0.6386, Adjusted R-squared:  0.6349 
## F-statistic: 173.2 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="simple-linear-regression.html#cb26-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Level 1 LM summary&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Level 1 LM summary&quot;</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="simple-linear-regression.html#cb28-1" tabindex="-1"></a><span class="fu">summary</span>(outRegNoiLe1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = yNoiLe1 ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -276.39  -69.79   17.82   73.26  209.52 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 985.2762    31.5963   31.18   &lt;2e-16 ***
## x             5.3080     0.4186   12.68   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 97.58 on 98 degrees of freedom
## Multiple R-squared:  0.6213, Adjusted R-squared:  0.6174 
## F-statistic: 160.8 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="simple-linear-regression.html#cb30-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Level 2 LM summary&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Level 2 LM summary&quot;</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="simple-linear-regression.html#cb32-1" tabindex="-1"></a><span class="fu">summary</span>(outRegNoiLe2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = yNoiLe2 ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -624.54 -137.77   11.28  160.65  514.78 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1034.0315    70.8342  14.598  &lt; 2e-16 ***
## x              4.5527     0.9385   4.851 4.63e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 218.8 on 98 degrees of freedom
## Multiple R-squared:  0.1936, Adjusted R-squared:  0.1854 
## F-statistic: 23.53 on 1 and 98 DF,  p-value: 4.63e-06</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="simple-linear-regression.html#cb34-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Level 3 LM summary&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Level 3 LM summary&quot;</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="simple-linear-regression.html#cb36-1" tabindex="-1"></a><span class="fu">summary</span>(outRegNoiLe3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = yNoiLe3 ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1313.46  -275.87    77.13   322.09   959.94 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  923.902    135.408   6.823 7.43e-10 ***
## x              6.393      1.794   3.564 0.000567 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 418.2 on 98 degrees of freedom
## Multiple R-squared:  0.1147, Adjusted R-squared:  0.1057 
## F-statistic:  12.7 on 1 and 98 DF,  p-value: 0.0005672</code></pre>
<p>here where we observe that the point cloud is more dispersed and looks less than a line
the more noise is added, but the estimated regression line changes only a little bit.
We can also see how the <span class="math inline">\(R^2\)</span> becomes smaller as more noise is added.</p>
<hr />
</div>
<div id="residual-analysis" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Residual Analysis<a href="simple-linear-regression.html#residual-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>While the coefficient of determination (<span class="math inline">\(R^2\)</span>) provides a measure of how well the data fit a linear model, it cannot explain why the fit is good or poor. To gain insights into potential problems with the model, the most straightforward approach is to examine the residuals or estimated errors. Residual analysis helps identify issues that can compromise the validity of the model’s assumptions.</p>
<p>Currently, we focus on four common problems that residual analysis can reveal:</p>
<ul>
<li><p><strong>The regression function is not truly linear.</strong><br />
If the relationship between the predictor and response is nonlinear, then a linear model will not capture the true pattern, leading to systematic patterns in the residuals.</p></li>
<li><p><strong>The variance of the error terms is not constant (heteroscedasticity).</strong><br />
Non-constant variance means that the spread of the residuals varies with the fitted values or predictor, which violates a key assumption of linear regression and can affect inference.</p></li>
<li><p><strong>There are outliers.</strong><br />
Outliers are observations that deviate markedly from the overall pattern and can disproportionately influence the estimated regression coefficients or inflate measures of error.</p></li>
<li><p><strong>Important variables are omitted.</strong><br />
Omitting relevant variables leads to biased residuals and can cause the model to systematically under- or over-predict responses based on unaccounted factors.</p></li>
</ul>
<hr />
<div id="non-linear-regression-function" class="section level3 hasAnchor" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> Non-linear regression function<a href="simple-linear-regression.html#non-linear-regression-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In many real-world situations, the relationship between the variables of interest may not be well described by a simple straight line. While linear regression is a powerful and widely used technique, it relies on the assumption that the true relationship between the predictor and response variables is linear.</p>
<p>However, when this assumption does not hold, the linear model can lead to poor fits, systematic bias, and misleading inferences. To better understand the nature of the relationship, it is important to examine the fitted regression function and related residuals carefully.</p>
<p>Consider the example of analyzing the relationship between burger price and the number of burgers sold.</p>
<p><img src="_main_files/figure-html/burgers-residuals-1.png" width="672" />
Here we can clearly appreciate that there is something wrong. The residuals clearly
indicate that a non linear relationship is present in the data. One can solve these
problem by transforming one or both of the variables and then applying linear
regression. Common transformations functions <span class="math inline">\(g\)</span> are (but are not limited to):</p>
<ul>
<li><span class="math inline">\(g(x) = x^2\)</span></li>
<li><span class="math inline">\(g(x) = \sqrt{x}\)</span></li>
<li><span class="math inline">\(g(x) = log(x)\)</span></li>
</ul>
<p>This transformations can be applied to the independent variable, to the dependent
variable of both.</p>
<p>In the next example we work with <span class="math inline">\(log(\text{Burgers Sold})\)</span> instead of directly
working with “Burgers Sold”.</p>
<p><img src="_main_files/figure-html/burgers-residuals-tr2-1.png" width="672" /></p>
<p>In this case there seems to be a much better fit. We can compare this to the
following transformation <span class="math inline">\(\log{(\text{Price})}\)</span>:</p>
<p><img src="_main_files/figure-html/burgers-residuals-tr1-1.png" width="672" /></p>
<p>It this case, these transformations improve the fit of the model, however which one
is the best one?</p>
<p>One can check the <span class="math inline">\(R^2\)</span> of the different transformations:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="simple-linear-regression.html#cb38-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Burger Data.csv&quot;</span>)</span>
<span id="cb38-2"><a href="simple-linear-regression.html#cb38-2" tabindex="-1"></a></span>
<span id="cb38-3"><a href="simple-linear-regression.html#cb38-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Price</span>
<span id="cb38-4"><a href="simple-linear-regression.html#cb38-4" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Burgers</span>
<span id="cb38-5"><a href="simple-linear-regression.html#cb38-5" tabindex="-1"></a></span>
<span id="cb38-6"><a href="simple-linear-regression.html#cb38-6" tabindex="-1"></a>outRegOri <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb38-7"><a href="simple-linear-regression.html#cb38-7" tabindex="-1"></a>outRegTr1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(y) <span class="sc">~</span> x)</span>
<span id="cb38-8"><a href="simple-linear-regression.html#cb38-8" tabindex="-1"></a>outRegTr2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">log</span>(x))</span>
<span id="cb38-9"><a href="simple-linear-regression.html#cb38-9" tabindex="-1"></a></span>
<span id="cb38-10"><a href="simple-linear-regression.html#cb38-10" tabindex="-1"></a><span class="fu">summary</span>(outRegOri)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -17.310  -5.637  -0.553   2.899  47.182 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  76.9130     2.0098   38.27   &lt;2e-16 ***
## x            -4.3417     0.2194  -19.79   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.522 on 98 degrees of freedom
## Multiple R-squared:  0.7999, Adjusted R-squared:  0.7978 
## F-statistic: 391.7 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="simple-linear-regression.html#cb40-1" tabindex="-1"></a><span class="fu">summary</span>(outRegTr1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(y) ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.39409 -0.08669  0.00065  0.09325  0.37302 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  4.518050   0.034072  132.60   &lt;2e-16 ***
## x           -0.109240   0.003719  -29.37   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1445 on 98 degrees of freedom
## Multiple R-squared:  0.898,  Adjusted R-squared:  0.897 
## F-statistic: 862.8 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="simple-linear-regression.html#cb42-1" tabindex="-1"></a><span class="fu">summary</span>(outRegTr2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ log(x))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -12.4312  -2.8468   0.1724   3.1830   9.8330 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  99.6184     1.4927   66.74   &lt;2e-16 ***
## log(x)      -29.8274     0.7237  -41.22   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.449 on 98 degrees of freedom
## Multiple R-squared:  0.9455, Adjusted R-squared:  0.9449 
## F-statistic:  1699 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We will see other ways to choose the transformation later.</p>
<p>Why non-linearity matters:</p>
<ul>
<li><strong>Violation of assumptions:</strong> Non-linearity violates the key assumption of linear regression that the expected response changes linearly with the predictor. This can lead to biased estimates and unreliable inference.</li>
<li><strong>Inadequate modeling:</strong> A simple straight line may oversimplify the true relationship, missing important features such as thresholds, plateaus, or other non-linear behaviors.</li>
<li><strong>Need for model refinement:</strong> Detecting non-linearity encourages the use of more flexible modeling approaches, such as polynomial regression, spline regression, or other non-linear models that better capture the underlying pattern.</li>
</ul>
<p>Addressing non-linearity:</p>
<ul>
<li><p><strong>Visualization:</strong> Always examine scatterplots of the data accompanied by the fitted regression line and residuals plots to identify non-linear patterns visually.</p></li>
<li><p><strong>Transformations:</strong> Applying data transformations (e.g., logarithmic, square root) to one or both variables can sometimes linearize the relationship.</p></li>
<li><p><strong>Non-linear models:</strong> Use specialized non-linear regression models that explicitly accommodate curves and other complex relationships.</p></li>
</ul>
<p>In summary, recognizing when the relationship is non-linear is crucial for building accurate models. By analyzing the fitted line and residuals plots, analysts can identify these cases and take appropriate steps to improve their modeling approach, leading to better understanding and more reliable predictions.</p>
<hr />
</div>
<div id="heteroscedasticity" class="section level3 hasAnchor" number="4.7.2">
<h3><span class="header-section-number">4.7.2</span> Heteroscedasticity<a href="simple-linear-regression.html#heteroscedasticity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In linear regression modeling, one of the key assumptions is that the variability (or variance) of the errors remains constant across all levels of the explanatory variable. This assumption is known as homoscedasticity. When this assumption is violated–that is, when the variance of the errors is not constant but instead depends on the level of the predictor–it is referred to as <strong>heteroscedasticity</strong>.</p>
<p><strong>Definition and intuition</strong><br />
Heteroscedasticity occurs when the spread or dispersion of the residuals changes systematically with the value of the independent variable. For example, as the value of the predictor increases, the errors or deviations in the response variable may become larger or smaller, leading to a funnel-shaped or other non-uniform pattern in the residuals plot.</p>
<p><strong>An illustrative example</strong><br />
Consider a new data set with the heights of several children. Suppose we believe there is a linear relationship between a child’s age and height: as children grow older, their height increases. However, it is also reasonable to expect that the variability in height differences increases for older children than for younger children: taller children tend to have more variation in height than shorter children.<br />
This expectation can be visualized in a simulated data set where the residuals (the deviations from the fitted line) tend to increase with age, forming a funnel shape in the residuals plot. Such a pattern indicates heteroscedasticity, where the error variance depends on the predictor.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="simple-linear-regression.html#cb44-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Height Data.csv&quot;</span>)</span>
<span id="cb44-2"><a href="simple-linear-regression.html#cb44-2" tabindex="-1"></a></span>
<span id="cb44-3"><a href="simple-linear-regression.html#cb44-3" tabindex="-1"></a>outReg <span class="ot">&lt;-</span> <span class="fu">lm</span>(Height <span class="sc">~</span> Age, <span class="at">data =</span> dat)</span>
<span id="cb44-4"><a href="simple-linear-regression.html#cb44-4" tabindex="-1"></a></span>
<span id="cb44-5"><a href="simple-linear-regression.html#cb44-5" tabindex="-1"></a><span class="co"># Original Data</span></span>
<span id="cb44-6"><a href="simple-linear-regression.html#cb44-6" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb44-7"><a href="simple-linear-regression.html#cb44-7" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> dat<span class="sc">$</span>Age,</span>
<span id="cb44-8"><a href="simple-linear-regression.html#cb44-8" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>Height,</span>
<span id="cb44-9"><a href="simple-linear-regression.html#cb44-9" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Age (years)&quot;</span>,</span>
<span id="cb44-10"><a href="simple-linear-regression.html#cb44-10" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Height (in)&quot;</span>,</span>
<span id="cb44-11"><a href="simple-linear-regression.html#cb44-11" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Original Data&quot;</span>,</span>
<span id="cb44-12"><a href="simple-linear-regression.html#cb44-12" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb44-13"><a href="simple-linear-regression.html#cb44-13" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outReg<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb44-14"><a href="simple-linear-regression.html#cb44-14" tabindex="-1"></a>       <span class="at">b   =</span> outReg<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb44-15"><a href="simple-linear-regression.html#cb44-15" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb44-16"><a href="simple-linear-regression.html#cb44-16" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb44-17"><a href="simple-linear-regression.html#cb44-17" tabindex="-1"></a><span class="co"># Residuals</span></span>
<span id="cb44-18"><a href="simple-linear-regression.html#cb44-18" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> dat<span class="sc">$</span>Age,</span>
<span id="cb44-19"><a href="simple-linear-regression.html#cb44-19" tabindex="-1"></a>     <span class="at">y =</span> outReg<span class="sc">$</span>residuals,</span>
<span id="cb44-20"><a href="simple-linear-regression.html#cb44-20" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Age (years)&quot;</span>,</span>
<span id="cb44-21"><a href="simple-linear-regression.html#cb44-21" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals (in)&quot;</span>,</span>
<span id="cb44-22"><a href="simple-linear-regression.html#cb44-22" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb44-23"><a href="simple-linear-regression.html#cb44-23" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb44-24"><a href="simple-linear-regression.html#cb44-24" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb44-25"><a href="simple-linear-regression.html#cb44-25" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/height-data-residuals-1.png" width="672" /></p>
<p>While one can infer this error behavior from the original scatter plot, the residuals plot makes this pattern much more clearly visible. In the residual plot, it is evident that the errors increase with the age of the children. While this is not inherently problematic when using least squares estimation, it may be advisable to consider alternative methods that do not penalize all errors equally—particularly the larger errors at later ages—such as weighted least squares.</p>
<p>While heteroscedasticity does not bias the estimates of the regression coefficients themselves, it affects the accuracy and precision of these estimates and the associated inference metrics. Specifically, standard errors may be underestimated or overestimated, leading to incorrect conclusions about the significance of predictors.</p>
<p>Several strategies can be used to address heteroscedasticity, including:</p>
<ul>
<li>Applying data transformations (e.g., logarithmic, square root) to stabilize the variance.<br />
</li>
<li>Using heteroscedasticity-robust standard errors that adjust the inference procedures to account for non-constant variance.<br />
</li>
<li>Employing alternative modeling approaches such as weighted least squares, where observations are weighted inversely proportional to their variance, or explicitly modeling the variance function.</li>
</ul>
<hr />
</div>
<div id="outliers" class="section level3 hasAnchor" number="4.7.3">
<h3><span class="header-section-number">4.7.3</span> Outliers<a href="simple-linear-regression.html#outliers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the context of linear regression analysis, <strong>outliers</strong> are data points that do not conform to the general pattern established by the majority of the observations. While most data points may align well with the fitted regression line, a few observations might deviate substantially from this pattern, standing out as unusual or extreme relative to the rest.</p>
<p>Outliers can arise for various reasons, including measurement errors, data entry mistakes, or genuinely rare or unusual cases. Regardless of their origin, these outlying points can have a disproportionate influence on the results of the regression analysis, often skewing the estimated coefficients, inflating the residuals, and affecting overall model fit.</p>
<p>It is important to identify and understand outliers because they can mislead the analysis. They may unduly influence the estimated relationship between the predictor and response variables, leading to biased estimates of the slope and intercept. Moreover, outliers can inflate measures of error, such as the residual sum of squares, making the model seem less accurate than it truly is for the majority of the data.</p>
<p>Detecting outliers typically involves examining residuals, especially those that are unusually large or small compared to typical residuals. Residual plots can reveal these points as isolated dots that are far away from the bulk of the data cloud. Statistical rules and diagnostic tools can also be used to flag potential outliers for further investigation.</p>
<p>Once identified, outliers demand careful consideration. Some outliers may be due to data collection or entry errors and should be corrected or removed. Others may represent genuine but rare phenomena, and understanding their context is essential before deciding how to handle them in the analysis.</p>
<p>Dealing with outliers involves balancing the goal of accurately modeling the typical relationship while acknowledging that outliers can carry important information or indicate special cases. Sometimes, it is appropriate to analyze the data both with and without outliers, or to apply robust regression methods designed to reduce the influence of extreme observations.</p>
<p>In the next example, we work with the Wine data set and add outliers, to see the
effect this might have on the estimation.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="simple-linear-regression.html#cb45-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Wine Data.csv&quot;</span>)</span>
<span id="cb45-2"><a href="simple-linear-regression.html#cb45-2" tabindex="-1"></a></span>
<span id="cb45-3"><a href="simple-linear-regression.html#cb45-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Glasses</span>
<span id="cb45-4"><a href="simple-linear-regression.html#cb45-4" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Years</span>
<span id="cb45-5"><a href="simple-linear-regression.html#cb45-5" tabindex="-1"></a></span>
<span id="cb45-6"><a href="simple-linear-regression.html#cb45-6" tabindex="-1"></a>ymin <span class="ot">&lt;-</span> <span class="fu">min</span>(y, <span class="fu">max</span>(y) <span class="sc">-</span> <span class="dv">22</span>)</span>
<span id="cb45-7"><a href="simple-linear-regression.html#cb45-7" tabindex="-1"></a>ymax <span class="ot">&lt;-</span> <span class="fu">max</span>(y, <span class="fu">min</span>(y) <span class="sc">+</span> <span class="dv">22</span>)</span>
<span id="cb45-8"><a href="simple-linear-regression.html#cb45-8" tabindex="-1"></a>xmin <span class="ot">&lt;-</span> <span class="fu">min</span>(x)</span>
<span id="cb45-9"><a href="simple-linear-regression.html#cb45-9" tabindex="-1"></a>xmax <span class="ot">&lt;-</span> <span class="fu">max</span>(x)</span>
<span id="cb45-10"><a href="simple-linear-regression.html#cb45-10" tabindex="-1"></a></span>
<span id="cb45-11"><a href="simple-linear-regression.html#cb45-11" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb45-12"><a href="simple-linear-regression.html#cb45-12" tabindex="-1"></a><span class="co"># No Outliers</span></span>
<span id="cb45-13"><a href="simple-linear-regression.html#cb45-13" tabindex="-1"></a>outRegNoo <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb45-14"><a href="simple-linear-regression.html#cb45-14" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb45-15"><a href="simple-linear-regression.html#cb45-15" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb45-16"><a href="simple-linear-regression.html#cb45-16" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb45-17"><a href="simple-linear-regression.html#cb45-17" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb45-18"><a href="simple-linear-regression.html#cb45-18" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb45-19"><a href="simple-linear-regression.html#cb45-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Life Expectancy&quot;</span>,</span>
<span id="cb45-20"><a href="simple-linear-regression.html#cb45-20" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;No Outliers&quot;</span>,</span>
<span id="cb45-21"><a href="simple-linear-regression.html#cb45-21" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb45-22"><a href="simple-linear-regression.html#cb45-22" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegNoo<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb45-23"><a href="simple-linear-regression.html#cb45-23" tabindex="-1"></a>       <span class="at">b   =</span> outRegNoo<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb45-24"><a href="simple-linear-regression.html#cb45-24" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb45-25"><a href="simple-linear-regression.html#cb45-25" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb45-26"><a href="simple-linear-regression.html#cb45-26" tabindex="-1"></a></span>
<span id="cb45-27"><a href="simple-linear-regression.html#cb45-27" tabindex="-1"></a><span class="co"># Outlier on the left side</span></span>
<span id="cb45-28"><a href="simple-linear-regression.html#cb45-28" tabindex="-1"></a><span class="co"># Adds Observation</span></span>
<span id="cb45-29"><a href="simple-linear-regression.html#cb45-29" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(x, <span class="dv">0</span>)</span>
<span id="cb45-30"><a href="simple-linear-regression.html#cb45-30" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(y, <span class="dv">90</span>)</span>
<span id="cb45-31"><a href="simple-linear-regression.html#cb45-31" tabindex="-1"></a>outRegLef <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb45-32"><a href="simple-linear-regression.html#cb45-32" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb45-33"><a href="simple-linear-regression.html#cb45-33" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb45-34"><a href="simple-linear-regression.html#cb45-34" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb45-35"><a href="simple-linear-regression.html#cb45-35" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb45-36"><a href="simple-linear-regression.html#cb45-36" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb45-37"><a href="simple-linear-regression.html#cb45-37" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Life Expectancy&quot;</span>,</span>
<span id="cb45-38"><a href="simple-linear-regression.html#cb45-38" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Outlier Left&quot;</span>,</span>
<span id="cb45-39"><a href="simple-linear-regression.html#cb45-39" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb45-40"><a href="simple-linear-regression.html#cb45-40" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x   =</span> x[<span class="dv">21</span>],</span>
<span id="cb45-41"><a href="simple-linear-regression.html#cb45-41" tabindex="-1"></a>       <span class="at">y   =</span> y[<span class="dv">21</span>],</span>
<span id="cb45-42"><a href="simple-linear-regression.html#cb45-42" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">16</span>,</span>
<span id="cb45-43"><a href="simple-linear-regression.html#cb45-43" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb45-44"><a href="simple-linear-regression.html#cb45-44" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegLef<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb45-45"><a href="simple-linear-regression.html#cb45-45" tabindex="-1"></a>       <span class="at">b   =</span> outRegLef<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb45-46"><a href="simple-linear-regression.html#cb45-46" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb45-47"><a href="simple-linear-regression.html#cb45-47" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb45-48"><a href="simple-linear-regression.html#cb45-48" tabindex="-1"></a><span class="co"># Outlier on the right side</span></span>
<span id="cb45-49"><a href="simple-linear-regression.html#cb45-49" tabindex="-1"></a><span class="co"># Adds Observation</span></span>
<span id="cb45-50"><a href="simple-linear-regression.html#cb45-50" tabindex="-1"></a>x[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb45-51"><a href="simple-linear-regression.html#cb45-51" tabindex="-1"></a>y[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">68</span></span>
<span id="cb45-52"><a href="simple-linear-regression.html#cb45-52" tabindex="-1"></a>outRegRig <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb45-53"><a href="simple-linear-regression.html#cb45-53" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb45-54"><a href="simple-linear-regression.html#cb45-54" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb45-55"><a href="simple-linear-regression.html#cb45-55" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb45-56"><a href="simple-linear-regression.html#cb45-56" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb45-57"><a href="simple-linear-regression.html#cb45-57" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb45-58"><a href="simple-linear-regression.html#cb45-58" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Life Expectancy&quot;</span>,</span>
<span id="cb45-59"><a href="simple-linear-regression.html#cb45-59" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Right Outliers&quot;</span>,</span>
<span id="cb45-60"><a href="simple-linear-regression.html#cb45-60" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb45-61"><a href="simple-linear-regression.html#cb45-61" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x   =</span> x[<span class="dv">21</span>],</span>
<span id="cb45-62"><a href="simple-linear-regression.html#cb45-62" tabindex="-1"></a>       <span class="at">y   =</span> y[<span class="dv">21</span>],</span>
<span id="cb45-63"><a href="simple-linear-regression.html#cb45-63" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">16</span>,</span>
<span id="cb45-64"><a href="simple-linear-regression.html#cb45-64" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb45-65"><a href="simple-linear-regression.html#cb45-65" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegRig<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb45-66"><a href="simple-linear-regression.html#cb45-66" tabindex="-1"></a>       <span class="at">b   =</span> outRegRig<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb45-67"><a href="simple-linear-regression.html#cb45-67" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb45-68"><a href="simple-linear-regression.html#cb45-68" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb45-69"><a href="simple-linear-regression.html#cb45-69" tabindex="-1"></a><span class="co"># Outlier on the right side</span></span>
<span id="cb45-70"><a href="simple-linear-regression.html#cb45-70" tabindex="-1"></a><span class="co"># Adds Observation</span></span>
<span id="cb45-71"><a href="simple-linear-regression.html#cb45-71" tabindex="-1"></a>x[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb45-72"><a href="simple-linear-regression.html#cb45-72" tabindex="-1"></a>y[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">95</span></span>
<span id="cb45-73"><a href="simple-linear-regression.html#cb45-73" tabindex="-1"></a>outRegCen <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb45-74"><a href="simple-linear-regression.html#cb45-74" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb45-75"><a href="simple-linear-regression.html#cb45-75" tabindex="-1"></a>     <span class="at">y    =</span> y,</span>
<span id="cb45-76"><a href="simple-linear-regression.html#cb45-76" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb45-77"><a href="simple-linear-regression.html#cb45-77" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb45-78"><a href="simple-linear-regression.html#cb45-78" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb45-79"><a href="simple-linear-regression.html#cb45-79" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Life Expectancy&quot;</span>,</span>
<span id="cb45-80"><a href="simple-linear-regression.html#cb45-80" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Center Outliers&quot;</span>,</span>
<span id="cb45-81"><a href="simple-linear-regression.html#cb45-81" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb45-82"><a href="simple-linear-regression.html#cb45-82" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x   =</span> x[<span class="dv">21</span>],</span>
<span id="cb45-83"><a href="simple-linear-regression.html#cb45-83" tabindex="-1"></a>       <span class="at">y   =</span> y[<span class="dv">21</span>],</span>
<span id="cb45-84"><a href="simple-linear-regression.html#cb45-84" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">16</span>,</span>
<span id="cb45-85"><a href="simple-linear-regression.html#cb45-85" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb45-86"><a href="simple-linear-regression.html#cb45-86" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> outRegCen<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb45-87"><a href="simple-linear-regression.html#cb45-87" tabindex="-1"></a>       <span class="at">b   =</span> outRegCen<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb45-88"><a href="simple-linear-regression.html#cb45-88" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb45-89"><a href="simple-linear-regression.html#cb45-89" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/wine-outliers-1.png" width="672" />
The presence of outliers, will also be more clear when looking at the residuals.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="simple-linear-regression.html#cb46-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Glasses</span>
<span id="cb46-2"><a href="simple-linear-regression.html#cb46-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Years</span>
<span id="cb46-3"><a href="simple-linear-regression.html#cb46-3" tabindex="-1"></a></span>
<span id="cb46-4"><a href="simple-linear-regression.html#cb46-4" tabindex="-1"></a>ymin <span class="ot">&lt;-</span> <span class="fu">min</span>(outRegNoo<span class="sc">$</span>residuals, outRegLef<span class="sc">$</span>residuals, outRegRig<span class="sc">$</span>residuals, outRegCen<span class="sc">$</span>residuals)</span>
<span id="cb46-5"><a href="simple-linear-regression.html#cb46-5" tabindex="-1"></a>ymax <span class="ot">&lt;-</span> <span class="fu">max</span>(outRegNoo<span class="sc">$</span>residuals, outRegLef<span class="sc">$</span>residuals, outRegRig<span class="sc">$</span>residuals, outRegCen<span class="sc">$</span>residuals)</span>
<span id="cb46-6"><a href="simple-linear-regression.html#cb46-6" tabindex="-1"></a>xmin <span class="ot">&lt;-</span> <span class="fu">min</span>(x)</span>
<span id="cb46-7"><a href="simple-linear-regression.html#cb46-7" tabindex="-1"></a>xmax <span class="ot">&lt;-</span> <span class="fu">max</span>(x)</span>
<span id="cb46-8"><a href="simple-linear-regression.html#cb46-8" tabindex="-1"></a></span>
<span id="cb46-9"><a href="simple-linear-regression.html#cb46-9" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb46-10"><a href="simple-linear-regression.html#cb46-10" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> dat<span class="sc">$</span>Glasses,</span>
<span id="cb46-11"><a href="simple-linear-regression.html#cb46-11" tabindex="-1"></a>     <span class="at">y    =</span> outRegNoo<span class="sc">$</span>residuals,</span>
<span id="cb46-12"><a href="simple-linear-regression.html#cb46-12" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb46-13"><a href="simple-linear-regression.html#cb46-13" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb46-14"><a href="simple-linear-regression.html#cb46-14" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb46-15"><a href="simple-linear-regression.html#cb46-15" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb46-16"><a href="simple-linear-regression.html#cb46-16" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;No Outliers&quot;</span>,</span>
<span id="cb46-17"><a href="simple-linear-regression.html#cb46-17" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb46-18"><a href="simple-linear-regression.html#cb46-18" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb46-19"><a href="simple-linear-regression.html#cb46-19" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb46-20"><a href="simple-linear-regression.html#cb46-20" tabindex="-1"></a></span>
<span id="cb46-21"><a href="simple-linear-regression.html#cb46-21" tabindex="-1"></a><span class="co"># Outlier on the left side</span></span>
<span id="cb46-22"><a href="simple-linear-regression.html#cb46-22" tabindex="-1"></a>x[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb46-23"><a href="simple-linear-regression.html#cb46-23" tabindex="-1"></a>y[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">90</span></span>
<span id="cb46-24"><a href="simple-linear-regression.html#cb46-24" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb46-25"><a href="simple-linear-regression.html#cb46-25" tabindex="-1"></a>     <span class="at">y    =</span> outRegLef<span class="sc">$</span>residuals,</span>
<span id="cb46-26"><a href="simple-linear-regression.html#cb46-26" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb46-27"><a href="simple-linear-regression.html#cb46-27" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb46-28"><a href="simple-linear-regression.html#cb46-28" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb46-29"><a href="simple-linear-regression.html#cb46-29" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb46-30"><a href="simple-linear-regression.html#cb46-30" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Left Outlier&quot;</span>,</span>
<span id="cb46-31"><a href="simple-linear-regression.html#cb46-31" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb46-32"><a href="simple-linear-regression.html#cb46-32" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb46-33"><a href="simple-linear-regression.html#cb46-33" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb46-34"><a href="simple-linear-regression.html#cb46-34" tabindex="-1"></a><span class="co"># Outlier on the right side</span></span>
<span id="cb46-35"><a href="simple-linear-regression.html#cb46-35" tabindex="-1"></a><span class="co"># Adds Observation</span></span>
<span id="cb46-36"><a href="simple-linear-regression.html#cb46-36" tabindex="-1"></a>x[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb46-37"><a href="simple-linear-regression.html#cb46-37" tabindex="-1"></a>y[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">68</span></span>
<span id="cb46-38"><a href="simple-linear-regression.html#cb46-38" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb46-39"><a href="simple-linear-regression.html#cb46-39" tabindex="-1"></a>     <span class="at">y    =</span> outRegRig<span class="sc">$</span>residuals,</span>
<span id="cb46-40"><a href="simple-linear-regression.html#cb46-40" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb46-41"><a href="simple-linear-regression.html#cb46-41" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb46-42"><a href="simple-linear-regression.html#cb46-42" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb46-43"><a href="simple-linear-regression.html#cb46-43" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb46-44"><a href="simple-linear-regression.html#cb46-44" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Left Outlier&quot;</span>,</span>
<span id="cb46-45"><a href="simple-linear-regression.html#cb46-45" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb46-46"><a href="simple-linear-regression.html#cb46-46" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb46-47"><a href="simple-linear-regression.html#cb46-47" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb46-48"><a href="simple-linear-regression.html#cb46-48" tabindex="-1"></a><span class="co"># Outlier on the right side</span></span>
<span id="cb46-49"><a href="simple-linear-regression.html#cb46-49" tabindex="-1"></a><span class="co"># Adds Observation</span></span>
<span id="cb46-50"><a href="simple-linear-regression.html#cb46-50" tabindex="-1"></a>x[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb46-51"><a href="simple-linear-regression.html#cb46-51" tabindex="-1"></a>y[<span class="dv">21</span>] <span class="ot">&lt;-</span> <span class="dv">95</span></span>
<span id="cb46-52"><a href="simple-linear-regression.html#cb46-52" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> x,</span>
<span id="cb46-53"><a href="simple-linear-regression.html#cb46-53" tabindex="-1"></a>     <span class="at">y    =</span> outRegCen<span class="sc">$</span>residuals,</span>
<span id="cb46-54"><a href="simple-linear-regression.html#cb46-54" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb46-55"><a href="simple-linear-regression.html#cb46-55" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb46-56"><a href="simple-linear-regression.html#cb46-56" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Average Glasses per Week&quot;</span>,</span>
<span id="cb46-57"><a href="simple-linear-regression.html#cb46-57" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb46-58"><a href="simple-linear-regression.html#cb46-58" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Left Outlier&quot;</span>,</span>
<span id="cb46-59"><a href="simple-linear-regression.html#cb46-59" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb46-60"><a href="simple-linear-regression.html#cb46-60" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb46-61"><a href="simple-linear-regression.html#cb46-61" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/wine-res-1.png" width="672" />
And we can also see how the <span class="math inline">\(R^2\)</span> changes:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="simple-linear-regression.html#cb47-1" tabindex="-1"></a><span class="fu">summary</span>(outRegNoo)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.6443 -1.4398 -0.3390  0.9071  4.8057 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  73.7154     0.8674  84.988  &lt; 2e-16 ***
## x             2.4686     0.4364   5.656 2.29e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.162 on 18 degrees of freedom
## Multiple R-squared:   0.64,  Adjusted R-squared:   0.62 
## F-statistic:    32 on 1 and 18 DF,  p-value: 2.295e-05</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="simple-linear-regression.html#cb49-1" tabindex="-1"></a><span class="fu">summary</span>(outRegLef)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.550 -2.296 -1.413  1.440 14.028 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  75.9724     1.5101   50.31   &lt;2e-16 ***
## x             1.5258     0.7786    1.96   0.0649 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.056 on 19 degrees of freedom
## Multiple R-squared:  0.1682, Adjusted R-squared:  0.1244 
## F-statistic: 3.841 on 1 and 19 DF,  p-value: 0.06486</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="simple-linear-regression.html#cb51-1" tabindex="-1"></a><span class="fu">summary</span>(outRegRig)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.228  -1.428  -0.211   2.283   5.654 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  75.0353     1.4815  50.648   &lt;2e-16 ***
## x             1.2981     0.6965   1.864   0.0779 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.803 on 19 degrees of freedom
## Multiple R-squared:  0.1545, Adjusted R-squared:   0.11 
## F-statistic: 3.473 on 1 and 19 DF,  p-value: 0.0779</code></pre>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="simple-linear-regression.html#cb53-1" tabindex="-1"></a><span class="fu">summary</span>(outRegCen)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4964 -2.2377 -0.9105  0.9596 15.4953 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  74.1257     1.6870  43.939  &lt; 2e-16 ***
## x             2.6895     0.8486   3.169  0.00505 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.214 on 19 degrees of freedom
## Multiple R-squared:  0.3459, Adjusted R-squared:  0.3114 
## F-statistic: 10.05 on 1 and 19 DF,  p-value: 0.005048</code></pre>
<p>Outliers located at the edges of the explanatory variable range often possess higher leverage, which means they have a disproportionate influence on the estimated regression coefficients, including the slope. Leverage points are data points that are far from the center of the predictor distribution and can exert undue influence on the fitted line.</p>
<hr />
</div>
<div id="variables-ommited" class="section level3 hasAnchor" number="4.7.4">
<h3><span class="header-section-number">4.7.4</span> Variables Ommited<a href="simple-linear-regression.html#variables-ommited" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When analyzing the residuals from a regression model, one of the key assumptions is that there should be no systematic pattern or structure in the residual plots. If the residuals appear randomly scattered around zero, it suggests that the model has effectively captured all the systematic variation in the response variable, and no important information has been left out.</p>
<p>However, in practice, the absence of an apparent pattern in the residuals may be misleading if some relevant variables have been omitted from the model. Sometimes, the pattern is only visible when residuals are examined in relation to other variables not included as predictors. This situation indicates that the model may be missing important factors that explain variability in the response, leading to biased or incomplete conclusions.</p>
<p>To illustrate this concept, consider the example of analyzing children’s heights. Suppose the initial model predicts height based solely on age. When you examine the residuals, they might seem to lack any obvious pattern (beyond heteroscedasticity), suggesting a reasonable fit. Yet, if the data set also includes the sex of the children as an additional variable, and this variable affects height, then ignoring sex in the model can cause a pattern to emerge in the residuals when they are grouped or compared by sex.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="simple-linear-regression.html#cb55-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Height Data.csv&quot;</span>)</span>
<span id="cb55-2"><a href="simple-linear-regression.html#cb55-2" tabindex="-1"></a></span>
<span id="cb55-3"><a href="simple-linear-regression.html#cb55-3" tabindex="-1"></a><span class="co"># Fits linear Model</span></span>
<span id="cb55-4"><a href="simple-linear-regression.html#cb55-4" tabindex="-1"></a>outReg <span class="ot">&lt;-</span> <span class="fu">lm</span>(dat<span class="sc">$</span>Height <span class="sc">~</span> dat<span class="sc">$</span>Age)</span>
<span id="cb55-5"><a href="simple-linear-regression.html#cb55-5" tabindex="-1"></a></span>
<span id="cb55-6"><a href="simple-linear-regression.html#cb55-6" tabindex="-1"></a><span class="co"># Plots residuals for each sex</span></span>
<span id="cb55-7"><a href="simple-linear-regression.html#cb55-7" tabindex="-1"></a>ymin <span class="ot">&lt;-</span> <span class="fu">min</span>(outReg<span class="sc">$</span>residuals)</span>
<span id="cb55-8"><a href="simple-linear-regression.html#cb55-8" tabindex="-1"></a>ymax <span class="ot">&lt;-</span> <span class="fu">max</span>(outReg<span class="sc">$</span>residuals)</span>
<span id="cb55-9"><a href="simple-linear-regression.html#cb55-9" tabindex="-1"></a></span>
<span id="cb55-10"><a href="simple-linear-regression.html#cb55-10" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb55-11"><a href="simple-linear-regression.html#cb55-11" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> dat<span class="sc">$</span>Age[dat<span class="sc">$</span>Sex <span class="sc">==</span> <span class="dv">1</span>],</span>
<span id="cb55-12"><a href="simple-linear-regression.html#cb55-12" tabindex="-1"></a>     <span class="at">y =</span> outReg<span class="sc">$</span>residuals[dat<span class="sc">$</span>Sex <span class="sc">==</span> <span class="dv">1</span>],</span>
<span id="cb55-13"><a href="simple-linear-regression.html#cb55-13" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb55-14"><a href="simple-linear-regression.html#cb55-14" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Age&quot;</span>,</span>
<span id="cb55-15"><a href="simple-linear-regression.html#cb55-15" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb55-16"><a href="simple-linear-regression.html#cb55-16" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Males&quot;</span>,</span>
<span id="cb55-17"><a href="simple-linear-regression.html#cb55-17" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb55-18"><a href="simple-linear-regression.html#cb55-18" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb55-19"><a href="simple-linear-regression.html#cb55-19" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb55-20"><a href="simple-linear-regression.html#cb55-20" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> dat<span class="sc">$</span>Age[dat<span class="sc">$</span>Sex <span class="sc">==</span> <span class="dv">0</span>],</span>
<span id="cb55-21"><a href="simple-linear-regression.html#cb55-21" tabindex="-1"></a>     <span class="at">y =</span> outReg<span class="sc">$</span>residuals[dat<span class="sc">$</span>Sex <span class="sc">==</span> <span class="dv">0</span>],</span>
<span id="cb55-22"><a href="simple-linear-regression.html#cb55-22" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb55-23"><a href="simple-linear-regression.html#cb55-23" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Age&quot;</span>,</span>
<span id="cb55-24"><a href="simple-linear-regression.html#cb55-24" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb55-25"><a href="simple-linear-regression.html#cb55-25" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Females&quot;</span>,</span>
<span id="cb55-26"><a href="simple-linear-regression.html#cb55-26" tabindex="-1"></a>     <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb55-27"><a href="simple-linear-regression.html#cb55-27" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb55-28"><a href="simple-linear-regression.html#cb55-28" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/height-other-var-1.png" width="672" /></p>
<p>Specifically, when residuals are plotted separately for boys and girls, a clear pattern may appear: residuals for boys tend to be systematically higher or lower than those for girls. This indicates that sex is an important variable that was omitted from the model. Failing to include such relevant variables can lead to biased estimates of the relationship between the predictor (age) and the response (height), and potentially misleading inferences.</p>
<p>This example underscores the importance of considering all relevant variables when building a regression model. Omitting an important variable—such as sex in this case—can result in patterns in the residuals that reveal the model’s inadequacy. These patterns suggest the model is misspecified and that including additional, relevant predictors could significantly improve its accuracy and interpretability.</p>
<p>In practice, examining residuals against other variables, not just the predictor, provides crucial diagnostic information. Detecting such patterns alerts analysts to possible omitted variables, prompting further investigation, model refinement, and ultimately, more valid and reliable conclusions.</p>
<hr />
</div>
</div>
<div id="cross-validation" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> Cross-Validation<a href="simple-linear-regression.html#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When testing different models, a good idea to evaluate the performance of each
model beyond <span class="math inline">\(R^2\)</span>, is to separate your data set into a training set and a
validation or test set. Doing this is called cross-validation. There are several
alternatives to doing cross-validation, here are a few of the most relevant ones:</p>
<ol style="list-style-type: decimal">
<li><strong>Holdout Method (Train/Test Split)</strong>
<ul>
<li><strong>Description</strong>: The data is split into two (or three) sets: training and test (and sometimes validation). The model is trained on the training set and evaluated on the test set.</li>
<li><strong>Use Case</strong>: Simple to implement, but has high variance. The performance may depend on the specific split.</li>
</ul></li>
<li><strong>K-Fold Cross-Validation</strong>
<ul>
<li><strong>Description</strong>: The data is divided into <span class="math inline">\(k\)</span> equal-sized folds (subsets). The model is trained on <span class="math inline">\(k-1\)</span> folds and tested on the remaining fold. This process is repeated <span class="math inline">\(k\)</span> times, with each fold used as the test set once.</li>
<li><strong>Use Case</strong>:
<ul>
<li>Works well for most applications and balances the bias-variance tradeoff.</li>
<li>Common choices for <span class="math inline">\(k\)</span>: 5, 10.</li>
</ul></li>
</ul></li>
<li><strong>Leave-One-Out Cross-Validation (LOOCV)</strong>
<ul>
<li><strong>Description</strong>: Each data point is used once as a test set, and the rest of the data is used as the training set. This results in <span class="math inline">\(n\)</span> iterations, where <span class="math inline">\(n\)</span> is the number of samples.</li>
<li><strong>Use Case</strong>: Good when the dataset is small, but can be computationally expensive for large datasets.</li>
</ul></li>
<li><strong>Leave-P-Out Cross-Validation (LPOCV)</strong>
<ul>
<li><strong>Description</strong>: Similar to LOOCV, but instead of leaving out one data point, <span class="math inline">\(p\)</span> data points are left out. This creates <span class="math inline">\(\binom{n}{p}\)</span> different training/testing splits.</li>
<li><strong>Use Case</strong>: Rarely used due to its high computational cost for large datasets but might be useful in specific scenarios.</li>
</ul></li>
<li><strong>Stratified K-Fold Cross-Validation</strong>
<ul>
<li><strong>Description</strong>: Similar to K-fold cross-validation but ensures that each fold has the same proportion of each class in classification tasks (i.e., balanced classes in each fold).</li>
<li><strong>Use Case</strong>: Useful for imbalanced datasets in classification problems.</li>
</ul></li>
<li><strong>Repeated K-Fold Cross-Validation</strong>
<ul>
<li><strong>Description</strong>: A variation of K-Fold Cross-Validation where the process is repeated multiple times with different random splits.</li>
<li><strong>Use Case</strong>: Provides more robust estimates of model performance, particularly when the dataset is small.</li>
</ul></li>
</ol>
<p>Here is an example of <span class="math inline">\(k\)</span>-fold Cross-Validation using the “Burger Data” set.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="simple-linear-regression.html#cb56-1" tabindex="-1"></a><span class="do">### Cross Validation Example</span></span>
<span id="cb56-2"><a href="simple-linear-regression.html#cb56-2" tabindex="-1"></a></span>
<span id="cb56-3"><a href="simple-linear-regression.html#cb56-3" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb56-4"><a href="simple-linear-regression.html#cb56-4" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">&quot;Burger Data.csv&quot;</span>)</span>
<span id="cb56-5"><a href="simple-linear-regression.html#cb56-5" tabindex="-1"></a></span>
<span id="cb56-6"><a href="simple-linear-regression.html#cb56-6" tabindex="-1"></a><span class="co"># Saves Variables</span></span>
<span id="cb56-7"><a href="simple-linear-regression.html#cb56-7" tabindex="-1"></a>x <span class="ot">&lt;-</span> dat<span class="sc">$</span>Price</span>
<span id="cb56-8"><a href="simple-linear-regression.html#cb56-8" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>Burgers</span>
<span id="cb56-9"><a href="simple-linear-regression.html#cb56-9" tabindex="-1"></a></span>
<span id="cb56-10"><a href="simple-linear-regression.html#cb56-10" tabindex="-1"></a><span class="co"># Number of Observations</span></span>
<span id="cb56-11"><a href="simple-linear-regression.html#cb56-11" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb56-12"><a href="simple-linear-regression.html#cb56-12" tabindex="-1"></a></span>
<span id="cb56-13"><a href="simple-linear-regression.html#cb56-13" tabindex="-1"></a><span class="co"># Number of Folds</span></span>
<span id="cb56-14"><a href="simple-linear-regression.html#cb56-14" tabindex="-1"></a>numFol <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb56-15"><a href="simple-linear-regression.html#cb56-15" tabindex="-1"></a><span class="co"># Fold Size</span></span>
<span id="cb56-16"><a href="simple-linear-regression.html#cb56-16" tabindex="-1"></a>sizFol <span class="ot">&lt;-</span> <span class="fu">round</span>(n <span class="sc">/</span> numFol)</span>
<span id="cb56-17"><a href="simple-linear-regression.html#cb56-17" tabindex="-1"></a></span>
<span id="cb56-18"><a href="simple-linear-regression.html#cb56-18" tabindex="-1"></a><span class="co"># List Containing the Fold indices</span></span>
<span id="cb56-19"><a href="simple-linear-regression.html#cb56-19" tabindex="-1"></a>fol <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb56-20"><a href="simple-linear-regression.html#cb56-20" tabindex="-1"></a><span class="co"># Select Fold Indeces</span></span>
<span id="cb56-21"><a href="simple-linear-regression.html#cb56-21" tabindex="-1"></a><span class="co"># Initialization</span></span>
<span id="cb56-22"><a href="simple-linear-regression.html#cb56-22" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n</span>
<span id="cb56-23"><a href="simple-linear-regression.html#cb56-23" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>numFol){</span>
<span id="cb56-24"><a href="simple-linear-regression.html#cb56-24" tabindex="-1"></a>  <span class="co"># Computes the remaining number of indices</span></span>
<span id="cb56-25"><a href="simple-linear-regression.html#cb56-25" tabindex="-1"></a>  numInd   <span class="ot">&lt;-</span> <span class="fu">length</span>(ind)</span>
<span id="cb56-26"><a href="simple-linear-regression.html#cb56-26" tabindex="-1"></a>  <span class="co"># Randomly selects indices from the remaining indeces</span></span>
<span id="cb56-27"><a href="simple-linear-regression.html#cb56-27" tabindex="-1"></a>  indFol   <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(ind), <span class="at">size =</span> sizFol, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb56-28"><a href="simple-linear-regression.html#cb56-28" tabindex="-1"></a>  <span class="co"># Saves the inidices to the list of Fold Indeces</span></span>
<span id="cb56-29"><a href="simple-linear-regression.html#cb56-29" tabindex="-1"></a>  fol[[i]] <span class="ot">&lt;-</span> ind[indFol]</span>
<span id="cb56-30"><a href="simple-linear-regression.html#cb56-30" tabindex="-1"></a>  <span class="co"># Removes the indices from the indeces vector</span></span>
<span id="cb56-31"><a href="simple-linear-regression.html#cb56-31" tabindex="-1"></a>  ind      <span class="ot">&lt;-</span> ind[<span class="sc">-</span> indFol]</span>
<span id="cb56-32"><a href="simple-linear-regression.html#cb56-32" tabindex="-1"></a>}</span>
<span id="cb56-33"><a href="simple-linear-regression.html#cb56-33" tabindex="-1"></a></span>
<span id="cb56-34"><a href="simple-linear-regression.html#cb56-34" tabindex="-1"></a><span class="co"># Models to try</span></span>
<span id="cb56-35"><a href="simple-linear-regression.html#cb56-35" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(x, <span class="fu">log</span>(x),      x, <span class="fu">log</span>(x))</span>
<span id="cb56-36"><a href="simple-linear-regression.html#cb56-36" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">cbind</span>(y,      y, <span class="fu">log</span>(y), <span class="fu">log</span>(y))</span>
<span id="cb56-37"><a href="simple-linear-regression.html#cb56-37" tabindex="-1"></a></span>
<span id="cb56-38"><a href="simple-linear-regression.html#cb56-38" tabindex="-1"></a><span class="co"># Number of Models to compare</span></span>
<span id="cb56-39"><a href="simple-linear-regression.html#cb56-39" tabindex="-1"></a>numMod <span class="ot">&lt;-</span> <span class="fu">dim</span>(Y)[<span class="dv">2</span>]</span>
<span id="cb56-40"><a href="simple-linear-regression.html#cb56-40" tabindex="-1"></a><span class="co"># Saves the validation metric for each model and each fold </span></span>
<span id="cb56-41"><a href="simple-linear-regression.html#cb56-41" tabindex="-1"></a>matMet <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="cn">NA</span>, <span class="at">nrow =</span> numFol, <span class="at">ncol =</span> numMod)</span>
<span id="cb56-42"><a href="simple-linear-regression.html#cb56-42" tabindex="-1"></a></span>
<span id="cb56-43"><a href="simple-linear-regression.html#cb56-43" tabindex="-1"></a></span>
<span id="cb56-44"><a href="simple-linear-regression.html#cb56-44" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb56-45"><a href="simple-linear-regression.html#cb56-45" tabindex="-1"></a><span class="co"># Loops through the models</span></span>
<span id="cb56-46"><a href="simple-linear-regression.html#cb56-46" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>numMod){</span>
<span id="cb56-47"><a href="simple-linear-regression.html#cb56-47" tabindex="-1"></a>  yMod <span class="ot">&lt;-</span> Y[, k]</span>
<span id="cb56-48"><a href="simple-linear-regression.html#cb56-48" tabindex="-1"></a>  xMod <span class="ot">&lt;-</span> X[, k]</span>
<span id="cb56-49"><a href="simple-linear-regression.html#cb56-49" tabindex="-1"></a>  <span class="co"># Plots Transformed Data (All)</span></span>
<span id="cb56-50"><a href="simple-linear-regression.html#cb56-50" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="at">x =</span> xMod,</span>
<span id="cb56-51"><a href="simple-linear-regression.html#cb56-51" tabindex="-1"></a>       <span class="at">y =</span> yMod,</span>
<span id="cb56-52"><a href="simple-linear-regression.html#cb56-52" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Price ($)&quot;</span>,</span>
<span id="cb56-53"><a href="simple-linear-regression.html#cb56-53" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">&quot;Burgers&quot;</span>,</span>
<span id="cb56-54"><a href="simple-linear-regression.html#cb56-54" tabindex="-1"></a>       <span class="at">main =</span> <span class="fu">paste0</span>(<span class="st">&quot;Model &quot;</span>, k),</span>
<span id="cb56-55"><a href="simple-linear-regression.html#cb56-55" tabindex="-1"></a>       <span class="at">pch  =</span> <span class="dv">16</span>)</span>
<span id="cb56-56"><a href="simple-linear-regression.html#cb56-56" tabindex="-1"></a>  <span class="co"># Plots Regression Line (All)</span></span>
<span id="cb56-57"><a href="simple-linear-regression.html#cb56-57" tabindex="-1"></a>  outReg   <span class="ot">&lt;-</span> <span class="fu">lm</span>(yMod <span class="sc">~</span> xMod)</span>
<span id="cb56-58"><a href="simple-linear-regression.html#cb56-58" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">a   =</span> outReg<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb56-59"><a href="simple-linear-regression.html#cb56-59" tabindex="-1"></a>         <span class="at">b   =</span> outReg<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb56-60"><a href="simple-linear-regression.html#cb56-60" tabindex="-1"></a>         <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb56-61"><a href="simple-linear-regression.html#cb56-61" tabindex="-1"></a>         <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb56-62"><a href="simple-linear-regression.html#cb56-62" tabindex="-1"></a>  metFol <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="at">length =</span> numFol)</span>
<span id="cb56-63"><a href="simple-linear-regression.html#cb56-63" tabindex="-1"></a>  <span class="co"># Loops through the folds</span></span>
<span id="cb56-64"><a href="simple-linear-regression.html#cb56-64" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>numFol){</span>
<span id="cb56-65"><a href="simple-linear-regression.html#cb56-65" tabindex="-1"></a>    <span class="co"># At Each Fold do Linear Regression without the test set</span></span>
<span id="cb56-66"><a href="simple-linear-regression.html#cb56-66" tabindex="-1"></a>    outReg   <span class="ot">&lt;-</span> <span class="fu">lm</span>(yMod[<span class="sc">-</span>fol[[i]]] <span class="sc">~</span> xMod[<span class="sc">-</span>fol[[i]]])</span>
<span id="cb56-67"><a href="simple-linear-regression.html#cb56-67" tabindex="-1"></a>    <span class="co"># Evaluate Out of Sample</span></span>
<span id="cb56-68"><a href="simple-linear-regression.html#cb56-68" tabindex="-1"></a>    yHatOut   <span class="ot">&lt;-</span> outReg<span class="sc">$</span>coefficients[<span class="dv">1</span>] <span class="sc">+</span> outReg<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">*</span> xMod[fol[[i]]] </span>
<span id="cb56-69"><a href="simple-linear-regression.html#cb56-69" tabindex="-1"></a>    <span class="cf">if</span>(k <span class="sc">&gt;=</span> <span class="dv">3</span> ){</span>
<span id="cb56-70"><a href="simple-linear-regression.html#cb56-70" tabindex="-1"></a>      metEva    <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">exp</span>(yMod[fol[[i]]]) <span class="sc">-</span> <span class="fu">exp</span>(yHatOut))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb56-71"><a href="simple-linear-regression.html#cb56-71" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb56-72"><a href="simple-linear-regression.html#cb56-72" tabindex="-1"></a>      metEva    <span class="ot">&lt;-</span> <span class="fu">mean</span>((yMod[fol[[i]]] <span class="sc">-</span> yHatOut)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb56-73"><a href="simple-linear-regression.html#cb56-73" tabindex="-1"></a>    }</span>
<span id="cb56-74"><a href="simple-linear-regression.html#cb56-74" tabindex="-1"></a>    metFol[i] <span class="ot">&lt;-</span> metEva</span>
<span id="cb56-75"><a href="simple-linear-regression.html#cb56-75" tabindex="-1"></a>  }</span>
<span id="cb56-76"><a href="simple-linear-regression.html#cb56-76" tabindex="-1"></a>  matMet[, k] <span class="ot">&lt;-</span> metFol</span>
<span id="cb56-77"><a href="simple-linear-regression.html#cb56-77" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="_main_files/figure-html/cross-validation-1.png" width="672" /></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="simple-linear-regression.html#cb57-1" tabindex="-1"></a><span class="co"># Box plot of the MSE</span></span>
<span id="cb57-2"><a href="simple-linear-regression.html#cb57-2" tabindex="-1"></a><span class="fu">boxplot</span>(matMet, <span class="at">outline =</span> <span class="cn">FALSE</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Models&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;MSE&quot;</span>)</span>
<span id="cb57-3"><a href="simple-linear-regression.html#cb57-3" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">at =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>), <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;y~x&quot;</span>, <span class="st">&quot;y~log(x)&quot;</span>, <span class="st">&quot;log(y)~x&quot;</span>, <span class="st">&quot;log(y)~log(x)&quot;</span>))</span>
<span id="cb57-4"><a href="simple-linear-regression.html#cb57-4" tabindex="-1"></a></span>
<span id="cb57-5"><a href="simple-linear-regression.html#cb57-5" tabindex="-1"></a><span class="co"># Displays the Results of Cross-Validation</span></span>
<span id="cb57-6"><a href="simple-linear-regression.html#cb57-6" tabindex="-1"></a>tab <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">colMeans</span>(matMet))</span>
<span id="cb57-7"><a href="simple-linear-regression.html#cb57-7" tabindex="-1"></a>tab <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">c</span>(<span class="st">&quot;Model 1&quot;</span>, <span class="st">&quot;Model 2&quot;</span>, <span class="st">&quot;Model 3&quot;</span>, <span class="st">&quot;Model 4&quot;</span>), tab)</span>
<span id="cb57-8"><a href="simple-linear-regression.html#cb57-8" tabindex="-1"></a>tab <span class="ot">&lt;-</span> <span class="fu">cbind</span>(tab, <span class="fu">apply</span>(<span class="at">X =</span> matMet, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> sd))</span>
<span id="cb57-9"><a href="simple-linear-regression.html#cb57-9" tabindex="-1"></a><span class="fu">colnames</span>(tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Model&quot;</span>, <span class="st">&quot;Mean&quot;</span>, <span class="st">&quot;SD&quot;</span>)</span>
<span id="cb57-10"><a href="simple-linear-regression.html#cb57-10" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(tab, <span class="at">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="right">Mean</th>
<th align="right">SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Model 1</td>
<td align="right">78.38</td>
<td align="right">88.40</td>
</tr>
<tr class="even">
<td align="left">Model 2</td>
<td align="right">19.74</td>
<td align="right">8.72</td>
</tr>
<tr class="odd">
<td align="left">Model 3</td>
<td align="right">39.28</td>
<td align="right">46.96</td>
</tr>
<tr class="even">
<td align="left">Model 4</td>
<td align="right">364.82</td>
<td align="right">1013.20</td>
</tr>
</tbody>
</table>
<p><img src="_main_files/figure-html/cross-validation-2.png" width="672" />
When performing cross-validation, it is essential to calculate error metrics in the original units of the response variable before any transformation. Transformations like scaling or logging change the scale of the data, which can make errors appear smaller or larger in the transformed units and lead to misleading comparisons.</p>
<p>For example, compare the scale of the dependent variable in Models 3 and 4 with those in Models 1 and 2. Since the range of values after transforming in Models 3 and 4 is smaller, the squared errors tend to be smaller simply because of this change, making it unfair to compare these errors to those from Models 1 and 2. Similarly, if income is measured in dollars and then log-transformed, errors in the transformed scale do not directly reflect dollar amounts. To evaluate model performance correctly, predictions should be transformed back to the original units before computing errors or other accuracy metrics.</p>
<p>Always perform validation in the original units to ensure that results are meaningful and comparable in real-world terms. Keep the transformation parameters consistent for new data, and report errors in the original units for clear, interpretable results.</p>
<hr />
</div>
<div id="weighted-least-squares" class="section level2 hasAnchor" number="4.9">
<h2><span class="header-section-number">4.9</span> Weighted Least Squares<a href="simple-linear-regression.html#weighted-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Weighted Least Squares is a generalization of Ordinary Least Squares designed to handle situations in which the assumption of constant error variance is violated. When the variability of the errors differs across observations—i.e., when heteroscedasticity is present—OLS estimators remain unbiased but are no longer the most efficient linear estimators. WLS addresses this inefficiency by incorporating information about the relative precision of individual observations into the fitting procedure.</p>
<p>Motivation and intuition<br />
- Heteroscedasticity means some observations are inherently more variable than others. Treating all observations as equally informative gives undue influence to noisy points and reduces overall estimation precision.<br />
- WLS assigns each observation a weight that reflects its reliability: observations with smaller error variance receive larger weight, and those with larger error variance receive smaller weight. In effect, the method downweights noisy observations and upweights precise ones, producing estimates that place greater trust in the most informative data.<br />
- In the ideal case, weights are chosen to be inversely related to the true error variances; this makes the weighted estimator the most efficient among linear unbiased estimators under the model.</p>
<p>In a simple linear regression model with one independent variable, OLS minimizes the sum of squared residuals:</p>
<p><span class="math display">\[ \min \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \min \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2
\]</span></p>
<p>Weighted Least Squares (WLS) corrects for heteroscedasticity by minimizing the weighted sum of squared residuals:
<span class="math display">\[
\min \sum_{i=1}^{n} w_i (y_i - \hat{y}_i)^2 = \min \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i)^2
\]</span></p>
<div class="proposition">
<p><span id="prp:prp-weighted-least-squares-estimators" class="proposition"><strong>Proposition 4.13  (Weighted Least Squares Estimators) </strong></span>The solution to the Weithed Least Squares problem:
<span class="math display">\[
\min_{\beta_0,\beta_1} \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i)^2
\]</span>
is given by:
<span class="math display">\[\hat{\beta}_1^w = \frac{\sum_{i=1}^{n} w_i x_i y_i - \frac{\sum_{i=1}^{n} w_i x_i \sum_{i=1}^{n} w_i y_i}{\sum_{i=1}^{n} w_i}}{\sum_{i=1}^{n} w_i x_i^2 - \frac{\left( \sum_{i=1}^{n} w_i x_i \right)^2}{\sum_{i=1}^{n} w_i}},\]</span></p>
<p><span class="math display">\[\hat{\beta}_0^w = \frac{\sum_{i=1}^{n} w_i y_i - \beta_1^w \sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^{n} w_i}.\]</span></p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-13" class="proof"><em>Proof</em>. </span>First define the weighted residual sum of squares, the objective function, as:
<span class="math display">\[Q_w(\beta_0, \beta_1) = \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i)^2\]</span>
We aim to minimize <span class="math inline">\(Q(\beta_0, \beta_1)\)</span> with respect to <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.
We will do so by finding its critical points.
<span class="math display">\[\begin{align*}
\frac{\partial Q_w}{\partial \beta_0}  
  &amp;= \frac{\partial}{\partial \beta_0} \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i)^2
    &amp; \text{def. } Q_w \\
  &amp;= \sum_{i=1}^{n} \frac{\partial}{\partial \beta_0} w_i (y_i - \beta_0 - \beta_1 x_i)^2
    &amp; \text{lin. dev.} \\
  &amp;= \sum_{i=1}^{n} -2  w_i (y_i - \beta_0 - \beta_1 x_i)
    &amp; \\
  &amp;= \sum_{i=1}^{n} -2  \left( w_i y_i - w_i \beta_0 - w_i \beta_1 x_i \right)
    &amp; \\
  &amp;= -2  \sum_{i=1}^{n} w_i y_i + 2 \beta_0 \sum_{i=1}^{n} w_i + 2 \beta_1 \sum_{i=1}^{n} w_i x_i
    &amp; \text{lin. sum} \\
  &amp;= -2  \left( \sum_{i=1}^{n} w_i y_i - \beta_0 \sum_{i=1}^{n} w_i - \beta_1 \sum_{i=1}^{n} w_i x_i \right)
    &amp; \\
\end{align*}\]</span>
Similarly
<span class="math display">\[\begin{align*}
\frac{\partial Q_w}{\partial \beta_1}  
  &amp;= \frac{\partial}{\partial \beta_1} \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i)^2
    &amp; \text{def. } Q_w \\
  &amp;= \sum_{i=1}^{n} \frac{\partial}{\partial \beta_1} w_i (y_i - \beta_0 - \beta_1 x_i)^2
    &amp; \text{lin. dev.} \\
  &amp;= \sum_{i=1}^{n} -2  w_i (y_i - \beta_0 - \beta_1 x_i) x_i
    &amp; \\
  &amp;= \sum_{i=1}^{n} -2  \left( w_i y_i  x_i - \beta_0 w_i  x_i - w_i \beta_1 x_i^2 \right)
    &amp; \\
  &amp;= -2  \sum_{i=1}^{n} w_i y_i x_i + 2 \beta_0 \sum_{i=1}^{n} w_i x_i + 2 \beta_1 \sum_{i=1}^{n} w_i x_i^2
    &amp; \text{lin. sum} \\
  &amp;= -2  \left( \sum_{i=1}^{n} w_i y_i x_i - \beta_0 \sum_{i=1}^{n} w_i x_i - \beta_1 \sum_{i=1}^{n} w_i x_i^2 \right)
    &amp; \\
\end{align*}\]</span>
Making them equal to zero, we have:</p>
<p><span class="math display">\[\begin{align*}
-2  
  &amp; \left( \sum_{i=1}^{n} w_i y_i - \beta_0 \sum_{i=1}^{n} w_i - \beta_1 \sum_{i=1}^{n} w_i x_i \right) = 0 \\
  &amp; \implies \sum_{i=1}^{n} w_i y_i = \beta_0 \sum_{i=1}^{n} w_i + \beta_1 \sum_{i=1}^{n} w_i x_i \tag{1}
\end{align*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align*}
-2  
  &amp; \left( \sum_{i=1}^{n} w_i y_i x_i - \beta_0 \sum_{i=1}^{n} w_i x_i - \beta_1 \sum_{i=1}^{n} w_i x_i^2 \right) = 0 \\
  &amp; \implies \sum_{i=1}^{n} w_i y_i x_i = \beta_0 \sum_{i=1}^{n} w_i x_i + \beta_1 \sum_{i=1}^{n} w_i x_i^2 \tag{2}
\end{align*}\]</span></p>
<p>Now we solve the system of two linear equations.
First, we solve for <span class="math inline">\(\beta_1\)</span> by multiplying equation (1) by <span class="math inline">\(\frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^n w_i}\)</span> and subtract it from equation (2) to eliminate <span class="math inline">\(\beta_0\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\sum_{i=1}^{n}
  &amp; w_i y_i = \beta_0 \sum_{i=1}^{n} w_i + \beta_1 \sum_{i=1}^{n} w_i x_i
    &amp; \\
  &amp;\implies \frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^n w_i} \sum_{i=1}^{n} w_i y_i = \frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^n w_i} \left( \beta_0 \sum_{i=1}^{n} w_i + \beta_1 \sum_{i=1}^{n} w_i x_i \right)
    &amp; \\
  &amp;\implies \frac{\sum_{i=1}^{n} w_i x_i \sum_{i=1}^{n} w_i y_i}{\sum_{i=1}^n w_i} = \beta_0 \sum_{i=1}^{n} w_i x_i + \beta_1 \frac{\left(\sum_{i=1}^{n} w_i x_i \right)^2}{\sum_{i=1}^n w_i}
    &amp; \\
  &amp;\implies \frac{\sum_{i=1}^{n} w_i x_i \sum_{i=1}^{n} w_i y_i}{\sum_{i=1}^n w_i} - \sum_{i=1}^{n} w_i y_i x_i = \beta_1 \frac{\left(\sum_{i=1}^{n} w_i x_i \right)^2}{\sum_{i=1}^n w_i} - \beta_1 \sum_{i=1}^{n} w_i x_i^2
    &amp; \text{sub. } (2) \\
  &amp;\implies \beta_1 \left( \sum_{i=1}^{n} w_i x_i^2 - \frac{\left( \sum_{i=1}^{n} w_i x_i \right)^2}{\sum_{i=1}^{n} w_i} \right) = \sum_{i=1}^{n} w_i y_i x_i - \frac{\sum_{i=1}^{n} w_i x_i \sum_{i=1}^{n} w_i y_i}{\sum_{i=1}^{n} w_i}
    &amp; \\
  &amp;\implies \beta_1 = \frac{\sum_{i=1}^{n} w_i x_i y_i - \frac{\sum_{i=1}^{n} w_i x_i \sum_{i=1}^{n} w_i y_i}{\sum_{i=1}^{n} w_i}}{\sum_{i=1}^{n} w_i x_i^2 - \frac{\left( \sum_{i=1}^{n} w_i x_i \right)^2}{\sum_{i=1}^{n} w_i}}
    &amp; \\
\end{align*}\]</span>
Then the weighted estimators for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are:
<span class="math display">\[\hat{\beta}_1^w = \frac{\sum_{i=1}^{n} w_i x_i y_i - \frac{\sum_{i=1}^{n} w_i x_i \sum_{i=1}^{n} w_i y_i}{\sum_{i=1}^{n} w_i}}{\sum_{i=1}^{n} w_i x_i^2 - \frac{\left( \sum_{i=1}^{n} w_i x_i \right)^2}{\sum_{i=1}^{n} w_i}},\]</span></p>
<p><span class="math display">\[\hat{\beta}_0^w = \frac{\sum_{i=1}^{n} w_i y_i - \beta_1 \sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^{n} w_i}.\]</span></p>
</div>
</details>
<p>How weights are determined?</p>
<ul>
<li>When the error variances are known a priori, weights can be set directly in inverse proportion to those variances.<br />
</li>
<li>In practice, true variances are usually unknown. Weights are therefore often based on estimates or proxies for the variances—obtained from repeated measurements, external information, or preliminary residual analysis. Iterative procedures can be used to refine weights if needed.<br />
</li>
<li>Choosing appropriate weights is crucial: good weights improve efficiency and inference, whereas poorly chosen weights can degrade performance.</li>
</ul>
<p>Implementation and relationship to OLS:</p>
<ul>
<li>WLS reduces to OLS when all weights are equal; thus OLS is a special case of WLS.<br />
</li>
<li>Implementation typically entails reweighting observations and solving the weighted normal equations; many statistical software packages provide direct WLS fitting routines.<br />
</li>
<li>It is important to record and use the same weights for prediction and validation to maintain consistency.</li>
</ul>
<p>Diagnostics and when to consider WLS:</p>
<ul>
<li>Residual plots that show changing spread with the predictor or fitted values suggest heteroscedasticity and motivate consideration of WLS.<br />
</li>
<li>Even when heteroscedasticity is modest, WLS can improve precision; where heteroscedasticity is severe, it can substantially alter conclusions.</li>
</ul>
<p>Limitations and cautions:</p>
<ul>
<li>Weight specification is critical. If weights do not reflect the actual variance structure, WLS may not improve and can even worsen inference.<br />
</li>
<li>When reliable information about the variance structure is unavailable, robust (heteroscedasticity‑consistent) standard errors or other methods may be preferable to poorly specified weighting.<br />
</li>
<li>WLS addresses nonconstant variance but does not by itself resolve other model problems such as nonlinearity, omitted variables, or influential outliers.</li>
</ul>
<p><strong>Summary</strong></p>
<p>Weighted Least Squares adapts the least‑squares framework to give more influence to precise observations and less to noisy ones, restoring efficiency when error variances vary across observations. The guiding principle is straightforward: lower variance implies higher weight, and higher variance implies lower weight. Effective use of WLS depends on sensible choices or estimates of the weights and appropriate calculation of standard errors for valid inference.</p>
<hr />
</div>
<div id="model-in-matrix-form" class="section level2 hasAnchor" number="4.10">
<h2><span class="header-section-number">4.10</span> Model in Matrix Form<a href="simple-linear-regression.html#model-in-matrix-form" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can specify the same model as in <a href="simple-linear-regression.html#slr-model">Model</a>, in matrix form as follows:</p>
<p><span class="math display">\[\mathbf{y}= \mathbf{X}\boldsymbol{\beta}+ \mathbf{e}\]</span>
where:</p>
<ul>
<li><span class="math inline">\(\mathbf{y}={y_1,\ldots,y_n}&#39;\)</span></li>
<li><span class="math inline">\(\mathbf{e}={e_1,\ldots,e_n}&#39;\)</span></li>
<li><span class="math inline">\(\boldsymbol{\beta}={\beta_0,\beta_1}&#39;\)</span></li>
<li><span class="math inline">\(\mathbf{X}=\begin{bmatrix}𝟙  &amp; \mathbf{x}\end{bmatrix}\)</span></li>
<li><span class="math inline">\(𝟙={1,\ldots,1}&#39;\)</span> (1 <span class="math inline">\(n\)</span>-times).</li>
</ul>
<p>In this way we can re-write our minimization problem as follows:</p>
<p><span class="math display">\[ \min_{\boldsymbol{\beta}} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \min_{\boldsymbol{\beta}} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2
= \min_{\boldsymbol{\beta}} \sum_{i=1}^{n} (\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})\]</span>
equivalent to:
<span class="math display">\[\min_{\boldsymbol{\beta}} Q(\boldsymbol{\beta}) = \left( \mathbf{y}&#39;\mathbf{y}- \mathbf{y}&#39;\mathbf{X}\boldsymbol{\beta}- \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{y}+ \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{X}\boldsymbol{\beta}\right)\]</span></p>
<div class="proposition">
<p><span id="prp:prp-lse-matrix-solution" class="proposition"><strong>Proposition 4.14  </strong></span>The solution to the Least Squres minimization problem in matrix form is given by:
<span class="math display">\[\hat{\boldsymbol{\beta}} = \left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}\mathbf{X}&#39;\mathbf{y}\]</span></p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-14" class="proof"><em>Proof</em>. </span>The procedure to find the estimators is very similar, working with the gradient
instead of the partial derivatives:
<span class="math display">\[\nabla_\beta Q = \mathbf{0} \]</span>
Now
<span class="math display">\[\nabla_\beta Q = -\mathbf{X}&#39;\mathbf{y}-\mathbf{X}&#39;\mathbf{y}+ 2 \mathbf{X}&#39; \mathbf{X}\boldsymbol{\beta}\]</span>
Then, making the gradient equal to zero, we have:
<span class="math display">\[\begin{align*}
\nabla_\beta Q
  &amp;= 0
    &amp; \\
  &amp; \implies -\mathbf{X}&#39;\mathbf{y}-\mathbf{X}&#39;\mathbf{y}+ 2 \mathbf{X}&#39; \mathbf{X}\boldsymbol{\beta}= 0
    &amp; \\
  &amp; \implies 2 \mathbf{X}&#39; \mathbf{X}\boldsymbol{\beta}= 2 \mathbf{X}&#39;\mathbf{y}
    &amp; \\
  &amp; \implies \mathbf{X}&#39; \mathbf{X}\boldsymbol{\beta}= \mathbf{X}&#39;\mathbf{y}
    &amp; \\
  &amp; \implies \boldsymbol{\beta}= \left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}\mathbf{X}&#39;\mathbf{y}
\end{align*}\]</span>
Notice how we find the estimates much easier. It is also easy to show that a
minimum is attained by computing the <strong>Hessian Matrix</strong> of <span class="math inline">\(Q\)</span>
The Hessian matrix is given by:
<span class="math display">\[H_Q = 2 \mathbf{X}&#39; \mathbf{X}\]</span>
Notice that
<span class="math display">\[ \mathbf{X}&#39; \mathbf{X}\]</span>
is <a href="prerequisites.html#positive-definite-matrix">positive-definite</a> if <span class="math inline">\(\mathbf{x}\)</span> is not a constant vector.
This shows that the solution is indeed a minimum.</p>
</div>
</details>
<div class="proposition">
<p><span id="prp:prp-lse-matrix-solution-equal" class="proposition"><strong>Proposition 4.15  </strong></span>The solution to the Least Squres minimization problem in matrix form is given by
is equivalent to the solution in of the model expressed in scalar form.</p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em>. </span>We need to show that each of the components of <span class="math inline">\(\hat{\boldsymbol{\beta}} = (\hat{\beta}_0,\hat{\beta_1})\)</span>
are the same of the scalar solution. To do so, we need to go through the matrix
computations.
<span class="math display">\[\begin{align*}
\mathbf{X}
  &amp;= \begin{bmatrix}
      1 &amp; x_1 \\
      1 &amp; x_2 \\
      \vdots &amp; \vdots \\
      1 &amp; x_n
    \end{bmatrix} \\
  &amp;= \begin{bmatrix}
      𝟙  &amp; \mathbf{x}
     \end{bmatrix} \in \mathbb{R}^{n \times 2}
\end{align*}\]</span></p>
<p>Then we have that:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{X}&#39;
  &amp;= \begin{bmatrix}
      𝟙&#39; \\
      \mathbf{x}&#39;
     \end{bmatrix} \in \mathbb{R}^{2 \times n}
\end{align*}\]</span></p>
<p>Multiplying <span class="math inline">\(\mathbf{X}&#39;\)</span> by <span class="math inline">\(\mathbf{X}\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{X}&#39; \mathbf{X}
  &amp;= \begin{bmatrix}
      𝟙&#39; \\
      \mathbf{x}&#39;
     \end{bmatrix}
     \begin{bmatrix}
      𝟙 &amp; \mathbf{x}
     \end{bmatrix} \\
  &amp;= \begin{bmatrix}
      𝟙&#39;𝟙 &amp; 𝟙&#39;\mathbf{x}\\
      \mathbf{x}&#39;𝟙&amp; \mathbf{x}&#39;\mathbf{x}
     \end{bmatrix} \\
  &amp;= \begin{bmatrix}
        n                &amp; \sum_{i=1}^n x_i   \\
        \sum_{i=1}^n x_i &amp; \sum_{i=1}^n x_i^2
     \end{bmatrix} \\
  &amp;= \begin{bmatrix}
        n        &amp; n\bar{x}   \\
        n\bar{x} &amp; \sum_{i=1}^n x_i^2
     \end{bmatrix}
\end{align*}\]</span></p>
<p>Similarly, we have that:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{X}&#39; \mathbf{y}
  &amp;= \begin{bmatrix}
      𝟙&#39; \\
      \mathbf{x}&#39;
     \end{bmatrix}
     \mathbf{y}\\
  &amp;= \begin{bmatrix}
      𝟙&#39;\mathbf{y}\\
      \mathbf{x}&#39;\mathbf{y}
     \end{bmatrix} \\
  &amp;= \begin{bmatrix}
      \sum_{i=1}^n y_i \\
      \sum_{i=1}^n x_i y_i
     \end{bmatrix} \\
  &amp;= \begin{bmatrix}
      n \bar{y} \\
      \sum_{i=1}^n x_i y_i
     \end{bmatrix} \\
\end{align*}\]</span></p>
<p>To compute the inverse of <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span> we use the <a href="prerequisites.html#special-case-2-by-2-matrix">Formula for the Inverse of a 2 by 2 Matrix</a>:</p>
<p><span class="math display">\[
   (\mathbf{X}&#39; \mathbf{X})^{-1} = \frac{1}{n \sum_{i=1}^n x_i^2 - n^2 \left( \bar{x} \right)^2}
   \begin{bmatrix}
   \sum_{i=1}^n x_i^2 &amp; - n \bar{x} \\
   - n \bar{x}        &amp; n
   \end{bmatrix}
\]</span></p>
<p>Combining all of this operations we have that:</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}
  &amp;= (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
    &amp; \\
  &amp;= \frac{1}{n \sum_{i=1}^n x_i^2 - n^2 \left( \bar{x} \right)^2}
    \begin{bmatrix}
      \sum_{i=1}^n x_i^2 &amp; - n \bar{x} \\
      - n \bar{x}        &amp; n
    \end{bmatrix}
    \begin{bmatrix}
      n \bar{y} \\
      \sum_{i=1}^n x_i y_i
    \end{bmatrix}
    &amp; \\
  &amp;= \frac{1}{n \left( \sum_{i=1}^n x_i^2 - n \left( \bar{x} \right)^2 \right)}
    \begin{bmatrix}
      n \bar{y} \sum_{i=1}^n x_i^2 - n \bar{x} \sum_{i=1}^n x_i y_i \\
      -n^2 \bar{y} \bar{x} + n \sum_{i=1}^n x_i y_i  
    \end{bmatrix}
    &amp; \\
  &amp;= \frac{1}{n(n-1)S_{xx}}
    \begin{bmatrix}
      n \left( \bar{y} \sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i y_i \right) \\
      n \left( \sum_{i=1}^n x_i y_i - n \bar{y} \bar{x} \right)   
    \end{bmatrix}
    &amp; S{xx}=\frac{\sum_{i=1}^n x_i^2 - n \left( \bar{x} \right)^2}{n-1} \\
  &amp;= \frac{1}{n(n-1)S_{xx}}
    \begin{bmatrix}
      n \left( \bar{y} \sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i y_i \right) \\
      n (n-1) S{xy}   
    \end{bmatrix}
    &amp; S{xy}=\frac{\sum_{i=1}^n x_iy_i - n \bar{x}\bar{y}}{n-1} \\
\end{align*}\]</span></p>
<p>Now, notice that:
<span class="math display">\[\begin{align*}
  \bar{y} \sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i y_i
    &amp;= \bar{y} \sum_{i=1}^n x_i^2 -n \bar{y}\bar{x}^2 + n \bar{y}\bar{x}^2 - \bar{x} \sum_{i=1}^n x_i y_i
      &amp; \text{adding 0} \\
    &amp;= \bar{y} \left(\sum_{i=1}^n x_i^2 -n \bar{x}^2 \right) + \bar{x}\left(n \bar{y}\bar{x} - \sum_{i=1}^n x_i y_i \right)
      &amp; \\
    &amp;= \bar{y} (n-1)S_{xx} - \bar{x}(n-1)S_{yx}
      &amp; S{xx}=\frac{\sum_{i=1}^n x_i^2 - n \left( \bar{x} \right)^2}{n-1}, S{xy}=\frac{\sum_{i=1}^n x_iy_i - n \bar{x}\bar{y}}{n-1} \\
    &amp;= (n-1)\left(\bar{y} S_{xx} - S_{yx} \bar{x} \right)
      &amp; S{xx}=\frac{\sum_{i=1}^n x_i^2 - n \left( \bar{x} \right)^2}{n-1}, S{xy}=\frac{\sum_{i=1}^n x_iy_i - n \bar{x}\bar{y}}{n-1} \\
\end{align*}\]</span>
Then we have that:
<span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}
  &amp;= \frac{1}{n(n-1)S_{xx}}
    \begin{bmatrix}
      n (n-1)\left(\bar{y} S_{xx} - S_{yx} \bar{x} \right) \\
      n (n-1) S{xy}   
    \end{bmatrix}
    &amp; \\
  &amp;= \begin{bmatrix}
      \frac{n (n-1)\left(\bar{y} S_{xx} - S_{yx} \bar{x} \right)}{n(n-1)S_{xx}} \\
      \frac{n (n-1) S{xy}}{n(n-1)S_{xx}}   
    \end{bmatrix}
    &amp; \\
  &amp;= \begin{bmatrix}
      \frac{\left(\bar{y} S_{xx} - S_{yx} \bar{x} \right)}{S_{xx}} \\
      \frac{S{xy}}{S_{xx}}   
    \end{bmatrix}
    &amp; \\
  &amp;= \begin{bmatrix}
      \frac{\bar{y} S_{xx}}{S_{xx}} - \frac{S_{yx} \bar{x}}{S_{xx}} \\
      \hat{\beta}_1   
    \end{bmatrix}
    &amp; \hat{\beta}_1 = \frac{S{xy}}{S_{xx}}\\
  &amp;= \begin{bmatrix}
      \bar{y} - \bar{x}\frac{S_{yx}}{S_{xx}} \\
      \hat{\beta}_1   
    \end{bmatrix}
    &amp; \\
  &amp;= \begin{bmatrix}
      \bar{y} - \bar{x}\hat{\beta}_1 \\
      \hat{\beta}_1   
    \end{bmatrix}
    &amp; \hat{\beta}_1 = \frac{S{xy}}{S_{xx}}\\
\end{align*}\]</span></p>
</div>
</details>
<hr />
<div id="weighted-least-squares-in-matrix-form" class="section level3 hasAnchor" number="4.10.1">
<h3><span class="header-section-number">4.10.1</span> Weighted Least Squares in Matrix Form<a href="simple-linear-regression.html#weighted-least-squares-in-matrix-form" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In a similar way we can solve the weighted least squares problem in matrix form
by noticing that we can write the Weighted Least Squares function as follows:</p>
<p><span class="math display">\[Q_w = \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_i)^2 = (\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;\mathbf{W}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) = \mathbf{y}&#39;\mathbf{W}\mathbf{y}-\mathbf{y}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}- \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{W}\mathbf{y}+ \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}\]</span></p>
<p>where, <span class="math inline">\(\mathbf{W}\)</span> is a diagonal matrix with diagonal entries <span class="math inline">\(w_1,\ldots,w_n\)</span>. That is</p>
<p><span class="math display">\[\mathbf{W}= \left[\begin{matrix}
          w_1    &amp; 0      &amp; \ldots &amp; 0      \\
          0      &amp; w_2    &amp; \ldots &amp; 0      \\
          0      &amp;   0    &amp; \ldots &amp; 0      \\
          \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
          0      &amp;      0 &amp; \ldots &amp; w_n    
        \end{matrix} \right]\]</span></p>
<div class="proposition">
<p><span id="prp:prp-weighted-least-squares-estimators-matrix" class="proposition"><strong>Proposition 4.16  (Weighted Least Squares Estimators in Matrix Form) </strong></span>The solution to the Weithed Least Squares problem in matrix form:
<span class="math display">\[
\min_{\boldsymbol{\beta}} (\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;\mathbf{W}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})
\]</span>
is given by:
<span class="math display">\[\hat{\boldsymbol{\beta}}^w = \left(\mathbf{X}&#39;\mathbf{W}\mathbf{X}\right)^{-1}\mathbf{X}&#39;\mathbf{W}\mathbf{y}\]</span></p>
</div>
<details>
<summary>
<span style="color:green">Proof</span>
</summary>
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>Note we can write the objective function as:
<span class="math display">\[Q_w = \mathbf{y}&#39;\mathbf{W}\mathbf{y}-\mathbf{y}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}- \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{W}\mathbf{y}+ \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}\]</span>
then computing the gradient we have that:
<span class="math display">\[\begin{align*}
  \nabla_\boldsymbol{\beta}Q_w
    &amp;=\frac{d Q_w}{d \boldsymbol{\beta}}
      &amp; \\
    &amp;= \frac{d}{d \boldsymbol{\beta}} \left[\mathbf{y}&#39;\mathbf{W}\mathbf{y}-\mathbf{y}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}- \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{W}\mathbf{y}+ \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}\right]
      &amp; \\
    &amp;=  \frac{d}{d \boldsymbol{\beta}} \mathbf{y}&#39;\mathbf{W}\mathbf{y}- \frac{d}{d \boldsymbol{\beta}} \mathbf{y}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}- \frac{d}{d \boldsymbol{\beta}} \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{W}\mathbf{y}+ \frac{d}{d \boldsymbol{\beta}} \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}
      &amp; \text{lin. derivative} \\
    &amp;=  0 - \mathbf{X}&#39;\mathbf{W}\mathbf{y}- \mathbf{X}&#39;\mathbf{W}\mathbf{y}+ 2 \mathbf{X}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}
      &amp;  \\
    &amp;=  2 \mathbf{X}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}- 2 \mathbf{X}&#39;\mathbf{W}\mathbf{y}
      &amp;  \\
\end{align*}\]</span></p>
<p>Then making the gradient equalt to <span class="math inline">\(0\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
  \nabla_\boldsymbol{\beta}Q_w
    &amp;= 0
      &amp; \\
    &amp;\implies 2 \mathbf{X}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}- 2 \mathbf{X}&#39;\mathbf{W}\mathbf{y}
      &amp; \\
    &amp;\implies \mathbf{X}&#39;\mathbf{W}\mathbf{X}\boldsymbol{\beta}= \mathbf{X}&#39;\mathbf{W}\mathbf{y}
      &amp; \\
    &amp;\implies \boldsymbol{\beta}= \left(\mathbf{X}&#39;\mathbf{W}\mathbf{X}\right)^{-1} \mathbf{X}&#39;\mathbf{W}\mathbf{y}
      &amp; \\
\end{align*}\]</span></p>
<p>And the Hessian Matrix is given by:</p>
<p><span class="math display">\[H_{Q_w} = 2 \mathbf{X}&#39;\mathbf{W}\mathbf{X}\]</span></p>
<p>which is positive-definite under the same conditions that before and if the entries
of <span class="math inline">\(\mathbf{W}\)</span> are positive.</p>
</div>
</details>
<p>Again, using matrix notation, the computations become much more easier.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="polynomial-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
