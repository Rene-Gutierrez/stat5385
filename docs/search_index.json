[["index.html", "1 Stat 5385/6385", " 1 Stat 5385/6385 This is the website for STAT5385. It will contain the relevant information for the course and lecture notes and code seen in class. Find the syllabus here: Syllabus "],["prerequisites.html", "2 Prerequisites 2.1 General Math 2.2 Linear Algebra 2.3 Probability 2.4 Statistics 2.5 Calculus", " 2 Prerequisites Before diving into the course, it’s important to have a solid understanding of the following foundational concepts. These are categorized into five key topics: General Math Linear Algebra Probability Statistics Calculus You can check some of the requirements on Chapter 1 of the textbook. 2.1 General Math You should be familiar with the summation operator \\(\\sum\\). This operator is defined as follows: \\[\\sum_{i=1}^n x_i = x_1 + x_2 + \\ldots + x_n \\] Key properties of the summation operator include: Linearity: \\[\\sum_{i=1}^N (a + b x_i) = aN + b \\sum_{i=1}^N x_i\\] Additivity: \\[\\sum_{i=1}^N (x_i + y_i) = \\sum_{i=1}^N x_i + \\sum_{i=1}^N y_i\\] 2.2 Linear Algebra You should be familiar with the following linear algebra concepts: Matrices Determinants Eigenvalues and Eigenvectors Diagonalization Vector Spaces Linear Transformations 2.3 Probability Key probability concepts to understand include: Expected Value Variance Covariance Correlation Joint, Marginal, and Conditional Distributions Independence Central Limit Theorem Distributions: Normal Chi-Squared (\\(\\chi^2\\)) t-distribution F-distribution 2.4 Statistics Essential statistical concepts include: Point Estimation: Maximum Likelihood Least Squares Estimation Properties of Point Estimators: Unbiased Consistent Minimum Variance Interval Estimation Hypothesis Testing 2.5 Calculus Key calculus topics include: Gradients Optimization "],["introduction.html", "3 Introduction 3.1 Examples", " 3 Introduction Check chapters 2.1-2.2 of the textbook. Linear regression is a statistical method used to analyze the relationship between at least two variables: one dependent variable and at least one independent variable. This technique is closely tied to the correlation coefficient, which measures the strength and direction of a linear relationship between variables. In this document, we will explore how these concepts are connected. Linear regression is typically applied for three main purposes: Description: To describe the relationship between the variables under analysis. Control: To predict how changes in the independent variables will affect the dependent variable. Prediction: To forecast the value of the dependent variable based on new observations of the independent variables. 3.1 Examples 3.1.1 Ad Spending Imagine you are a newly hired data scientist at a mattress company. Your manager asks you to analyze the relationship between Google ad spending and mattress sales revenue. To illustrate this scenario, I have simulated a dataset: # Ad Spending Example # Set Seed set.seed(8272024) # Data Simulation x &lt;- rnorm(n = 100, mean = 70, sd = 30) y &lt;- 1000 + 5 * x + rnorm(n = 100, mean = 0, sd = 100) # Creates the Data Frame datAd &lt;- data.frame(cbind(y,x)) # Names the Variables colnames(datAd) &lt;- c(&quot;Revenue&quot;, &quot;Ad Spending&quot;) # Saves to csv write.csv(x = datAd[, c(1, 2)], file = &quot;Ad spending Data.csv&quot;, row.names = FALSE) You can load the dataset as follows: dat &lt;- read.csv(file = &quot;Ad spending Data.csv&quot;) To visualize the data: dat &lt;- read.csv(file = &quot;Ad spending Data.csv&quot;) plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) Next, you can perform a linear regression analysis: outReg &lt;- lm(Revenue ~ Ad.Spending, data = dat) The most important results from the regression analysis can be summarized as follows: summary(outReg) ## ## Call: ## lm(formula = Revenue ~ Ad.Spending, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -248.394 -58.805 3.782 63.577 196.745 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 997.3894 28.8185 34.61 &lt;2e-16 *** ## Ad.Spending 5.0247 0.3818 13.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 89.01 on 98 degrees of freedom ## Multiple R-squared: 0.6386, Adjusted R-squared: 0.6349 ## F-statistic: 173.2 on 1 and 98 DF, p-value: &lt; 2.2e-16 For now, let’s focus on the estimate for the intercept and the Ad.Spending coefficient. The intercept indicates the expected revenue when ad spending is zero. Based on your analysis, you observe that even without an ad campaign, mattress sales generate \\(r outReg\\)coefficients[1]. The coefficient for Ad.Spending shows that each dollar spent on ads increases revenue by \\(r outReg\\)coefficients[2]. To visualize the regression line: plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) The red regression line represents the “best fit” for these variables. In this context, “best fit” means the line that minimizes the sum of the squared distances from each point to the line. For comparison, here are examples of other lines that do not fit as well: Different slope (coefficient for the Ad.Spending variable): plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(a = outReg$coefficients[1], b = 6, col = &#39;blue&#39;, lwd = 2) Different intercept: plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(a = 1100, b = outReg$coefficients[2], col = &#39;blue&#39;, lwd = 2) We can expand the range for ad spending and revenue on the plot, as follows: plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;, ylim = c(800, 2500), xlim = c(0, 200)) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(h = 0, v = 0) Within the range of observed ad spending values (r round(min(dat\\(Ad.Spending)) to r round(max(dat\\)Ad.Spending))), we can be reasonably confident in the relationship between ad spending and revenue. However, what happens when we consider values outside this range? While we can predict and control revenue to some extent by adjusting ad spending within the observed range, this confidence may not extend to values beyond it. For instance, it’s plausible that after reaching a certain level of ad spending, the market could become saturated, resulting in diminishing or even no additional revenue despite increased ad spending. Therefore, we should be cautious when extrapolating beyond the observed data, as the relationship may not hold under different conditions. 3.1.2 Wine and Life Expectancy In the previous example, we were able to manipulate the independent variable to influence the outcome. However, there are situations where our primary goal is simply to describe the relationship between two variables, without aiming to control them. In the following example, I generate a dataset that illustrates the relationship between wine consumption and life expectancy in years for an imaginary country. Here’s how the data is simulated: # Wine and Life Expectancy # Set Seed set.seed(8272024) # Data Simulation x &lt;- rbinom(n = 20, size = 20, prob = 0.1) y &lt;- 75 + 1.5 * x + rnorm(n = 20, mean = 0, sd = 3) # Creates the Data Frame datWin &lt;- data.frame(cbind(y,x)) # Names the Variables colnames(datWin) &lt;- c(&quot;Years&quot;, &quot;Glasses&quot;) # Saves to csv write.csv(datWin[, c(1, 2)], file = &quot;Wine Data.csv&quot;, row.names = FALSE) Here’s an improved version of your text: You should be able to generate the exact same data if you copy and paste the code and run it on your computer. Although the data is randomly simulated, I’ve set a seed to ensure that the simulation can be replicated consistently. We can read and plot the data in the as follows: # Reads the Data dat &lt;- read.csv(file = &quot;Wine Data.csv&quot;) # Plots the Data plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) Once again, we can perform a linear regression to examine the relationship between the variables and visualize it with a regression line: # Performs Linear Regression outReg &lt;- lm(Years ~ Glasses, data = dat) # Plots plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) Given the regression line, can we confidently conclude that increasing wine consumption leads to a longer life expectancy? 3.1.3 Burger Demand Imagine you work at a burger franchise where prices change daily. As the franchise manager, you want to predict the demand for burgers at a given price. Over the past 100 days, you’ve collected data on the number of burgers sold at each price set by the franchise’s main office. Here’s what your data looks like: # Reads the Data dat &lt;- read.csv(file = &quot;Burger Data.csv&quot;) # Plots the Data plot(x = dat$Price, y = dat$Burgers, xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers Sold&quot;) Is this dataset a good candidate for linear regression? While it’s true that we can always fit a line to any dataset, the real question is whether that line meaningfully represents the relationship between the variables. Here’s what happens when we apply linear regression to this data: # Performs Linear Regression outReg &lt;- lm(Burgers ~ Price, data = dat) # Plots plot(x = dat$Price, y = dat$Burgers, xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers Sold&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) Do you think this is a good fit for the data? The regression line appears to be inadequate for both low and high price points. Additionally, since the number of burgers sold must be positive, the regression line should not intersect the x-axis. "],["simple-linear-regression.html", "4 Simple Linear Regression 4.1 Model 4.2 Least Squares Estimation", " 4 Simple Linear Regression Simple linear regression (SLR) is a linear regression model with a single explanatory variable. It focuses on the linear relationship between one independent variable and one dependent variable, making it the most basic form of linear regression analysis. 4.1 Model The model for simple linear regression is as follows: \\[y_i = \\beta_0 + \\beta_1 x_i + e_i, \\quad i\\in\\{1,\\ldots,n\\}\\] where: \\(y_i\\) represents the \\(i\\)-th observation of the dependent variable. \\(x_i\\) represents the \\(i\\)-th observation of the independent variable. \\(e_i\\) represents the \\(i\\)-th observation of the error term. \\(\\beta_0\\) is the intercept of the linear model, or regression line. \\(\\beta_1\\) is the slope of the linear model, or regression line. \\(n\\) is the number of observations for both variables. Note that we are not making any assumptions about the error terms. In the case of the wine example, we generated the data based on the following linear model: \\[y_i = 75 + 1.5 x_i + e_i \\] dat &lt;- read.csv(file = &quot;Wine Data.csv&quot;) plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) abline(a = 75, b = 1.5, col = &#39;red&#39;, lwd = 2) abline(v = 0, lwd = 2) text(x = 0.25, y = 76, expression(beta[0] ~ &quot;=75&quot;)) text(x = 3.25, y = 79, expression(beta[1] ~ &quot;=1.5&quot;)) segments(x0 = c(2, 3), x1 = c(3, 3), y0 = c(78, 78), y1 = c(78, 79.5), lwd = 2, col = &#39;blue&#39;) In this case, the intercept \\(\\beta_0\\) is meaningful, as it represents the expected number of years a person would live if they didn’t drink wine at all. However, depending on the data, the intercept may or may not have a meaningful interpretation. The slope \\(\\beta_1\\) indicates that for each additional glass of wine consumed per week, our model predicts an increase of 1.5 years in life expectancy. In practice, we rarely know the true regression line. Instead, it must be estimated from the data. The goal is to find the “best” line that fits the data, where “best” means the line that minimizes the sum of squared errors (SSE) between the observed values and the values predicted by the model. 4.2 Least Squares Estimation As explained before, we want to minimize the SSE, we can create a function of \\(\\beta_0\\) and \\(\\beta_1\\) with this sum as follows: \\[Q(\\beta_0, \\beta_1) = \\sum_{i=1}^n (e_i(\\beta_0, \\beta_1))^2 = \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\] and we can find the minimum of this function easily since it is a differentiable function. We can find the both components of the gradient and equal them to zero to find the critical points. We start with \\(\\beta_0\\): \\[\\begin{align*} \\frac{\\partial Q}{\\partial \\beta_0} &amp;= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\ &amp;= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i^2 + \\beta_0^2 + \\beta_1^2 x_i^2 - 2 \\beta_0 y_i - 2 \\beta_1 x_i y_i + 2 \\beta_0 \\beta_1 x_i) \\\\ &amp;= \\sum_{i = 1}^n (2 \\beta_0 - 2 y_i + 2 \\beta_1 x_i) \\\\ &amp;= -2 \\left( n \\beta_0 - n \\bar{y} + n\\beta_1 \\bar{x} \\right) \\end{align*}\\] where we have adopted the notation: \\(\\bar{x} = \\frac{1}{n}\\sum_{i}^n x_i\\) and \\(\\bar{y} = \\frac{1}{n}\\sum_{i}^n y_i\\). \\[\\begin{align} \\frac{\\partial Q}{\\partial \\beta_0} = 0 &amp;\\iff -2 \\left( n \\beta_0 - n \\bar{y} + n\\beta_1 \\bar{x} \\right) = 0 \\notag \\\\ &amp;\\iff \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\tag{1} \\end{align}\\] And we can do a similar thing for \\(\\beta_1\\): \\[\\begin{align*} \\frac{\\partial Q}{\\partial \\beta_1} &amp;= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\ &amp;= \\sum_{i = 1}^n 2(y_i -\\beta_0 - \\beta_1 x_i)(-x_i) \\\\ &amp;= -2\\sum_{i = 1}^n y_i x_i + 2 \\beta_0 \\sum_{i = 1}^n x_i + 2 \\beta_1 \\sum_{i = 1}^n x_i^2 \\\\ &amp;= -2\\sum_{i = 1}^n y_i x_i + 2 n \\beta_0 \\bar{x} + 2 \\beta_1 \\sum_{i = 1}^n x_i^2 \\end{align*}\\] then: \\[\\begin{align} \\frac{\\partial Q}{\\partial \\beta_1} = 0 &amp;\\iff -2\\sum_{i = 1}^n y_i x_i + 2 n \\beta_0 \\bar{x} + 2 \\beta_1 \\sum_{i = 1}^n x_i^2 = 0 \\notag \\\\ &amp;\\iff \\sum_{i = 1}^n y_i x_i = n \\beta_0 \\bar{x} + \\beta_1 \\sum_{i = 1}^n x_i^2 \\tag{2} \\end{align}\\] Now, substituting (1) into (2) we have that \\[\\begin{align*} \\sum_{i = 1}^n y_i x_i &amp;= n (\\bar{y} - \\beta_1 \\bar{x}) \\bar{x} + \\beta_1 \\sum_{i = 1}^n x_i^2 \\\\ &amp;= n \\bar{y} \\bar{x} - n \\beta_1 \\bar{x}^2 + \\beta_1 \\sum_{i = 1}^n x_i^2 \\\\ &amp;= n \\bar{y} \\bar{x} + \\beta_1 \\left( \\sum_{i = 1}^n x_i^2 - n \\bar{x}^2 \\right) \\end{align*}\\] Then, \\[ \\beta_1 = \\frac{\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}}{\\sum_{i = 1}^n x_i^2 - n \\bar{x}^2} \\] so, the only critical point for \\(Q(\\beta_0,\\beta_1)\\) is when: \\[ \\hat{\\beta}_1 = \\frac{\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}}{\\sum_{i = 1}^n x_i^2 - n \\bar{x}^2} \\] \\[ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] where we use \\(\\hat{}\\), to denote the specific critical point. It remains to see if this is indeed a minimum. One can check the second order conditions. Now, if we introduce the notation for sample variance and covariance: \\[ S^2_{xx} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 \\] \\[ S_{xy} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\] and note the following: \\[\\begin{align*} \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i^2 - 2\\bar{x}x_i + \\bar{x}^2) \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_i^2 - 2\\bar{x}\\sum_{i=1}^n x_i + \\sum_{i=1}^n \\bar{x}^2 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_i^2 - 2\\bar{x}(n\\bar{x}) + n \\bar{x}^2 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_i^2 - 2n\\bar{x}^2 + n \\bar{x}^2 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_i^2 - n\\bar{x}^2 \\end{align*}\\] and \\[\\begin{align*} \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_iy_i - \\bar{x}y_i - \\bar{y}x_i + \\bar{x}\\bar{y}) \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_iy_i - \\bar{x} \\sum_{i=1}^ny_i - \\bar{y} \\sum_{i=1}^n x_i + \\sum_{i=1}^n \\bar{x}\\bar{y} \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_iy_i - n\\bar{x} \\bar{y} - n\\bar{y} \\bar{x} + n \\bar{x}\\bar{y} \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_iy_i - n\\bar{x} \\bar{y} \\\\ \\end{align*}\\] then we can express \\(\\hat{\\beta}_1\\) as: \\[\\hat{\\beta}_1 = \\frac{(n-1)S_{xy}}{(n-1)S_{xx}^2}=\\frac{S_{xy}}{S_{xx}^2} \\] Now notice that in order to find the Least Squares estimates you don’t require the complete data set, but only require the following quantities: \\(\\bar{y}\\). \\(\\bar{x}\\). \\(S_{xx}^2\\). \\(S_{xy}\\). 4.2.1 Other estimated quantites If we use the Least squares estimates in the regression equation, we can derive other estimated quantities: The estimated value for observation \\(i\\): \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] and the estimated error: \\[ \\hat{e}_i = y_i - \\hat{y}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\] And we can also compare our estimated regression line (blue) with the real regression line (red) in the following as follows: outReg &lt;- lm(Years ~ Glasses, data = dat) plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) abline(a = 75, b = 1.5, col = &#39;red&#39;, lwd = 2) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;blue&#39;, lwd = 2) 4.2.2 Properties of the Estimates The estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are linear combinations of \\(\\mathbf{y} = (y_1,\\ldots,y_n)&#39;\\). To see this, notice the following: \\[\\begin{align*} \\sum{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\\\ &amp;= \\sum{i=1}^n x_i y_i - n \\bar{x} \\bar{y} \\\\ &amp;= \\sum{i=1}^n x_i y_i - \\bar{x} \\sum_{i=1}^n y_i \\\\ &amp;= \\sum{i=1}^n x_i y_i - \\sum_{i=1}^n \\bar{x} y_i \\\\ &amp;= \\sum{i=1}^n (x_i y_i - \\bar{x} y_i) \\\\ &amp;= \\sum{i=1}^n (x_i - \\bar{x}) y_i \\\\ \\end{align*}\\] Then \\[ \\hat{\\beta}_1 = \\frac{\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) y_i}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\sum_{i=1}^n\\frac{(x_i - \\bar{x}) }{\\sum_{i=1}^n (x_i - \\bar{x})^2}y_i \\] and similarly: \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} = \\sum_{i=1}^n \\frac{y_i}{n} - \\sum_{i=1}^n\\frac{(x_i - \\bar{x}) }{\\sum_{j = 1}^n x_j^2 - n \\bar{x}^2}y_i \\bar{x} = \\sum_{i=1}^n \\left( \\frac{1}{n} - \\frac{(x_i - \\bar{x}) }{\\sum_{j = 1}^n x_j^2 - n \\bar{x}^2} \\bar{x} \\right)y_i \\] Also, notice that the sum of the errors is \\(0\\). \\[\\begin{align*} \\sum_{i=1}^n \\hat{e}_i &amp;= \\sum_{i=1}^n(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) \\\\ &amp;= \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\hat{\\beta}_0 - \\hat{\\beta}_1 \\sum_{i=1}^n x_i \\\\ &amp;= n\\bar{y} - n \\hat{\\beta}_0 - n \\hat{\\beta}_1 \\bar{x} \\\\ &amp;= n\\bar{y} - n (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) - n \\hat{\\beta}_1 \\bar{x} \\\\ &amp;= n\\bar{y} - n \\bar{y} + n \\hat{\\beta}_1 \\bar{x} - n \\hat{\\beta}_1 \\bar{x} \\\\ &amp;= 0 \\end{align*}\\] If we let \\(\\hat{\\mathbf{e}} = (\\hat{e}_i,\\ldots,\\hat{e}_n)&#39;\\) and \\(\\mathbf{x}=(x_1,\\ldots,x_n)&#39;\\), two vectors of size \\(n\\), then we have that \\(\\hat{\\mathbf{e}}\\) and \\(\\mathbf{x}\\) are orthogonal. That is: \\[\\begin{align*} \\langle \\hat{\\mathbf{e}}, \\mathbf{x}\\rangle &amp;= \\sum_{i=1}^{n} \\hat{e}_i x_i \\\\ &amp;= \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)x_i \\\\ &amp;= \\sum_{i=1}^{n} (y_i x_i - \\hat{\\beta}_0x_i - \\hat{\\beta}_1 x_i x_i) \\\\ &amp;= \\sum_{i=1}^{n} y_i x_i - \\sum_{i=1}^{n} \\hat{\\beta}_0x_i - \\sum_{i=1}^{n} \\hat{\\beta}_1 x_i x_i \\\\ &amp;= \\sum_{i=1}^{n} y_i - n \\hat{\\beta}_0 \\bar{x} - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2 \\\\ &amp;= \\sum_{i=1}^{n} y_i - n (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\bar{x} - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2 \\\\ &amp;= \\sum_{i=1}^{n} y_i - n \\bar{y} \\bar{x} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2 \\\\ &amp;= \\sum_{i=1}^{n} y_i - n \\bar{y} \\bar{x} - \\hat{\\beta}_1 (\\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2) \\\\ &amp;= \\sum_{i=1}^{n} y_i - n \\bar{y} \\bar{x} - \\frac{\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}}{\\sum_{i = 1}^n x_i^2 - n \\bar{x}^2}(\\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2) \\\\ &amp;= \\sum_{i=1}^{n} y_i - n \\bar{y} \\bar{x} - (\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}) \\\\ &amp;=0 \\end{align*}\\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
