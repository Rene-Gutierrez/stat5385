[["index.html", "1 Stat 5385/6385", " 1 Stat 5385/6385 This is the website for STAT5385. It will contain the relevant information for the course and lecture notes and code seen in class. Find the syllabus here: Syllabus "],["prerequisites.html", "2 Prerequisites 2.1 General Math 2.2 Linear Algebra 2.3 Probability 2.4 Statistics 2.5 Calculus", " 2 Prerequisites Before diving into the course, it’s important to have a solid understanding of the following foundational concepts. These are categorized into five key topics: General Math Linear Algebra Probability Statistics Calculus You can check some of the requirements on Chapter 1 of the textbook. 2.1 General Math You should be familiar with the summation operator \\(\\sum\\). This operator is defined as follows: \\[\\sum_{i=1}^n x_i = x_1 + x_2 + \\ldots + x_n \\] Key properties of the summation operator include: Linearity: \\[\\sum_{i=1}^N (a + b x_i) = aN + b \\sum_{i=1}^N x_i\\] Additivity: \\[\\sum_{i=1}^N (x_i + y_i) = \\sum_{i=1}^N x_i + \\sum_{i=1}^N y_i\\] 2.2 Linear Algebra You should be familiar with the following linear algebra concepts: Linear Independence Full Rank Matrix Inverse Matrix Positive Definite Matrix Determinants Eigenvalues and Eigenvectors Diagonalization Vector Spaces Linear Transformations 2.2.1 Linear Independence Linear independence is a fundamental concept in linear algebra that describes a set of vectors where no vector can be written as a linear combination of the others. In other words, the vectors are not “redundant,” meaning none of the vectors depends on any other in the set. 2.2.1.1 Definition: A set of vectors \\(\\{ \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n \\}\\) in a vector space is linearly independent if the only solution to the equation: \\[ c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n = \\mathbf{0} \\] is when all the scalar coefficients \\(c_1, c_2, \\ldots, c_n\\) are zero, i.e., \\(c_1 = c_2 = \\cdots = c_n = 0\\). If any of the coefficients can be non-zero while still satisfying this equation, then the vectors are linearly dependent. 2.2.1.2 Example: Consider two vectors in \\(\\mathbb{R}^2\\): \\[ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\] These vectors are linearly independent because there is no way to express one as a multiple of the other. The only solution to: \\[ c_1 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + c_2 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\] is \\(c_1 = 0\\) and \\(c_2 = 0\\). In contrast, if: \\[ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} \\] These vectors are linearly dependent, because \\(\\mathbf{v}_2 = 2 \\mathbf{v}_1\\). Therefore, you can express \\(mathbf{v}_2\\) as a linear combination of \\(\\mathbf{v}_1\\). 2.2.1.3 Key Points: Linearly independent vectors carry distinct information and cannot be derived from each other. Linearly dependent vectors are redundant because one or more can be expressed as a combination of others. In a set of linearly independent vectors, removing any vector would reduce the span of the vector space they cover. 2.2.1.4 Importance: Linear independence is crucial in determining the rank of a matrix. In systems of equations, linear independence of the rows or columns determines if the system has a unique solution. In vector spaces, the dimension of the space is the maximum number of linearly independent vectors. 2.2.2 Full Rank Matrix A full rank matrix is a matrix in which the rank is equal to the largest possible value for that matrix, meaning: For an \\(m \\times n\\) matrix \\(A\\), the rank is the maximum number of linearly independent rows or columns. If the rank is equal to \\(m\\) (the number of rows), the matrix has full row rank. If the rank is equal to \\(n\\) (the number of columns), the matrix has full column rank. 2.2.2.1 For a square matrix (\\(m = n\\)): A square matrix is full rank if its rank is equal to its dimension, i.e., if the matrix is invertible. In this case, \\(\\text{rank}(\\mathbf{A}) = n\\), meaning all rows and columns are linearly independent, and the matrix has an inverse. 2.2.2.2 For a rectangular matrix (\\(m \\neq n\\)): A matrix is full rank if the rank equals the smaller of the number of rows or columns. For an \\(m \\times n\\) matrix, the rank is at most \\(\\min(m, n)\\). If the matrix has full row rank, all rows are linearly independent. If the matrix has full column rank, all columns are linearly independent. 2.2.2.3 Example: Consider the matrix: \\[ \\mathbf{A}= \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\] This is a \\(2 \\times 3\\) matrix. Since its two rows are linearly independent, it has full row rank, with rank = 2 (the number of rows). However, it does not have full column rank because it has only two independent rows for three columns. 2.2.2.4 Key Properties: A full rank matrix has no redundant rows or columns (no row or column can be written as a linear combination of others). A square matrix with full rank is invertible (non-singular). For a rectangular matrix, full rank implies the matrix has maximal independent information in terms of its rows or columns. 2.2.2.5 Importance: Full rank matrices are crucial in solving systems of linear equations. A system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) has a unique solution if \\(\\mathbf{A}\\) is a square, full rank matrix. In linear algebra and machine learning, the rank provides insight into the dimensionality and the independence of the data or transformation matrix. 2.2.3 Inverse Matrix An inverse matrix of a square matrix \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is a matrix that, when multiplied by \\(\\mathbf{A}\\), results in the identity matrix \\(I\\). This relationship is expressed as: \\[ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A}= \\mathbf{I} \\] where \\(\\mathbf{I}\\) is the identity matrix, and its diagonal elements are 1, with all off-diagonal elements being 0. 2.2.3.1 Conditions for a Matrix to Have an Inverse: The matrix \\(\\mathbf{A}\\) must be square, meaning it has the same number of rows and columns. The matrix \\(\\mathbf{A}\\) must be non-singular, meaning its determinant is non-zero (\\(|\\mathbf{A}| \\neq 0\\)). 2.2.3.1.1 Properties of the Inverse Matrix: Uniqueness: If a matrix has an inverse, it is unique. Inverse of a Product: The inverse of the product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is given by \\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}\\). Inverse of the Inverse: \\((\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\\). Transpose of the Inverse: \\((\\mathbf{A}^{-1})&#39; = (\\mathbf{A}&#39;)^{-1}\\). 2.2.3.2 Special Case: For a \\(2 \\times 2\\) matrix: \\[ \\mathbf{A}= \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\] The inverse of \\(\\mathbf{A}\\) (if \\(|\\mathbf{A}|=\\det(\\mathbf{A}) \\neq 0\\)) is: \\[ A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix} \\] where \\(ad - bc\\) is the determinant of the matrix \\(\\mathbf{A}\\). 2.2.4 Positive Definite Matrix A positive definite matrix is a symmetric matrix \\(\\mathbf{A}\\) where, for any non-zero vector \\(\\mathbf{x}\\), the following condition holds: \\[ \\mathbf{x}&#39; \\mathbf{A}\\mathbf{x} &gt; 0 \\] 2.2.4.1 Key Properties: Symmetry: The matrix \\(\\mathbf{A}\\) must be symmetric, meaning \\(\\mathbf{A}= \\mathbf{A}&#39;\\). Positive quadratic form: For any non-zero vector \\(\\mathbf{x}\\), the quadratic form \\(\\mathbf{x}&#39; \\mathbf{A}\\mathbf{x}\\) must yield a positive value. 2.2.4.2 Characteristics of a Positive Definite Matrix: All the eigenvalues of a positive definite matrix are positive. The determinants of the leading principal minors (submatrices) of the matrix are positive. The diagonal elements of a positive definite matrix are positive. 2.2.4.3 Example: The matrix: \\[ \\mathbf{A}= \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\] is positive definite, because for any non-zero vector \\(\\mathbf{x}\\), \\(\\mathbf{x}&#39; \\mathbf{A}\\mathbf{x} &gt; 0\\). For instance, if \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), then: \\[ \\mathbf{x}&#39; \\mathbf{A}\\mathbf{x} = \\begin{bmatrix} 1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 6 &gt; 0 \\] 2.3 Probability Key probability concepts to understand include: Expected Value Variance Covariance Correlation Joint, Marginal, and Conditional Distributions Independence Central Limit Theorem Distributions: Normal Chi-Squared (\\(\\chi^2\\)) t-distribution F-distribution 2.4 Statistics Essential statistical concepts include: Point Estimation: Maximum Likelihood Least Squares Estimation Properties of Point Estimators: Unbiased Consistent Minimum Variance Interval Estimation Hypothesis Testing 2.5 Calculus Key calculus topics include: Gradient Hessian Matrix Calculus Optimization 2.5.1 Gradient The gradient of a function is a vector that contains the partial derivatives of the function with respect to each of its variables. It points in the direction of the steepest ascent of the function, and its magnitude indicates the rate of change in that direction. For a scalar function \\(f(x_1, x_2, \\ldots, x_n)\\), where \\(x_1, x_2, \\ldots, x_n\\) are the variables, the gradient is defined as: \\[ \\nabla f = \\frac{d}{d \\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} \\] 2.5.1.1 Key Points: Direction: The gradient points in the direction of the greatest increase of the function. Magnitude: The magnitude of the gradient represents how fast the function increases in that direction. Zero Gradient: If \\(\\nabla f = 0\\), it indicates that the function has a critical point, which could be a local minimum, maximum, or saddle point. 2.5.1.2 Example: For a function \\(f(x, y) = x^2 + y^2\\), the gradient is: \\[ \\nabla f = \\begin{bmatrix} \\frac{\\partial}{\\partial x} (x^2 + y^2) \\\\ \\frac{\\partial}{\\partial y} (x^2 + y^2) \\end{bmatrix} = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix} \\] This shows that the gradient points outward from the origin, and its magnitude increases as \\(x\\) and \\(y\\) increase. 2.5.1.3 Applications: In optimization, the gradient is used to find the minimum or maximum of a function (e.g., in gradient descent, a common optimization algorithm). In vector calculus, the gradient is used to describe the slope or rate of change of scalar fields (such as temperature, pressure, or altitude in physical applications). 2.5.2 Hessian Matrix The Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function. It describes the local curvature of a multivariable function and is used to assess the nature of critical points (i.e., whether they are minima, maxima, or saddle points). For a scalar function \\(f(x_1, x_2, \\ldots, x_n)\\), the Hessian matrix \\(\\mathbf{H}\\) is defined as: \\[ \\mathbf{H}(f) = \\frac{d}{d \\mathbf{x} d\\mathbf{x}&#39;} f(\\mathbf{x}) =\\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} \\] 2.5.2.1 Key Properties: The Hessian is symmetric if the second-order partial derivatives are continuous (by Clairaut’s theorem, also called Schwarz’s theorem). It provides important information about the local behavior of the function, particularly around critical points where the gradient is zero. Eigenvalues of the Hessian matrix determine the type of critical points: If all eigenvalues are positive, the function has a local minimum. If all eigenvalues are negative, the function has a local maximum. If some eigenvalues are positive and others are negative, the function has a saddle point. 2.5.2.2 Example: For a function \\(f(x, y) = x^2 + xy + y^2\\), the Hessian matrix is: \\[ \\mathbf{H}(f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} &amp; \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} &amp; \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\] 2.5.3 Applications: In optimization, the Hessian is used to assess the convexity or concavity of a function, which helps in identifying the nature of critical points. In machine learning, it is used to optimize loss functions and can be part of second-order optimization methods like Newton’s method. In economics and engineering, the Hessian helps in analyzing systems involving multiple variables and understanding how they interact with each other. 2.5.4 Matrix Calculus You need to know the following matrix calculus operations: \\[ \\frac{d}{d \\mathbf{x}} \\left(\\mathbf{c}&#39;\\mathbf{x}\\right) \\] \\[ \\frac{d}{d \\mathbf{x}} \\left(\\mathbf{x}&#39;\\mathbf{A}\\mathbf{x}\\right) \\] \\[ \\frac{d}{d \\mathbf{x} d\\mathbf{x}&#39;} \\left(\\mathbf{x}&#39;\\mathbf{A}\\mathbf{x}\\right) \\] Let \\(\\mathbf{c}\\) be a constant vector and \\(\\mathbf{x}\\) be a variable vector, both of size \\(n \\times 1\\). We want to compute the derivative of the product: \\[ f(\\mathbf{x}) = \\mathbf{c}&#39; \\mathbf{x} \\] Where: \\[ \\mathbf{c}&#39; \\mathbf{x} = \\sum_{i=1}^{n} c_i x_i \\] To differentiate \\(f(\\mathbf{x}) = \\mathbf{c}&#39; \\mathbf{x}\\) with respect to the variable vector \\(\\mathbf{x}\\), we take the derivative of each component separately: \\[ \\nabla f = \\frac{d}{d \\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} (\\mathbf{c}&#39; \\mathbf{x}) \\\\ \\frac{\\partial }{\\partial x_2} (\\mathbf{c}&#39; \\mathbf{x}) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} (\\mathbf{c}&#39; \\mathbf{x}) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} \\left(\\sum_{i=1}^{n} c_i x_i\\right) \\\\ \\frac{\\partial }{\\partial x_2} \\left(\\sum_{i=1}^{n} c_i x_i\\right) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} \\left(\\sum_{i=1}^{n} c_i x_i\\right) \\end{bmatrix} \\] Since \\(\\mathbf{c}\\) is a constant vector, the derivative of each term \\(c_i x_i\\) is simply \\(c_i\\), that is: \\[ \\frac{d}{d x_j} \\left(\\sum_{i=1}^{n} c_i x_i\\right) = c_j \\] Thus, the derivative of the entire sum is the vector: \\[ \\frac{d}{d \\mathbf{x}} \\left( \\mathbf{c}&#39; \\mathbf{x} \\right) = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix} = \\mathbf{c} \\] Now, let’s go through the derivative of the quadratic form \\(f(\\mathbf{x}) = \\mathbf{x}&#39; \\mathbf{A}\\mathbf{x}\\), where: \\(\\mathbf{x}\\) is a variable vector of size \\(n \\times 1\\), \\(\\mathbf{A}\\) is a constant, symmetric matrix of size \\(n \\times n\\). \\[ f(\\mathbf{x}) = \\mathbf{x}&#39; \\mathbf{A}\\mathbf{x} \\] First, expand the quadratic form: \\[ f(\\mathbf{x}) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j \\] Then \\[ \\nabla f = \\frac{d}{d \\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} (\\mathbf{x}&#39; \\mathbf{A}\\mathbf{x}) \\\\ \\frac{\\partial }{\\partial x_2} (\\mathbf{x}&#39; \\mathbf{A}\\mathbf{x}) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} (\\mathbf{x}&#39; \\mathbf{A}\\mathbf{x}) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\frac{\\partial }{\\partial x_2} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\end{bmatrix} \\] For each component \\(x_k\\) in the vector \\(\\mathbf{x}\\), the derivative of \\(f(\\mathbf{x})\\) is: \\[ \\frac{\\partial}{\\partial x_k} f(\\mathbf{x}) = \\frac{\\partial}{\\partial x_k} \\left( \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j \\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\frac{\\partial}{\\partial x_k} x_i a_{ij} x_j \\] Each term \\(x_i a_{ij} x_j\\) has two components that depend on \\(\\mathbf{x}\\): If \\(i = j = k\\), the derivative with respect to \\(x_k\\) is: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = 2 a_{kk} x_k \\] If \\(i \\neq j\\) and \\(i = k\\), the derivative with respect to \\(x_k\\) is: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = a_{kj} x_j \\] - Similarly, if \\(i \\neq j\\) and \\(j = k\\), the derivative with respect to \\(x_k\\) is: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = a_{ik} x_i \\] - Finally, if \\(i \\neq k\\) and \\(j \\neq k\\), then: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = 0 \\] Then \\[ \\frac{\\partial}{\\partial x_k} f(\\mathbf{x}) = 2 a_{kk} x_k + \\sum_{i \\neq k} a_{ik} x_i + \\sum_{j \\neq k} a_{kj} x_j \\] Now since \\(\\mathbf{A}\\) is symmetric (\\(a_{ij} = a_{ji}\\)), then: \\[\\begin{align*} \\frac{\\partial}{\\partial x_k} f(\\mathbf{x}) &amp;= 2 a_{kk} x_k + \\sum_{i \\neq k} a_{ik} x_i + \\sum_{i \\neq k} a_{ik} x_i \\\\ &amp;= 2 a_{kk} x_k + 2\\sum_{i \\neq k} a_{ik} x_i \\\\ &amp;= 2 \\left(\\sum_{i \\neq k} a_{ik} x_i + a_{kk}x_k \\right) \\\\ &amp;= 2 \\left(\\sum_{i = 1}^n a_{ki} x_i\\right) \\end{align*}\\] Then: \\[ \\nabla f = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\frac{\\partial }{\\partial x_2} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\end{bmatrix} = \\begin{bmatrix} 2 \\sum_{i = 1}^n a_{1i} x_i \\\\ 2 \\sum_{i = 1}^n a_{2i} x_i \\\\ \\vdots \\\\ 2 \\sum_{i = 1}^n a_{ni} x_i \\end{bmatrix} = 2 \\mathbf{A}\\mathbf{x} \\] Finally for the second derivative we have that: In general, the Hessian matrix of a scalar function \\(f(\\mathbf{x})\\), where \\(\\mathbf{x} \\in \\mathbb{R}^n\\) is a vector of variables, is a matrix that contains all the second-order partial derivatives of the function. It is defined as: \\[ \\mathbf{H}(f) = \\frac{d^2 f}{d\\mathbf{x}d\\mathbf{x}&#39;} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1}\\left(\\frac{d f}{d\\mathbf{x}}\\right)&#39; \\\\ \\frac{\\partial }{\\partial x_2}\\left(\\frac{d f}{d\\mathbf{x}}\\right)&#39; \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n}\\left(\\frac{d f}{d\\mathbf{x}}\\right)&#39; \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1}\\left(2\\mathbf{A}\\mathbf{x}\\right)&#39; \\\\ \\frac{\\partial }{\\partial x_2}\\left(2\\mathbf{A}\\mathbf{x}\\right)&#39; \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n}\\left(2\\mathbf{A}\\mathbf{x}\\right)&#39; \\end{bmatrix} \\] Now \\[ \\frac{\\partial }{\\partial x_k}\\left(2\\mathbf{A}\\mathbf{x}\\right) = 2\\frac{\\partial }{\\partial x_k}\\begin{bmatrix} \\sum_{i = 1}^n a_{1i} x_i \\\\ \\sum_{i = 1}^n a_{2i} x_i \\\\ \\vdots \\\\ \\sum_{i = 1}^n a_{ni} x_i \\end{bmatrix} = 2 \\begin{bmatrix} \\frac{\\partial }{\\partial x_k} \\left(\\sum_{i = 1}^n a_{1i} x_i \\right)\\\\ \\frac{\\partial }{\\partial x_k} \\left(\\sum_{i = 1}^n a_{2i} x_i \\right)\\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_k} \\left(\\sum_{i = 1}^n a_{ni} x_i \\right) \\end{bmatrix} = 2 \\begin{bmatrix} a_{k1} \\\\ a_{k2} \\\\ \\vdots \\\\ a_{kn} \\end{bmatrix} \\] Then \\[ \\mathbf{H}(f) = \\frac{d^2 f}{d\\mathbf{x}d\\mathbf{x}&#39;} = \\begin{bmatrix} 2\\begin{bmatrix} a_{11} \\\\ a_{12} \\\\ \\vdots \\\\ a_{1n} \\end{bmatrix}&#39; \\\\ 2\\begin{bmatrix} a_{21} \\\\ a_{22} \\\\ \\vdots \\\\ a_{2n} \\end{bmatrix}&#39; \\\\ \\vdots \\\\ 2\\begin{bmatrix} a_{n1} \\\\ a_{n2} \\\\ \\vdots \\\\ a_{nn} \\end{bmatrix}&#39; \\end{bmatrix} = 2\\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} \\end{bmatrix} = 2 \\mathbf{A} \\] "],["introduction.html", "3 Introduction 3.1 Examples", " 3 Introduction Check chapters 2.1-2.2 of the textbook. Linear regression is a statistical method used to analyze the relationship between at least two variables: one dependent variable and at least one independent variable. This technique is closely tied to the correlation coefficient, which measures the strength and direction of a linear relationship between variables. In this document, we will explore how these concepts are connected. Linear regression is typically applied for three main purposes: Description: To describe the relationship between the variables under analysis. Control: To predict how changes in the independent variables will affect the dependent variable. Prediction: To forecast the value of the dependent variable based on new observations of the independent variables. 3.1 Examples 3.1.1 Ad Spending Imagine you are a newly hired data scientist at a mattress company. Your manager asks you to analyze the relationship between Google ad spending and mattress sales revenue. To illustrate this scenario, I have simulated a dataset: # Ad Spending Example # Set Seed set.seed(8272024) # Data Simulation x &lt;- rnorm(n = 100, mean = 70, sd = 30) y &lt;- 1000 + 5 * x + rnorm(n = 100, mean = 0, sd = 100) # Creates the Data Frame datAd &lt;- data.frame(cbind(y,x)) # Names the Variables colnames(datAd) &lt;- c(&quot;Revenue&quot;, &quot;Ad Spending&quot;) # Saves to csv write.csv(x = datAd[, c(1, 2)], file = &quot;Ad spending Data.csv&quot;, row.names = FALSE) You can load the dataset as follows: dat &lt;- read.csv(file = &quot;Ad spending Data.csv&quot;) To visualize the data: dat &lt;- read.csv(file = &quot;Ad spending Data.csv&quot;) plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) Next, you can perform a linear regression analysis: outReg &lt;- lm(Revenue ~ Ad.Spending, data = dat) The most important results from the regression analysis can be summarized as follows: summary(outReg) ## ## Call: ## lm(formula = Revenue ~ Ad.Spending, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -248.394 -58.805 3.782 63.577 196.745 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 997.3894 28.8185 34.61 &lt;2e-16 *** ## Ad.Spending 5.0247 0.3818 13.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 89.01 on 98 degrees of freedom ## Multiple R-squared: 0.6386, Adjusted R-squared: 0.6349 ## F-statistic: 173.2 on 1 and 98 DF, p-value: &lt; 2.2e-16 For now, let’s focus on the estimate for the intercept and the Ad.Spending coefficient. The intercept indicates the expected revenue when ad spending is zero. Based on your analysis, you observe that even without an ad campaign, mattress sales generate \\(r outReg\\)coefficients[1]. The coefficient for Ad.Spending shows that each dollar spent on ads increases revenue by \\(r outReg\\)coefficients[2]. To visualize the regression line: plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) The red regression line represents the “best fit” for these variables. In this context, “best fit” means the line that minimizes the sum of the squared distances from each point to the line. For comparison, here are examples of other lines that do not fit as well: Different slope (coefficient for the Ad.Spending variable): plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(a = outReg$coefficients[1], b = 6, col = &#39;blue&#39;, lwd = 2) Different intercept: plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(a = 1100, b = outReg$coefficients[2], col = &#39;blue&#39;, lwd = 2) We can expand the range for ad spending and revenue on the plot, as follows: plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;, ylim = c(800, 2500), xlim = c(0, 200)) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(h = 0, v = 0) Within the range of observed ad spending values (r round(min(dat\\(Ad.Spending)) to r round(max(dat\\)Ad.Spending))), we can be reasonably confident in the relationship between ad spending and revenue. However, what happens when we consider values outside this range? While we can predict and control revenue to some extent by adjusting ad spending within the observed range, this confidence may not extend to values beyond it. For instance, it’s plausible that after reaching a certain level of ad spending, the market could become saturated, resulting in diminishing or even no additional revenue despite increased ad spending. Therefore, we should be cautious when extrapolating beyond the observed data, as the relationship may not hold under different conditions. 3.1.2 Wine and Life Expectancy In the previous example, we were able to manipulate the independent variable to influence the outcome. However, there are situations where our primary goal is simply to describe the relationship between two variables, without aiming to control them. In the following example, I generate a dataset that illustrates the relationship between wine consumption and life expectancy in years for an imaginary country. Here’s how the data is simulated: # Wine and Life Expectancy # Set Seed set.seed(8272024) # Data Simulation x &lt;- rbinom(n = 20, size = 20, prob = 0.1) y &lt;- 75 + 1.5 * x + rnorm(n = 20, mean = 0, sd = 3) # Creates the Data Frame datWin &lt;- data.frame(cbind(y,x)) # Names the Variables colnames(datWin) &lt;- c(&quot;Years&quot;, &quot;Glasses&quot;) # Saves to csv write.csv(datWin[, c(1, 2)], file = &quot;Wine Data.csv&quot;, row.names = FALSE) Here’s an improved version of your text: You should be able to generate the exact same data if you copy and paste the code and run it on your computer. Although the data is randomly simulated, I’ve set a seed to ensure that the simulation can be replicated consistently. We can read and plot the data in the as follows: # Reads the Data dat &lt;- read.csv(file = &quot;Wine Data.csv&quot;) # Plots the Data plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) Once again, we can perform a linear regression to examine the relationship between the variables and visualize it with a regression line: # Performs Linear Regression outReg &lt;- lm(Years ~ Glasses, data = dat) # Plots plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) Given the regression line, can we confidently conclude that increasing wine consumption leads to a longer life expectancy? 3.1.3 Burger Demand Imagine you work at a burger franchise where prices change daily. As the franchise manager, you want to predict the demand for burgers at a given price. Over the past 100 days, you’ve collected data on the number of burgers sold at each price set by the franchise’s main office. Here’s what your data looks like: # Reads the Data dat &lt;- read.csv(file = &quot;Burger Data.csv&quot;) # Plots the Data plot(x = dat$Price, y = dat$Burgers, xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers Sold&quot;) Is this dataset a good candidate for linear regression? While it’s true that we can always fit a line to any dataset, the real question is whether that line meaningfully represents the relationship between the variables. Here’s what happens when we apply linear regression to this data: # Performs Linear Regression outReg &lt;- lm(Burgers ~ Price, data = dat) # Plots plot(x = dat$Price, y = dat$Burgers, xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers Sold&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) Do you think this is a good fit for the data? The regression line appears to be inadequate for both low and high price points. Additionally, since the number of burgers sold must be positive, the regression line should not intersect the x-axis. "],["simple-linear-regression.html", "4 Simple Linear Regression 4.1 Model 4.2 Least Squares Estimation 4.3 Properties of the Estimates 4.4 Centering and Standarizing the Data 4.5 Coefficient of Determination 4.6 Residual Analysis 4.7 Cross-Validation 4.8 Weighted Least Squares 4.9 Model in Matrix Form", " 4 Simple Linear Regression Simple linear regression (SLR) is a linear regression model with a single explanatory variable. It focuses on the linear relationship between one independent variable and one dependent variable, making it the most basic form of linear regression analysis. 4.1 Model The model for simple linear regression is as follows: \\[y_i = \\beta_0 + \\beta_1 x_i + e_i, \\quad i\\in\\{1,\\ldots,n\\}\\] where: \\(y_i\\) represents the \\(i\\)-th observation of the dependent variable. \\(x_i\\) represents the \\(i\\)-th observation of the independent variable. \\(e_i\\) represents the \\(i\\)-th observation of the error term. \\(\\beta_0\\) is the intercept of the linear model, or regression line. \\(\\beta_1\\) is the slope of the linear model, or regression line. \\(n\\) is the number of observations for both variables. Note that we are not making any assumptions about the error terms. In the case of the wine example, we generated the data based on the following linear model: \\[y_i = 75 + 1.5 x_i + e_i \\] dat &lt;- read.csv(file = &quot;Wine Data.csv&quot;) plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) abline(a = 75, b = 1.5, col = &#39;red&#39;, lwd = 2) abline(v = 0, lwd = 2) text(x = 0.25, y = 76, expression(beta[0] ~ &quot;=75&quot;)) text(x = 3.25, y = 79, expression(beta[1] ~ &quot;=1.5&quot;)) segments(x0 = c(2, 3), x1 = c(3, 3), y0 = c(78, 78), y1 = c(78, 79.5), lwd = 2, col = &#39;blue&#39;) In this case, the intercept \\(\\beta_0\\) is meaningful, as it represents the expected number of years a person would live if they didn’t drink wine at all. However, depending on the data, the intercept may or may not have a meaningful interpretation. The slope \\(\\beta_1\\) indicates that for each additional glass of wine consumed per week, our model predicts an increase of 1.5 years in life expectancy. In practice, we rarely know the true regression line. Instead, it must be estimated from the data. The goal is to find the “best” line that fits the data, where “best” means the line that minimizes the sum of squared errors (SSE) between the observed values and the values predicted by the model. 4.2 Least Squares Estimation As explained before, we want to minimize the SSE, we can create a function of \\(\\beta_0\\) and \\(\\beta_1\\) with this sum as follows: \\[Q(\\beta_0, \\beta_1) = \\sum_{i=1}^n (e_i(\\beta_0, \\beta_1))^2 = \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\] and we can find the minimum of this function easily since it is a differentiable function. We can find the both components of the gradient and equal them to zero to find the critical points. We start with \\(\\beta_0\\): \\[\\begin{align*} \\frac{\\partial Q}{\\partial \\beta_0} &amp;= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\ &amp;= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i^2 + \\beta_0^2 + \\beta_1^2 x_i^2 - 2 \\beta_0 y_i - 2 \\beta_1 x_i y_i + 2 \\beta_0 \\beta_1 x_i) \\\\ &amp;= \\sum_{i = 1}^n (2 \\beta_0 - 2 y_i + 2 \\beta_1 x_i) \\\\ &amp;= -2 \\left( n \\beta_0 - n \\bar{y} + n\\beta_1 \\bar{x} \\right) \\end{align*}\\] where we have adopted the notation: \\(\\bar{x} = \\frac{1}{n}\\sum_{i}^n x_i\\) and \\(\\bar{y} = \\frac{1}{n}\\sum_{i}^n y_i\\). \\[\\begin{align} \\frac{\\partial Q}{\\partial \\beta_0} = 0 &amp;\\iff -2 \\left( n \\beta_0 - n \\bar{y} + n\\beta_1 \\bar{x} \\right) = 0 \\notag \\\\ &amp;\\iff \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\tag{1} \\end{align}\\] And we can do a similar thing for \\(\\beta_1\\): \\[\\begin{align*} \\frac{\\partial Q}{\\partial \\beta_1} &amp;= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\ &amp;= \\sum_{i = 1}^n 2(y_i -\\beta_0 - \\beta_1 x_i)(-x_i) \\\\ &amp;= -2\\sum_{i = 1}^n y_i x_i + 2 \\beta_0 \\sum_{i = 1}^n x_i + 2 \\beta_1 \\sum_{i = 1}^n x_i^2 \\\\ &amp;= -2\\sum_{i = 1}^n y_i x_i + 2 n \\beta_0 \\bar{x} + 2 \\beta_1 \\sum_{i = 1}^n x_i^2 \\end{align*}\\] then: \\[\\begin{align} \\frac{\\partial Q}{\\partial \\beta_1} = 0 &amp;\\iff -2\\sum_{i = 1}^n y_i x_i + 2 n \\beta_0 \\bar{x} + 2 \\beta_1 \\sum_{i = 1}^n x_i^2 = 0 \\notag \\\\ &amp;\\iff \\sum_{i = 1}^n y_i x_i = n \\beta_0 \\bar{x} + \\beta_1 \\sum_{i = 1}^n x_i^2 \\tag{2} \\end{align}\\] Now, substituting (1) into (2) we have that \\[\\begin{align*} \\sum_{i = 1}^n y_i x_i &amp;= n (\\bar{y} - \\beta_1 \\bar{x}) \\bar{x} + \\beta_1 \\sum_{i = 1}^n x_i^2 \\\\ &amp;= n \\bar{y} \\bar{x} - n \\beta_1 \\bar{x}^2 + \\beta_1 \\sum_{i = 1}^n x_i^2 \\\\ &amp;= n \\bar{y} \\bar{x} + \\beta_1 \\left( \\sum_{i = 1}^n x_i^2 - n \\bar{x}^2 \\right) \\end{align*}\\] Then, \\[ \\beta_1 = \\frac{\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}}{\\sum_{i = 1}^n x_i^2 - n \\bar{x}^2} \\] so, the only critical point for \\(Q(\\beta_0,\\beta_1)\\) is when: \\[ \\hat{\\beta}_1 = \\frac{\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}}{\\sum_{i = 1}^n x_i^2 - n \\bar{x}^2} \\] \\[ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] where we use \\(\\hat{}\\), to denote the specific critical point. It remains to see if this is indeed a minimum. One can check the second order conditions. Now, if we introduce the notation for sample variance and covariance: \\[ S^2_{xx} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 \\] \\[ S_{xy} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\] and note the following: \\[\\begin{align*} \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i^2 - 2\\bar{x}x_i + \\bar{x}^2) \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_i^2 - 2\\bar{x}\\sum_{i=1}^n x_i + \\sum_{i=1}^n \\bar{x}^2 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_i^2 - 2\\bar{x}(n\\bar{x}) + n \\bar{x}^2 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_i^2 - 2n\\bar{x}^2 + n \\bar{x}^2 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_i^2 - n\\bar{x}^2 \\end{align*}\\] and \\[\\begin{align*} \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_iy_i - \\bar{x}y_i - \\bar{y}x_i + \\bar{x}\\bar{y}) \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_iy_i - \\bar{x} \\sum_{i=1}^ny_i - \\bar{y} \\sum_{i=1}^n x_i + \\sum_{i=1}^n \\bar{x}\\bar{y} \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_iy_i - n\\bar{x} \\bar{y} - n\\bar{y} \\bar{x} + n \\bar{x}\\bar{y} \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_iy_i - n\\bar{x} \\bar{y} \\\\ \\end{align*}\\] then we can express \\(\\hat{\\beta}_1\\) as: \\[\\hat{\\beta}_1 = \\frac{(n-1)S_{xy}}{(n-1)S_{xx}^2}=\\frac{S_{xy}}{S_{xx}^2} \\] Now notice that in order to find the Least Squares estimates you don’t require the complete data set, but only require the following quantities: \\(\\bar{y}\\). \\(\\bar{x}\\). \\(S_{xx}^2\\). \\(S_{xy}\\). 4.2.1 Other estimated quantites If we use the Least squares estimates in the regression equation, we can derive other estimated quantities: The estimated value for observation \\(i\\): \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] and the estimated error: \\[ \\hat{e}_i = y_i - \\hat{y}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\] And we can also compare our estimated regression line (blue) with the real regression line (red) in the following as follows: outReg &lt;- lm(Years ~ Glasses, data = dat) plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) abline(a = 75, b = 1.5, col = &#39;red&#39;, lwd = 2) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;blue&#39;, lwd = 2) 4.3 Properties of the Estimates The estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are linear combinations of \\(\\mathbf{y} = (y_1,\\ldots,y_n)&#39;\\). To see this, notice the following: \\[\\begin{align*} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\\\ &amp;= \\sum_{i=1}^n x_i y_i - n \\bar{x} \\bar{y} \\\\ &amp;= \\sum_{i=1}^n x_i y_i - \\bar{x} \\sum_{i=1}^n y_i \\\\ &amp;= \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n \\bar{x} y_i \\\\ &amp;= \\sum_{i=1}^n (x_i y_i - \\bar{x} y_i) \\\\ &amp;= \\sum_{i=1}^n (x_i - \\bar{x}) y_i \\\\ \\end{align*}\\] Then \\[ \\hat{\\beta}_1 = \\frac{\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) y_i}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\sum_{i=1}^n\\frac{(x_i - \\bar{x}) }{\\sum_{i=1}^n (x_i - \\bar{x})^2}y_i \\] and similarly: \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} = \\sum_{i=1}^n \\frac{y_i}{n} - \\sum_{i=1}^n\\frac{(x_i - \\bar{x}) }{\\sum_{j = 1}^n x_j^2 - n \\bar{x}^2}y_i \\bar{x} = \\sum_{i=1}^n \\left( \\frac{1}{n} - \\frac{(x_i - \\bar{x}) }{\\sum_{j = 1}^n x_j^2 - n \\bar{x}^2} \\bar{x} \\right)y_i \\] Also, notice that the sum of the errors is \\(0\\). \\[\\begin{align*} \\sum_{i=1}^n \\hat{e}_i &amp;= \\sum_{i=1}^n(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) \\\\ &amp;= \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\hat{\\beta}_0 - \\hat{\\beta}_1 \\sum_{i=1}^n x_i \\\\ &amp;= n\\bar{y} - n \\hat{\\beta}_0 - n \\hat{\\beta}_1 \\bar{x} \\\\ &amp;= n\\bar{y} - n (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) - n \\hat{\\beta}_1 \\bar{x} \\\\ &amp;= n\\bar{y} - n \\bar{y} + n \\hat{\\beta}_1 \\bar{x} - n \\hat{\\beta}_1 \\bar{x} \\\\ &amp;= 0 \\end{align*}\\] If we let \\(\\hat{\\mathbf{e}} = (\\hat{e}_i,\\ldots,\\hat{e}_n)&#39;\\) and \\(\\mathbf{x}=(x_1,\\ldots,x_n)&#39;\\), two vectors of size \\(n\\), then we have that \\(\\hat{\\mathbf{e}}\\) and \\(\\mathbf{x}\\) are orthogonal. That is: \\[\\begin{align*} \\langle \\hat{\\mathbf{e}}, \\mathbf{x}\\rangle &amp;= \\sum_{i=1}^{n} \\hat{e}_i x_i \\\\ &amp;= \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)x_i \\\\ &amp;= \\sum_{i=1}^{n} (y_i x_i - \\hat{\\beta}_0x_i - \\hat{\\beta}_1 x_i x_i) \\\\ &amp;= \\sum_{i=1}^{n} y_i x_i - \\sum_{i=1}^{n} \\hat{\\beta}_0x_i - \\sum_{i=1}^{n} \\hat{\\beta}_1 x_i x_i \\\\ &amp;= \\sum_{i=1}^{n} y_i - n \\hat{\\beta}_0 \\bar{x} - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2 \\\\ &amp;= \\sum_{i=1}^{n} y_i - n (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\bar{x} - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2 \\\\ &amp;= \\sum_{i=1}^{n} y_i - n \\bar{y} \\bar{x} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2 \\\\ &amp;= \\sum_{i=1}^{n} y_i - n \\bar{y} \\bar{x} - \\hat{\\beta}_1 (\\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2) \\\\ &amp;= \\sum_{i=1}^{n} y_i - n \\bar{y} \\bar{x} - \\frac{\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}}{\\sum_{i = 1}^n x_i^2 - n \\bar{x}^2}(\\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2) \\\\ &amp;= \\sum_{i=1}^{n} y_i - n \\bar{y} \\bar{x} - (\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}) \\\\ &amp;=0 \\end{align*}\\] The same applies to \\(\\hat{\\mathbf{y}}= (\\hat{y}_1,\\ldots,\\hat{\\mathbf{y}}_n)&#39;\\) and \\(\\hat{\\mathbf{e}}\\), as we can see: \\[\\begin{align*} \\langle \\hat{\\mathbf{e}}, \\hat{\\mathbf{y}}\\rangle &amp;= \\sum_{i=1}^{n} \\hat{e}_i \\hat{y}_i \\\\ &amp;= \\sum_{i=1}^{n} \\hat{e}_i(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\\\ &amp;= \\sum_{i=1}^{n} (\\hat{e}_i \\hat{\\beta}_0 + he_i \\hat{\\beta}_1 x_i) \\\\ &amp;= \\hat{\\beta}_0 \\sum_{i=1}^{n} \\hat{e}_i + \\hat{\\beta}_1 \\sum_{i=1}^{n} he_i x_i \\\\ &amp;= \\hat{\\beta}_1 \\langle \\hat{\\mathbf{e}}, \\mathbf{x}\\rangle \\\\ &amp;= 0 \\end{align*}\\] Finally, the average of \\(\\hat{\\mathbf{y}}\\) and \\(\\mathbf{y}\\) are the same, to see this notice: \\[\\begin{align*} \\frac{1}{n} \\sum_{i=1}^n \\hat{y}_i &amp;= \\frac{1}{n} \\sum_{i=1}^n (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\\\ &amp;= \\frac{1}{n} (n \\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum_{i=1}^n x_i) \\\\ &amp;= \\frac{1}{n} (n \\hat{\\beta}_0 + n \\hat{\\beta}_1 \\mathbf{x}) \\\\ &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\mathbf{x}\\\\ &amp;= \\bar{y} - \\hat{\\beta}_1 \\mathbf{x}+ \\hat{\\beta}_1 \\mathbf{x}\\\\ &amp;= \\bar{y} \\\\ \\end{align*}\\] 4.4 Centering and Standarizing the Data Some transformations of the data can help the regression analysis or make it more intuitive. There are 2 main transformations of the data: centering and standardization. Consider observations \\(x_1,\\ldots,x_n\\), then the centered version of observation \\(i\\) is given by: \\[x_i&#39; = x_i - \\bar{x}\\] The new observations \\(x_1&#39;,\\ldots,x_n&#39;\\) are centered and their mean is \\(0\\). \\[\\bar{x}&#39; = \\frac{1}{n} \\sum_{i=1}^n x_i&#39; = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x}) = \\frac{1}{n} \\left(\\sum_{i=1}^n x_i - \\sum_{i=1}^n \\bar{x} \\right) = \\frac{1}{n} \\left(n\\bar{x} - n \\bar{x} \\right) = 0\\] Also, let us see that the variance of the standardized variables is the same as the variance of the original observations. \\[ S_{xx}&#39; = \\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39; - \\bar{x}&#39;) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x}) = S_{xx} \\] So the variance of the observations is not affected by the centering. If we center another set of observations \\(y_1,\\ldots,y_n\\), and compute the covariance, we have that it also doesn’t change. \\[S_{xy}&#39; = \\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39; - \\bar{x})(y_i&#39; - \\bar{x}) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;)(y_i&#39;) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{x}) = S_{xy}\\] The standardized version of observation \\(i\\) is given by: \\[x_i&#39;&#39; = \\frac{x_i - \\bar{x}}{\\sqrt{S_{xx}}} = \\frac{x_i&#39;}{\\sqrt{S_{xx}}}\\] The standardized observations have mean of 0 and variance 1. Let’s see first that the sample mean is \\[ \\bar{x}_i&#39;&#39; = \\frac{1}{n} \\sum_{i=1}^n x_i&#39;&#39; = \\frac{1}{n} \\sum_{i=1}^n \\frac{x_i&#39;}{\\sqrt{S_{xx}}} = \\frac{1}{\\sqrt{S_{xx}}}\\frac{1}{n} \\sum_{i=1}^n x_i&#39; = \\frac{1}{\\sqrt{S_{xx}}} \\bar{x}&#39; = 0\\] Now let us see that the variance of the standardized observations is 1. \\[\\begin{align*} S_{xx}&#39;&#39; &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;&#39; - \\bar{x}&#39;&#39;)^2 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;&#39;)^2 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n \\left(\\frac{x_i - \\bar{x}}{\\sqrt{S_{xx}}}\\right)^2 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n \\frac{(x_i - \\bar{x})^2}{S_{xx}} \\\\ &amp;= \\frac{1}{S_{xx}} \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 \\\\ &amp;= \\frac{1}{S_{xx}}S_{xx} \\\\ &amp;= 1 \\end{align*}\\] Now, let us introduce the sample correlation as: \\[ r_{xy} = \\frac{S_{xy}}{\\sqrt{S_{xx}{S_{yy}}}} \\] If we standardize two sets of observations, then the covaraince of the standardized version is the correlation of the standardized version. Let us see it: \\[\\begin{align*} S_{xy}&#39;&#39; &amp;= \\frac{1}{n-1} \\sum_{i=1} (x_i&#39;&#39; - \\bar{x}&#39;&#39;)(y_i&#39;&#39; - \\bar{x}&#39;&#39;) \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1} (x_i&#39;&#39;)(y_i&#39;&#39;) \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1} \\left(\\frac{x_i - \\bar{x}}{\\sqrt{S_{xx}}}\\right) \\left(\\frac{y_i - \\bar{y}}{\\sqrt{S_{yy}}}\\right) \\\\ &amp;= \\frac{1}{\\sqrt{S_{yy}}\\sqrt{S_{xx}}}\\frac{1}{n-1} \\sum_{i=1} (x_i - \\bar{x}) (y_i - \\bar{y}) \\\\ &amp;= \\frac{1}{\\sqrt{S_{yy}}\\sqrt{S_{xx}}} S_{xy} \\\\ &amp;= r_{xy} \\end{align*}\\] With this results, we can analyze the effects of following 3 scenarios on the estimated coefficients: Independent variable centered. Both, Independent and dependent variable centered. Both, Independent and dependent variable standardized. 4.4.1 Independent variable centered Lets compute the value for \\(\\beta_1\\) when the data is centered. \\[\\hat{\\beta}_1&#39; = \\frac{S_{xy}&#39;}{S_{xx}&#39;} = \\frac{S_{xy}}{S_{xx}} = \\hat{\\beta}_1 \\] So centering the data doesn’t change the value of the estimated slope. \\[ \\hat{\\beta}_0&#39; = \\bar{y} - \\hat{\\beta}_1&#39; \\bar{x}&#39; = \\bar{y} - \\hat{\\beta}_1&#39; 0 = \\bar{y} \\] So centering the data, makes the estimated intercept to coincide with the mean of the independent variable. We can see this in one of our example data sets, looking at the ad spending data we can perform linear regression on the original data and the centered data: # Read Data dat &lt;- read.csv(&quot;Ad spending Data.csv&quot;) # Assign data x &lt;- dat$Ad.Spending y &lt;- dat$Revenue # Centers x xCen &lt;- x - mean(x) # Linear regression on the original data outRegOri &lt;- lm(y ~ x) # Linear regression on the centered independent variable data outRegCen &lt;- lm(y ~ xCen) # Plots ## Two plots in the same image par(mfrow = c(1, 2)) ## Original data # Plots the points plot(x = x, y = y, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;) # Plots the regression line abline(a = outRegOri$coefficients[1], b = outRegOri$coefficients[2], col = &#39;red&#39;, lwd = 2) ## Independent Variable centered data # Plots the points plot(x = xCen, y = y, xlab = &quot;Ad spending (Centered)&quot;, ylab = &quot;Revenue&quot;) # Plots the regression line abline(a = outRegCen$coefficients[1], b = outRegCen$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(v = 0, lwd = 2) So we can appreciate that centering the independent variable just shifts the data horizontally so the mean will be at zero. 4.4.2 Both Variables centered Now lets see the effects when both variables are centered. Here, we will denote the estimates again with one prime, that is \\(\\hat{\\beta}&#39;\\). Again, the estimate of the slope doesn’t change: \\[\\hat{\\beta}_1&#39; = \\frac{S_{xy}&#39;}{S_{xx}&#39;} = \\frac{S_{xy}}{S_{xx}} = \\hat{\\beta}_1 \\] while the estimate of the intercept becomes zero (the new mean of the centered dependent variable) \\[ \\hat{\\beta}_0&#39; = \\bar{y}&#39; - \\hat{\\beta}_1&#39; \\bar{x}&#39; = \\bar{y}&#39; = 0 \\] The effect of this transformation can be observed, here: # Read Data dat &lt;- read.csv(&quot;Ad spending Data.csv&quot;) # Assign data x &lt;- dat$Ad.Spending y &lt;- dat$Revenue # Centers x and y xCen &lt;- x - mean(x) yCen &lt;- y - mean(y) # Linear regression on the original data outRegOri &lt;- lm(y ~ x) # Linear regression on the centered data outRegCen &lt;- lm(yCen ~ xCen) # Plots ## Two plots in the same image par(mfrow = c(1, 2)) ## Original data # Plots the points plot(x = x, y = y, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;) # Plots the regression line abline(a = outRegOri$coefficients[1], b = outRegOri$coefficients[2], col = &#39;red&#39;, lwd = 2) ## Centered data # Plots the points plot(x = xCen, y = yCen, xlab = &quot;Ad spending (Centered)&quot;, ylab = &quot;Revenue&quot;) # Plots the regression line abline(a = outRegCen$coefficients[1], b = outRegCen$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(v = 0, lwd = 2) abline(h = 0, lwd = 2) 4.4.3 Independent and dependent variable standardized Again we start we the slope estimate: \\[\\hat{\\beta}_1&#39;&#39; = \\frac{S_{xy}&#39;&#39;}{S_{xx}&#39;&#39;} = \\frac{r_{xy}}{1} =r_{xy} \\] so, the estimate of the slope is the sample correlation of the original observations. Again, we can see this graphically: # Read Data dat &lt;- read.csv(&quot;Ad spending Data.csv&quot;) # Assign data x &lt;- dat$Ad.Spending y &lt;- dat$Revenue # Standardizes x and y xSta &lt;- (x - mean(x))/sqrt(var(x)) ySta &lt;- (y - mean(y))/sqrt(var(y)) # Linear regression on the original data outRegOri &lt;- lm(y ~ x) # Linear regression on the standard data outRegSta &lt;- lm(ySta ~ xSta) # Plots ## Two plots in the same image par(mfrow = c(1, 2)) ## Original data # Plots the points plot(x = x, y = y, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;) # Plots the regression line abline(a = outRegOri$coefficients[1], b = outRegOri$coefficients[2], col = &#39;red&#39;, lwd = 2) ## Standard data # Plots the points plot(x = xSta, y = ySta, xlab = &quot;Ad spending (Centered)&quot;, ylab = &quot;Revenue&quot;) # Plots the regression line abline(a = outRegSta$coefficients[1], b = outRegSta$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(v = 0, lwd = 2) abline(h = 0, lwd = 2) Now, let us see that the correlation is always in the interval \\((-1,1)\\). To see this, notice the following: \\[\\begin{align*} \\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;&#39; + y_i&#39;&#39;)^2 &amp;= \\frac{1}{n-1} \\sum_{i=1}^n \\left((x_i&#39;&#39;)^2 + 2x_i&#39;&#39; y_i&#39;&#39; + (y_i&#39;&#39;)^2 \\right) \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i&#39;&#39;)^2}{n-1} + 2\\frac{\\sum_{i=1}^n x_i&#39;&#39;y_i&#39;&#39;}{n-1} + \\frac{\\sum_{i=1}^n (y_i&#39;&#39;)^2}{n-1} \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i&#39;&#39; - \\bar{x}&#39;&#39;)^2}{n-1} + 2\\frac{\\sum_{i=1}^n (x_i&#39;&#39; - \\bar{x}&#39;&#39;)(y_i&#39;&#39; - \\bar{y}&#39;&#39;)}{n-1} + \\frac{\\sum_{i=1}^n (y_i&#39;&#39; - \\bar{y}&#39;&#39;)^2}{n-1} \\\\ &amp;= S_{xx}&#39;&#39; + 2 S_{xy}&#39;&#39; + S_{yy}&#39;&#39; \\\\ &amp;= 1 + 2r_{xy} + 1 \\\\ &amp;= 2(1 + r_{xy}) \\end{align*}\\] In a similar way it can be shown that: \\[\\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;&#39; + y_i&#39;&#39;)^2 = 2(1 - r_{xy})\\] Now since, \\[\\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;&#39; + y_i&#39;&#39;)^2 \\geq 0\\] then we have that \\[\\begin{align*} \\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;&#39; + y_i&#39;&#39;)^2 \\geq 0 &amp;\\implies 2(1 + r_{xy}) \\geq 0 \\\\ &amp;\\implies 1 + r_{xy} \\geq 0 \\\\ &amp;\\implies r_{xy} \\geq -1 \\\\ \\end{align*}\\] Similarly, since \\[\\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;&#39; - y_i&#39;&#39;)^2 \\geq 0\\] implies \\[r_{xy} \\leq 1 \\] then, we have that: \\[ -1 \\leq r_{xy} \\leq 1\\] which implies that the slope of the regression analysis after standarizing both variables is going to be in the interval \\((-1, 1)\\). 4.5 Coefficient of Determination So far, we have been concerned on findin the “best” line, that is estimating the coefficients that minimize the sum of squared errors. We have find some derivated estimated values and some properties of these values. However, we hanven’t analized how good is our estimation. To see how well we are doing we will look at the coeficient of determination. To do so we will first introduce some quantities: Total Sum of Squares \\[SS_{tot} = \\sum_{i=1}^n (y_i - \\bar{y})\\] Is a measure of total variability of the dependent variable. You can think of it in many ways: As a proxy for uncertainty. The bigger the more uncertainty in the dependent varaible. It is proportional to the sample varaince. It is what you get if when you solve the following minimization problem: \\[ \\min_{a} \\sum_{i=1}^n (y_i - \\beta_0)^2 \\] that is, the best you can do to minimize the sum of squares without having access to the independent variables \\(x_1,\\ldots,x_n\\). Residual Sum of Squares \\[ SS_{res} = \\sum_{i=1}^n(\\hat{e}_i)^2 = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\] The residual Sum of Squares is the minimum value of our optimization problem. It is the the amount of variability that no matter what we do we will have remaing even after finding the “best” line. Explained Sum of Squares \\[ SS_{reg} = \\sum_{i=1}^n(\\hat{y}_i - \\bar{y}) = \\sum_{i=1}^n(\\hat{y}_i - \\hat{\\bar{y}}) \\] The variability of the fitted value. This is the variability we can explain with our regression model. These 3 quantities are related by: \\[ SS_{tot} = SS_{reg} + SS_{res} \\] That is the total variability si the sum of the variability that is explained by the regression model and the variability we can’t explain with the regression model. \\[\\begin{align*} SS_{tot} &amp;= \\sum_{i=1}^n(y_i - \\bar{y})^2 = \\sum_{i=1}^n(y_i - \\hat{y}_i + \\hat{y}_i - \\bar{y})^2 \\\\ &amp;= \\sum_{i=1}^n\\left((y_i - \\hat{y}_i)^2 + 2(y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) + (\\hat{y}_i - \\bar{y})^2\\right) \\\\ &amp;= \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 + 2 \\sum_{i=1}^n(y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) + \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2 \\\\ &amp;= SS_{res} + 2 \\sum_{i=1}^n(y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) + SS_{reg} \\\\ &amp;= SS_{res} + 2 \\sum_{i=1}^n\\hat{e}_i(\\hat{y}_i - \\bar{y}) + SS_{reg} \\\\ &amp;= SS_{res} + 2 \\sum_{i=1}^n(\\hat{e}_i\\hat{y}_i - \\hat{e}_i\\bar{y}) + SS_{reg} \\\\ &amp;= SS_{res} + 2 \\sum_{i=1}^n\\hat{e}_i\\hat{y}_i - 2 \\sum_{i=1}^n\\hat{e}_i\\bar{y} + SS_{reg} \\\\ &amp;= SS_{res} + 2(0) - 2 \\bar{y} \\sum_{i=1}^n\\hat{e}_i + SS_{reg} \\\\ &amp;= SS_{res} - 2 \\bar{y} (0) + SS_{reg} \\\\ &amp;= SS_{res} + SS_{reg} \\\\ \\end{align*}\\] Coefficient of Determination \\[ R^2 = \\frac{SS_{reg}}{SS_{tot}} = 1 - \\frac{SS_{res}}{SS_{tot}} \\] The coefficient of determination then can be explained as the percentage of the total varaibility that can be explained with linear regression. As a percentage, it has to be a number between 0 and 1. To see this let us show that: \\[ R^2 = r_{xy}^2 \\] To see this, first let us express \\(SS_{reg}\\) in a more convinient way: \\[\\begin{align*} SS_{reg} &amp;= \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2 \\\\ &amp;= \\sum_{i=1}^n(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i - \\bar{y})^2 \\\\ &amp;= \\sum_{i=1}^n(\\bar{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 x_i - \\bar{y})^2 \\\\ &amp;= \\sum_{i=1}^n(\\hat{\\beta}_1 x_i - \\hat{\\beta}_1 \\bar{x})^2 \\\\ &amp;= \\sum_{i=1}^n\\hat{\\beta}_1^2(x_i - \\bar{x})^2 \\\\ &amp;= \\hat{\\beta}_1^2 \\sum_{i=1}^n(x_i - \\bar{x})^2 \\\\ &amp;= \\hat{\\beta}_1^2 S_{xx} (n-1) \\\\ &amp;= \\left( \\frac{S_{xy}}{S_{xx}} \\right)^2 S_{xx} (n-1) \\\\ &amp;= \\frac{S_{xy}^2}{S_{xx}^2} S_{xx} (n-1) \\\\ &amp;= \\frac{S_{xy}^2}{S_{xx}} (n-1) \\\\ \\end{align*}\\] Then, we can see that: \\[\\begin{align*} R^2 &amp;= \\frac{SS_{reg}}{SS_{tot}} \\\\ &amp;= \\frac{\\frac{S_{xy}^2}{S_{xx}} (n-1)}{\\sum_{i=1}^n(y_i - \\bar{y})^2} \\\\ &amp;= \\frac{\\frac{S_{xy}^2}{S_{xx}} (n-1)}{S_{yy}(n-1)} \\\\ &amp;= \\frac{S_{xy}^2}{S_{xx}S_{yy}} \\\\ &amp;= \\left( \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}} \\right)^2 \\\\ &amp;= r_{xy}^2 \\end{align*}\\] The bigger the \\(R^2\\), the better is the fit of our linear model. The \\(R^2\\) can be low for 2 reasons: The first one is if our data is not linear, then a linear model will explain little about the relationship (some times a linear model can be a good approximation of a non-linear model). The second reason, is when the data is noisy. This can reduce the \\(R^2\\) even when we know the relationship between the variables is linear. As an example of noisy data, recall the Ad spending data. I actually generated the data under a linear model, so the relationship between the variables is linear. You can verify this be looking at the code where the data is generated in the introduction. Next I show the effects of adding additional noise to the data, at 3 levels: Level1: Small Noise. Level2: Medium Noise. Level3: High noise. # Read Data dat &lt;- read.csv(&quot;Ad spending Data.csv&quot;) # Assign data x &lt;- dat$Ad.Spending y &lt;- dat$Revenue # Adds Noise yNoiLe1 &lt;- y + rnorm(n = 100, sd = 50) yNoiLe2 &lt;- y + rnorm(n = 100, sd = 200) yNoiLe3 &lt;- y + rnorm(n = 100, sd = 500) # Auxiliary Variables ymax &lt;- max(y, yNoiLe1, yNoiLe2, yNoiLe3) ymin &lt;- min(y, yNoiLe1, yNoiLe2, yNoiLe3) xmax = max(x) xmin = min(x) # Performs Linear Regression outRegOri &lt;- lm(y ~ x) outRegNoiLe1 &lt;- lm(yNoiLe1 ~ x) outRegNoiLe2 &lt;- lm(yNoiLe2 ~ x) outRegNoiLe3 &lt;- lm(yNoiLe3 ~ x) # Plots par(mfrow = c(2, 2)) plot(x = x, y = y, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;, main = &quot;Original Data&quot;, ylim = c(ymin, ymax), xlim = c(xmin, xmax)) # Plots the regression line abline(a = outRegOri$coefficients[1], b = outRegOri$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = x, y = yNoiLe1, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;, main = &quot;Small Noise Added&quot;, ylim = c(ymin, ymax), xlim = c(xmin, xmax)) # Plots the regression line abline(a = outRegNoiLe1$coefficients[1], b = outRegNoiLe1$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = x, y = yNoiLe2, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;, main = &quot;Medium Noise Added&quot;, ylim = c(ymin, ymax), xlim = c(xmin, xmax)) # Plots the regression line abline(a = outRegNoiLe2$coefficients[1], b = outRegNoiLe2$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = x, y = yNoiLe3, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;, main = &quot;High Noise Added&quot;, ylim = c(ymin, ymax), xlim = c(xmin, xmax)) # Plots the regression line abline(a = outRegNoiLe3$coefficients[1], b = outRegNoiLe3$coefficients[2], col = &#39;red&#39;, lwd = 2) print(&quot;Original Data LM summary&quot;) ## [1] &quot;Original Data LM summary&quot; summary(outRegOri) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -248.394 -58.805 3.782 63.577 196.745 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 997.3894 28.8185 34.61 &lt;2e-16 *** ## x 5.0247 0.3818 13.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 89.01 on 98 degrees of freedom ## Multiple R-squared: 0.6386, Adjusted R-squared: 0.6349 ## F-statistic: 173.2 on 1 and 98 DF, p-value: &lt; 2.2e-16 print(&quot;Level 1 LM summary&quot;) ## [1] &quot;Level 1 LM summary&quot; summary(outRegNoiLe1) ## ## Call: ## lm(formula = yNoiLe1 ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -225.09 -62.78 -11.97 73.73 254.06 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1004.0513 30.9057 32.49 &lt;2e-16 *** ## x 4.9766 0.4095 12.15 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 95.45 on 98 degrees of freedom ## Multiple R-squared: 0.6011, Adjusted R-squared: 0.5971 ## F-statistic: 147.7 on 1 and 98 DF, p-value: &lt; 2.2e-16 print(&quot;Level 2 LM summary&quot;) ## [1] &quot;Level 2 LM summary&quot; summary(outRegNoiLe2) ## ## Call: ## lm(formula = yNoiLe2 ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -588.4 -122.2 8.4 136.1 449.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1007.1088 64.7010 15.566 &lt; 2e-16 *** ## x 4.9009 0.8573 5.717 1.17e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 199.8 on 98 degrees of freedom ## Multiple R-squared: 0.2501, Adjusted R-squared: 0.2424 ## F-statistic: 32.68 on 1 and 98 DF, p-value: 1.173e-07 print(&quot;Level 3 LM summary&quot;) ## [1] &quot;Level 3 LM summary&quot; summary(outRegNoiLe3) ## ## Call: ## lm(formula = yNoiLe3 ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1157.74 -264.68 -27.29 239.51 894.85 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 998.139 134.510 7.421 4.28e-11 *** ## x 4.948 1.782 2.776 0.00659 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 415.4 on 98 degrees of freedom ## Multiple R-squared: 0.0729, Adjusted R-squared: 0.06344 ## F-statistic: 7.706 on 1 and 98 DF, p-value: 0.006592 where we observe that the point cloud is more dispersed and looks less than a line the more noise is added, but the estimated regression line changes only a little bit. We can also see how the \\(R^2\\) becomes smaller as more noise is added. 4.6 Residual Analysis While the coefficient of determination can tell us how good is the fit of the data, it can’t tell us why is it good or bad. The easiest way to check for any problems is to check the residuals or estimated errors. Right now, we will focus on 4 problems: The regression function is not linear. The variance of the error terms is not constant. There are outliers. Important variables are ommited. 4.6.1 Non-linear regression function Sometimes the relationship between the two variables that we are analyzing is not linear. Looking at the example of the relationship between burger price and burgers sold we can look at the fitted regression line and the residuals plot. Here we can clearly appreciate that there is something wrong. The residuals clearly indicate that a non linear relationship is present in the data. One can solve these problem by transforming one or both of the variables and then applying linear regression. Common transformations functions \\(g\\) are (but are not limited to): \\(g(x) = x^2\\) \\(g(x) = \\sqrt{x}\\) \\(g(x) = log(x)\\) This transformations can be applied to the independent variable, to the dependent variable of both. In the next example we work with \\(log(\\text{Burgers Sold})\\) instead of directly working with “Burgers Sold”. In this case there seems to be a much better fit. We can compare this to the following transformation \\(\\log{(\\text{Price})}\\): It is this these transformations improve the fit of the model, however which one is the best one? One can check the \\(R^2\\) of the different transformations: dat &lt;- read.csv(&quot;Burger Data.csv&quot;) x &lt;- dat$Price y &lt;- dat$Burgers outRegOri &lt;- lm(y ~ x) outRegTr1 &lt;- lm(log(y) ~ x) outRegTr2 &lt;- lm(y ~ log(x)) summary(outRegOri) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.310 -5.637 -0.553 2.899 47.182 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 76.9130 2.0098 38.27 &lt;2e-16 *** ## x -4.3417 0.2194 -19.79 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.522 on 98 degrees of freedom ## Multiple R-squared: 0.7999, Adjusted R-squared: 0.7978 ## F-statistic: 391.7 on 1 and 98 DF, p-value: &lt; 2.2e-16 summary(outRegTr1) ## ## Call: ## lm(formula = log(y) ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.39409 -0.08669 0.00065 0.09325 0.37302 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.518050 0.034072 132.60 &lt;2e-16 *** ## x -0.109240 0.003719 -29.37 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1445 on 98 degrees of freedom ## Multiple R-squared: 0.898, Adjusted R-squared: 0.897 ## F-statistic: 862.8 on 1 and 98 DF, p-value: &lt; 2.2e-16 summary(outRegTr2) ## ## Call: ## lm(formula = y ~ log(x)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.4312 -2.8468 0.1724 3.1830 9.8330 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 99.6184 1.4927 66.74 &lt;2e-16 *** ## log(x) -29.8274 0.7237 -41.22 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.449 on 98 degrees of freedom ## Multiple R-squared: 0.9455, Adjusted R-squared: 0.9449 ## F-statistic: 1699 on 1 and 98 DF, p-value: &lt; 2.2e-16 We will see other ways to choose the transformation later. 4.6.2 Heteroscedasticity Sometimes, errors are expected to increase depending on the explanatory variable. For example, it could be expected that errors for bigger values of the explanatory variable will be also bigger. As an example, consider a new data set with the Height of several children. One could expect there is a linear relationship between the age of a child and the height. However, one also would expect that height difference are bigger for older children than for younger children. We can see this in the following simulated data: dat &lt;- read.csv(&quot;Height Data.csv&quot;) outReg &lt;- lm(Height ~ Age, data = dat) # Original Data par(mfrow = c(1, 2)) plot(x = dat$Age, y = dat$Height, xlab = &quot;Age (years)&quot;, ylab = &quot;Height (in)&quot;, main = &quot;Original Data&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) # Residuals plot(x = dat$Age, y = outReg$residuals, xlab = &quot;Age (years)&quot;, ylab = &quot;Residuals (in)&quot;, main = &quot;Residuals&quot;) abline(h = 0, lwd = 2) While one can infer this error behavior in the original scatter plot, the residual plot makes this pattern much more clearly. In the residual plot, it is pretty clear that the errors increase with the age of the children. While this is not a problem in itself when doing least squares estimation, it might be better to consider other alternatives that do not penalize the same way the errors at young age than the errors at a later age. We will see this with weighted least squares. 4.6.3 Outliers Sometimes the data follows a linear regression for most of the observations, but there might be some observations that do not follow the linear relationship. In the next example, we work with the Wine data set and add outliers, to see the effect this might have on the estimation. dat &lt;- read.csv(&quot;Wine Data.csv&quot;) x &lt;- dat$Glasses y &lt;- dat$Years ymin &lt;- min(y, max(y) - 22) ymax &lt;- max(y, min(y) + 22) xmin &lt;- min(x) xmax &lt;- max(x) par(mfrow = c(2, 2)) # No Outliers outRegNoo &lt;- lm(y ~ x) plot(x = x, y = y, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Life Expectancy&quot;, main = &quot;No Outliers&quot;) abline(a = outRegNoo$coefficients[1], b = outRegNoo$coefficients[2], col = &#39;red&#39;, lwd = 2) # Outlier on the left side # Adds Observation x &lt;- c(x, 0) y &lt;- c(y, 90) outRegLef &lt;- lm(y ~ x) plot(x = x, y = y, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Life Expectancy&quot;, main = &quot;Outlier Left&quot;) points(x = x[21], y = y[21], pch = 16, col = &#39;blue&#39;) abline(a = outRegLef$coefficients[1], b = outRegLef$coefficients[2], col = &#39;red&#39;, lwd = 2) # Outlier on the right side # Adds Observation x[21] &lt;- 4 y[21] &lt;- 68 outRegRig &lt;- lm(y ~ x) plot(x = x, y = y, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Life Expectancy&quot;, main = &quot;Right Outliers&quot;) points(x = x[21], y = y[21], pch = 16, col = &#39;blue&#39;) abline(a = outRegRig$coefficients[1], b = outRegRig$coefficients[2], col = &#39;red&#39;, lwd = 2) # Outlier on the right side # Adds Observation x[21] &lt;- 2 y[21] &lt;- 95 outRegCen &lt;- lm(y ~ x) plot(x = x, y = y, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Life Expectancy&quot;, main = &quot;Center Outliers&quot;) points(x = x[21], y = y[21], pch = 16, col = &#39;blue&#39;) abline(a = outRegCen$coefficients[1], b = outRegCen$coefficients[2], col = &#39;red&#39;, lwd = 2) The presence of outliers, will also be more clear when looking at the residuals. x &lt;- dat$Glasses y &lt;- dat$Years ymin &lt;- min(outRegNoo$residuals, outRegLef$residuals, outRegRig$residuals, outRegCen$residuals) ymax &lt;- max(outRegNoo$residuals, outRegLef$residuals, outRegRig$residuals, outRegCen$residuals) xmin &lt;- min(x) xmax &lt;- max(x) par(mfrow = c(2, 2)) plot(x = dat$Glasses, y = outRegNoo$residuals, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Residuals&quot;, main = &quot;No Outliers&quot;) abline(h = 0, lwd = 2) # Outlier on the left side x[21] &lt;- 0 y[21] &lt;- 90 plot(x = x, y = outRegLef$residuals, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Residuals&quot;, main = &quot;Left Outlier&quot;) abline(h = 0, lwd = 2) # Outlier on the right side # Adds Observation x[21] &lt;- 4 y[21] &lt;- 68 plot(x = x, y = outRegRig$residuals, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Residuals&quot;, main = &quot;Left Outlier&quot;) abline(h = 0, lwd = 2) # Outlier on the right side # Adds Observation x[21] &lt;- 2 y[21] &lt;- 95 plot(x = x, y = outRegCen$residuals, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Residuals&quot;, main = &quot;Left Outlier&quot;) abline(h = 0, lwd = 2) And we can also see how the \\(R^2\\) changes: summary(outRegNoo) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6443 -1.4398 -0.3390 0.9071 4.8057 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 73.7154 0.8674 84.988 &lt; 2e-16 *** ## x 2.4686 0.4364 5.656 2.29e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.162 on 18 degrees of freedom ## Multiple R-squared: 0.64, Adjusted R-squared: 0.62 ## F-statistic: 32 on 1 and 18 DF, p-value: 2.295e-05 summary(outRegLef) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.550 -2.296 -1.413 1.440 14.028 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 75.9724 1.5101 50.31 &lt;2e-16 *** ## x 1.5258 0.7786 1.96 0.0649 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.056 on 19 degrees of freedom ## Multiple R-squared: 0.1682, Adjusted R-squared: 0.1244 ## F-statistic: 3.841 on 1 and 19 DF, p-value: 0.06486 summary(outRegRig) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.228 -1.428 -0.211 2.283 5.654 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 75.0353 1.4815 50.648 &lt;2e-16 *** ## x 1.2981 0.6965 1.864 0.0779 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.803 on 19 degrees of freedom ## Multiple R-squared: 0.1545, Adjusted R-squared: 0.11 ## F-statistic: 3.473 on 1 and 19 DF, p-value: 0.0779 summary(outRegCen) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4964 -2.2377 -0.9105 0.9596 15.4953 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 74.1257 1.6870 43.939 &lt; 2e-16 *** ## x 2.6895 0.8486 3.169 0.00505 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.214 on 19 degrees of freedom ## Multiple R-squared: 0.3459, Adjusted R-squared: 0.3114 ## F-statistic: 10.05 on 1 and 19 DF, p-value: 0.005048 4.6.4 Variables Ommited When looking at the residuals, there should be no aparent pattern. Sometimes, the pattern is only noticeable when considering other variables. We can see this with the Height data set. The data includes the sex of the children. If you fit least squares without taking this into consideration, a pattern is appears in the residuals when you look at the residuals for each sex. dat &lt;- read.csv(&quot;Height Data.csv&quot;) # Fits linear Model outReg &lt;- lm(dat$Height ~ dat$Age) # Plots residuals for each sex ymin &lt;- min(outReg$residuals) ymax &lt;- max(outReg$residuals) par(mfrow=c(1, 2)) plot(x = dat$Age[dat$Sex == 1], y = outReg$residuals[dat$Sex == 1], ylim = c(ymin, ymax), xlab = &quot;Age&quot;, ylab = &quot;Residuals&quot;, main = &quot;Males&quot;) abline(h = 0, lwd = 2) plot(x = dat$Age[dat$Sex == 0], y = outReg$residuals[dat$Sex == 0], ylim = c(ymin, ymax), xlab = &quot;Age&quot;, ylab = &quot;Residuals&quot;, main = &quot;Females&quot;) abline(h = 0, lwd = 2) 4.7 Cross-Validation When testing different models, a good idea to evaluate the performance of each model beyond \\(R^2\\), is to separate your data set into a training set and a validation or test set. Doing this is called cross-validation. There are several alternatives to doing cross-validation, here are a few of the most relevant ones: Holdout Method (Train/Test Split) Description: The data is split into two (or three) sets: training and test (and sometimes validation). The model is trained on the training set and evaluated on the test set. Use Case: Simple to implement, but has high variance. The performance may depend on the specific split. K-Fold Cross-Validation Description: The data is divided into \\(k\\) equal-sized folds (subsets). The model is trained on \\(k-1\\) folds and tested on the remaining fold. This process is repeated \\(k\\) times, with each fold used as the test set once. Use Case: Works well for most applications and balances the bias-variance tradeoff. Common choices for \\(k\\): 5, 10. Leave-One-Out Cross-Validation (LOOCV) Description: Each data point is used once as a test set, and the rest of the data is used as the training set. This results in \\(n\\) iterations, where \\(n\\) is the number of samples. Use Case: Good when the dataset is small, but can be computationally expensive for large datasets. Leave-P-Out Cross-Validation (LPOCV) Description: Similar to LOOCV, but instead of leaving out one data point, \\(p\\) data points are left out. This creates \\(\\binom{n}{p}\\) different training/testing splits. Use Case: Rarely used due to its high computational cost for large datasets but might be useful in specific scenarios. Stratified K-Fold Cross-Validation Description: Similar to K-fold cross-validation but ensures that each fold has the same proportion of each class in classification tasks (i.e., balanced classes in each fold). Use Case: Useful for imbalanced datasets in classification problems. Repeated K-Fold Cross-Validation Description: A variation of K-Fold Cross-Validation where the process is repeated multiple times with different random splits. Use Case: Provides more robust estimates of model performance, particularly when the dataset is small. Here is an example of \\(k\\)-fold Cross-Validation using the “Burger Data” set. ### Cross Validation Example # Read Data dat &lt;- read.csv(file = &quot;Burger Data.csv&quot;) # Saves Variables x &lt;- dat$Price y &lt;- dat$Burgers # Number of Observations n &lt;- length(y) # Number of Folds numFol &lt;- 10 # Fold Size sizFol &lt;- round(n / numFol) # List Containing the Fold indices fol &lt;- list() # Select Fold Indeces # Initialization ind &lt;- 1:n for(i in 1:numFol){ # Computes the remaining number of indices numInd &lt;- length(ind) # Randomly selects indices from the remaining indeces indFol &lt;- sample(x = 1:length(ind), size = sizFol, replace = FALSE) # Saves the inidices to the list of Fold Indeces fol[[i]] &lt;- ind[indFol] # Removes the indices from the indeces vector ind &lt;- ind[- indFol] } # Models to try X &lt;- cbind(x, log(x), x, log(x)) Y &lt;- cbind(y, y, log(y), log(y)) # Number of Models to compare numMod &lt;- dim(Y)[2] # Saves the validation metric for each model and each fold matMet &lt;- matrix(data = NA, nrow = numFol, ncol = numMod) par(mfrow=c(2, 2)) # Loops through the models for(k in 1:numMod){ yMod &lt;- Y[, k] xMod &lt;- X[, k] plot(x = xMod, y = yMod, xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers&quot;, main = paste0(&quot;Model &quot;, k)) metFol &lt;- numeric(length = numFol) # Loops through the folds for(i in 1:numFol){ # At Each Fold do Linear Regression without the test set outReg &lt;- lm(yMod[-fol[[i]]] ~ xMod[-fol[[i]]]) # Evaluate Out of Sample yHatOut &lt;- outReg$coefficients[1] + outReg$coefficients[2] * xMod[fol[[i]]] if(k &gt;= 3 ){ metEva &lt;- mean((exp(yMod[fol[[i]]]) - exp(yHatOut))^2) } else { metEva &lt;- mean((yMod[fol[[i]]] - yHatOut)^2) } metFol[i] &lt;- metEva } matMet[, k] &lt;- metFol } 4.8 Weighted Least Squares Weighted Least Squares (WLS) is an extension of ordinary least squares (OLS) used when the assumption of constant variance (homoscedasticity) is violated. In cases of heteroscedasticity (unequal variances), OLS produces inefficient estimates. WLS solves this by assigning weights to each observation, giving more importance to data points with lower variance. This leads to more reliable estimates in the presence of heteroscedasticity, making WLS valuable for improving regression models when error variances vary. Lower variance → Higher weight Higher variance → Lower weight In a simple linear regression model with one independent variable, OLS minimizes the sum of squared residuals: \\[ \\min \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\min \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\] Weighted Least Squares (WLS) corrects for heteroscedasticity by minimizing the weighted sum of squared residuals: \\[ \\min \\sum_{i=1}^{n} w_i (y_i - \\hat{y}_i)^2 = \\min \\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i)^2 \\] Step 1: Define the weighted residual sum of squares The objective function is: \\[Q_w(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i)^2\\] We aim to minimize \\(Q(\\beta_0, \\beta_1)\\) with respect to \\(\\beta_0\\) and \\(\\beta_1\\). Step 2: Take the partial derivatives with respect to $ _0 $ and $ _1 $: \\[\\frac{\\partial Q_w}{\\partial \\beta_0} = -2 \\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i)\\] Setting the derivative equal to zero: \\[\\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i) = 0 \\] \\[\\sum_{i=1}^{n} w_i y_i = \\sum_{i=1}^{n} w_i (\\beta_0 + \\beta_1 x_i)\\] \\[\\beta_0 \\sum_{i=1}^{n} w_i + \\beta_1 \\sum_{i=1}^{n} w_i x_i = \\sum_{i=1}^{n} w_i y_i \\tag{3}\\] Now with respect to \\(\\beta_1\\): \\[\\frac{\\partial Q_w}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i) x_i\\] Setting the derivative equal to zero: \\[-2 \\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i) x_i = 0\\] \\[\\sum_{i=1}^{n} w_i y_i x_i = \\sum_{i=1}^{n} w_i (\\beta_0 + \\beta_1 x_i) x_i\\] \\[\\beta_0 \\sum_{i=1}^{n} w_i x_i + \\beta_1 \\sum_{i=1}^{n} w_i x_i^2 = \\sum_{i=1}^{n} w_i y_i x_i \\tag{4}\\] Now we solve the system of two linear equations: \\(\\beta_0 \\sum_{i=1}^{n} w_i + \\beta_1 \\sum_{i=1}^{n} w_i x_i = \\sum_{i=1}^{n} w_i y_i\\) \\(\\beta_0 \\sum_{i=1}^{n} w_i x_i + \\beta_1 \\sum_{i=1}^{n} w_i x_i^2 = \\sum_{i=1}^{n} w_i y_i x_i\\) Solve for \\(\\beta_1\\) Multiply equation (3) by \\(\\frac{\\sum_{i=1}^{n} w_i x_i}{\\sum_{i=1}^n w_i}\\) and subtract it from equation (4) to eliminate \\(\\beta_0\\): \\[\\beta_1 \\left( \\sum_{i=1}^{n} w_i x_i^2 - \\frac{\\left( \\sum_{i=1}^{n} w_i x_i \\right)^2}{\\sum_{i=1}^{n} w_i} \\right) = \\sum_{i=1}^{n} w_i y_i x_i - \\frac{\\sum_{i=1}^{n} w_i x_i \\sum_{i=1}^{n} w_i y_i}{\\sum_{i=1}^{n} w_i}\\] Thus, \\(\\beta_1\\) is: \\[\\beta_1 = \\frac{\\sum_{i=1}^{n} w_i x_i y_i - \\frac{\\sum_{i=1}^{n} w_i x_i \\sum_{i=1}^{n} w_i y_i}{\\sum_{i=1}^{n} w_i}}{\\sum_{i=1}^{n} w_i x_i^2 - \\frac{\\left( \\sum_{i=1}^{n} w_i x_i \\right)^2}{\\sum_{i=1}^{n} w_i}}\\] Then the weighted estimators for \\(\\beta_0\\) and \\(\\beta_1\\) are: \\[\\hat{\\beta}_{w,1} = \\frac{\\sum_{i=1}^{n} w_i x_i y_i - \\frac{\\sum_{i=1}^{n} w_i x_i \\sum_{i=1}^{n} w_i y_i}{\\sum_{i=1}^{n} w_i}}{\\sum_{i=1}^{n} w_i x_i^2 - \\frac{\\left( \\sum_{i=1}^{n} w_i x_i \\right)^2}{\\sum_{i=1}^{n} w_i}},\\] \\[\\hat{\\beta}_{w,0} = \\frac{\\sum_{i=1}^{n} w_i y_i - \\beta_1 \\sum_{i=1}^{n} w_i x_i}{\\sum_{i=1}^{n} w_i}.\\] 4.9 Model in Matrix Form We can specify the same model as in Model, in matrix form as follows: \\[\\mathbf{y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}\\] where: \\(\\mathbf{y}={y_1,\\ldots,y_n}&#39;\\) \\(\\mathbf{e}={e_1,\\ldots,e_n}&#39;\\) \\(\\boldsymbol{\\beta}={\\beta_0,\\beta_1}&#39;\\) \\(\\mathbf{X}=[\\mathbb{1}_n \\mathbf{x}]\\) \\(\\mathbb{1}_n={1,\\ldots,1}&#39;\\) (1 \\(n\\)-times). In this way we can re-write our minimization problem as follows: \\[ \\min \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\min \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 = \\min \\sum_{i=1}^{n} (\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})\\] equivalent to: \\[\\min \\mathbf{y}&#39;\\mathbf{y}- \\mathbf{y}&#39;\\mathbf{X}\\boldsymbol{\\beta}- \\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{y}+ \\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{X}\\boldsymbol{\\beta}\\] The procedure to find the estimators is very similar, working with the gradient instead of the partial derivatives: \\[\\nabla_\\beta Q = \\mathbf{0} \\] then: \\[-\\mathbf{X}&#39;\\mathbf{y}-\\mathbf{X}&#39;\\mathbf{y}+ 2 \\mathbf{X}&#39; \\mathbf{X}\\boldsymbol{\\beta}= \\mathbf{0}\\] \\[2 \\mathbf{X}&#39; \\mathbf{X}\\boldsymbol{\\beta}= 2 \\mathbf{X}&#39;\\mathbf{y}\\] \\[\\mathbf{X}&#39; \\mathbf{X}\\boldsymbol{\\beta}= \\mathbf{X}&#39;\\mathbf{y}\\] \\[\\boldsymbol{\\beta}= \\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}\\mathbf{X}&#39;\\mathbf{y}\\] Notice how we find the estimates much easier. It is also easy to show that a minimum is attained by computing the Hessian Matrix of \\(Q\\) \\[H_Q = 2 \\mathbf{X}&#39; \\mathbf{X}\\] and noticing that: \\[\\mathbf{X}&#39; \\mathbf{X}\\] is positive-definite if \\(\\mathbf{x}\\) is not a constant vector. We can verify that indeed the estimate \\(\\hat{\\boldsymbol{\\beta}}\\) is the same as before by manually doing the computations for this case: We have: \\[ \\mathbf{X} = \\begin{bmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{bmatrix} \\] \\[ \\mathbf{X}&#39; = \\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1 \\\\ x_1 &amp; x_2 &amp; \\cdots &amp; x_n \\end{bmatrix} \\] Multiplying \\(\\mathbf{X}&#39;\\) by \\(\\mathbf{X}\\): \\[ \\mathbf{X}&#39; \\mathbf{X} = \\begin{bmatrix} n &amp; \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i &amp; \\sum_{i=1}^n x_i^2 \\end{bmatrix} \\] Similarly, we have: \\[ \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\] Multiplying \\(\\mathbf{X}&#39;\\) by \\(\\mathbf{y}\\): \\[ \\mathbf{X}&#39; \\mathbf{y} = \\begin{bmatrix} \\sum_{i=1}^n y_i \\\\ \\sum_{i=1}^n x_i y_i \\end{bmatrix} \\] To compute the inverse of \\(\\mathbf{X}&#39;\\mathbf{X}\\) we have that the determinant is: \\[ |\\mathbf{X}&#39; \\mathbf{X}|=\\text{det}(\\mathbf{X}^\\top \\mathbf{X}) = n \\sum_{i=1}^n x_i^2 - \\left( \\sum_{i=1}^n x_i \\right)^2 \\] Then, the inverse is: \\[ (\\mathbf{X}&#39; \\mathbf{X})^{-1} = \\frac{1}{n \\sum_{i=1}^n x_i^2 - \\left( \\sum_{i=1}^n x_i \\right)^2} \\begin{bmatrix} \\sum_{i=1}^n x_i^2 &amp; - \\sum_{i=1}^n x_i \\\\ - \\sum_{i=1}^n x_i &amp; n \\end{bmatrix} \\] Then, we have that: \\[\\begin{align*} (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} &amp;= \\frac{1}{n \\sum_{i=1}^n x_i^2 - \\left( \\sum_{i=1}^n x_i \\right)^2} \\begin{bmatrix} \\sum_{i=1}^n x_i^2 &amp; - \\sum_{i=1}^n x_i \\\\ - \\sum_{i=1}^n x_i &amp; n \\end{bmatrix} \\begin{bmatrix} \\sum_{i=1}^n y_i \\\\ \\sum_{i=1}^n x_i y_i \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} \\frac{1}{n \\sum_{i=1}^n x_i^2 - \\left( \\sum_{i=1}^n x_i \\right)^2} \\left( \\sum_{i=1}^n x_i^2 \\sum_{i=1}^n y_i - \\sum_{i=1}^n x_i \\sum_{i=1}^n x_i y_i \\right) \\\\ \\frac{1}{n \\sum_{i=1}^n x_i^2 - \\left( \\sum_{i=1}^n x_i \\right)^2} \\left( - \\sum_{i=1}^n x_i \\sum_{i=1}^n y_i + n \\sum_{i=1}^n x_i y_i \\right) \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} \\frac{1}{n \\sum_{i=1}^n x_i^2 - (n \\bar{x})^2} \\left( n \\bar{y} \\sum_{i=1}^n x_i^2 - n \\bar{x} \\sum_{i=1}^n x_i y_i \\right) \\\\ \\frac{1}{n \\sum_{i=1}^n x_i^2 - (n \\bar{x})^2} \\left( - n \\bar{x}n \\bar{y} + n \\sum_{i=1}^n x_i y_i \\right) \\end{bmatrix} \\\\ \\end{align*}\\] Now simplifying, the first row, we have that: \\[\\begin{align*} \\frac{n \\bar{y} \\sum_{i=1}^n x_i^2 - n \\bar{x} \\sum_{i=1}^n x_i y_i}{n \\sum_{i=1}^n x_i^2 - (n \\bar{x})^2} &amp;= \\frac{1}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2} \\left( \\bar{y} \\sum_{i=1}^n x_i^2 - \\bar{x} \\sum_{i=1}^n x_i y_i \\right) \\\\ &amp;= \\frac{1}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2} \\left( \\bar{y} \\sum_{i=1}^n x_i^2 - \\bar{y} n\\bar{x}^2 + \\bar{y} n\\bar{x}^2 - \\bar{x} \\sum_{i=1}^n x_i y_i \\right) \\\\ &amp;= \\frac{1}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2} \\left( \\bar{y} \\sum_{i=1}^n x_i^2 - \\bar{y} n\\bar{x}^2 + \\bar{y} n\\bar{x}^2 - \\bar{x} \\sum_{i=1}^n x_i y_i \\right) \\\\ &amp;= \\frac{1}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2} \\left( \\bar{y} \\left(\\sum_{i=1}^n x_i^2 - n\\bar{x}^2 \\right) + \\bar{x} \\left(n\\bar{y}\\bar{x} - \\sum_{i=1}^n x_i y_i \\right) \\right) \\\\ &amp;= \\bar{y} + \\bar{x} \\frac{n\\bar{y}\\bar{x} - \\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2} \\\\ &amp;= \\bar{y} + \\bar{x} \\frac{-(n-1)S_{xy}}{(n-1)S_{xx}} \\\\ &amp;= \\bar{y} - \\beta_1 \\bar{x} \\end{align*}\\] And simplifying the second row, we have: \\[\\frac{1}{n \\sum_{i=1}^n x_i^2 - (n \\bar{x})^2} \\left( - n \\bar{x}n \\bar{y} + n \\sum_{i=1}^n x_i y_i \\right) = \\frac{1}{\\sum_{i=1}^n x_i^2 - n \\bar{x}^2} \\left( - n\\bar{x}\\bar{y} + \\sum_{i=1}^n x_i y_i \\right) = \\frac{S_{xy}}{S_{xx}} = \\beta_1\\] So both expressions are equivalent. 4.9.1 Weighted Least Squares in Matrix Form In a similar way we can solve the weighted least squares problem in matrix form as follows: \\[Q_w = \\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i)^2 = (\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;\\mathbf{W}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{y}&#39;\\mathbf{W}\\mathbf{y}-\\mathbf{y}&#39;\\mathbf{W}\\mathbf{X}\\boldsymbol{\\beta}- \\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{W}\\mathbf{y}+ \\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{W}\\mathbf{X}\\boldsymbol{\\beta}\\] where, \\(\\mathbf{W}\\) is a diagonal matrix with diagonal entries \\(w_1,\\ldots,w_n\\). then the gradient is: \\[ \\nabla_\\beta Q_w = -2 \\mathbf{X}&#39;\\mathbf{W}\\mathbf{y}+ 2\\mathbf{X}&#39;\\mathbf{W}\\mathbf{X}\\boldsymbol{\\beta}\\] then, making it equal to zero, we have that: \\[ 2\\mathbf{X}&#39;\\mathbf{W}\\mathbf{X}\\boldsymbol{\\beta}= 2 \\mathbf{X}&#39;\\mathbf{W}\\mathbf{y}\\] \\[\\mathbf{X}&#39;\\mathbf{W}\\mathbf{X}\\boldsymbol{\\beta}= \\mathbf{X}&#39;\\mathbf{W}\\mathbf{y}\\] \\[\\boldsymbol{\\beta}= \\left(\\mathbf{X}&#39;\\mathbf{W}\\mathbf{X}\\right)^{-1}\\mathbf{X}&#39;\\mathbf{W}\\mathbf{y}\\] And the Hessian Matrix is given by: \\[H_{Q_w} = 2 \\mathbf{X}&#39;\\mathbf{W}\\mathbf{X}\\] which is positive-definite under the same conditions that before and if the entries of \\(\\mathbf{W}\\) are positive. "],["polynomial-regression.html", "5 Polynomial Regression", " 5 Polynomial Regression Polynomial regression is an extension of linear regression where the relationship between the independent variable \\(x\\) and the dependent variable \\(y\\) is modeled as an \\(n\\)-th degree polynomial. Instead of fitting a straight line, it fits a curve that can capture more complex patterns in the data. In polynomial regression, the model takes the form: \\[ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_k x^k + \\epsilon \\] Where: - \\(y\\) is the dependent variable, - \\(x\\) is the independent variable, - \\(\\beta_0, \\beta_1, \\dots, \\beta_n\\) are the coefficients, - \\(\\epsilon\\) is the error term. Polynomial regression is useful when the data exhibits a nonlinear trend that cannot be well approximated by a straight line. By increasing the degree of the polynomial, the model can fit more complex patterns, though caution is needed to avoid overfitting. There are two main scenarios for using polynomial regression: The underlying process is inherently polynomial. A polynomial provides a good approximation for the relationship. If you know that the phenomenon you’re modeling follows a polynomial function, there’s no need to worry about overfitting. You can confidently use the polynomial degree that best fits the known relationship. However, if you’re using polynomial regression to approximate a nonlinear relationship between the independent and dependent variables, be cautious. Overfitting can become a significant issue, particularly outside the range of the data used for model fitting, where predictions can be unreliable. In the second scenario, it’s common to limit the polynomial degree to 2nd or 3rd order to prevent overfitting while still capturing key nonlinear patterns. We use our Burger Data set, to see an example of using polynomial regression to approximate a non-linear relationship. We will use a second degree polynomial to fit the data. dat &lt;- read.csv(&quot;Burger Data.csv&quot;) # Polynomial Regression Fit outRegPol &lt;- lm(Burgers ~ Price + I(Price^2), data = dat) # Data Limits xmin &lt;- min(dat$Price) xmax &lt;- max(dat$Price) ymin &lt;- min(dat$Burgers) ymax &lt;- max(dat$Burgers) # Scatter plot plot(x = dat$Price, y = dat$Burgers, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers Sold&quot;) par(new = TRUE) # Plots real non-linear relationship curve(expr = 100 - 30 * log(x), from = xmin, to = xmax, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;&quot;, ylab = &quot;&quot;, col = &#39;red&#39;, lwd = 2) par(new = TRUE) # Plots approximated polynomial relationship curve(expr = outRegPol$coefficients[1] + outRegPol$coefficients[2] * x + outRegPol$coefficients[3] * x^2, from = xmin, to = xmax, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;&quot;, ylab = &quot;&quot;, col = &#39;blue&#39;, lwd = 2) The second-degree polynomial approximation provides a reasonably good fit to the data. The red line represents the true nonlinear relationship, while the blue line shows the polynomial approximation. However, if we restrict the dataset to prices below $10, the fit noticeably worsens. This demonstrates a limitation of the polynomial model: while it might fit well within certain ranges, it can lead to unrealistic predictions outside those bounds. For instance, the model might predict that burger sales will eventually rise again as prices increase, which contradicts typical market expectations. dat &lt;- read.csv(&quot;Burger Data.csv&quot;) # Select only Prices bellow 10 dollars sel &lt;- dat$Price &lt; 10 datRes &lt;- dat[sel, ] # Polynomial Regression Fit outRegPolRes &lt;- lm(Burgers ~ Price + I(Price^2), data = datRes) # Data Limits xmin &lt;- min(dat$Price) xmax &lt;- max(dat$Price) ymin &lt;- min(dat$Burgers) ymax &lt;- max(dat$Burgers) # Scatter plot plot(x = dat$Price, y = dat$Burgers, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers Sold&quot;, main = &quot;Polynomial fit with Restricted data (Price &lt; 10)&quot;) par(new = TRUE) # Plots real non-linear relationship curve(expr = 100 - 30 * log(x), from = xmin, to = xmax, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;&quot;, ylab = &quot;&quot;, col = &#39;red&#39;, lwd = 2) par(new = TRUE) # Plots approximated polynomial relationship curve(expr = outRegPolRes$coefficients[1] + outRegPolRes$coefficients[2] * x + outRegPolRes$coefficients[3] * x^2, from = xmin, to = xmax, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;&quot;, ylab = &quot;&quot;, col = &#39;blue&#39;, lwd = 2) We can also check the \\(R^2\\) of the restricted fit. summary(outRegPolRes) ## ## Call: ## lm(formula = Burgers ~ Price + I(Price^2), data = datRes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.0809 -3.5462 -0.0635 3.0957 13.8374 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 116.0882 3.2929 35.254 &lt; 2e-16 *** ## Price -18.3910 1.2519 -14.691 &lt; 2e-16 *** ## I(Price^2) 1.0709 0.1105 9.695 4.13e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.062 on 63 degrees of freedom ## Multiple R-squared: 0.9131, Adjusted R-squared: 0.9104 ## F-statistic: 331.2 on 2 and 63 DF, p-value: &lt; 2.2e-16 And notice that, the \\(R^2\\) with the restricted data is pretty good, however we see that outside the restricted range the fit is pretty bad. Although polynomial regression is technically a form of multivariate linear regression—since it involves multiple polynomial terms of a single independent variable—it can still be analyzed using the tools discussed in the next chapter. Despite having multiple polynomial terms, the model has only one original independent variable that has been transformed. Consequently, the relationship between the transformed variable and the dependent variable can be effectively visualized using a scatter plot. "],["multiple-linear-regression.html", "6 Multiple Linear Regression 6.1 Introduction 6.2 Example 6.3 Least Squares Estimation 6.4 Properties of the Estimates 6.5 Multiple \\(R^2\\) 6.6 Geometric Interpretation of Multiple Linear Regression", " 6 Multiple Linear Regression 6.1 Introduction Multiple regression is a statistical technique used to model the relationship between a dependent variable and two or more independent variables. It extends simple linear regression by allowing for a more complex analysis of how various factors impact an outcome. The general form of the multiple regression equation is: \\[ y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\ldots + \\beta_p x_{p,i} + e_i \\quad i=\\{1,\\ldots,n\\} \\] Where: - \\(y\\) is the dependent variable. - \\(\\beta_0\\) is the intercept. - \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are the coefficients of the independent variables \\(X_1, X_2, \\ldots, X_p\\). - $e represents the error term. This technique is widely used across various fields, including economics, social sciences, and healthcare, to control for multiple factors and enhance prediction accuracy. However, it requires careful attention to assumptions. Beyond linearity, now we have to consider issues like independence, multicollinearity, and variable selection which can affect the results. We already have seen an example of Multiple linear regression when we worked with Polynomial regression. However, multiple linear regression is more general. 6.2 Example Consider, for example, the task of explaining a country’s GDP using other economic variables such as inflation, unemployment, reference interest rate, government spending (as a percentage of GDP), and exports (as a percentage of GDP). In this case, visualization is not as straightforward, and visually inspecting these relationships is much less practical. Nevertheless, with this number of variables, we can visually explore the relationships between them as follows: # Reads Data dat &lt;- read.csv(file = &quot;Gdp Data.csv&quot;) # Plot the scatterplots for each pair of variables pairs(dat) Here we can see, that some independent variables are more related to GDP and some independent variables are more related between themselves. This is valuable information that will help us to develop the right linear model with this variables. We can also observe the correlation between these variables as follows: # Computes the correlation between variables cor(dat) ## gdp inf une int gov exp ## gdp 1.0000000 0.875131082 -0.74874795 0.6964256 0.22172279 0.173602651 ## inf 0.8751311 1.000000000 -0.78173033 0.8292061 0.31103644 0.005685918 ## une -0.7487479 -0.781730327 1.00000000 -0.3642453 -0.16674407 0.010553855 ## int 0.6964256 0.829206121 -0.36424525 1.0000000 0.21389456 0.015699798 ## gov 0.2217228 0.311036436 -0.16674407 0.2138946 1.00000000 0.018475446 ## exp 0.1736027 0.005685918 0.01055386 0.0156998 0.01847545 1.000000000 We can also fit simple linear regression with each one of the independent variables. Inflation Rate # Fits with Inflation outRegInf &lt;- lm(gdp ~ inf, data = dat) varVal &lt;- dat$inf out &lt;- outRegInf varNam &lt;- &quot;Inflation Rate&quot; # Plots Regression Line and Scatterplot and residuals plot par(mfrow = c(1, 2)) plot(x = varVal, y = dat$gd, xlab = varNam, ylab = &quot;GDP&quot;) abline(a = out$coefficients[1], b = out$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = varVal, y = out$residuals, xlab = varNam, ylab = &quot;Residuals&quot;) abline(h = 0, lwd = 2) Unemployment Rate # Fits with Inflation outRegUne &lt;- lm(gdp ~ une, data = dat) varVal &lt;- dat$une out &lt;- outRegUne varNam &lt;- &quot;Unemplyment Rate&quot; # Plots Regression Line and Scatterplot and residuals plot par(mfrow = c(1, 2)) plot(x = varVal, y = dat$gd, xlab = varNam, ylab = &quot;GDP&quot;) abline(a = out$coefficients[1], b = out$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = varVal, y = out$residuals, xlab = varNam, ylab = &quot;Residuals&quot;) abline(h = 0, lwd = 2) Interest Rate # Fits with Inflation outRegInt &lt;- lm(gdp ~ int, data = dat) varVal &lt;- dat$int out &lt;- outRegInt varNam &lt;- &quot;Interest Rate&quot; # Plots Regression Line and Scatterplot and residuals plot par(mfrow = c(1, 2)) plot(x = varVal, y = dat$gd, xlab = varNam, ylab = &quot;GDP&quot;) abline(a = out$coefficients[1], b = out$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = varVal, y = out$residuals, xlab = varNam, ylab = &quot;Residuals&quot;) abline(h = 0, lwd = 2) Goverment Spending # Fits with Inflation outRegGov &lt;- lm(gdp ~ gov, data = dat) varVal &lt;- dat$gov out &lt;- outRegGov varNam &lt;- &quot;Goverment Spending&quot; # Plots Regression Line and Scatterplot and residuals plot par(mfrow = c(1, 2)) plot(x = varVal, y = dat$gd, xlab = varNam, ylab = &quot;GDP&quot;) abline(a = out$coefficients[1], b = out$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = varVal, y = out$residuals, xlab = varNam, ylab = &quot;Residuals&quot;) abline(h = 0, lwd = 2) Exports # Fits with Inflation outRegExp &lt;- lm(gdp ~ exp, data = dat) varVal &lt;- dat$exp out &lt;- outRegExp varNam &lt;- &quot;Exports&quot; # Plots Regression Line and Scatterplot and residuals plot par(mfrow = c(1, 2)) plot(x = varVal, y = dat$gd, xlab = varNam, ylab = &quot;GDP&quot;) abline(a = out$coefficients[1], b = out$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = varVal, y = out$residuals, xlab = varNam, ylab = &quot;Residuals&quot;) abline(h = 0, lwd = 2) All of them seem like good candidates for a linear relationship with the GDP, however when we use them all together, a more careful analysis should be made. We can see the summary reports for the individual regressions and the regression with all independent variables as follows: outRegAll &lt;- lm(gdp ~ inf + une + int + gov + exp, data = dat) # Summary All print(&quot;All Independent Variables&quot;) ## [1] &quot;All Independent Variables&quot; summary(outRegAll) ## ## Call: ## lm(formula = gdp ~ inf + une + int + gov + exp, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.56610 -0.38300 -0.00634 0.36630 1.22542 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.555301 0.380440 4.088 6.41e-05 *** ## inf 0.312218 0.090012 3.469 0.000647 *** ## une -0.377334 0.129275 -2.919 0.003938 ** ## int 0.177827 0.128312 1.386 0.167403 ## gov -0.008483 0.010361 -0.819 0.413950 ## exp 0.064657 0.011930 5.420 1.80e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5024 on 190 degrees of freedom ## Multiple R-squared: 0.8096, Adjusted R-squared: 0.8046 ## F-statistic: 161.6 on 5 and 190 DF, p-value: &lt; 2.2e-16 print(&quot;Only Inflation Rate&quot;) ## [1] &quot;Only Inflation Rate&quot; summary(outRegInf) ## ## Call: ## lm(formula = gdp ~ inf, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.40960 -0.38896 0.03562 0.37998 1.33364 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.02551 0.08705 11.78 &lt;2e-16 *** ## inf 0.49489 0.01965 25.19 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5514 on 194 degrees of freedom ## Multiple R-squared: 0.7659, Adjusted R-squared: 0.7646 ## F-statistic: 634.5 on 1 and 194 DF, p-value: &lt; 2.2e-16 print(&quot;Only Unemployment Rate&quot;) ## [1] &quot;Only Unemployment Rate&quot; summary(outRegUne) ## ## Call: ## lm(formula = gdp ~ une, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.42276 -0.49693 0.02667 0.49525 2.79562 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.0868 0.2046 29.74 &lt;2e-16 *** ## une -1.0400 0.0661 -15.73 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7554 on 194 degrees of freedom ## Multiple R-squared: 0.5606, Adjusted R-squared: 0.5584 ## F-statistic: 247.5 on 1 and 194 DF, p-value: &lt; 2.2e-16 print(&quot;Only Interest Rate&quot;) ## [1] &quot;Only Interest Rate&quot; summary(outRegInt) ## ## Call: ## lm(formula = gdp ~ int, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.27807 -0.50801 -0.00257 0.50336 2.69719 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.31600 0.32323 -4.071 6.8e-05 *** ## int 0.86483 0.06398 13.517 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8178 on 194 degrees of freedom ## Multiple R-squared: 0.485, Adjusted R-squared: 0.4824 ## F-statistic: 182.7 on 1 and 194 DF, p-value: &lt; 2.2e-16 print(&quot;Only Government Spending&quot;) ## [1] &quot;Only Government Spending&quot; summary(outRegGov) ## ## Call: ## lm(formula = gdp ~ gov, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4370 -0.6442 -0.1258 0.7429 3.1839 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.37117 0.51450 2.665 0.00835 ** ## gov 0.06464 0.02041 3.167 0.00179 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.111 on 194 degrees of freedom ## Multiple R-squared: 0.04916, Adjusted R-squared: 0.04426 ## F-statistic: 10.03 on 1 and 194 DF, p-value: 0.001789 print(&quot;Only Exports&quot;) ## [1] &quot;Only Exports&quot; summary(outRegExp) ## ## Call: ## lm(formula = gdp ~ exp, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0084 -0.6679 -0.1133 0.6581 3.0810 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.32709 0.27818 8.365 1.16e-14 *** ## exp 0.06540 0.02664 2.455 0.015 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.122 on 194 degrees of freedom ## Multiple R-squared: 0.03014, Adjusted R-squared: 0.02514 ## F-statistic: 6.028 on 1 and 194 DF, p-value: 0.01496 As we can see, the values for the coefficients can change when doing simple linear regression and multiple linear regression. If the changes are very dramatic (like change in the sign of the coefficient) further inspection is necessary for that variable. 6.3 Least Squares Estimation For least squares estimation, we need to solve the problem: \\[ \\min_\\boldsymbol{\\beta}Q(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - \\hat{y}(\\boldsymbol{\\beta}))^2 = (\\mathbf{y}- \\hat{\\mathbf{y}})&#39;(\\mathbf{y}- \\hat{\\mathbf{y}}) = (\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\] The representation in matrix notation of the problem, allows us to use the same expression to solve this problem as with simple linear regression. The solution is obtained in the exact same way, and is given by: \\[ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39; \\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y} \\] however in this case: \\[ \\hat{\\boldsymbol{\\beta}} = \\left(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2,\\ldots,\\hat{\\beta}_p\\right)&#39; \\] this is the reason, working in matrix form is very useful. 6.4 Properties of the Estimates As with simple linear regression, we can consider several estimates: \\(\\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\beta}\\) the estimates of the observations, \\(\\hat{\\mathbf{e}} = \\mathbf{y}- \\hat{\\mathbf{y}} = \\mathbf{y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) the estimates of the errors. We also note that: \\[ \\hat{\\mathbf{y}} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{y}= \\mathbf{H}y \\] where \\(\\mathbf{H}\\) is called the hat matrix, because it transforms \\(\\mathbf{y}\\) into \\(\\hat{\\mathbf{y}}\\), or the projection matrix. We will see that: \\(\\hat{\\boldsymbol{\\beta}}\\) is a linear combination of \\(y\\). The sum of the estimated errors is equal to zero, \\(\\sum_{i=1}^n \\hat{e_i} = 0\\). \\(\\hat{\\mathbf{e}}\\) and \\(\\hat{\\mathbf{x}_j}\\) are orthogonal for \\(j=\\{1,\\ldots,p\\}\\). \\(\\hat{\\mathbf{e}}\\) and \\(\\hat{\\mathbf{y}}\\) are orthogonal. \\(\\bar{y} = \\hat{\\bar{y}}\\). To see that \\(\\hat{\\boldsymbol{\\beta}}\\) is a linear combination of \\(y\\), we need to express \\(\\hat{\\boldsymbol{\\beta}}\\) as follows: \\[ \\hat{\\boldsymbol{\\beta}} = \\mathbf{A}\\mathbf{y} \\] for some matrix \\(\\mathbf{A}\\). This is very easy to do, we just let \\(\\mathbf{A}= (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\), so: \\[ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}= \\mathbf{A}\\mathbf{y} \\] Now to see that the sum of the estimated errors is equal to zero, \\(\\sum_{i=1}^n \\hat{e_i} = 0\\), we notice that we need to show that: \\[ \\hat{\\mathbf{e}}&#39; \\mathbf{1}= 0 \\] To do so we notice that: \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y} &amp;\\implies (\\mathbf{X}&#39;\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}&#39;\\mathbf{y}\\\\ &amp;\\implies \\mathbf{X}&#39;\\mathbf{y}- \\mathbf{X}&#39;\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}\\\\ &amp;\\implies \\mathbf{X}&#39;\\left(\\mathbf{y}- \\hat{\\mathbf{y}}\\right) = \\mathbf{0}\\\\ &amp;\\implies \\mathbf{X}&#39;\\hat{\\mathbf{e}} = \\mathbf{0} \\end{align*}\\] Now focusing on the product \\(\\mathbf{X}&#39;\\hat{\\mathbf{e}}\\) we have that: \\[ \\mathbf{X}&#39;\\hat{\\mathbf{e}} = \\left[\\begin{matrix} \\mathbf{1}&#39; \\\\ \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\vdots \\\\ \\mathbf{x}_p \\end{matrix}\\right] \\hat{\\mathbf{e}} = \\left[\\begin{matrix} \\mathbf{1}&#39; \\hat{\\mathbf{e}} \\\\ \\mathbf{x}_1 \\hat{\\mathbf{e}} \\\\ \\mathbf{x}_2 \\hat{\\mathbf{e}} \\\\ \\vdots \\\\ \\mathbf{x}_p \\hat{\\mathbf{e}} \\end{matrix}\\right] \\] So we have that: \\[ \\left[\\begin{matrix} \\mathbf{1}&#39; \\hat{\\mathbf{e}} \\\\ \\mathbf{x}_1 \\hat{\\mathbf{e}} \\\\ \\mathbf{x}_2 \\hat{\\mathbf{e}} \\\\ \\vdots \\\\ \\mathbf{x}_p \\hat{\\mathbf{e}} \\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{matrix}\\right] \\] So from the first line of this result, we have that: \\[ \\mathbf{1}&#39; \\hat{\\mathbf{e}} = 0 \\] which is the result we wanted to proof. Now, to show that \\(\\hat{\\mathbf{e}}\\) and \\(\\hat{\\mathbf{x}_j}\\) are orthogonal for \\(j=\\{1,\\ldots,p\\}\\), we use again on: \\[ \\left[\\begin{matrix} \\mathbf{1}&#39; \\hat{\\mathbf{e}} \\\\ \\mathbf{x}_1 \\hat{\\mathbf{e}} \\\\ \\mathbf{x}_2 \\hat{\\mathbf{e}} \\\\ \\vdots \\\\ \\mathbf{x}_p \\hat{\\mathbf{e}} \\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{matrix}\\right] \\] And notice that lines 2 to \\(p+1\\) proof this results, that is \\[ \\mathbf{x}_i &#39; \\hat{\\mathbf{e}} = 0 \\quad i=\\{1,\\ldots,p\\} \\] Now to show that \\(\\hat{\\mathbf{e}}\\) and \\(\\hat{\\mathbf{y}}\\) are orthogonal, we show that: \\[ \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{y}} = 0 \\] Now \\[\\begin{align*} \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{y}} &amp;= (\\mathbf{y}- \\hat{\\mathbf{y}})&#39;\\hat{\\mathbf{y}} \\\\ &amp;= \\left(\\mathbf{y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\right)&#39;\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\ &amp;= \\left(\\mathbf{y}- \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}\\right)&#39;\\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}\\\\ &amp;= \\mathbf{y}&#39; \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}- \\mathbf{y}&#39; \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{y}\\\\ &amp;= \\mathbf{y}&#39; \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}- \\mathbf{y}&#39; \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{y}\\\\ &amp;= 0 \\end{align*}\\] Finally, to show that \\(\\bar{y} = \\hat{\\bar{y}}\\), we use: \\[ \\hat{\\mathbf{e}}&#39;\\mathbf{1}= (\\mathbf{y}- \\hat{\\mathbf{y}})&#39;\\mathbf{1}= \\mathbf{y}&#39;\\mathbf{1}- \\hat{\\mathbf{y}}&#39;\\mathbf{1}= \\sum_{i=1}^ny_i - \\sum_{i=1}^n\\hat{y}_i = n\\bar{y} - n\\hat{\\bar{y}} \\] since \\(\\hat{\\mathbf{e}}&#39;\\mathbf{1}= 0\\), then we have that \\[ n\\bar{y} - n\\hat{\\bar{y}} = 0 \\implies n\\bar{y} = n\\hat{\\bar{y}} \\implies \\bar{y} = \\hat{\\bar{y}} \\] 6.5 Multiple \\(R^2\\) As with simple linear regression we can explain the total variability, by decomposing the variability in two parts, the regression variability and the error variability. First, we define this concepts: Total Sum of Squares \\(SS_{tot}\\): The total sum of squares measures the total variability in \\(\\mathbf{y}\\): \\[ SS_{tot} = (\\mathbf{y} - \\bar{y} \\mathbf{1})&#39; (\\mathbf{y} - \\bar{y} \\mathbf{1}) \\] Residual Sum of Squares \\(SS_{res}\\): The residual sum of squares measures the unexplained variability in the regression model: \\[ SS_{res} = (\\mathbf{y} - \\hat{\\mathbf{y}})&#39; (\\mathbf{y} - \\hat{\\mathbf{y}}) \\] Explained Sum of Squares \\(SS_{reg}\\) The explained sum of squares measures how much of the total variability is explained by the regression model. It is the difference between the predicted values and the mean of \\(\\mathbf{y}\\): \\[ SS_{reg} = (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1})&#39; (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) \\] As with simple linear regression, it can be shown that: \\[ SS_{tot} = SS_{reg} + SS_{res} \\] To see this, we start form \\(SS_{tot}\\), and do the adding and subtracting trick: \\[\\begin{align*} SS_{tot} &amp;= (\\mathbf{y} - \\bar{y} \\mathbf{1})&#39; (\\mathbf{y} - \\bar{y} \\mathbf{1}) \\\\ &amp;= (\\mathbf{y} - \\hat{\\mathbf{y}} + \\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1})&#39; (\\mathbf{y} - \\hat{\\mathbf{y}} + \\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) \\\\ &amp;= (\\mathbf{y} - \\hat{\\mathbf{y}})&#39; (\\mathbf{y} - \\hat{\\mathbf{y}}) + (\\mathbf{y} - \\hat{\\mathbf{y}})&#39; (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) + (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1})&#39; (\\mathbf{y} - \\hat{\\mathbf{y}}\\mathbf{1}) + (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1})&#39; (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) \\end{align*}\\] Now, notice that: \\[ (\\mathbf{y} - \\hat{\\mathbf{y}})&#39; (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) = \\hat{\\mathbf{e}}&#39; (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) = \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{y}} - \\bar{y}\\hat{\\mathbf{e}}&#39; \\mathbf{1} = 0 - \\bar{y}0 = 0 \\] And similarly for \\((\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1})&#39; (\\mathbf{y} - \\hat{\\mathbf{y}}\\mathbf{1}) = 0\\), then: \\[ SS_{tot} = (\\mathbf{y} - \\hat{\\mathbf{y}})&#39; (\\mathbf{y} - \\hat{\\mathbf{y}}) + (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1})&#39; (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) = SS_{reg} + SS_{res} \\] The multiple \\(R^2\\) is the variability explained by the regression with respect to the total variability and can be expressed as: \\[ R^2 = \\frac{SS_{reg}}{SS_{tot}} \\] or using the previous expression \\[ 1 = \\frac{SS_{tot}}{SS_{tot}} = \\frac{SS_{reg}}{SS_{tot}} + \\frac{SS_{res}}{SS_{tot}} = R^2 + \\frac{SS_{res}}{SS_{tot}} \\implies R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\] Finally, we work on the expressions of \\(SS_{res}\\) and \\(SS_{tot}\\), to express them in terms of projection matrices. First note that: \\[ \\mathbf{y}- \\hat{\\mathbf{y}} = \\mathbf{y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{y}- \\mathbf{H}\\mathbf{y}= (\\mathbf{I}- \\mathbf{H})\\mathbf{y} \\] and also notice that \\((\\mathbf{I}- \\mathbf{H})\\) is symmetric and: \\[ (\\mathbf{I}- \\mathbf{H})(\\mathbf{I}- \\mathbf{H}) = \\mathbf{I}-\\mathbf{H}- \\mathbf{H}+ \\mathbf{H}\\mathbf{H} \\] and \\[ \\mathbf{H}\\mathbf{H}= \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; = \\mathbf{H} \\] this means \\(\\mathbf{H}\\) is idempotent. In fact, all projection matrices are idempotent. Then, we have that: \\[ (\\mathbf{I}- \\mathbf{H})(\\mathbf{I}- \\mathbf{H}) = \\mathbf{I}-\\mathbf{H}- \\mathbf{H}+ \\mathbf{H}= \\mathbf{I}-\\mathbf{H}- \\mathbf{H} \\] which makes \\(\\mathbf{I}- \\mathbf{H}\\) also idempotent. Therefore: \\[ SS_{res} = (\\mathbf{y}- \\hat{\\mathbf{y}})&#39;(\\mathbf{y}- \\hat{\\mathbf{y}}) = ((\\mathbf{I}- \\mathbf{H})\\mathbf{y})&#39;((\\mathbf{I}- \\mathbf{H})\\mathbf{y}) = \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}= \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y} \\] And we can do a similar trick for the \\(SS_{tot}\\) by writing \\(\\bar{y} \\mathbf{1}\\) as a result of projecting \\(\\mathbf{y}\\) with a design matrix \\(\\mathbf{1}\\): \\[ \\bar{y} \\mathbf{1}= \\mathbf{1}\\bar{y} = \\mathbf{1}\\frac{1}{n} \\sum_{i=1}^n y_i = \\mathbf{1}\\frac{1}{n}\\mathbf{1}&#39; \\mathbf{y}= \\mathbf{1}(\\mathbf{1}&#39;\\mathbf{1})^{-1}\\mathbf{1}&#39; \\mathbf{y} \\] where we use the fact that \\(\\mathbf{1}&#39;\\mathbf{1}= n\\). We call \\(\\mathbf{H}_0 = \\mathbf{1}(\\mathbf{1}&#39;\\mathbf{1})^{-1}\\mathbf{1}&#39;\\), since \\(\\mathbf{1}(\\mathbf{1}&#39;\\mathbf{1})^{-1}\\mathbf{1}&#39;\\) is a projection matrix. And since it is a projection matrix it is idempotent (it is also not difficult to check this manually) and \\(\\mathbf{I}- \\mathbf{H}_0\\) is also idempotent. So we can do: \\[ \\mathbf{y}- \\hat{y}\\mathbf{1}= \\mathbf{y}- \\mathbf{H}_0 \\mathbf{y}= (\\mathbf{I}-\\mathbf{H}_0)\\mathbf{y} \\] \\[ SS_{tot} = (\\mathbf{y}- \\bar{y}\\mathbf{1})&#39;(\\mathbf{y}- \\bar{y}\\mathbf{1}) = ((\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y})&#39;((\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}) = \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}= \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y} \\] so the \\(R^2\\) can be expressed as follows: \\[ R^2 = 1 - \\frac{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}}{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}} \\] When written like this, it is easy to see that: \\[ \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}= \\min_\\boldsymbol{\\beta}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\] the solution to this minimization problem, since we are using the optimal value \\(\\hat{\\boldsymbol{\\beta}}\\). And \\[ \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}= \\min_{\\beta_0} (\\mathbf{y}- \\mathbf{X}_0 \\beta_0)&#39;(\\mathbf{y}- \\mathbf{X}_0 \\beta_0) \\] where \\(\\mathbf{X}_0\\) is just a matrix with one column \\(\\mathbf{1}\\). Now, we also have that: \\[ \\min_\\boldsymbol{\\beta}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\leq \\min_{\\beta_0} (\\mathbf{y}- \\mathbf{X}_0 \\beta_0)&#39;(\\mathbf{y}- \\mathbf{X}_0 \\beta_0) \\] therefore \\[ \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}\\leq \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y} \\] and since both of them are quadratic forms, we have that: \\(\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}, \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}\\geq 0\\) then: \\[0 \\leq \\frac{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}}{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}} \\leq 0\\] then: \\[0 \\leq R^2 \\leq 0\\]. Where we use the fact that all symmetric idempotent matrices are symmetric positive semi-definite. Another interpretation of \\(R^2\\) is the percentage of the variability explained by multiple regression of a “poor man’s regression” in which you don’t have independent variables (that is you are independent variable poor). In this way, we can define \\[ \\bar{y} \\mathbf{1}= \\hat{\\mathbf{y}}_0 \\] the “poor man’s prediction”, of which \\(\\mathbf{H}_0\\) is it’s projection matrix (or hat matrix). 6.6 Geometric Interpretation of Multiple Linear Regression Multiple linear regression can be thought as projecting \\(\\mathbf{y}\\) in the column space of the design matrix \\(\\mathbf{X}\\). The following diagram pictures multiple linear regression. Here we can see several components: \\(\\mathbf{y}\\) is the vector of observations. Is a vector in \\(\\mathbb{R}^n\\). The grey hyper-plane is the column space generated by \\(\\mathbf{X}\\), a sub-space of \\(\\mathbb{R}^n\\). The multiple regression prediction \\(\\hat{\\mathbf{y}}\\) of \\(\\mathbf{y}\\) is the projection of \\(\\mathbf{y}\\) on the space generated by the column of \\(\\mathbf{X}\\). The poor man’s prediction \\(\\hat{\\mathbf{y}}_0\\), in the column space of \\(\\mathbf{X}\\) (since, one of the columns is \\(\\mathbf{1}\\)), but in most cases it is different to \\(\\hat{\\mathbf{y}}\\) (the closest vector in the column space of \\(\\mathbf{X}\\) to \\(\\mathbf{y}\\)). We notice that the differences: \\(\\mathbf{y}- \\hat{\\mathbf{y}}\\). \\(\\mathbf{y}- \\hat{\\mathbf{y}}_0\\) \\(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_0\\) form a right triangle, then it must be that: \\[ ||\\mathbf{y}- \\hat{\\mathbf{y}}||^2 = ||\\mathbf{y}- \\hat{\\mathbf{y}}_0||^2 + ||\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_0||^2 \\] which is the same as \\[ (\\mathbf{y}- \\hat{\\mathbf{y}})&#39;(\\mathbf{y}- \\hat{\\mathbf{y}}) = (\\mathbf{y}- \\hat{\\mathbf{y}}_0)&#39;(\\mathbf{y}- \\hat{\\mathbf{y}}_0) + (\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_0)&#39;(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_0) \\] that can be expressed as: \\[ SS_{tot} = SS_{res} + SS_{reg} \\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
