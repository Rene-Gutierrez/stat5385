[["index.html", "1 Stat 5385/6385 Fall 2025 1.1 Calendar", " 1 Stat 5385/6385 Fall 2025 This is the website for STAT5385. It will contain the relevant information for the course and lecture notes and code seen in class. Find the syllabus here: Syllabus 1.1 Calendar 1.1.1 Important Dates 1.1.2 Class Schedule Class 1: Class Introduction Class 2: Regression Introduction Class 3: Intro to Simple Linear Regression Class 4: Properties of the Simple Linear Regression Problem "],["prerequisites.html", "2 Prerequisites 2.1 General Math 2.2 Linear Algebra 2.3 Calculus 2.4 Probability 2.5 Statistics", " 2 Prerequisites Before diving into the course, it’s important to have a solid understanding of the following foundational concepts. These are categorized into five key topics: General Math Linear Algebra Probability Statistics Calculus You can check some of the requirements on Chapter 1 of the textbook. 2.1 General Math You should be familiar with the summation operator \\(\\sum\\). This operator is defined as follows: \\[\\sum_{i=1}^n x_i = x_1 + x_2 + \\ldots + x_n \\] Key properties of the summation operator include: Linearity: \\[\\sum_{i=1}^N (a + b x_i) = aN + b \\sum_{i=1}^N x_i\\] Additivity: \\[\\sum_{i=1}^N (x_i + y_i) = \\sum_{i=1}^N x_i + \\sum_{i=1}^N y_i\\] 2.2 Linear Algebra You should be familiar with the following linear algebra concepts: Linear Independence Column Space of a Matrix Rank of a Matrix Full Rank Matrix Inverse Matrix Positive Definite Matrix Singular Value Decomposition Eigendecomposition Idempotent Matrix Determinant of a Matrix 2.2.1 Linear Independence Linear independence is a fundamental concept in linear algebra that describes a set of vectors where no vector can be written as a linear combination of the others. In other words, the vectors are not “redundant,” meaning none of the vectors depends on any other in the set. 2.2.1.1 Definition: A set of vectors \\(\\{ \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n \\}\\) in a vector space is linearly independent if the only solution to the equation: \\[ c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n = \\mathbf{0} \\] is when all the scalar coefficients \\(c_1, c_2, \\ldots, c_n\\) are zero, i.e., \\(c_1 = c_2 = \\cdots = c_n = 0\\). If any of the coefficients can be non-zero while still satisfying this equation, then the vectors are linearly dependent. 2.2.1.2 Example: Consider two vectors in \\(\\mathbb{R}^2\\): \\[ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\] These vectors are linearly independent because there is no way to express one as a multiple of the other. The only solution to: \\[ c_1 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + c_2 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\] is \\(c_1 = 0\\) and \\(c_2 = 0\\). In contrast, if: \\[ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} \\] These vectors are linearly dependent, because \\(\\mathbf{v}_2 = 2 \\mathbf{v}_1\\). Therefore, you can express \\(mathbf{v}_2\\) as a linear combination of \\(\\mathbf{v}_1\\). 2.2.1.3 Key Points: Linearly independent vectors carry distinct information and cannot be derived from each other. Linearly dependent vectors are redundant because one or more can be expressed as a combination of others. In a set of linearly independent vectors, removing any vector would reduce the span of the vector space they cover. 2.2.1.4 Importance: Linear independence is crucial in determining the rank of a matrix. In systems of equations, linear independence of the rows or columns determines if the system has a unique solution. In vector spaces, the dimension of the space is the maximum number of linearly independent vectors. 2.2.2 Column Space of a Matrix The column space of a matrix is the set of all possible linear combinations of its columns. If you have a matrix \\(\\mathbf{A}\\) with \\(n\\) rows and \\(p\\) columns, the column space of \\(\\mathbf{A}\\), denoted as Col(\\(\\mathbf{A}\\)), consists of all vectors in \\(\\mathbb{R}^n\\) that can be expressed as a linear combination of the columns of \\(\\mathbf{A}\\). 2.2.2.1 Definition: Given a matrix \\(\\mathbf{A}\\) with columns \\(\\mathbf{a}_1, \\mathbf{a}_2, \\dots, \\mathbf{a}_p\\), the column space of \\(\\mathbf{A}\\) is defined as: \\[ \\text{Col}(\\mathbf{A}) = \\left\\{ \\mathbf{y} \\in \\mathbb{R}^n \\mid \\mathbf{y} = \\mathbf{A} \\mathbf{c} \\text{ for some } \\mathbf{c} \\in \\mathbb{R}^p \\right\\} \\] This means the column space is the span of the columns of \\(\\mathbf{A}\\), or equivalently, all vectors that can be written as \\(\\mathbf{y} = c_1 \\mathbf{a}_1 + c_2 \\mathbf{a}_2 + \\dots + c_p \\mathbf{a}_p\\), where \\(c_1, c_2, \\dots, c_p\\) are scalars. 2.2.2.2 Properties: The column space of \\(\\mathbf{A}\\) is a subspace of \\(\\mathbb{R}^n\\). The dimension of the column space of \\(\\mathbf{A}\\) is called the rank of the matrix and corresponds to the number of linearly independent columns in \\(\\mathbf{A}\\). The column space provides valuable information about the linear independence and span of the columns of a matrix. 2.2.2.3 Geometric Interpretation: In geometric terms, the column space represents the set of all possible vectors that can be “reached” by linearly combining the columns of the matrix. For example: - For a matrix with 2 columns in \\(\\mathbb{R}^3\\), the column space will be a plane in \\(\\mathbb{R}^3\\) if the columns are linearly independent. - For a matrix with 3 columns in \\(\\mathbb{R}^2\\), the column space will span all of \\(\\mathbb{R}^2\\) (if the columns are linearly independent) or a line (if they are dependent). 2.2.3 Rank of a Matrix The rank of a matrix is the dimension of its column space, which is the number of linearly independent columns in the matrix. Alternatively, it is also the dimension of the row space, which is the number of linearly independent rows. 2.2.3.1 Definition: For a matrix \\(\\mathbf{A}\\), the rank is defined as: \\[ \\text{rank}(\\mathbf{A}) = \\dim(\\text{Col}(\\mathbf{A})) = \\dim(\\text{Row}(\\mathbf{A})) \\] This is the maximum number of linearly independent rows or columns in the matrix. In other words, it tells you how many of the matrix’s columns (or rows) are not redundant and cannot be written as a linear combination of the others. 2.2.3.2 Key Points: The rank of a matrix \\(\\mathbf{A}\\) is denoted as rank(\\(\\mathbf{A}\\)). It measures the number of independent directions in the column space or row space. Full rank: A matrix is said to have full rank if its rank is equal to the smaller of the number of rows or columns. For an \\(m \\times n\\) matrix: If \\(\\text{rank}(\\mathbf{A}) = m\\) (number of rows), it has full row rank. If \\(\\text{rank}(\\mathbf{A}) = n\\) (number of columns), it has full column rank. Rank-deficient: If the rank of the matrix is less than the smaller of the number of rows or columns, the matrix is called rank-deficient, meaning that some of its rows or columns are linearly dependent. \\(\\text{rank}(\\mathbf{A}) = \\text{rank}(\\mathbf{A}&#39;) = \\text{rank}(\\mathbf{A}&#39; \\mathbf{A}) = \\text{rank}(\\mathbf{A}\\mathbf{A}&#39;)\\) 2.2.3.3 Example: Consider the matrix: \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix} \\] The rank of \\(\\mathbf{A}\\) is 2 because two of the rows (or columns) are linearly independent, but the third row (or column) is a linear combination of the others. 2.2.3.4 Properties: The rank of a matrix is always less than or equal to the minimum of the number of rows and columns: \\[ \\text{rank}(\\mathbf{A}) \\leq \\min(m, n) \\] The rank of a matrix is equal to the number of non-zero singular values in its singular value decomposition (SVD). In square matrices, the rank gives insight into whether the matrix is invertible. A square matrix is invertible if and only if it has full rank. 2.2.4 Full Rank Matrix A full rank matrix is a matrix in which the rank is equal to the largest possible value for that matrix, meaning: For an \\(m \\times n\\) matrix \\(A\\), the rank is the maximum number of linearly independent rows or columns. If the rank is equal to \\(m\\) (the number of rows), the matrix has full row rank. If the rank is equal to \\(n\\) (the number of columns), the matrix has full column rank. 2.2.4.1 For a square matrix (\\(m = n\\)): A square matrix is full rank if its rank is equal to its dimension, i.e., if the matrix is invertible. In this case, \\(\\text{rank}(\\mathbf{A}) = n\\), meaning all rows and columns are linearly independent, and the matrix has an inverse. 2.2.4.2 For a rectangular matrix (\\(m \\neq n\\)): A matrix is full rank if the rank equals the smaller of the number of rows or columns. For an \\(m \\times n\\) matrix, the rank is at most \\(\\min(m, n)\\). If the matrix has full row rank, all rows are linearly independent. If the matrix has full column rank, all columns are linearly independent. 2.2.4.3 Example: Consider the matrix: \\[ \\mathbf{A}= \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\] This is a \\(2 \\times 3\\) matrix. Since its two rows are linearly independent, it has full row rank, with rank = 2 (the number of rows). However, it does not have full column rank because it has only two independent rows for three columns. 2.2.4.4 Key Properties: A full rank matrix has no redundant rows or columns (no row or column can be written as a linear combination of others). A square matrix with full rank is invertible (non-singular). For a rectangular matrix, full rank implies the matrix has maximal independent information in terms of its rows or columns. 2.2.4.5 Importance: Full rank matrices are crucial in solving systems of linear equations. A system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) has a unique solution if \\(\\mathbf{A}\\) is a square, full rank matrix. In linear algebra and machine learning, the rank provides insight into the dimensionality and the independence of the data or transformation matrix. 2.2.5 Inverse Matrix An inverse matrix of a square matrix \\(\\mathbf{A}\\), denoted as \\(\\mathbf{A}^{-1}\\), is a matrix that, when multiplied by \\(\\mathbf{A}\\), results in the identity matrix \\(I\\). This relationship is expressed as: \\[ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A}= \\mathbf{I} \\] where \\(\\mathbf{I}\\) is the identity matrix, and its diagonal elements are 1, with all off-diagonal elements being 0. 2.2.5.1 Conditions for a Matrix to Have an Inverse: The matrix \\(\\mathbf{A}\\) must be square, meaning it has the same number of rows and columns. The matrix \\(\\mathbf{A}\\) must be non-singular, meaning its determinant is non-zero (\\(|\\mathbf{A}| \\neq 0\\)). 2.2.5.2 Properties of the Inverse Matrix: Uniqueness: If a matrix has an inverse, it is unique. Inverse of a Product: The inverse of the product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is given by \\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}\\). Inverse of the Inverse: \\((\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\\). Transpose of the Inverse: \\((\\mathbf{A}^{-1})&#39; = (\\mathbf{A}&#39;)^{-1}\\). 2.2.5.3 Special Case 2 by 2 Matrix For a \\(2 \\times 2\\) matrix: \\[ \\mathbf{A}= \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\] The inverse of \\(\\mathbf{A}\\) (if \\(|\\mathbf{A}|=\\det(\\mathbf{A}) \\neq 0\\)) is: \\[ A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix} \\] where \\(ad - bc\\) is the determinant of the matrix \\(\\mathbf{A}\\). 2.2.5.4 Special Case 2 by 2 Block Matrix The inverse of a \\(2 \\times 2\\) block matrix can be expressed under certain conditions. Let’s consider a block matrix \\(\\mathbf{M}\\) of the form: \\[ \\mathbf{M} = \\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{B} \\\\ \\mathbf{C} &amp; \\mathbf{D} \\end{bmatrix} \\] where: - \\(\\mathbf{A}\\) and \\(\\mathbf{D}\\) are themselves square matrices, and \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) are matrices (not necessarily square). Then the inverse of \\(\\mathbf{M}\\) is given by: \\[ \\mathbf{M}^{-1} = \\begin{bmatrix} \\mathbf{A}^{-1} + \\mathbf{A}^{-1} \\mathbf{B} \\mathbf{S}^{-1} \\mathbf{C} \\mathbf{A}^{-1} &amp; -\\mathbf{A}^{-1} \\mathbf{B} \\mathbf{S}^{-1} \\\\ -\\mathbf{S}^{-1} \\mathbf{C} \\mathbf{A}^{-1} &amp; \\mathbf{S}^{-1} \\end{bmatrix} \\] where \\(\\mathbf{S}\\) is the Schur complement of \\(\\mathbf{A}\\) and is defined as: \\[ \\mathbf{S} = \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} \\] 2.2.5.4.1 Conditions for the Inverse to Exist: \\(\\mathbf{A}\\) must be invertible, The Schur complement \\(\\mathbf{S}\\) must also be invertible. 2.2.5.4.2 Explanation of the Terms: \\(\\mathbf{A}^{-1}\\): The inverse of matrix \\(\\mathbf{A}\\), \\(\\mathbf{S}^{-1}\\): The inverse of the Schur complement \\(\\mathbf{S}\\), which can be interpreted as the “effective” part of matrix \\(\\mathbf{D}\\) once the contribution of \\(\\mathbf{A}\\) has been removed. This formula generalizes the concept of inverting a matrix when it’s partitioned into blocks. 2.2.5.5 Sherman-Morrison Formula The Sherman-Morrison formula provides a way to compute the inverse of a matrix after it has been updated by a rank-one modification. Specifically, it addresses the situation where a matrix A has been updated by the outer product of two vectors u and v. The formula is: \\[ (\\mathbf{A} + \\mathbf{u} \\mathbf{v}^T)^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1} \\mathbf{u} \\mathbf{v}^T \\mathbf{A}^{-1}}{1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{u}} \\] 2.2.5.5.1 Requirements: A must be an invertible matrix. The scalar \\(1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{u}\\) must not be zero. 2.2.5.5.2 Explanation of the terms: A is an invertible \\(n \\times n\\) matrix. u and v are \\(n \\times 1\\) column vectors. The outer product \\(\\mathbf{u} \\mathbf{v}^T\\) is an \\(n \\times n\\) rank-one matrix. The term \\(1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{u}\\) is a scalar. This formula is useful in situations where you need to efficiently update the inverse of a matrix after a low-rank modification, rather than recomputing the inverse from scratch. 2.2.6 Positive Definite Matrix A positive definite matrix is a symmetric matrix \\(\\mathbf{A}\\) where, for any non-zero vector \\(\\mathbf{x}\\), the following condition holds: \\[ \\mathbf{x}&#39; \\mathbf{A}\\mathbf{x} &gt; 0 \\] 2.2.6.1 Key Properties: Symmetry: The matrix \\(\\mathbf{A}\\) must be symmetric, meaning \\(\\mathbf{A}= \\mathbf{A}&#39;\\). Positive quadratic form: For any non-zero vector \\(\\mathbf{x}\\), the quadratic form \\(\\mathbf{x}&#39; \\mathbf{A}\\mathbf{x}\\) must yield a positive value. 2.2.6.2 Characteristics of a Positive Definite Matrix: All the eigenvalues of a positive definite matrix are positive. The determinants of the leading principal minors (submatrices) of the matrix are positive. The diagonal elements of a positive definite matrix are positive. \\(\\mathbf{A}\\) is invertible. \\(\\mathbf{A}^{-1}\\) is also positive definite matrix. 2.2.6.3 Example: The matrix: \\[ \\mathbf{A}= \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\] is positive definite, because for any non-zero vector \\(\\mathbf{x}\\), \\(\\mathbf{x}&#39; \\mathbf{A}\\mathbf{x} &gt; 0\\). For instance, if \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), then: \\[ \\mathbf{x}&#39; \\mathbf{A}\\mathbf{x} = \\begin{bmatrix} 1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 6 &gt; 0 \\] 2.2.7 Singular Value Decomposition Singular Value Decomposition (SVD) is a fundamental matrix factorization technique used in linear algebra to break down a matrix into three distinct components. It provides valuable insight into the structure of a matrix and is widely used in applications like data compression, signal processing, and dimensionality reduction. 2.2.7.1 Definition: For any real (or complex) matrix \\(\\mathbf{A}\\) of size \\(m \\times n\\), the Singular Value Decomposition is given by: \\[ \\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39; \\] where: - \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, whose columns are called the left singular vectors. - \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) diagonal matrix, where the diagonal entries are the singular values of \\(\\mathbf{A}\\). The singular values are always non-negative and arranged in decreasing order. - \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix, whose columns are called the right singular vectors. 2.2.7.2 Interpretation of the Components: \\(\\mathbf{U}\\) represents the orthonormal basis for the column space of \\(\\mathbf{A}\\). \\(\\mathbf{V}\\) represents the orthonormal basis for the row space of \\(\\mathbf{A}\\). \\(\\mathbf{\\Sigma}\\) contains the singular values, which provide information about the importance or magnitude of the corresponding singular vectors. Large singular values indicate directions with significant data spread, while small or zero singular values correspond to directions with little or no data variation. 2.2.7.3 Geometric Interpretation: SVD can be viewed geometrically as a transformation where: 1. \\(\\mathbf{V}\\) applies a rotation or reflection in the input space. 2. \\(\\mathbf{\\Sigma}\\) stretches or compresses the data along certain axes. 3. \\(\\mathbf{U}\\) applies a final rotation or reflection in the output space. 2.2.7.4 Key Points: Rank: The number of non-zero singular values in \\(\\mathbf{\\Sigma}\\) equals the rank of the matrix \\(\\mathbf{A}\\). Dimensionality Reduction: By truncating small singular values in \\(\\mathbf{\\Sigma}\\), we can approximate \\(\\mathbf{A}\\) with a lower-rank matrix, which is useful in compressing data while retaining most of its structure. Condition Number: The ratio of the largest to the smallest non-zero singular value gives the condition number of the matrix, which indicates how sensitive a matrix is to numerical errors or perturbations. 2.2.7.5 Example: For a matrix \\(\\mathbf{A}\\) of size \\(3 \\times 2\\), the SVD would look like: \\[ \\mathbf{A} = \\mathbf{U} \\begin{bmatrix} \\sigma_1 &amp; 0 \\\\ 0 &amp; \\sigma_2 \\\\ 0 &amp; 0 \\end{bmatrix} \\mathbf{V}&#39; \\] where \\(\\sigma_1\\) and \\(\\sigma_2\\) are the singular values, and \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthogonal matrices. 2.2.7.6 Applications of SVD: Dimensionality Reduction: SVD is widely used in Principal Component Analysis (PCA) for reducing the dimensionality of large datasets. Low-Rank Approximations: In data compression, SVD helps to approximate matrices with fewer dimensions while maintaining the core structure. Solving Linear Systems: In cases where a matrix is close to singular, SVD can be used to solve linear systems more stably. Latent Semantic Analysis (LSA): In natural language processing, SVD is used to reduce the dimensionality of word-document matrices to capture latent relationships between words and documents. 2.2.8 Eigendecomposition Eigendecomposition is a matrix factorization technique used in linear algebra, where a square matrix is decomposed into its eigenvalues and eigenvectors. It is applicable to square matrices and provides deep insight into the matrix’s structure, particularly in understanding transformations, systems of linear equations, and differential equations. 2.2.8.1 Definition: Given a square matrix \\(\\mathbf{A}\\) of size \\(n \\times n\\), eigendecomposition is a factorization of the matrix into the following form: \\[ \\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1} \\] where: - \\(\\mathbf{V}\\) is the matrix of eigenvectors of \\(\\mathbf{A}\\), and each column of \\(\\mathbf{V}\\) is an eigenvector. - \\(\\mathbf{\\Lambda}\\) is a diagonal matrix of eigenvalues of \\(\\mathbf{A}\\), with each diagonal element corresponding to an eigenvalue of \\(\\mathbf{A}\\). - \\(\\mathbf{V}^{-1}\\) is the inverse of the matrix of eigenvectors. 2.2.8.2 Eigenvalues and Eigenvectors: Eigenvalue (\\(\\lambda\\)): A scalar \\(\\lambda\\) is an eigenvalue of \\(\\mathbf{A}\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that: \\[ \\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v} \\] In this case, \\(\\mathbf{v}\\) is called the eigenvector corresponding to the eigenvalue \\(\\lambda\\). Eigenvector: A non-zero vector \\(\\mathbf{v}\\) that remains parallel to itself (i.e., only scaled) when multiplied by \\(\\mathbf{A}\\) is called an eigenvector. 2.2.8.3 Conditions for Eigendecomposition: A matrix \\(\\mathbf{A}\\) is diagonalizable (i.e., it can be factored into \\(\\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1}\\)) if and only if it has \\(n\\) linearly independent eigenvectors. Not all matrices are diagonalizable. However, if \\(\\mathbf{A}\\) has \\(n\\) distinct eigenvalues, then it is guaranteed to be diagonalizable. Symmetric matrices are always diagonalizable. 2.2.8.4 Geometric Interpretation: Eigendecomposition reveals the directions (eigenvectors) along which the matrix transformation \\(\\mathbf{A}\\) acts as a simple scaling by the eigenvalues. Geometrically: - Eigenvectors point in directions that remain invariant under the transformation by \\(\\mathbf{A}\\). - The corresponding eigenvalues tell us how much the matrix stretches or compresses vectors in the direction of those eigenvectors. 2.2.8.5 Example: For a matrix \\(\\mathbf{A}\\): \\[ \\mathbf{A} = \\begin{bmatrix} 4 &amp; 1 \\\\ 2 &amp; 3 \\end{bmatrix} \\] The eigenvalues \\(\\lambda_1 = 5\\) and \\(\\lambda_2 = 2\\) can be found by solving the characteristic equation \\(\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0\\). Corresponding eigenvectors can then be computed, allowing the matrix to be diagonalized as: \\[ \\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1} \\] where \\(\\mathbf{\\Lambda} = \\text{diag}(5, 2)\\) and \\(\\mathbf{V}\\) is the matrix of eigenvectors. 2.2.8.6 Applications of Eigendecomposition: Diagonalization: Eigendecomposition allows matrices to be diagonalized, simplifying many computations (such as raising matrices to powers). Principal Component Analysis (PCA): In data science, eigendecomposition is used in PCA to find directions of maximum variance in data. Solving Differential Equations: Eigenvalues and eigenvectors are useful in solving systems of linear differential equations. Quantum Mechanics: In physics, eigenvalues and eigenvectors describe the measurable properties (like energy levels) of systems. In summary, eigendecomposition is a powerful tool in linear algebra that provides insight into how a matrix transforms space, offering valuable properties through its eigenvalues and eigenvectors. 2.2.9 Idempotent Matrix An idempotent matrix is a matrix that, when multiplied by itself, yields the same matrix. In other words, a matrix \\(\\mathbf{M}\\) is idempotent if it satisfies the condition: \\[ \\mathbf{M}^2 = \\mathbf{M} \\] 2.2.9.1 Key Properties of Idempotent Matrices: Eigenvalues: The eigenvalues of an idempotent matrix are either 0 or 1. This is because for an eigenvector \\(\\mathbf{v}\\) with eigenvalue \\(\\lambda\\), the equation \\(\\mathbf{M}^2 \\mathbf{v} = \\mathbf{M} \\mathbf{v}\\) simplifies to \\(\\lambda^2 \\mathbf{v} = \\lambda \\mathbf{v}\\), meaning \\(\\lambda(\\lambda - 1) = 0\\), so \\(\\lambda = 0\\) or \\(\\lambda = 1\\). Rank: The rank of an idempotent matrix \\(\\mathbf{M}\\) is equal to the trace of the matrix (the sum of the diagonal elements), which is also the number of eigenvalues equal to 1. Projection Interpretation: Idempotent matrices often represent projection matrices in linear algebra. A projection matrix projects vectors onto a subspace, and applying the projection multiple times doesn’t change the result beyond the first application, which is why it satisfies \\(\\mathbf{M}^2 = \\mathbf{M}\\). 2.2.9.2 Examples: Identity Matrix: The identity matrix \\(\\mathbf{I}\\) is idempotent because: \\[ \\mathbf{I}^2 = \\mathbf{I} \\] Zero Matrix: The zero matrix \\(\\mathbf{0}\\) is also idempotent because: \\[ \\mathbf{0}^2 = \\mathbf{0} \\] Projection Matrix: Consider a projection matrix onto the x-axis in 2D: \\[ \\mathbf{P} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\] This matrix is idempotent since: \\[ \\mathbf{P}^2 = \\mathbf{P} \\] 2.2.9.3 Use in Statistics: Idempotent matrices are commonly used in statistics, particularly in the context of regression analysis. For example, the hat matrix \\(\\mathbf{H}\\) in linear regression, which transforms the observed values into the predicted values, is idempotent: \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\] where \\(\\mathbf{X}\\) is the design matrix. In summary, idempotent matrices have unique properties and are frequently encountered in linear algebra, projections, and statistical applications. 2.2.10 Determinant of a Matrix The determinant of a matrix is a scalar value that provides important information about the properties of a square matrix. It is a fundamental concept in linear algebra, often used to determine whether a matrix is invertible, as well as to describe geometric transformations and volume scaling. 2.2.10.1 Key Charachteristics of the Determinant: Square Matrices Only: The determinant is only defined for square matrices (i.e., matrices with the same number of rows and columns). Invertibility: A matrix is invertible (i.e., it has an inverse) if and only if its determinant is non-zero. If the determinant is zero, the matrix is singular and does not have an inverse. Geometric Interpretation: The determinant represents the scaling factor of the linear transformation described by the matrix. For example, in two or three dimensions, the determinant tells you how much the matrix scales area or volume: A determinant of 1 means the matrix preserves the area (in 2D) or volume (in 3D). A determinant greater than 1 means the matrix scales the area or volume by that factor. A negative determinant indicates that the transformation also includes a reflection. Significance of Zero Determinant: If the determinant is zero, it means that the matrix maps some vectors to a lower-dimensional space. For instance, in 2D, it might map points onto a line, collapsing the area to zero. 2.2.10.2 Definition: For a 2x2 matrix: \\[ \\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\] The determinant of \\(\\mathbf{A}\\) is: \\[ \\det(\\mathbf{A}) = ad - bc \\] This formula gives the area scaling factor for the linear transformation represented by the matrix \\(\\mathbf{A}\\). For a 3x3 matrix: \\[ \\mathbf{B} = \\begin{bmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{bmatrix} \\] The determinant of \\(\\mathbf{B}\\) is: \\[ \\det(\\mathbf{B}) = a(ei - fh) - b(di - fg) + c(dh - eg) \\] This formula can be extended to higher dimensions using recursive expansion (called cofactor expansion or Laplace expansion). 2.2.10.3 Applications of the Determinant: Solving Linear Systems: The determinant is used in Cramer’s Rule, a method for solving systems of linear equations. If the determinant of the coefficient matrix is non-zero, the system has a unique solution. Eigenvalues and Eigenvectors: The determinant plays a key role in computing the eigenvalues of a matrix. The determinant of a matrix \\(\\mathbf{A} - \\lambda \\mathbf{I}\\), where \\(\\lambda\\) is a scalar and \\(\\mathbf{I}\\) is the identity matrix, gives the characteristic equation whose solutions are the eigenvalues. Volume and Area Calculations: In geometry, the determinant helps calculate the area (in 2D) or volume (in 3D) of a region after applying a linear transformation. Singular Value Decomposition (SVD) and Principal Component Analysis (PCA): The determinant is important in these techniques for understanding data structures, transformations, and dimensionality reduction. 2.2.10.4 Properties of the Determinant For a general block matrix of the form: \\[ \\mathbf{M} = \\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{B} \\\\ \\mathbf{C} &amp; \\mathbf{D} \\end{bmatrix} \\] where \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), \\(\\mathbf{C}\\), and \\(\\mathbf{D}\\) are submatrices, the determinant formula is more complex. If \\(\\mathbf{D}\\) is invertible, we can use the Schur complement to compute the determinant: \\[ \\det(\\mathbf{M}) = \\det(\\mathbf{D}) \\det(\\mathbf{A} - \\mathbf{B} \\mathbf{D}^{-1} \\mathbf{C}) \\] Here, \\(\\mathbf{A} - \\mathbf{B} \\mathbf{D}^{-1} \\mathbf{C}\\) is called the Schur complement of \\(\\mathbf{D}\\) in \\(\\mathbf{M}\\). For any invertible matrix \\(\\mathbf{X}\\in \\mathbb{R}^{p \\times p}\\) and matrices \\(\\mathbf{A}\\in\\mathbb{R}^{p\\times q}\\) and \\(\\mathbf{B}\\in\\mathbb{R}^{q\\times p}\\) we have, that: \\[\\det(\\mathbf{X}+ \\mathbf{A}\\mathbf{B}) = det(\\mathbf{X})\\det(\\mathbf{I}_q + \\mathbf{B}\\mathbf{X}^{-1}\\mathbf{A}) \\] 2.3 Calculus Key calculus topics include: Gradient Hessian Matrix Calculus Optimization 2.3.1 Gradient The gradient of a function is a vector that contains the partial derivatives of the function with respect to each of its variables. It points in the direction of the steepest ascent of the function, and its magnitude indicates the rate of change in that direction. For a scalar function \\(f(x_1, x_2, \\ldots, x_n)\\), where \\(x_1, x_2, \\ldots, x_n\\) are the variables, the gradient is defined as: \\[ \\nabla f = \\frac{d}{d \\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} \\] 2.3.1.1 Key Points: Direction: The gradient points in the direction of the greatest increase of the function. Magnitude: The magnitude of the gradient represents how fast the function increases in that direction. Zero Gradient: If \\(\\nabla f = 0\\), it indicates that the function has a critical point, which could be a local minimum, maximum, or saddle point. 2.3.1.2 Example: For a function \\(f(x, y) = x^2 + y^2\\), the gradient is: \\[ \\nabla f = \\begin{bmatrix} \\frac{\\partial}{\\partial x} (x^2 + y^2) \\\\ \\frac{\\partial}{\\partial y} (x^2 + y^2) \\end{bmatrix} = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix} \\] This shows that the gradient points outward from the origin, and its magnitude increases as \\(x\\) and \\(y\\) increase. 2.3.1.3 Applications: In optimization, the gradient is used to find the minimum or maximum of a function (e.g., in gradient descent, a common optimization algorithm). In vector calculus, the gradient is used to describe the slope or rate of change of scalar fields (such as temperature, pressure, or altitude in physical applications). 2.3.2 Hessian Matrix The Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function. It describes the local curvature of a multivariable function and is used to assess the nature of critical points (i.e., whether they are minima, maxima, or saddle points). For a scalar function \\(f(x_1, x_2, \\ldots, x_n)\\), the Hessian matrix \\(\\mathbf{H}\\) is defined as: \\[ \\mathbf{H}(f) = \\frac{d}{d \\mathbf{x} d\\mathbf{x}&#39;} f(\\mathbf{x}) =\\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} \\] 2.3.2.1 Key Properties: The Hessian is symmetric if the second-order partial derivatives are continuous (by Clairaut’s theorem, also called Schwarz’s theorem). It provides important information about the local behavior of the function, particularly around critical points where the gradient is zero. Eigenvalues of the Hessian matrix determine the type of critical points: If all eigenvalues are positive, the function has a local minimum. If all eigenvalues are negative, the function has a local maximum. If some eigenvalues are positive and others are negative, the function has a saddle point. 2.3.2.2 Example: For a function \\(f(x, y) = x^2 + xy + y^2\\), the Hessian matrix is: \\[ \\mathbf{H}(f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} &amp; \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} &amp; \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\] 2.3.3 Applications: In optimization, the Hessian is used to assess the convexity or concavity of a function, which helps in identifying the nature of critical points. In machine learning, it is used to optimize loss functions and can be part of second-order optimization methods like Newton’s method. In economics and engineering, the Hessian helps in analyzing systems involving multiple variables and understanding how they interact with each other. 2.3.4 Matrix Calculus You need to know the following matrix calculus operations: \\[ \\frac{d}{d \\mathbf{x}} \\left(\\mathbf{c}&#39;\\mathbf{x}\\right) \\] \\[ \\frac{d}{d \\mathbf{x}} \\left(\\mathbf{x}&#39;\\mathbf{A}\\mathbf{x}\\right) \\] \\[ \\frac{d}{d \\mathbf{x} d\\mathbf{x}&#39;} \\left(\\mathbf{x}&#39;\\mathbf{A}\\mathbf{x}\\right) \\] Let \\(\\mathbf{c}\\) be a constant vector and \\(\\mathbf{x}\\) be a variable vector, both of size \\(n \\times 1\\). We want to compute the derivative of the product: \\[ f(\\mathbf{x}) = \\mathbf{c}&#39; \\mathbf{x} \\] Where: \\[ \\mathbf{c}&#39; \\mathbf{x} = \\sum_{i=1}^{n} c_i x_i \\] To differentiate \\(f(\\mathbf{x}) = \\mathbf{c}&#39; \\mathbf{x}\\) with respect to the variable vector \\(\\mathbf{x}\\), we take the derivative of each component separately: \\[ \\nabla f = \\frac{d}{d \\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} (\\mathbf{c}&#39; \\mathbf{x}) \\\\ \\frac{\\partial }{\\partial x_2} (\\mathbf{c}&#39; \\mathbf{x}) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} (\\mathbf{c}&#39; \\mathbf{x}) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} \\left(\\sum_{i=1}^{n} c_i x_i\\right) \\\\ \\frac{\\partial }{\\partial x_2} \\left(\\sum_{i=1}^{n} c_i x_i\\right) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} \\left(\\sum_{i=1}^{n} c_i x_i\\right) \\end{bmatrix} \\] Since \\(\\mathbf{c}\\) is a constant vector, the derivative of each term \\(c_i x_i\\) is simply \\(c_i\\), that is: \\[ \\frac{d}{d x_j} \\left(\\sum_{i=1}^{n} c_i x_i\\right) = c_j \\] Thus, the derivative of the entire sum is the vector: \\[ \\frac{d}{d \\mathbf{x}} \\left( \\mathbf{c}&#39; \\mathbf{x} \\right) = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix} = \\mathbf{c} \\] Now, let’s go through the derivative of the quadratic form \\(f(\\mathbf{x}) = \\mathbf{x}&#39; \\mathbf{A}\\mathbf{x}\\), where: \\(\\mathbf{x}\\) is a variable vector of size \\(n \\times 1\\), \\(\\mathbf{A}\\) is a constant, symmetric matrix of size \\(n \\times n\\). \\[ f(\\mathbf{x}) = \\mathbf{x}&#39; \\mathbf{A}\\mathbf{x} \\] First, expand the quadratic form: \\[ f(\\mathbf{x}) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j \\] Then \\[ \\nabla f = \\frac{d}{d \\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} (\\mathbf{x}&#39; \\mathbf{A}\\mathbf{x}) \\\\ \\frac{\\partial }{\\partial x_2} (\\mathbf{x}&#39; \\mathbf{A}\\mathbf{x}) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} (\\mathbf{x}&#39; \\mathbf{A}\\mathbf{x}) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\frac{\\partial }{\\partial x_2} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\end{bmatrix} \\] For each component \\(x_k\\) in the vector \\(\\mathbf{x}\\), the derivative of \\(f(\\mathbf{x})\\) is: \\[ \\frac{\\partial}{\\partial x_k} f(\\mathbf{x}) = \\frac{\\partial}{\\partial x_k} \\left( \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j \\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\frac{\\partial}{\\partial x_k} x_i a_{ij} x_j \\] Each term \\(x_i a_{ij} x_j\\) has two components that depend on \\(\\mathbf{x}\\): If \\(i = j = k\\), the derivative with respect to \\(x_k\\) is: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = 2 a_{kk} x_k \\] If \\(i \\neq j\\) and \\(i = k\\), the derivative with respect to \\(x_k\\) is: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = a_{kj} x_j \\] - Similarly, if \\(i \\neq j\\) and \\(j = k\\), the derivative with respect to \\(x_k\\) is: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = a_{ik} x_i \\] - Finally, if \\(i \\neq k\\) and \\(j \\neq k\\), then: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = 0 \\] Then \\[ \\frac{\\partial}{\\partial x_k} f(\\mathbf{x}) = 2 a_{kk} x_k + \\sum_{i \\neq k} a_{ik} x_i + \\sum_{j \\neq k} a_{kj} x_j \\] Now since \\(\\mathbf{A}\\) is symmetric (\\(a_{ij} = a_{ji}\\)), then: \\[\\begin{align*} \\frac{\\partial}{\\partial x_k} f(\\mathbf{x}) &amp;= 2 a_{kk} x_k + \\sum_{i \\neq k} a_{ik} x_i + \\sum_{i \\neq k} a_{ik} x_i \\\\ &amp;= 2 a_{kk} x_k + 2\\sum_{i \\neq k} a_{ik} x_i \\\\ &amp;= 2 \\left(\\sum_{i \\neq k} a_{ik} x_i + a_{kk}x_k \\right) \\\\ &amp;= 2 \\left(\\sum_{i = 1}^n a_{ki} x_i\\right) \\end{align*}\\] Then: \\[ \\nabla f = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\frac{\\partial }{\\partial x_2} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\end{bmatrix} = \\begin{bmatrix} 2 \\sum_{i = 1}^n a_{1i} x_i \\\\ 2 \\sum_{i = 1}^n a_{2i} x_i \\\\ \\vdots \\\\ 2 \\sum_{i = 1}^n a_{ni} x_i \\end{bmatrix} = 2 \\mathbf{A}\\mathbf{x} \\] Finally for the second derivative we have that: In general, the Hessian matrix of a scalar function \\(f(\\mathbf{x})\\), where \\(\\mathbf{x} \\in \\mathbb{R}^n\\) is a vector of variables, is a matrix that contains all the second-order partial derivatives of the function. It is defined as: \\[ \\mathbf{H}(f) = \\frac{d^2 f}{d\\mathbf{x}d\\mathbf{x}&#39;} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1}\\left(\\frac{d f}{d\\mathbf{x}}\\right)&#39; \\\\ \\frac{\\partial }{\\partial x_2}\\left(\\frac{d f}{d\\mathbf{x}}\\right)&#39; \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n}\\left(\\frac{d f}{d\\mathbf{x}}\\right)&#39; \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1}\\left(2\\mathbf{A}\\mathbf{x}\\right)&#39; \\\\ \\frac{\\partial }{\\partial x_2}\\left(2\\mathbf{A}\\mathbf{x}\\right)&#39; \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n}\\left(2\\mathbf{A}\\mathbf{x}\\right)&#39; \\end{bmatrix} \\] Now \\[ \\frac{\\partial }{\\partial x_k}\\left(2\\mathbf{A}\\mathbf{x}\\right) = 2\\frac{\\partial }{\\partial x_k}\\begin{bmatrix} \\sum_{i = 1}^n a_{1i} x_i \\\\ \\sum_{i = 1}^n a_{2i} x_i \\\\ \\vdots \\\\ \\sum_{i = 1}^n a_{ni} x_i \\end{bmatrix} = 2 \\begin{bmatrix} \\frac{\\partial }{\\partial x_k} \\left(\\sum_{i = 1}^n a_{1i} x_i \\right)\\\\ \\frac{\\partial }{\\partial x_k} \\left(\\sum_{i = 1}^n a_{2i} x_i \\right)\\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_k} \\left(\\sum_{i = 1}^n a_{ni} x_i \\right) \\end{bmatrix} = 2 \\begin{bmatrix} a_{k1} \\\\ a_{k2} \\\\ \\vdots \\\\ a_{kn} \\end{bmatrix} \\] Then \\[ \\mathbf{H}(f) = \\frac{d^2 f}{d\\mathbf{x}d\\mathbf{x}&#39;} = \\begin{bmatrix} 2\\begin{bmatrix} a_{11} \\\\ a_{12} \\\\ \\vdots \\\\ a_{1n} \\end{bmatrix}&#39; \\\\ 2\\begin{bmatrix} a_{21} \\\\ a_{22} \\\\ \\vdots \\\\ a_{2n} \\end{bmatrix}&#39; \\\\ \\vdots \\\\ 2\\begin{bmatrix} a_{n1} \\\\ a_{n2} \\\\ \\vdots \\\\ a_{nn} \\end{bmatrix}&#39; \\end{bmatrix} = 2\\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} \\end{bmatrix} = 2 \\mathbf{A} \\] 2.4 Probability Key probability concepts to understand include: Expected Value Variance Cross-Covariance Matrix Multivariate Normal Distribution 2.4.1 Expected Value The expected value (or mean) of a random vector is a fundamental concept in multivariate statistics. Just as the expected value of a random variable provides a measure of the “center” or “average” of the distribution, the expected value of a random vector captures the central location of a multidimensional distribution. 2.4.1.1 Definition Let \\(\\mathbf{X}\\) be a random vector in \\(\\mathbb{R}^n\\), represented as: \\[ \\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}, \\] where \\(X_1, X_2, \\dots, X_n\\) are its components. The expected value of \\(\\mathbf{X}\\), denoted by \\(\\mathbb{E}[\\mathbf{X}]\\), is defined as the vector of the expected values of each component: \\[ \\mathbb{E}[\\mathbf{X}] = \\begin{bmatrix} \\mathbb{E}[X_1] \\\\ \\mathbb{E}[X_2] \\\\ \\vdots \\\\ \\mathbb{E}[X_n] \\end{bmatrix}. \\] This vector \\(\\mathbb{E}[\\mathbf{X}]\\) is also called the mean vector of \\(\\mathbf{X}\\). 2.4.1.2 Key Properties of the Expected Value of a Random Vector Linearity: For any scalar \\(a\\) and random vectors \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), \\[ \\mathbb{E}[a\\mathbf{X} + \\mathbf{Y}] = a\\mathbb{E}[\\mathbf{X}] + \\mathbb{E}[\\mathbf{Y}]. \\] Expectation of a Constant Vector: If \\(\\mathbf{c}\\) is a constant vector in \\(\\mathbb{R}^n\\), then the expectation is simply the vector itself: \\[ \\mathbb{E}[\\mathbf{c}] = \\mathbf{c}. \\] Expectation of a Linear Transformation: If \\(\\mathbf{A}\\) is an \\(m \\times n\\) constant matrix, then for a random vector \\(\\mathbf{X}\\) in \\(\\mathbb{R}^n\\), \\[ \\mathbb{E}[\\mathbf{A} \\mathbf{X}] = \\mathbf{A} \\mathbb{E}[\\mathbf{X}]. \\] 2.4.2 Variance 2.4.2.1 Definition Let \\(\\mathbf{X}\\) be a random vector in \\(\\mathbb{R}^n\\), represented as: \\[ \\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}, \\] where each \\(X_i\\) is a random variable. The variance-covariance matrix of \\(\\mathbf{X}\\), denoted by \\(\\mathbb{V}(\\mathbf{X})\\) or \\(\\boldsymbol{\\Sigma}\\), is defined as: \\[ \\mathbb{V}(\\mathbf{X}) = \\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])&#39;]. \\] The \\((i, j)\\) entry of \\(\\mathbb{V}(\\mathbf{X})\\) is given by \\(\\mathbb{C}(X_i, X_j)\\), representing the covariance between components \\(X_i\\) and \\(X_j\\). If \\(\\mathbf{X}\\) has \\(n\\) components, \\(\\mathbb{V}(\\mathbf{X})\\) will be an \\(n \\times n\\) symmetric matrix where: - Diagonal entries represent the variances of each component, i.e., \\(\\mathbb{V}(X_i) = \\mathbb{C}(X_i, X_i)\\). - Off-diagonal entries represent the covariances between different components, i.e., \\(\\mathbb{C}(X_i, X_j)\\) for \\(i \\neq j\\). 2.4.2.2 Key Properties of the Variance-Covariance Matrix Symmetry: The variance-covariance matrix is symmetric, meaning: \\[ \\mathbb{V}(\\mathbf{X}) = \\mathbb{V}(\\mathbf{X})^T. \\] Non-negativity: The variance-covariance matrix is positive semi-definite, which implies: \\[ \\mathbf{z}^T \\mathbb{V}(\\mathbf{X}) \\mathbf{z} \\geq 0 \\quad \\text{for any vector } \\mathbf{z} \\in \\mathbb{R}^n. \\] This property indicates that variances (the diagonal entries) are always non-negative. Variance of a Linear Transformation: If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, then the variance of the transformed random vector \\(\\mathbf{A} \\mathbf{X}\\) is given by: \\[ \\mathbb{V}(\\mathbf{A} \\mathbf{X}) = \\mathbf{A} \\, \\mathbb{V}(\\mathbf{X}) \\, \\mathbf{A}^T. \\] Variance of Independent Random Variables: If the components of \\(\\mathbf{X}\\) are mutually independent, the off-diagonal entries of \\(\\mathbb{V}(\\mathbf{X})\\) (the covariances) are zero, resulting in a diagonal covariance matrix. Variance of the Sum of Two Arbitrary Random Vectors: If \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are two arbitrary random vectors in \\(\\mathbb{R}^n\\), the variance of their sum is given by: \\[ \\mathbb{V}(\\mathbf{X} + \\mathbf{Y}) = \\mathbb{V}(\\mathbf{X}) + \\mathbb{V}(\\mathbf{Y}) + 2 \\, \\mathbb{C}(\\mathbf{X}, \\mathbf{Y}), \\] where \\(\\mathbb{C}(\\mathbf{X}, \\mathbf{Y})\\) is the cross-covariance matrix between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\): \\[ \\mathbb{C}(\\mathbf{X}, \\mathbf{Y}) = \\mathbb{E}\\left[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])(\\mathbf{Y} - \\mathbb{E}[\\mathbf{Y}])&#39;\\right]. \\] Variance of the Sum of Two Independent Random Vectors: If \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are two independent random vectors in \\(\\mathbb{R}^n\\), then the variance of their sum is given by the sum of their individual variances: \\[ \\mathbb{V}(\\mathbf{X} + \\mathbf{Y}) = \\mathbb{V}(\\mathbf{X}) + \\mathbb{V}(\\mathbf{Y}). \\] Since \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are independent, their covariance \\(\\mathbb{C}(\\mathbf{X}, \\mathbf{Y})\\) is zero, simplifying the variance of the sum. Trace of the Variance: A related measure of the variance of a random vector \\(\\mathbf{X}\\), is given by \\[ \\text{tr}(\\mathbb{V}[\\mathbf{X}]) = \\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])&#39;(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])] \\] 2.4.3 Cross-Covariance Matrix The cross-covariance matrix measures the covariance between two random vectors, providing insights into the linear relationships between different components of these vectors. Given two random vectors \\(\\mathbf{X} \\in \\mathbb{R}^n\\) and \\(\\mathbf{Y} \\in \\mathbb{R}^m\\), the cross-covariance matrix between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) captures how each component of \\(\\mathbf{X}\\) varies with each component of \\(\\mathbf{Y}\\). 2.4.3.1 Definition Let \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) be two random vectors with mean vectors \\(\\mathbb{E}[\\mathbf{X}] = \\boldsymbol{\\mu}_{\\mathbf{X}}\\) and \\(\\mathbb{E}[\\mathbf{Y}] = \\boldsymbol{\\mu}_{\\mathbf{Y}}\\). The cross-covariance matrix of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) is defined as: \\[ \\mathbb{C}(\\mathbf{X}, \\mathbf{Y}) = \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu}_{\\mathbf{X}})(\\mathbf{Y} - \\boldsymbol{\\mu}_{\\mathbf{Y}})^T]. \\] This matrix is of dimension \\(n \\times m\\), where each element \\((\\mathbb{C}(\\mathbf{X}, \\mathbf{Y}))_{ij}\\) represents the covariance between the \\(i\\)-th component of \\(\\mathbf{X}\\) and the \\(j\\)-th component of \\(\\mathbf{Y}\\): \\[ (\\mathbb{C}(\\mathbf{X}, \\mathbf{Y}))_{ij} = \\mathbb{C}(X_i, Y_j) = \\mathbb{E}[(X_i - \\mu_{X_i})(Y_j - \\mu_{Y_j})]. \\] 2.4.3.2 Key Properties Symmetry in Covariance: If \\(\\mathbf{X} = \\mathbf{Y}\\), then \\(\\mathbb{C}(\\mathbf{X}, \\mathbf{Y})\\) reduces to the covariance matrix of \\(\\mathbf{X}\\), which is symmetric. For distinct vectors \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), the cross-covariance matrix \\(\\mathbb{C}(\\mathbf{X}, \\mathbf{Y})\\) is generally not symmetric. Relationship with Joint Covariance Matrix: If \\(\\mathbf{Z} = \\begin{bmatrix} \\mathbf{X} \\\\ \\mathbf{Y} \\end{bmatrix}\\) is a combined random vector, then the covariance matrix of \\(\\mathbf{Z}\\) is: \\[ \\mathbb{C}(\\mathbf{Z}) = \\begin{bmatrix} \\mathbb{C}(\\mathbf{X}) &amp; \\mathbb{C}(\\mathbf{X}, \\mathbf{Y}) \\\\ \\mathbb{C}(\\mathbf{Y}, \\mathbf{X}) &amp; \\mathbb{C}(\\mathbf{Y}) \\end{bmatrix}. \\] Linearity: If \\(a\\) and \\(b\\) are constants and \\(\\mathbf{X}_1\\) and \\(\\mathbf{X}_2\\) are random vectors of the same dimension as \\(\\mathbf{X}\\), then: \\[ \\mathbb{C}(a\\mathbf{X}_1 + b\\mathbf{X}_2, \\mathbf{Y}) = a \\mathbb{C}(\\mathbf{X}_1, \\mathbf{Y}) + b \\mathbb{C}(\\mathbf{X}_2, \\mathbf{Y}). \\] Furthermore, if \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are matrices of compatible dimensions, and \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are random vectors then the cross-covariance of \\(\\mathbf{A}\\mathbf{X}\\) and \\(\\mathbf{B}\\mathbf{Y}\\) is given by: \\[ \\mathbb{C}(\\mathbf{A}\\mathbf{X}, \\mathbf{B}\\mathbf{Y}) = \\mathbf{A} \\mathbb{C}(\\mathbf{X}, \\mathbf{Y}) \\mathbf{B}^T. \\] Zero Cross-Covariance and Independence: If \\(\\mathbb{C}(\\mathbf{X}, \\mathbf{Y}) = \\mathbf{0}\\), it implies that \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are uncorrelated, but it does not necessarily imply independence unless \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are jointly normally distributed. 2.4.4 Multivariate Normal Distribution The multivariate normal distribution generalizes the concept of the normal distribution to multiple dimensions, describing the behavior of random vectors whose elements are jointly normally distributed. It is widely used in statistics and machine learning due to its well-behaved properties and its applicability to modeling correlations between variables. 2.4.4.1 Definition A random vector \\(\\mathbf{X} = (X_1, X_2, \\dots, X_n)^T\\) is said to follow a multivariate normal distribution if it has a probability density function of the form: \\[ f(\\mathbf{X}) = \\frac{1}{(2\\pi)^{n/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{X} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{X} - \\boldsymbol{\\mu}) \\right), \\] where: \\(\\mathbf{X} \\in \\mathbb{R}^n\\) is the random vector, \\(\\boldsymbol{\\mu} \\in \\mathbb{R}^n\\) is the mean vector, \\(\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}\\) is the covariance matrix, assumed to be symmetric and positive definite. This distribution is denoted as \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\). 2.4.4.2 Key Properties Marginal Distributions: Any subset of the components of \\(\\mathbf{X}\\) is also normally distributed. If \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), then any partition of \\(\\mathbf{X}\\) results in a marginal distribution that is also multivariate normal. Affine Transformation: For a matrix \\(\\mathbf{A}\\) and vector \\(\\mathbf{b}\\) of appropriate dimensions, the affine transformation \\(\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{b}\\) also follows a multivariate normal distribution, \\(\\mathbf{Y} \\sim \\mathcal{N}(\\mathbf{A} \\boldsymbol{\\mu} + \\mathbf{b}, \\mathbf{A} \\boldsymbol{\\Sigma} \\mathbf{A}^T)\\). Conditional Distributions: For a partitioned vector \\(\\mathbf{X} = (\\mathbf{X}_1, \\mathbf{X}_2)^T\\), with corresponding partitions of the mean vector and covariance matrix: \\[ \\boldsymbol{\\mu} = \\begin{pmatrix} \\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2 \\end{pmatrix}, \\quad \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\boldsymbol{\\Sigma}_{11} &amp; \\boldsymbol{\\Sigma}_{12} \\\\ \\boldsymbol{\\Sigma}_{21} &amp; \\boldsymbol{\\Sigma}_{22} \\end{pmatrix}, \\] the conditional distribution \\(\\mathbf{X}_1 | \\mathbf{X}_2 = \\mathbf{x}_2\\) is also multivariate normal: \\[ \\mathbf{X}_1 | \\mathbf{X}_2 = \\mathbf{x}_2 \\sim \\mathcal{N}(\\boldsymbol{\\mu}_1 + \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1} (\\mathbf{x}_2 - \\boldsymbol{\\mu}_2), \\boldsymbol{\\Sigma}_{11} - \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1} \\boldsymbol{\\Sigma}_{21}). \\] Independence and Uncorrelatedness: For a multivariate normal distribution, zero covariance implies independence. If two components \\(X_i\\) and \\(X_j\\) (or two subvectors) have a covariance of zero, they are independent. Moment Generating Function: The moment generating function of \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) is: \\[ M_{\\mathbf{X}}(\\mathbf{t}) = \\exp\\left( \\mathbf{t}^T \\boldsymbol{\\mu} + \\frac{1}{2} \\mathbf{t}^T \\boldsymbol{\\Sigma} \\mathbf{t} \\right). \\] Maximum Entropy: Among all distributions with a given mean vector and covariance matrix, the multivariate normal distribution has the maximum entropy, making it the most “uninformative” or spread out. 2.4.5 \\(\\chi^2\\) Distribution The chi-squared distribution is a probability distribution that arises naturally in statistics, particularly in hypothesis testing and estimation problems. It is defined as the distribution of a sum of the squares of independent standard normal random variables. Formally, let \\(Z_1, Z_2, \\dots, Z_k\\) be \\(k\\) independent random variables, each following a standard normal distribution (mean \\(0\\) and variance \\(1\\)). Then, the sum of their squares: \\[ Q = Z_1^2 + Z_2^2 + \\cdots + Z_k^2 \\] follows a chi-squared distribution with \\(k\\) degrees of freedom. We denote this as: \\[ Q \\sim \\chi^2_k \\] 2.4.5.1 Key Properties of the Chi-Squared Distribution Degrees of Freedom (\\(k\\)): The parameter \\(k\\) determines the shape of the distribution. Larger \\(k\\) values result in a distribution that becomes more symmetric and approaches a normal distribution (as \\(k \\to \\infty\\)). Mean and Variance: Mean: \\(\\mathbb{E}[Q] = k\\) Variance: \\(\\mathbb{V}[Q] = 2k\\) Additivity: If \\(Q_1 \\sim \\chi^2_{k_1}\\) and \\(Q_2 \\sim \\chi^2_{k_2}\\) are independent, then \\(Q_1 + Q_2 \\sim \\chi^2_{k_1 + k_2}\\). Special Case: For \\(k = 1\\), the chi-squared distribution is simply the square of a standard normal variable, \\(Q = Z^2\\). The chi-squared distribution is widely used in statistical tests, such as the chi-squared test for independence or goodness-of-fit, and in the construction of confidence intervals for variances. 2.4.6 \\(t\\) Distribution The t-distribution, also known as Student’s t-distribution, is a probability distribution that arises frequently in hypothesis testing, particularly when the sample size is small, and the population standard deviation is unknown. It is defined as the distribution of a standard normal random variable divided by the square root of an independent chi-squared random variable, scaled by its degrees of freedom. 2.4.6.1 Definition Let: - \\(Z\\) be a standard normal random variable (\\(Z \\sim N(0, 1)\\)), - \\(Q\\) be a chi-squared random variable with \\(k\\) degrees of freedom (\\(Q \\sim \\chi^2_k\\)), independent of \\(Z\\). Then, the random variable: \\[ T = \\frac{Z}{\\sqrt{\\frac{Q}{k}}} \\] follows a t-distribution with \\(k\\) degrees of freedom. We write: \\[ T \\sim t_k \\] 2.4.6.2 Key Properties of the t-Distribution Degrees of Freedom (\\(k\\)): The parameter \\(k\\) determines the shape of the t-distribution. As \\(k\\) increases, the t-distribution approaches the standard normal distribution. Symmetry: The t-distribution is symmetric about zero, similar to the normal distribution. Tails: The t-distribution has heavier tails than the normal distribution, meaning it gives more probability to extreme values. This reflects increased uncertainty when estimating the population mean with small sample sizes. Moments: Mean: \\(\\mathbb{E}[T] = 0\\) for \\(k &gt; 1\\) Variance: \\(\\mathbb{V}[T] = \\frac{k}{k-2}\\) for \\(k &gt; 2\\) Higher moments exist only for \\(k &gt; m\\), where \\(m\\) is the order of the moment. 2.4.6.3 Applications The t-distribution is widely used in: 1. t-tests for hypothesis testing, such as testing means of small samples. 2. Constructing confidence intervals for a population mean when the population standard deviation is unknown. 3. Regression analysis, where it appears in tests for regression coefficients. The t-distribution plays a fundamental role in statistics, bridging the gap between small-sample and large-sample inference. 2.4.7 \\(F\\) Distribution The F-distribution, also known as Fisher-Snedecor distribution, arises frequently in statistics, especially in hypothesis testing and variance analysis (ANOVA). It is defined as the distribution of the ratio of two independent chi-squared random variables, each divided by their respective degrees of freedom. 2.4.7.1 Definition Let: - \\(Q_1 \\sim \\chi^2_{k_1}\\), a chi-squared random variable with \\(k_1\\) degrees of freedom, - \\(Q_2 \\sim \\chi^2_{k_2}\\), a chi-squared random variable with \\(k_2\\) degrees of freedom, - \\(Q_1\\) and \\(Q_2\\) are independent. Then, the random variable: \\[ F = \\frac{\\frac{Q_1}{k_1}}{\\frac{Q_2}{k_2}} \\] follows an F-distribution with \\(k_1\\) and \\(k_2\\) degrees of freedom. We write: \\[ F \\sim F(k_1, k_2) \\] 2.4.7.2 Key Properties of the F-Distribution Degrees of Freedom: The parameters \\(k_1\\) and \\(k_2\\) determine the shape of the F-distribution. \\(k_1\\) (numerator degrees of freedom) is associated with the variability of the first chi-squared variable. \\(k_2\\) (denominator degrees of freedom) is associated with the variability of the second chi-squared variable. Support: \\(F\\) is defined for \\(F \\geq 0\\). Asymmetry: The F-distribution is not symmetric; it is skewed to the right, especially for small degrees of freedom. As \\(k_1\\) and \\(k_2\\) increase, it approaches a normal distribution. Mean: \\(\\mathbb{E}[F] = \\frac{k_2}{k_2 - 2}\\) for \\(k_2 &gt; 2\\). Variance: \\(\\mathbb{V}[F] = \\frac{2k_2^2 (k_1 + k_2 - 2)}{k_1 (k_2 - 2)^2 (k_2 - 4)}\\) for \\(k_2 &gt; 4\\). Special Cases: When \\(k_1 = 1\\), the F-distribution is equivalent to the square of a t-distribution with \\(k_2\\) degrees of freedom: \\(F(1, k_2) = t_{k_2}^2\\). 2.4.7.3 Applications ANOVA (Analysis of Variance): The F-statistic is used to test whether multiple groups have the same variance or means. Model Comparison: In regression, the F-distribution is used to compare nested models, assessing whether additional predictors improve the fit of the model. Hypothesis Testing: The F-distribution appears in tests of equality of variances (Levene’s test or Bartlett’s test). The F-distribution is essential in statistics for comparing variances and testing model adequacy, making it a cornerstone of many inferential procedures. 2.5 Statistics Essential statistical concepts include: Bias of an Estimator Unbiased Estimator Mean Square Error of an Estimator Consistent Minimum Variance Interval Estimation Hypothesis Testing 2.5.1 Bias of an Estimator The bias of an estimator \\(\\hat{\\mathbf{\\theta}}\\) for a vector parameter \\(\\mathbf{\\theta}\\) is defined as the difference between the expected value of the estimator and the true value of the parameter. Formally, if \\(\\mathbf{\\theta} \\in \\mathbb{R}^n\\) is a vector parameter, then the bias of the estimator \\(\\hat{\\mathbf{\\theta}}\\) is: \\[ \\text{Bias}(\\hat{\\mathbf{\\theta}}) = \\mathbb{E}[\\hat{\\mathbf{\\theta}}] - \\mathbf{\\theta}. \\] 2.5.1.1 Key Points Interpretation: Bias measures how far, on average, the estimator \\(\\hat{\\mathbf{\\theta}}\\) is from the true parameter \\(\\mathbf{\\theta}\\). If the bias is zero, the estimator is said to be unbiased. Component-wise Bias: For each component of the estimator \\(\\hat{\\mathbf{\\theta}} = (\\hat{\\theta}_1, \\dots, \\hat{\\theta}_n)^T\\), the bias can be expressed as: \\[ \\text{Bias}(\\hat{\\theta}_i) = \\mathbb{E}[\\hat{\\theta}_i] - \\theta_i, \\quad \\text{for each } i = 1, \\dots, n. \\] This component-wise breakdown is helpful for understanding how each part of the vector estimator deviates from the true values. Total Bias: The total bias can be viewed as a vector, summarizing the systematic error in each dimension of the estimator. 2.5.1.2 Example In practice, if \\(\\hat{\\mathbf{\\theta}}\\) is the sample mean vector of a random vector \\(\\mathbf{X}\\), and \\(\\mathbf{\\theta} = \\mathbb{E}[\\mathbf{X}]\\), then \\(\\text{Bias}(\\hat{\\mathbf{\\theta}})\\) will be zero, making the sample mean an unbiased estimator of the population mean. 2.5.2 Unbiased Estimator An unbiased estimator of a vector parameter is an estimator that, on average, accurately estimates the true value of the parameter. Formally, let \\(\\mathbf{\\theta} \\in \\mathbb{R}^n\\) be a vector parameter of interest, and let \\(\\hat{\\mathbf{\\theta}}\\) be an estimator of \\(\\mathbf{\\theta}\\). Then, \\(\\hat{\\mathbf{\\theta}}\\) is an unbiased estimator of \\(\\mathbf{\\theta}\\) if the following condition holds: \\[ \\mathbb{E}[\\hat{\\mathbf{\\theta}}] = \\mathbf{\\theta} \\quad \\forall \\boldsymbol{\\theta}\\in \\mathbb{R}^n. \\] 2.5.2.1 Key Points Component-wise Unbiasedness: Each component of \\(\\hat{\\mathbf{\\theta}}\\), say \\(\\hat{\\theta}_i\\), must be an unbiased estimator of the corresponding component \\(\\theta_i\\) of \\(\\mathbf{\\theta}\\): \\[ \\mathbb{E}[\\hat{\\theta}_i] = \\theta_i, \\quad \\text{for all } i = 1, \\dots, n. \\] No Systematic Bias: Unbiasedness implies that, on average, the estimator neither overestimates nor underestimates the true value of \\(\\mathbf{\\theta}\\) across repeated sampling. Applications: Unbiased estimators are crucial in statistics, as they provide estimations that are theoretically centered around the true parameter values, though they may vary due to sampling variation. An example is the sample mean \\(\\mathbf{\\bar{X}}\\) as an estimator for the population mean \\(\\mathbf{\\mu}\\) of a random vector \\(\\mathbf{X}\\), where \\(\\mathbb{E}[\\mathbf{\\bar{X}}] = \\mathbf{\\mu}\\). 2.5.3 Mean Square Error of an Estimator The Mean Square Error (MSE) of an estimator \\(\\hat{\\mathbf{\\theta}}\\) for a vector parameter \\(\\mathbf{\\theta}\\) measures the average squared deviation of the estimator from the true parameter. Formally, for a parameter vector \\(\\mathbf{\\theta} \\in \\mathbb{R}^n\\) and an estimator \\(\\hat{\\mathbf{\\theta}}\\), the MSE is defined as: \\[ \\mathbb{M}(\\hat{\\mathbf{\\theta}}) = \\mathbb{E} \\left[ \\|\\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}\\|^2 \\right], \\] where \\(\\|\\cdot\\|\\) denotes the Euclidean (or 2-norm) of a vector. Expanding this expression, we can write: \\[ \\mathbb{M}(\\hat{\\mathbf{\\theta}}) = \\mathbb{E} \\left[ (\\hat{\\mathbf{\\theta}} - \\mathbf{\\theta})^T (\\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}) \\right]. \\] 2.5.3.1 Key Components The MSE can be decomposed into two main parts: variance and squared bias. \\[ \\mathbb{M}(\\hat{\\mathbf{\\theta}}) = \\text{tr}(\\text{Var}(\\hat{\\mathbf{\\theta}})) + \\|\\text{Bias}(\\hat{\\mathbf{\\theta}})\\|^2, \\] where: - Variance: \\(\\text{Var}(\\hat{\\mathbf{\\theta}}) = \\mathbb{E}[(\\hat{\\mathbf{\\theta}} - \\mathbb{E}[\\hat{\\mathbf{\\theta}}])(\\hat{\\mathbf{\\theta}} - \\mathbb{E}[\\hat{\\mathbf{\\theta}}])^T]\\), representing the variability in the estimator. - Bias: \\(\\text{Bias}(\\hat{\\mathbf{\\theta}}) = \\mathbb{E}[\\hat{\\mathbf{\\theta}}] - \\mathbf{\\theta}\\), representing the systematic deviation from the true parameter. 2.5.3.2 Key Properties Unbiased Estimator: If \\(\\hat{\\mathbf{\\theta}}\\) is unbiased, then \\(\\text{Bias}(\\hat{\\mathbf{\\theta}}) = 0\\), and the MSE is simply the trace of the covariance matrix of \\(\\hat{\\mathbf{\\theta}}\\): \\[ \\mathbb{M}(\\hat{\\mathbf{\\theta}}) = \\text{tr}(\\text{Var}(\\hat{\\mathbf{\\theta}})). \\] Bias-Variance Trade-off: The MSE quantifies the balance between bias and variance. Lowering bias often increases variance and vice versa, a key concept in statistical estimation. Overall Error Metric: The MSE provides a comprehensive measure of the estimator’s accuracy, taking both variance and bias into account, making it an essential criterion in evaluating estimators. "],["introduction.html", "3 Introduction 3.1 Objectives of Linear Regression 3.2 Examples", " 3 Introduction 3.1 Objectives of Linear Regression Linear regression is a statistical method used to analyze the relationship between at least two variables: one dependent variable and at least one independent variable. This technique is closely tied to the correlation coefficient, which measures the strength and direction of a linear relationship between variables. In this document, we will explore how these concepts are connected. Linear regression is typically applied for three main purposes: Description: To describe the relationship between the variables under analysis. Control: To predict how changes in the independent variables will affect the dependent variable. Prediction: To forecast the value of the dependent variable based on new observations of the independent variables. 3.2 Examples 3.2.1 Ad Spending Imagine you are a newly hired data scientist at a mattress company. Your manager asks you to analyze the relationship between Google ad spending and mattress sales revenue. To illustrate this scenario, I have simulated a dataset: # Ad Spending Example # Set Seed set.seed(8272024) # Data Simulation x &lt;- rnorm(n = 100, mean = 70, sd = 30) y &lt;- 1000 + 5 * x + rnorm(n = 100, mean = 0, sd = 100) # Creates the Data Frame datAd &lt;- data.frame(cbind(y,x)) # Names the Variables colnames(datAd) &lt;- c(&quot;Revenue&quot;, &quot;Ad Spending&quot;) # Saves to csv write.csv(x = datAd[, c(1, 2)], file = &quot;Ad spending Data.csv&quot;, row.names = FALSE) You can load the dataset as follows: dat &lt;- read.csv(file = &quot;Ad spending Data.csv&quot;) To visualize the data: dat &lt;- read.csv(file = &quot;Ad spending Data.csv&quot;) plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) Next, you can perform a linear regression analysis: outReg &lt;- lm(Revenue ~ Ad.Spending, data = dat) The most important results from the regression analysis can be summarized as follows: summary(outReg) ## ## Call: ## lm(formula = Revenue ~ Ad.Spending, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -248.394 -58.805 3.782 63.577 196.745 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 997.3894 28.8185 34.61 &lt;2e-16 *** ## Ad.Spending 5.0247 0.3818 13.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 89.01 on 98 degrees of freedom ## Multiple R-squared: 0.6386, Adjusted R-squared: 0.6349 ## F-statistic: 173.2 on 1 and 98 DF, p-value: &lt; 2.2e-16 For now, let’s focus on the estimate for the intercept and the Ad.Spending coefficient. The intercept indicates the expected revenue when ad spending is zero. Based on your analysis, you observe that even without an ad campaign, mattress sales generate 997.3893514. The coefficient for Ad.Spending shows that each dollar spent on ads increases revenue by 5.0247439. To visualize the regression line: plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) The red regression line represents the “best fit” for these variables. In this context, “best fit” means the line that minimizes the sum of the squared distances from each point to the line. For comparison, here are examples of other lines that do not fit as well: Different slope (coefficient for the Ad.Spending variable): plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(a = outReg$coefficients[1], b = 6, col = &#39;blue&#39;, lwd = 2) Different intercept: plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(a = 1100, b = outReg$coefficients[2], col = &#39;blue&#39;, lwd = 2) We can expand the range for ad spending and revenue on the plot, as follows: plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;, ylim = c(800, 2500), xlim = c(0, 200)) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(h = 0, v = 0) Within the range of observed ad spending values 25 to 123, we can be reasonably confident in the relationship between ad spending and revenue. However, what happens when we consider values outside this range? While we can predict and control revenue to some extent by adjusting ad spending within the observed range, this confidence may not extend to values beyond it. For instance, it’s plausible that after reaching a certain level of ad spending, the market could become saturated, resulting in diminishing or even no additional revenue despite increased ad spending. Therefore, we should be cautious when extrapolating beyond the observed data, as the relationship may not hold under different conditions. 3.2.2 Wine and Life Expectancy In the previous example, we were able to manipulate the independent variable to influence the outcome. However, there are situations where our primary goal is simply to describe the relationship between two variables, without aiming to control them. In the following example, I generate a dataset that illustrates the relationship between wine consumption and life expectancy in years for an imaginary country. Here’s how the data is simulated: # Wine and Life Expectancy # Set Seed set.seed(8272024) # Data Simulation x &lt;- rbinom(n = 20, size = 20, prob = 0.1) y &lt;- 75 + 1.5 * x + rnorm(n = 20, mean = 0, sd = 3) # Creates the Data Frame datWin &lt;- data.frame(cbind(y,x)) # Names the Variables colnames(datWin) &lt;- c(&quot;Years&quot;, &quot;Glasses&quot;) # Saves to csv write.csv(datWin[, c(1, 2)], file = &quot;Wine Data.csv&quot;, row.names = FALSE) You should be able to generate the exact same data if you copy and paste the code and run it on your computer. Although the data is randomly simulated, I’ve set a seed to ensure that the simulation can be replicated consistently. We can read and plot the data in the as follows: # Reads the Data dat &lt;- read.csv(file = &quot;Wine Data.csv&quot;) # Plots the Data plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) Once again, we can perform a linear regression to examine the relationship between the variables and visualize it with a regression line: # Performs Linear Regression outReg &lt;- lm(Years ~ Glasses, data = dat) # Plots plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) Given the regression line, can we confidently conclude that increasing wine consumption leads to a longer life expectancy? 3.2.3 Burger Demand Imagine you work at a burger franchise where prices change daily. As the franchise manager, you want to predict the demand for burgers at a given price. Over the past 100 days, you’ve collected data on the number of burgers sold at each price set by the franchise’s main office. Here’s what your data looks like: # Reads the Data dat &lt;- read.csv(file = &quot;Burger Data.csv&quot;) # Plots the Data plot(x = dat$Price, y = dat$Burgers, xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers Sold&quot;) Is this dataset a good candidate for linear regression? While it’s true that we can always fit a line to any dataset, the real question is whether that line meaningfully represents the relationship between the variables. Here’s what happens when we apply linear regression to this data: # Performs Linear Regression outReg &lt;- lm(Burgers ~ Price, data = dat) # Plots plot(x = dat$Price, y = dat$Burgers, xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers Sold&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) Do you think this is a good fit for the data? The regression line appears to be inadequate for both low and high price points. Additionally, since the number of burgers sold must be positive, the regression line should not intersect the x-axis. "],["simple-linear-regression.html", "4 Simple Linear Regression 4.1 Intro to SLR 4.2 Least Squares Estimation 4.3 Properties of the Estimates 4.4 Centering and Standarizing the Data 4.5 Coefficient of Determination 4.6 Residual Analysis 4.7 Cross-Validation 4.8 Weighted Least Squares 4.9 Model in Matrix Form", " 4 Simple Linear Regression 4.1 Intro to SLR Simple linear regression (SLR) is a linear regression model with a single explanatory variable. It focuses on the linear relationship between one independent variable and one dependent variable, making it the most basic form of linear regression analysis. 4.1.1 A path not Taken The basic idea behind linear regression is to find the “best line” that describes the data. This idea admits several interpretations. One interpretation is to measure the distance between a candidate line and the data points. For a line \\(\\mathcal{l}\\) given by \\[ ax + by + c = 0 \\] the distance from a point \\((x_i,y_i)\\) to that line is \\[ d_i = d(\\mathcal{l},(x_i,y_i)) = \\frac{|ax_i + by_i + c|}{\\sqrt{a^2+b^2}}. \\] We could then pose the optimization problem \\[ \\min_{a,b,c} \\sum_{i=1}^n d_i. \\] However, those familiar with regression will recognize that this is not the standard linear regression formulation. That does not make the distance-minimization problem unreasonable, only that its solution will generally differ from the usual linear regression solution. Linear regression, as commonly defined, is a particular optimization choice made because of its convenient properties. In the next section we introduce a model that refines the notion of a “best line.” 4.1.2 SLR Model The model for simple linear regression is as follows: \\[y_i = \\beta_0 + \\beta_1 x_i + e_i, \\quad i\\in\\{1,\\ldots,n\\}\\] where: \\(y_i\\) represents the \\(i\\)-th observation of the dependent variable. \\(x_i\\) represents the \\(i\\)-th observation of the independent variable. \\(e_i\\) represents the \\(i\\)-th observation of the error term. \\(\\beta_0\\) is the intercept of the linear model, or regression line. \\(\\beta_1\\) is the slope of the linear model, or regression line. \\(n\\) is the number of observations for both variables. Notice that in this framework, the error is not measuring the distance from the point to the regression line, but the vertical distance from the point to the regression line. Also, note that we are not making any assumptions about the error terms. In the case of the wine example, we generated the data based on the following linear model: \\[y_i = 75 + 1.5 x_i + e_i \\] dat &lt;- read.csv(file = &quot;Wine Data.csv&quot;) plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) abline(a = 75, b = 1.5, col = &#39;red&#39;, lwd = 2) abline(v = 0, lwd = 2) text(x = 0.25, y = 76, expression(beta[0] ~ &quot;=75&quot;)) text(x = 3.25, y = 79, expression(beta[1] ~ &quot;=1.5&quot;)) segments(x0 = c(2, 3), x1 = c(3, 3), y0 = c(78, 78), y1 = c(78, 79.5), lwd = 2, col = &#39;blue&#39;) In this case, the intercept \\(\\beta_0\\) is meaningful, as it represents the expected number of years a person would live if they didn’t drink wine at all. However, depending on the data, the intercept may or may not have a meaningful interpretation. The slope \\(\\beta_1\\) indicates that for each additional glass of wine consumed per week, our model predicts an increase of 1.5 years in life expectancy. In practice, we rarely know the true regression line. Instead, it must be estimated from the data. The goal is to find the “best” line that fits the data, where “best” means the line that minimizes the sum of squared errors (SSE) between the observed values and the values predicted by the model. 4.1.3 Possible Optimization Problems Other than minimizing the sum of squared errors (SSE), we can consider the following alternative optimization problems. Here \\(e_i(\\beta_0,\\beta_1)=y_i-(\\beta_0+\\beta_1 x_i)\\) denotes the residual. 4.1.3.1 Minimizing the Sum of Absolute Errors \\[ \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n |e_i(\\beta_0, \\beta_1)| \\] This L1 criterion is common in the machine-learning community and is more robust to outliers than the SSE. Laplace was among the first to investigate this approach when developing early methods for fitting lines to data. 4.1.3.2 Minimizing the Maximum Error \\[ \\min_{\\beta_0, \\beta_1} \\max_{i=1}^n |e_i(\\beta_0, \\beta_1)| \\] The minimax approach focuses on reducing the largest error and is appropriate when large errors can be catastrophic. Laplace also explored this formulation in his early work. 4.1.3.3 Minimizing the Sum of Squared Errors \\[ \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n e_i(\\beta_0, \\beta_1)^2 \\] This is the standard formulation in simple linear regression (the least squares criterion), introduced by Legendre and Gauss (apparently independently). It remains widely used because of its convenient properties (closed-form solution, computational ease, and optimality under Gaussian error assumptions), which we will analyze later. 4.2 Least Squares Estimation As explained before, we want to minimize the SSE. Define \\[Q(\\beta_0, \\beta_1) = \\sum_{i=1}^n (e_i(\\beta_0, \\beta_1))^2 = \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\] Because Q is differentiable (in fact a convex quadratic), we find its minimum by setting the gradient to zero. The solution to this minimization problem is given by: \\[ \\hat{\\beta}_1 = \\frac{\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}}{\\sum_{i = 1}^n x_i^2 - n \\bar{x}^2} \\] \\[ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] where we use \\(\\hat{}\\), to denote the specific critical point. And we have adopted the standard notation: \\[\\bar{x} = \\frac{1}{n}\\sum_{i}^n x_i\\] and \\[\\bar{y} = \\frac{1}{n}\\sum_{i}^n y_i\\]. It remains to see if this is indeed a minimum. One can check the second order conditions or argue the quadratic form of the problem only admits a minimum. Derivation We start with \\(\\beta_0\\): \\[\\begin{align*} \\frac{\\partial Q}{\\partial \\beta_0} &amp;= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 &amp; \\text{def. } Q \\\\ &amp;= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i^2 + \\beta_0^2 + \\beta_1^2 x_i^2 - 2 \\beta_0 y_i - 2 \\beta_1 x_i y_i + 2 \\beta_0 \\beta_1 x_i) &amp; \\text{expanding square} \\\\ &amp;= \\sum_{i=1}^n \\frac{\\partial}{\\partial \\beta_0} (y_i^2 + \\beta_0^2 + \\beta_1^2 x_i^2 - 2 \\beta_0 y_i - 2 \\beta_1 x_i y_i + 2 \\beta_0 \\beta_1 x_i) &amp; \\text{linearity of derivative} \\\\ &amp;= \\sum_{i = 1}^n (2 \\beta_0 - 2 y_i + 2 \\beta_1 x_i) &amp; \\text{applying the derivative} \\\\ &amp;= 2 \\sum_{i = 1}^n \\beta_0 - 2 \\sum_{i = 1}^n y_i + 2 \\beta_1 \\sum_{i = 1}^n x_i &amp; \\text{linearity sum} \\\\ &amp;= 2 \\left( n \\beta_0 - n \\bar{y} + n \\beta_1 \\bar{x} \\right) &amp; \\text{def. } \\bar{y}, \\text{def. } \\bar{x} \\\\ \\end{align*}\\] Then we have that: \\[\\begin{align} \\frac{\\partial Q}{\\partial \\beta_0} = 0 &amp;\\iff -2 \\left( n \\beta_0 - n \\bar{y} + n\\beta_1 \\bar{x} \\right) = 0 \\notag \\\\ &amp;\\iff \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\tag{1} \\end{align}\\] And we can do a similar thing for \\(\\beta_1\\): \\[\\begin{align*} \\frac{\\partial Q}{\\partial \\beta_1} &amp;= \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 &amp; \\text{def. } Q \\\\ &amp;= \\sum_{i=1}^n \\frac{\\partial}{\\partial \\beta_1} (y_i - \\beta_0 - \\beta_1 x_i)^2 &amp; \\text{linwarity of derivative} \\\\ &amp;= \\sum_{i = 1}^n 2(y_i -\\beta_0 - \\beta_1 x_i)(-x_i) &amp; \\text{applying the derivative} \\\\ &amp;= -2\\sum_{i = 1}^n y_i x_i + 2 \\beta_0 \\sum_{i = 1}^n x_i + 2 \\beta_1 \\sum_{i = 1}^n x_i^2 &amp; \\text{linearity sum} \\\\ &amp;= -2\\sum_{i = 1}^n y_i x_i + 2 n \\beta_0 \\bar{x} + 2 \\beta_1 \\sum_{i = 1}^n x_i^2 &amp; \\text{def. } \\bar{x} \\\\ \\end{align*}\\] then: \\[\\begin{align} \\frac{\\partial Q}{\\partial \\beta_1} = 0 &amp;\\iff -2\\sum_{i = 1}^n y_i x_i + 2 n \\beta_0 \\bar{x} + 2 \\beta_1 \\sum_{i = 1}^n x_i^2 = 0 \\notag \\\\ &amp;\\iff \\sum_{i = 1}^n y_i x_i = n \\beta_0 \\bar{x} + \\beta_1 \\sum_{i = 1}^n x_i^2 \\tag{2} \\end{align}\\] Now, substituting (1) into (2) we have that \\[\\begin{align*} \\sum_{i = 1}^n y_i x_i &amp;= n (\\bar{y} - \\beta_1 \\bar{x}) \\bar{x} + \\beta_1 \\sum_{i = 1}^n x_i^2 \\\\ &amp;= n \\bar{y} \\bar{x} - n \\beta_1 \\bar{x}^2 + \\beta_1 \\sum_{i = 1}^n x_i^2 \\\\ &amp;= n \\bar{y} \\bar{x} + \\beta_1 \\left( \\sum_{i = 1}^n x_i^2 - n \\bar{x}^2 \\right) \\end{align*}\\] Then, \\[ \\beta_1 = \\frac{\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}}{\\sum_{i = 1}^n x_i^2 - n \\bar{x}^2} \\] so, the only critical point for \\(Q(\\beta_0,\\beta_1)\\) is when: \\[ \\hat{\\beta}_1 = \\frac{\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}}{\\sum_{i = 1}^n x_i^2 - n \\bar{x}^2} \\] \\[ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] Now, if we introduce the further notation for the sample variance and covariance: \\[ S^2_{xx} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 \\] \\[ S_{xy} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\] and note that: \\[\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n x_i^2 - n\\bar{x}^2 \\] Derivation \\[\\begin{align*} \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i^2 - 2\\bar{x}x_i + \\bar{x}^2) &amp; \\text{exp. square} \\\\ &amp;= \\frac{1}{n-1} \\left[ \\sum_{i=1}^n x_i^2 - 2\\bar{x}\\sum_{i=1}^n x_i + \\sum_{i=1}^n \\bar{x}^2 \\right] &amp; \\text{linearity sum} \\\\ &amp;= \\frac{1}{n-1} \\left[ \\sum_{i=1}^n x_i^2 - 2\\bar{x}(n\\bar{x}) + n \\bar{x}^2 \\right] &amp; \\text{def. } \\bar{x} \\\\ &amp;= \\frac{1}{n-1} \\left[ \\sum_{i=1}^n x_i^2 - 2n\\bar{x}^2 + n \\bar{x}^2 \\right] &amp; \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n x_i^2 - n\\bar{x}^2 &amp; \\\\ \\end{align*}\\] and \\[ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^n x_iy_i - n\\bar{x} \\bar{y} \\] Derivation \\[\\begin{align*} \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_iy_i - \\bar{x}y_i - \\bar{y}x_i + \\bar{x}\\bar{y}) &amp; \\text{exp. prod.} \\\\ &amp;= \\frac{1}{n-1} \\left[ \\sum_{i=1}^n x_iy_i - \\bar{x} \\sum_{i=1}^ny_i - \\bar{y} \\sum_{i=1}^n x_i + \\sum_{i=1}^n \\bar{x}\\bar{y} \\right] &amp; \\text{lin. sum} \\\\ &amp;= \\frac{1}{n-1} \\left[ \\sum_{i=1}^n x_iy_i - n\\bar{x} \\bar{y} - n\\bar{y} \\bar{x} + n \\bar{x}\\bar{y} \\right] &amp; \\text{def. } \\bar{y}, \\bar{x} \\\\ &amp;= \\frac{1}{n-1} \\left[ \\sum_{i=1}^n x_iy_i - n\\bar{x} \\bar{y} \\right] &amp; \\\\ \\end{align*}\\] Then we can express \\(\\hat{\\beta}_1\\) as: \\[\\hat{\\beta}_1 = \\frac{(n-1)S_{xy}}{(n-1)S_{xx}^2}=\\frac{S_{xy}}{S_{xx}} \\] Now notice that in order to find the Least Squares estimates you don’t require the complete data set, but only require the following quantities (Suffient Statistics): \\(\\bar{y}\\). \\(\\bar{x}\\). \\(S_{xx}\\). \\(S_{xy}\\). 4.2.1 Other estimated quantites If we substitute the least-squares estimates into the regression model we obtain the fitted (predicted) values and residuals. The fitted value for observation \\(i\\) is \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i, \\] and the residual (estimated error) is \\[ \\hat{e}_i = y_i - \\hat{y}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i. \\] And we can also compare our estimated regression line (blue) with the real regression line (red) in the following as follows: outReg &lt;- lm(Years ~ Glasses, data = dat) plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) abline(a = 75, b = 1.5, col = &#39;red&#39;, lwd = 2) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;blue&#39;, lwd = 2) 4.2.2 Properties of the SLR problem The least-squares solution in simple linear regression (SLR) enjoys several important and useful properties that are not generally shared by minimax or least-absolute-value fits (even for the same linear model). Below is a concise, polished list of these properties with brief explanations. Closed-form solution. The normal equations yield explicit formulas (e.g. \\(\\hat\\beta_1 = S_{xy}/S_{xx}\\), \\(\\hat\\beta_0=\\bar y-\\hat\\beta_1\\bar x\\)). Closed-form expressions provide direct insight into how the estimates depend on the data. Computationally cheap (O(n)). Only a few sums (or averages and second moments) are needed, so the estimates can be computed in linear time and with constant memory beyond the data summaries. Depends only on a few summary statistics (sufficient-type summaries). The estimates require only \\(\\bar x,\\bar y,S_{xx}^2,S_{xx}\\) 4.3 Properties of the Estimates Before analyzing more properties of the least squares problem, let us define the following variables: \\(\\mathbf{y} = (y_1,\\ldots,y_n)&#39;\\). \\(\\mathbf{x} = (x_1,\\ldots,x_n)&#39;\\). \\(\\mathbf{e} = (e_1,\\ldots,e_n)&#39;\\). \\(\\hat{\\mathbf{y}} = (\\hat{y}_1,\\ldots,\\hat{y}_n)&#39;\\). \\(\\hat{\\mathbf{e}} = (\\hat{e}_1,\\ldots,\\hat{e}_n)&#39;\\). \\(\\mathbb{1} = (1,\\ldots,1)&#39;\\). As a bonus, we can write the Linear regression model as follows: \\[ \\mathbf{y} = \\beta_0 \\mathbb{1} + \\beta_1 \\mathbf{x} + \\mathbf{e} \\] 4.3.1 \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are linear combinations of \\(\\mathbf{y}\\) The estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are linear combinations of \\(\\mathbf{y} = (y_1,\\ldots,y_n)&#39;\\). First note the following: \\[ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x}) y_i \\] Derivation \\[\\begin{align*} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) &amp;= \\sum_{i=1}^n x_i y_i - n \\bar{x} \\bar{y} &amp; \\text{dist. prod.} \\\\ &amp;= \\sum_{i=1}^n x_i y_i - \\bar{x} \\sum_{i=1}^n y_i &amp; \\text{def. } \\bar{y} \\\\ &amp;= \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n \\bar{x} y_i &amp; \\text{liniarity of sum} \\\\ &amp;= \\sum_{i=1}^n (x_i y_i - \\bar{x} y_i) &amp; \\text{ass. sum} \\\\ &amp;= \\sum_{i=1}^n (x_i - \\bar{x}) y_i &amp; \\text{factoring } y_i \\\\ \\end{align*}\\] Then \\[ \\hat{\\beta}_1 = \\frac{\\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) y_i}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\sum_{i=1}^n\\frac{(x_i - \\bar{x}) }{\\sum_{i=1}^n (x_i - \\bar{x})^2}y_i \\] and similarly: \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} = \\sum_{i=1}^n \\frac{y_i}{n} - \\sum_{i=1}^n\\frac{(x_i - \\bar{x}) }{\\sum_{j = 1}^n x_j^2 - n \\bar{x}^2}y_i \\bar{x} = \\sum_{i=1}^n \\left( \\frac{1}{n} - \\frac{(x_i - \\bar{x}) }{\\sum_{j = 1}^n x_j^2 - n \\bar{x}^2} \\bar{x} \\right)y_i \\] 4.3.2 The sum of the residuals is \\(0\\) That is: \\[ \\sum_{i=1}^n \\hat{e}_i = 0 \\] Derivation \\[\\begin{align*} \\sum_{i=1}^n \\hat{e}_i &amp;= \\sum_{i=1}^n(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) &amp; \\text{def. } \\hat{\\mathbf{e}}\\\\ &amp;= \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\hat{\\beta}_0 - \\hat{\\beta}_1 \\sum_{i=1}^n x_i &amp; \\text{linearity sum} \\\\ &amp;= n\\bar{y} - n \\hat{\\beta}_0 - n \\hat{\\beta}_1 \\bar{x} &amp; \\\\ &amp;= n\\bar{y} - n (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) - n \\hat{\\beta}_1 \\bar{x} &amp; \\text{def. } \\hat{\\beta}_0 \\\\ &amp;= n\\bar{y} - n \\bar{y} + n \\hat{\\beta}_1 \\bar{x} - n \\hat{\\beta}_1 \\bar{x} &amp; \\text{dist.} n \\\\ &amp;= 0 \\end{align*}\\] 4.3.3 \\(\\hat{\\mathbf{e}}\\) and \\(\\mathbf{x}\\) are orthogonal We need to show that: \\[ \\langle \\hat{\\mathbf{e}}, \\mathbf{x}\\rangle = 0 \\] Derivation \\[\\begin{align*} \\langle \\hat{\\mathbf{e}}, \\mathbf{x}\\rangle &amp;= \\sum_{i=1}^{n} \\hat{e}_i x_i &amp; \\text{def. dot prod.} \\\\ &amp;= \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)x_i &amp; \\text{def. } \\hat{e}_i \\\\ &amp;= \\sum_{i=1}^{n} (y_i x_i - \\hat{\\beta}_0x_i - \\hat{\\beta}_1 x_i x_i) &amp; \\text{dist. } x_i \\\\ &amp;= \\sum_{i=1}^{n} y_i x_i - \\hat{\\beta}_0 \\sum_{i=1}^{n} x_i - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i x_i &amp; \\text{linearity sum} \\\\ &amp;= \\sum_{i=1}^{n} y_i x_i - n \\hat{\\beta}_0 \\bar{x} - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2 &amp; \\text{def. } \\bar{x} \\\\ &amp;= \\sum_{i=1}^{n} y_i x_i - n (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\bar{x} - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2 &amp; \\text{def. } \\hat{\\beta}_0 \\\\ &amp;= \\sum_{i=1}^{n} y_i x_i - n \\bar{y} \\bar{x} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2 &amp; \\text{dist. } \\bar{x} \\\\ &amp;= \\sum_{i=1}^{n} y_i x_i - n \\bar{y} \\bar{x} - \\hat{\\beta}_1 (\\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2) &amp; \\text{fact. } \\hat{\\beta}_1 \\\\ &amp;= \\sum_{i=1}^{n} y_i x_i - n \\bar{y} \\bar{x} - \\frac{\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}}{\\sum_{i = 1}^n x_i^2 - n \\bar{x}^2}(\\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2) &amp; \\text{def. } \\hat{\\beta}_1 \\\\ &amp;= \\sum_{i=1}^{n} y_i x_i - n \\bar{y} \\bar{x} - (\\sum_{i = 1}^n y_i x_i - n \\bar{y} \\bar{x}) \\\\ &amp;=0 \\end{align*}\\] 4.3.4 \\(\\hat{\\mathbf{y}}\\) and \\(\\hat{\\mathbf{e}}\\) are orthogonal We need to show that: \\[ \\langle \\hat{\\mathbf{e}}, \\hat{\\mathbf{y}}\\rangle = 0 \\] Derivation \\[\\begin{align*} \\langle \\hat{\\mathbf{e}}, \\hat{\\mathbf{y}}\\rangle &amp;= \\sum_{i=1}^{n} \\hat{e}_i \\hat{y}_i &amp; \\text{def. dot prod.} \\\\ &amp;= \\sum_{i=1}^{n} \\hat{e}_i(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) &amp; \\text{def. } \\hat{y}_i \\\\ &amp;= \\sum_{i=1}^{n} (\\hat{e}_i \\hat{\\beta}_0 + \\hat{e}_i \\hat{\\beta}_1 x_i) &amp; \\text{dist. } \\hat{e}_i \\\\ &amp;= \\hat{\\beta}_0 \\sum_{i=1}^{n} \\hat{e}_i + \\hat{\\beta}_1 \\sum_{i=1}^{n} \\hat{e}_i x_i &amp; \\text{lin. sum } \\\\ &amp;= \\hat{\\beta}_1 \\sum_{i=1}^{n} \\hat{e}_i x_i &amp; \\sum_{i=1}^{n} \\hat{e}_i = 0\\\\ &amp;= \\hat{\\beta}_1 \\langle \\hat{\\mathbf{e}}, \\mathbf{x}\\rangle &amp; \\text{def. dot prod.}\\\\ &amp;= 0 &amp; \\langle \\hat{\\mathbf{e}}, \\mathbf{x}\\rangle = 0 \\\\ \\end{align*}\\] 4.3.5 The average of \\(\\hat{\\mathbf{y}}\\) and \\(\\mathbf{y}\\) are the same That is: \\[ \\frac{1}{n} \\sum_{i=1}^n \\hat{y}_i = \\bar{y} \\] Derivation \\[\\begin{align*} \\frac{1}{n} \\sum_{i=1}^n \\hat{y}_i &amp;= \\frac{1}{n} \\sum_{i=1}^n (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) &amp; \\text{def. } \\hat{y}_i \\\\ &amp;= \\frac{1}{n} (n \\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum_{i=1}^n x_i) &amp; \\text{dist. sum} \\\\ &amp;= \\frac{1}{n} (n \\hat{\\beta}_0 + n \\hat{\\beta}_1 \\mathbf{x}) &amp; \\text{dist. } n \\\\ &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\mathbf{x} &amp; \\text{def. } \\hat{\\beta}_0 \\\\ &amp;= \\bar{y} - \\hat{\\beta}_1 \\mathbf{x}+ \\hat{\\beta}_1 \\mathbf{x}\\\\ &amp; \\\\ &amp;= \\bar{y} \\\\ &amp; \\\\ \\end{align*}\\] All these properties of the estimates, are the result of solving the least squares problem. If another problem is solve, several of these properties, if not all of them, will be lost. 4.4 Centering and Standarizing the Data Some transformations of the data can help the regression analysis or make it more intuitive. There are 2 main transformations of the data: centering and standardization. Definition 4.1 (Centered variable) Given observations \\(\\{x_i\\}_{i=1}^n\\), then the centered version of observation \\(i\\), denoted by \\(x_i^c\\) is given by: \\[x_i^c = x_i - \\bar{x}\\] Proposition 4.1 The centered observations \\(\\{x_i^c\\}_{i=1}^n\\) have mean \\(0\\). Proof Proof. We need to show that: \\[ \\bar{x}^c = 0 \\] \\[\\begin{align*} \\bar{x}^c &amp;= \\frac{1}{n} \\sum_{i=1}^n x_i^c &amp; \\text{def. } \\bar{x}^c \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x}) &amp; \\text{def. } x_i^c \\\\ &amp;= \\frac{1}{n} \\left(\\sum_{i=1}^n x_i - \\sum_{i=1}^n \\bar{x} \\right) &amp; \\text{lin. sum} x_i^c \\\\ &amp;= \\frac{1}{n} \\left(n\\bar{x} - n \\bar{x} \\right) &amp; \\text{def. } \\bar{x} \\\\ &amp;= 0 \\\\ &amp; \\\\ \\end{align*}\\] Proposition 4.2 The centered observations \\(\\{x_i^c\\}_{i=1}^n\\) have the same sample variance as the original observations \\(\\{x_i\\}_{i=1}^n\\). Proof Proof. We need to show that: \\[ S_{xx}^c = S_{xx} \\] \\[\\begin{align*} S_{xx}^c &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i^c - \\bar{x}^c)^2 &amp; \\text{def. } S_{xx}^c \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i^c)^2 &amp; \\bar{x}^c = 0 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 &amp; \\text{def. } x_i^c \\\\ &amp;= S_{xx} &amp; \\text{def. } S_{xx} \\\\ \\end{align*}\\] So the variance of the observations is not affected by the centering. Proposition 4.3 The centered observations \\(\\{x_i^c\\}_{i=1}^n\\) and \\(\\{y_i^c\\}_{i=1}^n\\) have the same sample covariance as the original observations \\(\\{x_i\\}_{i=1}^n\\) and \\(\\{y_i\\}_{i=1}^n\\). Proof Proof. We need to show that: \\[ S_{xy}^c = S_{xy} \\] \\[\\begin{align*} S_{xy}^c &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i^c - \\bar{x})(y_i^c - \\bar{y}) &amp; \\text{def. } S_{xy}^c \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i^c)(y_i^c) &amp; \\bar{x}^c = 0,\\bar{y}^c = 0 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) &amp; \\text{def. } x_i^c,y_i^c \\\\ &amp;= S_{xy} &amp; \\text{def. } S_{xy} \\\\ \\end{align*}\\] So the covariance of the observations is also not affected by the centering. Definition 4.2 (Standarized variable) Given observations \\(\\{x_i\\}_{i=1}^n\\), then the standarized version of observation \\(i\\), denoted by \\(x_i^s\\), is given by: \\[x_i^s = \\frac{x_i - \\bar{x}}{\\sqrt{S_{xx}}} = \\frac{x_i^c}{\\sqrt{S_{xx}}}\\] Proposition 4.4 The standardized observations \\(\\{x_i^s\\}_{i=1}^n\\) have mean of \\(0\\) and variance \\(1\\). Proof Proof. We need to show that: \\[ \\bar{x}^s = 0 \\] and \\[ S_{xx}^s = 1 \\] First, let us show that \\(\\bar{x}^s = 0\\). \\[\\begin{align*} \\bar{x}^s = 0 &amp;= \\frac{1}{n} \\sum_{i=1}^n x_i^c &amp; \\text{def. } \\bar{x}^s \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n \\frac{x_i^c}{\\sqrt{S_{xx}}} &amp; \\text{def. } x_i^s \\\\ &amp;= = \\frac{1}{\\sqrt{S_{xx}}}\\frac{1}{n} \\sum_{i=1}^n x_i^c &amp; \\text{lin. sum} \\\\ &amp;= \\frac{1}{\\sqrt{S_{xx}}} \\bar{x}^c &amp; \\text{def. } \\bar{x}^c \\\\ &amp;= 0 &amp; \\bar{x}^c = 0 \\\\ \\end{align*}\\] Now let us see that the variance of the standardized observations is 1. \\[\\begin{align*} S_{xx}^s &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i^s - \\bar{x}^s)^2 &amp; \\text{def. } S_{xx}^s \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n (x_i^s)^2 &amp; \\bar{x}^s = 0 \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n \\left(\\frac{x_i - \\bar{x}}{\\sqrt{S_{xx}}}\\right)^2 &amp; \\text{def. } x_i^s \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1}^n \\frac{(x_i - \\bar{x})^2}{S_{xx}} &amp; \\\\ &amp;= \\frac{1}{S_{xx}} \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 &amp; \\text{lin. sum} \\\\ &amp;= \\frac{1}{S_{xx}}S_{xx} \\\\ &amp; \\text{def. } S_{xx} \\\\ &amp;= 1 &amp; \\\\ \\end{align*}\\] Now, let us introduce the sample correlation as: \\[ r_{xy} = \\frac{S_{xy}}{\\sqrt{S_{xx}{S_{yy}}}} \\] If we standardize two sets of observations, then the covaraince of the standardized version is the correlation of the standardized version. Let us see it: \\[\\begin{align*} S_{xy}&#39;&#39; &amp;= \\frac{1}{n-1} \\sum_{i=1} (x_i&#39;&#39; - \\bar{x}&#39;&#39;)(y_i&#39;&#39; - \\bar{x}&#39;&#39;) \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1} (x_i&#39;&#39;)(y_i&#39;&#39;) \\\\ &amp;= \\frac{1}{n-1} \\sum_{i=1} \\left(\\frac{x_i - \\bar{x}}{\\sqrt{S_{xx}}}\\right) \\left(\\frac{y_i - \\bar{y}}{\\sqrt{S_{yy}}}\\right) \\\\ &amp;= \\frac{1}{\\sqrt{S_{yy}}\\sqrt{S_{xx}}}\\frac{1}{n-1} \\sum_{i=1} (x_i - \\bar{x}) (y_i - \\bar{y}) \\\\ &amp;= \\frac{1}{\\sqrt{S_{yy}}\\sqrt{S_{xx}}} S_{xy} \\\\ &amp;= r_{xy} \\end{align*}\\] With this results, we can analyze the effects of following 3 scenarios on the estimated coefficients: Independent variable centered. Both, Independent and dependent variable centered. Both, Independent and dependent variable standardized. 4.4.1 Independent variable centered Lets compute the value for \\(\\beta_1\\) when the data is centered. \\[\\hat{\\beta}_1&#39; = \\frac{S_{xy}&#39;}{S_{xx}&#39;} = \\frac{S_{xy}}{S_{xx}} = \\hat{\\beta}_1 \\] So centering the data doesn’t change the value of the estimated slope. \\[ \\hat{\\beta}_0&#39; = \\bar{y} - \\hat{\\beta}_1&#39; \\bar{x}&#39; = \\bar{y} - \\hat{\\beta}_1&#39; 0 = \\bar{y} \\] So centering the data, makes the estimated intercept to coincide with the mean of the independent variable. We can see this in one of our example data sets, looking at the ad spending data we can perform linear regression on the original data and the centered data: # Read Data dat &lt;- read.csv(&quot;Ad spending Data.csv&quot;) # Assign data x &lt;- dat$Ad.Spending y &lt;- dat$Revenue # Centers x xCen &lt;- x - mean(x) # Linear regression on the original data outRegOri &lt;- lm(y ~ x) # Linear regression on the centered independent variable data outRegCen &lt;- lm(y ~ xCen) # Plots ## Two plots in the same image par(mfrow = c(1, 2)) ## Original data # Plots the points plot(x = x, y = y, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;) # Plots the regression line abline(a = outRegOri$coefficients[1], b = outRegOri$coefficients[2], col = &#39;red&#39;, lwd = 2) ## Independent Variable centered data # Plots the points plot(x = xCen, y = y, xlab = &quot;Ad spending (Centered)&quot;, ylab = &quot;Revenue&quot;) # Plots the regression line abline(a = outRegCen$coefficients[1], b = outRegCen$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(v = 0, lwd = 2) So we can appreciate that centering the independent variable just shifts the data horizontally so the mean will be at zero. 4.4.2 Both Variables centered Now lets see the effects when both variables are centered. Here, we will denote the estimates again with one prime, that is \\(\\hat{\\beta}&#39;\\). Again, the estimate of the slope doesn’t change: \\[\\hat{\\beta}_1&#39; = \\frac{S_{xy}&#39;}{S_{xx}&#39;} = \\frac{S_{xy}}{S_{xx}} = \\hat{\\beta}_1 \\] while the estimate of the intercept becomes zero (the new mean of the centered dependent variable) \\[ \\hat{\\beta}_0&#39; = \\bar{y}&#39; - \\hat{\\beta}_1&#39; \\bar{x}&#39; = \\bar{y}&#39; = 0 \\] The effect of this transformation can be observed, here: # Read Data dat &lt;- read.csv(&quot;Ad spending Data.csv&quot;) # Assign data x &lt;- dat$Ad.Spending y &lt;- dat$Revenue # Centers x and y xCen &lt;- x - mean(x) yCen &lt;- y - mean(y) # Linear regression on the original data outRegOri &lt;- lm(y ~ x) # Linear regression on the centered data outRegCen &lt;- lm(yCen ~ xCen) # Plots ## Two plots in the same image par(mfrow = c(1, 2)) ## Original data # Plots the points plot(x = x, y = y, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;) # Plots the regression line abline(a = outRegOri$coefficients[1], b = outRegOri$coefficients[2], col = &#39;red&#39;, lwd = 2) ## Centered data # Plots the points plot(x = xCen, y = yCen, xlab = &quot;Ad spending (Centered)&quot;, ylab = &quot;Revenue&quot;) # Plots the regression line abline(a = outRegCen$coefficients[1], b = outRegCen$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(v = 0, lwd = 2) abline(h = 0, lwd = 2) 4.4.3 Independent and dependent variable standardized Again we start we the slope estimate: \\[\\hat{\\beta}_1&#39;&#39; = \\frac{S_{xy}&#39;&#39;}{S_{xx}&#39;&#39;} = \\frac{r_{xy}}{1} =r_{xy} \\] so, the estimate of the slope is the sample correlation of the original observations. Again, we can see this graphically: # Read Data dat &lt;- read.csv(&quot;Ad spending Data.csv&quot;) # Assign data x &lt;- dat$Ad.Spending y &lt;- dat$Revenue # Standardizes x and y xSta &lt;- (x - mean(x))/sqrt(var(x)) ySta &lt;- (y - mean(y))/sqrt(var(y)) # Linear regression on the original data outRegOri &lt;- lm(y ~ x) # Linear regression on the standard data outRegSta &lt;- lm(ySta ~ xSta) # Plots ## Two plots in the same image par(mfrow = c(1, 2)) ## Original data # Plots the points plot(x = x, y = y, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;) # Plots the regression line abline(a = outRegOri$coefficients[1], b = outRegOri$coefficients[2], col = &#39;red&#39;, lwd = 2) ## Standard data # Plots the points plot(x = xSta, y = ySta, xlab = &quot;Ad spending (Centered)&quot;, ylab = &quot;Revenue&quot;) # Plots the regression line abline(a = outRegSta$coefficients[1], b = outRegSta$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(v = 0, lwd = 2) abline(h = 0, lwd = 2) Now, let us see that the correlation is always in the interval \\((-1,1)\\). To see this, notice the following: \\[\\begin{align*} \\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;&#39; + y_i&#39;&#39;)^2 &amp;= \\frac{1}{n-1} \\sum_{i=1}^n \\left((x_i&#39;&#39;)^2 + 2x_i&#39;&#39; y_i&#39;&#39; + (y_i&#39;&#39;)^2 \\right) \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i&#39;&#39;)^2}{n-1} + 2\\frac{\\sum_{i=1}^n x_i&#39;&#39;y_i&#39;&#39;}{n-1} + \\frac{\\sum_{i=1}^n (y_i&#39;&#39;)^2}{n-1} \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i&#39;&#39; - \\bar{x}&#39;&#39;)^2}{n-1} + 2\\frac{\\sum_{i=1}^n (x_i&#39;&#39; - \\bar{x}&#39;&#39;)(y_i&#39;&#39; - \\bar{y}&#39;&#39;)}{n-1} + \\frac{\\sum_{i=1}^n (y_i&#39;&#39; - \\bar{y}&#39;&#39;)^2}{n-1} \\\\ &amp;= S_{xx}&#39;&#39; + 2 S_{xy}&#39;&#39; + S_{yy}&#39;&#39; \\\\ &amp;= 1 + 2r_{xy} + 1 \\\\ &amp;= 2(1 + r_{xy}) \\end{align*}\\] In a similar way it can be shown that: \\[\\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;&#39; + y_i&#39;&#39;)^2 = 2(1 - r_{xy})\\] Now since, \\[\\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;&#39; + y_i&#39;&#39;)^2 \\geq 0\\] then we have that \\[\\begin{align*} \\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;&#39; + y_i&#39;&#39;)^2 \\geq 0 &amp;\\implies 2(1 + r_{xy}) \\geq 0 \\\\ &amp;\\implies 1 + r_{xy} \\geq 0 \\\\ &amp;\\implies r_{xy} \\geq -1 \\\\ \\end{align*}\\] Similarly, since \\[\\frac{1}{n-1} \\sum_{i=1}^n (x_i&#39;&#39; - y_i&#39;&#39;)^2 \\geq 0\\] implies \\[r_{xy} \\leq 1 \\] then, we have that: \\[ -1 \\leq r_{xy} \\leq 1\\] which implies that the slope of the regression analysis after standarizing both variables is going to be in the interval \\((-1, 1)\\). 4.5 Coefficient of Determination So far, we have been concerned on findin the “best” line, that is estimating the coefficients that minimize the sum of squared errors. We have find some derivated estimated values and some properties of these values. However, we hanven’t analized how good is our estimation. To see how well we are doing we will look at the coeficient of determination. To do so we will first introduce some quantities: Total Sum of Squares \\[SS_{tot} = \\sum_{i=1}^n (y_i - \\bar{y})\\] Is a measure of total variability of the dependent variable. You can think of it in many ways: As a proxy for uncertainty. The bigger the more uncertainty in the dependent varaible. It is proportional to the sample varaince. It is what you get if when you solve the following minimization problem: \\[ \\min_{a} \\sum_{i=1}^n (y_i - \\beta_0)^2 \\] that is, the best you can do to minimize the sum of squares without having access to the independent variables \\(x_1,\\ldots,x_n\\). Residual Sum of Squares \\[ SS_{res} = \\sum_{i=1}^n(\\hat{e}_i)^2 = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\] The residual Sum of Squares is the minimum value of our optimization problem. It is the the amount of variability that no matter what we do we will have remaing even after finding the “best” line. Explained Sum of Squares \\[ SS_{reg} = \\sum_{i=1}^n(\\hat{y}_i - \\bar{y}) = \\sum_{i=1}^n(\\hat{y}_i - \\hat{\\bar{y}}) \\] The variability of the fitted value. This is the variability we can explain with our regression model. These 3 quantities are related by: \\[ SS_{tot} = SS_{reg} + SS_{res} \\] That is the total variability si the sum of the variability that is explained by the regression model and the variability we can’t explain with the regression model. \\[\\begin{align*} SS_{tot} &amp;= \\sum_{i=1}^n(y_i - \\bar{y})^2 = \\sum_{i=1}^n(y_i - \\hat{y}_i + \\hat{y}_i - \\bar{y})^2 \\\\ &amp;= \\sum_{i=1}^n\\left((y_i - \\hat{y}_i)^2 + 2(y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) + (\\hat{y}_i - \\bar{y})^2\\right) \\\\ &amp;= \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 + 2 \\sum_{i=1}^n(y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) + \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2 \\\\ &amp;= SS_{res} + 2 \\sum_{i=1}^n(y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) + SS_{reg} \\\\ &amp;= SS_{res} + 2 \\sum_{i=1}^n\\hat{e}_i(\\hat{y}_i - \\bar{y}) + SS_{reg} \\\\ &amp;= SS_{res} + 2 \\sum_{i=1}^n(\\hat{e}_i\\hat{y}_i - \\hat{e}_i\\bar{y}) + SS_{reg} \\\\ &amp;= SS_{res} + 2 \\sum_{i=1}^n\\hat{e}_i\\hat{y}_i - 2 \\sum_{i=1}^n\\hat{e}_i\\bar{y} + SS_{reg} \\\\ &amp;= SS_{res} + 2(0) - 2 \\bar{y} \\sum_{i=1}^n\\hat{e}_i + SS_{reg} \\\\ &amp;= SS_{res} - 2 \\bar{y} (0) + SS_{reg} \\\\ &amp;= SS_{res} + SS_{reg} \\\\ \\end{align*}\\] Coefficient of Determination \\[ R^2 = \\frac{SS_{reg}}{SS_{tot}} = 1 - \\frac{SS_{res}}{SS_{tot}} \\] The coefficient of determination then can be explained as the percentage of the total varaibility that can be explained with linear regression. As a percentage, it has to be a number between 0 and 1. To see this let us show that: \\[ R^2 = r_{xy}^2 \\] To see this, first let us express \\(SS_{reg}\\) in a more convinient way: \\[\\begin{align*} SS_{reg} &amp;= \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2 \\\\ &amp;= \\sum_{i=1}^n(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i - \\bar{y})^2 \\\\ &amp;= \\sum_{i=1}^n(\\bar{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 x_i - \\bar{y})^2 \\\\ &amp;= \\sum_{i=1}^n(\\hat{\\beta}_1 x_i - \\hat{\\beta}_1 \\bar{x})^2 \\\\ &amp;= \\sum_{i=1}^n\\hat{\\beta}_1^2(x_i - \\bar{x})^2 \\\\ &amp;= \\hat{\\beta}_1^2 \\sum_{i=1}^n(x_i - \\bar{x})^2 \\\\ &amp;= \\hat{\\beta}_1^2 S_{xx} (n-1) \\\\ &amp;= \\left( \\frac{S_{xy}}{S_{xx}} \\right)^2 S_{xx} (n-1) \\\\ &amp;= \\frac{S_{xy}^2}{S_{xx}^2} S_{xx} (n-1) \\\\ &amp;= \\frac{S_{xy}^2}{S_{xx}} (n-1) \\\\ \\end{align*}\\] Then, we can see that: \\[\\begin{align*} R^2 &amp;= \\frac{SS_{reg}}{SS_{tot}} \\\\ &amp;= \\frac{\\frac{S_{xy}^2}{S_{xx}} (n-1)}{\\sum_{i=1}^n(y_i - \\bar{y})^2} \\\\ &amp;= \\frac{\\frac{S_{xy}^2}{S_{xx}} (n-1)}{S_{yy}(n-1)} \\\\ &amp;= \\frac{S_{xy}^2}{S_{xx}S_{yy}} \\\\ &amp;= \\left( \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}} \\right)^2 \\\\ &amp;= r_{xy}^2 \\end{align*}\\] The bigger the \\(R^2\\), the better is the fit of our linear model. The \\(R^2\\) can be low for 2 reasons: The first one is if our data is not linear, then a linear model will explain little about the relationship (some times a linear model can be a good approximation of a non-linear model). The second reason, is when the data is noisy. This can reduce the \\(R^2\\) even when we know the relationship between the variables is linear. As an example of noisy data, recall the Ad spending data. I actually generated the data under a linear model, so the relationship between the variables is linear. You can verify this be looking at the code where the data is generated in the introduction. Next I show the effects of adding additional noise to the data, at 3 levels: Level1: Small Noise. Level2: Medium Noise. Level3: High noise. # Read Data dat &lt;- read.csv(&quot;Ad spending Data.csv&quot;) # Assign data x &lt;- dat$Ad.Spending y &lt;- dat$Revenue # Adds Noise yNoiLe1 &lt;- y + rnorm(n = 100, sd = 50) yNoiLe2 &lt;- y + rnorm(n = 100, sd = 200) yNoiLe3 &lt;- y + rnorm(n = 100, sd = 500) # Auxiliary Variables ymax &lt;- max(y, yNoiLe1, yNoiLe2, yNoiLe3) ymin &lt;- min(y, yNoiLe1, yNoiLe2, yNoiLe3) xmax = max(x) xmin = min(x) # Performs Linear Regression outRegOri &lt;- lm(y ~ x) outRegNoiLe1 &lt;- lm(yNoiLe1 ~ x) outRegNoiLe2 &lt;- lm(yNoiLe2 ~ x) outRegNoiLe3 &lt;- lm(yNoiLe3 ~ x) # Plots par(mfrow = c(2, 2)) plot(x = x, y = y, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;, main = &quot;Original Data&quot;, ylim = c(ymin, ymax), xlim = c(xmin, xmax)) # Plots the regression line abline(a = outRegOri$coefficients[1], b = outRegOri$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = x, y = yNoiLe1, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;, main = &quot;Small Noise Added&quot;, ylim = c(ymin, ymax), xlim = c(xmin, xmax)) # Plots the regression line abline(a = outRegNoiLe1$coefficients[1], b = outRegNoiLe1$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = x, y = yNoiLe2, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;, main = &quot;Medium Noise Added&quot;, ylim = c(ymin, ymax), xlim = c(xmin, xmax)) # Plots the regression line abline(a = outRegNoiLe2$coefficients[1], b = outRegNoiLe2$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = x, y = yNoiLe3, xlab = &quot;Ad spending&quot;, ylab = &quot;Revenue&quot;, main = &quot;High Noise Added&quot;, ylim = c(ymin, ymax), xlim = c(xmin, xmax)) # Plots the regression line abline(a = outRegNoiLe3$coefficients[1], b = outRegNoiLe3$coefficients[2], col = &#39;red&#39;, lwd = 2) print(&quot;Original Data LM summary&quot;) ## [1] &quot;Original Data LM summary&quot; summary(outRegOri) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -248.394 -58.805 3.782 63.577 196.745 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 997.3894 28.8185 34.61 &lt;2e-16 *** ## x 5.0247 0.3818 13.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 89.01 on 98 degrees of freedom ## Multiple R-squared: 0.6386, Adjusted R-squared: 0.6349 ## F-statistic: 173.2 on 1 and 98 DF, p-value: &lt; 2.2e-16 print(&quot;Level 1 LM summary&quot;) ## [1] &quot;Level 1 LM summary&quot; summary(outRegNoiLe1) ## ## Call: ## lm(formula = yNoiLe1 ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -225.09 -62.78 -11.97 73.73 254.06 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1004.0513 30.9057 32.49 &lt;2e-16 *** ## x 4.9766 0.4095 12.15 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 95.45 on 98 degrees of freedom ## Multiple R-squared: 0.6011, Adjusted R-squared: 0.5971 ## F-statistic: 147.7 on 1 and 98 DF, p-value: &lt; 2.2e-16 print(&quot;Level 2 LM summary&quot;) ## [1] &quot;Level 2 LM summary&quot; summary(outRegNoiLe2) ## ## Call: ## lm(formula = yNoiLe2 ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -588.4 -122.2 8.4 136.1 449.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1007.1088 64.7010 15.566 &lt; 2e-16 *** ## x 4.9009 0.8573 5.717 1.17e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 199.8 on 98 degrees of freedom ## Multiple R-squared: 0.2501, Adjusted R-squared: 0.2424 ## F-statistic: 32.68 on 1 and 98 DF, p-value: 1.173e-07 print(&quot;Level 3 LM summary&quot;) ## [1] &quot;Level 3 LM summary&quot; summary(outRegNoiLe3) ## ## Call: ## lm(formula = yNoiLe3 ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1157.74 -264.68 -27.29 239.51 894.85 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 998.139 134.510 7.421 4.28e-11 *** ## x 4.948 1.782 2.776 0.00659 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 415.4 on 98 degrees of freedom ## Multiple R-squared: 0.0729, Adjusted R-squared: 0.06344 ## F-statistic: 7.706 on 1 and 98 DF, p-value: 0.006592 where we observe that the point cloud is more dispersed and looks less than a line the more noise is added, but the estimated regression line changes only a little bit. We can also see how the \\(R^2\\) becomes smaller as more noise is added. 4.6 Residual Analysis While the coefficient of determination can tell us how good is the fit of the data, it can’t tell us why is it good or bad. The easiest way to check for any problems is to check the residuals or estimated errors. Right now, we will focus on 4 problems: The regression function is not linear. The variance of the error terms is not constant. There are outliers. Important variables are ommited. 4.6.1 Non-linear regression function Sometimes the relationship between the two variables that we are analyzing is not linear. Looking at the example of the relationship between burger price and burgers sold we can look at the fitted regression line and the residuals plot. Here we can clearly appreciate that there is something wrong. The residuals clearly indicate that a non linear relationship is present in the data. One can solve these problem by transforming one or both of the variables and then applying linear regression. Common transformations functions \\(g\\) are (but are not limited to): \\(g(x) = x^2\\) \\(g(x) = \\sqrt{x}\\) \\(g(x) = log(x)\\) This transformations can be applied to the independent variable, to the dependent variable of both. In the next example we work with \\(log(\\text{Burgers Sold})\\) instead of directly working with “Burgers Sold”. In this case there seems to be a much better fit. We can compare this to the following transformation \\(\\log{(\\text{Price})}\\): It is this these transformations improve the fit of the model, however which one is the best one? One can check the \\(R^2\\) of the different transformations: dat &lt;- read.csv(&quot;Burger Data.csv&quot;) x &lt;- dat$Price y &lt;- dat$Burgers outRegOri &lt;- lm(y ~ x) outRegTr1 &lt;- lm(log(y) ~ x) outRegTr2 &lt;- lm(y ~ log(x)) summary(outRegOri) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.310 -5.637 -0.553 2.899 47.182 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 76.9130 2.0098 38.27 &lt;2e-16 *** ## x -4.3417 0.2194 -19.79 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.522 on 98 degrees of freedom ## Multiple R-squared: 0.7999, Adjusted R-squared: 0.7978 ## F-statistic: 391.7 on 1 and 98 DF, p-value: &lt; 2.2e-16 summary(outRegTr1) ## ## Call: ## lm(formula = log(y) ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.39409 -0.08669 0.00065 0.09325 0.37302 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.518050 0.034072 132.60 &lt;2e-16 *** ## x -0.109240 0.003719 -29.37 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1445 on 98 degrees of freedom ## Multiple R-squared: 0.898, Adjusted R-squared: 0.897 ## F-statistic: 862.8 on 1 and 98 DF, p-value: &lt; 2.2e-16 summary(outRegTr2) ## ## Call: ## lm(formula = y ~ log(x)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.4312 -2.8468 0.1724 3.1830 9.8330 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 99.6184 1.4927 66.74 &lt;2e-16 *** ## log(x) -29.8274 0.7237 -41.22 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.449 on 98 degrees of freedom ## Multiple R-squared: 0.9455, Adjusted R-squared: 0.9449 ## F-statistic: 1699 on 1 and 98 DF, p-value: &lt; 2.2e-16 We will see other ways to choose the transformation later. 4.6.2 Heteroscedasticity Sometimes, errors are expected to increase depending on the explanatory variable. For example, it could be expected that errors for bigger values of the explanatory variable will be also bigger. As an example, consider a new data set with the Height of several children. One could expect there is a linear relationship between the age of a child and the height. However, one also would expect that height difference are bigger for older children than for younger children. We can see this in the following simulated data: dat &lt;- read.csv(&quot;Height Data.csv&quot;) outReg &lt;- lm(Height ~ Age, data = dat) # Original Data par(mfrow = c(1, 2)) plot(x = dat$Age, y = dat$Height, xlab = &quot;Age (years)&quot;, ylab = &quot;Height (in)&quot;, main = &quot;Original Data&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) # Residuals plot(x = dat$Age, y = outReg$residuals, xlab = &quot;Age (years)&quot;, ylab = &quot;Residuals (in)&quot;, main = &quot;Residuals&quot;) abline(h = 0, lwd = 2) While one can infer this error behavior in the original scatter plot, the residual plot makes this pattern much more clearly. In the residual plot, it is pretty clear that the errors increase with the age of the children. While this is not a problem in itself when doing least squares estimation, it might be better to consider other alternatives that do not penalize the same way the errors at young age than the errors at a later age. We will see this with weighted least squares. 4.6.3 Outliers Sometimes the data follows a linear regression for most of the observations, but there might be some observations that do not follow the linear relationship. In the next example, we work with the Wine data set and add outliers, to see the effect this might have on the estimation. dat &lt;- read.csv(&quot;Wine Data.csv&quot;) x &lt;- dat$Glasses y &lt;- dat$Years ymin &lt;- min(y, max(y) - 22) ymax &lt;- max(y, min(y) + 22) xmin &lt;- min(x) xmax &lt;- max(x) par(mfrow = c(2, 2)) # No Outliers outRegNoo &lt;- lm(y ~ x) plot(x = x, y = y, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Life Expectancy&quot;, main = &quot;No Outliers&quot;) abline(a = outRegNoo$coefficients[1], b = outRegNoo$coefficients[2], col = &#39;red&#39;, lwd = 2) # Outlier on the left side # Adds Observation x &lt;- c(x, 0) y &lt;- c(y, 90) outRegLef &lt;- lm(y ~ x) plot(x = x, y = y, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Life Expectancy&quot;, main = &quot;Outlier Left&quot;) points(x = x[21], y = y[21], pch = 16, col = &#39;blue&#39;) abline(a = outRegLef$coefficients[1], b = outRegLef$coefficients[2], col = &#39;red&#39;, lwd = 2) # Outlier on the right side # Adds Observation x[21] &lt;- 4 y[21] &lt;- 68 outRegRig &lt;- lm(y ~ x) plot(x = x, y = y, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Life Expectancy&quot;, main = &quot;Right Outliers&quot;) points(x = x[21], y = y[21], pch = 16, col = &#39;blue&#39;) abline(a = outRegRig$coefficients[1], b = outRegRig$coefficients[2], col = &#39;red&#39;, lwd = 2) # Outlier on the right side # Adds Observation x[21] &lt;- 2 y[21] &lt;- 95 outRegCen &lt;- lm(y ~ x) plot(x = x, y = y, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Life Expectancy&quot;, main = &quot;Center Outliers&quot;) points(x = x[21], y = y[21], pch = 16, col = &#39;blue&#39;) abline(a = outRegCen$coefficients[1], b = outRegCen$coefficients[2], col = &#39;red&#39;, lwd = 2) The presence of outliers, will also be more clear when looking at the residuals. x &lt;- dat$Glasses y &lt;- dat$Years ymin &lt;- min(outRegNoo$residuals, outRegLef$residuals, outRegRig$residuals, outRegCen$residuals) ymax &lt;- max(outRegNoo$residuals, outRegLef$residuals, outRegRig$residuals, outRegCen$residuals) xmin &lt;- min(x) xmax &lt;- max(x) par(mfrow = c(2, 2)) plot(x = dat$Glasses, y = outRegNoo$residuals, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Residuals&quot;, main = &quot;No Outliers&quot;) abline(h = 0, lwd = 2) # Outlier on the left side x[21] &lt;- 0 y[21] &lt;- 90 plot(x = x, y = outRegLef$residuals, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Residuals&quot;, main = &quot;Left Outlier&quot;) abline(h = 0, lwd = 2) # Outlier on the right side # Adds Observation x[21] &lt;- 4 y[21] &lt;- 68 plot(x = x, y = outRegRig$residuals, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Residuals&quot;, main = &quot;Left Outlier&quot;) abline(h = 0, lwd = 2) # Outlier on the right side # Adds Observation x[21] &lt;- 2 y[21] &lt;- 95 plot(x = x, y = outRegCen$residuals, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Average Glasses per Week&quot;, ylab = &quot;Residuals&quot;, main = &quot;Left Outlier&quot;) abline(h = 0, lwd = 2) And we can also see how the \\(R^2\\) changes: summary(outRegNoo) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6443 -1.4398 -0.3390 0.9071 4.8057 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 73.7154 0.8674 84.988 &lt; 2e-16 *** ## x 2.4686 0.4364 5.656 2.29e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.162 on 18 degrees of freedom ## Multiple R-squared: 0.64, Adjusted R-squared: 0.62 ## F-statistic: 32 on 1 and 18 DF, p-value: 2.295e-05 summary(outRegLef) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.550 -2.296 -1.413 1.440 14.028 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 75.9724 1.5101 50.31 &lt;2e-16 *** ## x 1.5258 0.7786 1.96 0.0649 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.056 on 19 degrees of freedom ## Multiple R-squared: 0.1682, Adjusted R-squared: 0.1244 ## F-statistic: 3.841 on 1 and 19 DF, p-value: 0.06486 summary(outRegRig) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.228 -1.428 -0.211 2.283 5.654 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 75.0353 1.4815 50.648 &lt;2e-16 *** ## x 1.2981 0.6965 1.864 0.0779 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.803 on 19 degrees of freedom ## Multiple R-squared: 0.1545, Adjusted R-squared: 0.11 ## F-statistic: 3.473 on 1 and 19 DF, p-value: 0.0779 summary(outRegCen) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4964 -2.2377 -0.9105 0.9596 15.4953 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 74.1257 1.6870 43.939 &lt; 2e-16 *** ## x 2.6895 0.8486 3.169 0.00505 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.214 on 19 degrees of freedom ## Multiple R-squared: 0.3459, Adjusted R-squared: 0.3114 ## F-statistic: 10.05 on 1 and 19 DF, p-value: 0.005048 4.6.4 Variables Ommited When looking at the residuals, there should be no aparent pattern. Sometimes, the pattern is only noticeable when considering other variables. We can see this with the Height data set. The data includes the sex of the children. If you fit least squares without taking this into consideration, a pattern is appears in the residuals when you look at the residuals for each sex. dat &lt;- read.csv(&quot;Height Data.csv&quot;) # Fits linear Model outReg &lt;- lm(dat$Height ~ dat$Age) # Plots residuals for each sex ymin &lt;- min(outReg$residuals) ymax &lt;- max(outReg$residuals) par(mfrow=c(1, 2)) plot(x = dat$Age[dat$Sex == 1], y = outReg$residuals[dat$Sex == 1], ylim = c(ymin, ymax), xlab = &quot;Age&quot;, ylab = &quot;Residuals&quot;, main = &quot;Males&quot;) abline(h = 0, lwd = 2) plot(x = dat$Age[dat$Sex == 0], y = outReg$residuals[dat$Sex == 0], ylim = c(ymin, ymax), xlab = &quot;Age&quot;, ylab = &quot;Residuals&quot;, main = &quot;Females&quot;) abline(h = 0, lwd = 2) 4.7 Cross-Validation When testing different models, a good idea to evaluate the performance of each model beyond \\(R^2\\), is to separate your data set into a training set and a validation or test set. Doing this is called cross-validation. There are several alternatives to doing cross-validation, here are a few of the most relevant ones: Holdout Method (Train/Test Split) Description: The data is split into two (or three) sets: training and test (and sometimes validation). The model is trained on the training set and evaluated on the test set. Use Case: Simple to implement, but has high variance. The performance may depend on the specific split. K-Fold Cross-Validation Description: The data is divided into \\(k\\) equal-sized folds (subsets). The model is trained on \\(k-1\\) folds and tested on the remaining fold. This process is repeated \\(k\\) times, with each fold used as the test set once. Use Case: Works well for most applications and balances the bias-variance tradeoff. Common choices for \\(k\\): 5, 10. Leave-One-Out Cross-Validation (LOOCV) Description: Each data point is used once as a test set, and the rest of the data is used as the training set. This results in \\(n\\) iterations, where \\(n\\) is the number of samples. Use Case: Good when the dataset is small, but can be computationally expensive for large datasets. Leave-P-Out Cross-Validation (LPOCV) Description: Similar to LOOCV, but instead of leaving out one data point, \\(p\\) data points are left out. This creates \\(\\binom{n}{p}\\) different training/testing splits. Use Case: Rarely used due to its high computational cost for large datasets but might be useful in specific scenarios. Stratified K-Fold Cross-Validation Description: Similar to K-fold cross-validation but ensures that each fold has the same proportion of each class in classification tasks (i.e., balanced classes in each fold). Use Case: Useful for imbalanced datasets in classification problems. Repeated K-Fold Cross-Validation Description: A variation of K-Fold Cross-Validation where the process is repeated multiple times with different random splits. Use Case: Provides more robust estimates of model performance, particularly when the dataset is small. Here is an example of \\(k\\)-fold Cross-Validation using the “Burger Data” set. ### Cross Validation Example # Read Data dat &lt;- read.csv(file = &quot;Burger Data.csv&quot;) # Saves Variables x &lt;- dat$Price y &lt;- dat$Burgers # Number of Observations n &lt;- length(y) # Number of Folds numFol &lt;- 10 # Fold Size sizFol &lt;- round(n / numFol) # List Containing the Fold indices fol &lt;- list() # Select Fold Indeces # Initialization ind &lt;- 1:n for(i in 1:numFol){ # Computes the remaining number of indices numInd &lt;- length(ind) # Randomly selects indices from the remaining indeces indFol &lt;- sample(x = 1:length(ind), size = sizFol, replace = FALSE) # Saves the inidices to the list of Fold Indeces fol[[i]] &lt;- ind[indFol] # Removes the indices from the indeces vector ind &lt;- ind[- indFol] } # Models to try X &lt;- cbind(x, log(x), x, log(x)) Y &lt;- cbind(y, y, log(y), log(y)) # Number of Models to compare numMod &lt;- dim(Y)[2] # Saves the validation metric for each model and each fold matMet &lt;- matrix(data = NA, nrow = numFol, ncol = numMod) par(mfrow=c(2, 2)) # Loops through the models for(k in 1:numMod){ yMod &lt;- Y[, k] xMod &lt;- X[, k] plot(x = xMod, y = yMod, xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers&quot;, main = paste0(&quot;Model &quot;, k)) metFol &lt;- numeric(length = numFol) # Loops through the folds for(i in 1:numFol){ # At Each Fold do Linear Regression without the test set outReg &lt;- lm(yMod[-fol[[i]]] ~ xMod[-fol[[i]]]) # Evaluate Out of Sample yHatOut &lt;- outReg$coefficients[1] + outReg$coefficients[2] * xMod[fol[[i]]] if(k &gt;= 3 ){ metEva &lt;- mean((exp(yMod[fol[[i]]]) - exp(yHatOut))^2) } else { metEva &lt;- mean((yMod[fol[[i]]] - yHatOut)^2) } metFol[i] &lt;- metEva } matMet[, k] &lt;- metFol } 4.8 Weighted Least Squares Weighted Least Squares (WLS) is an extension of ordinary least squares (OLS) used when the assumption of constant variance (homoscedasticity) is violated. In cases of heteroscedasticity (unequal variances), OLS produces inefficient estimates. WLS solves this by assigning weights to each observation, giving more importance to data points with lower variance. This leads to more reliable estimates in the presence of heteroscedasticity, making WLS valuable for improving regression models when error variances vary. Lower variance → Higher weight Higher variance → Lower weight In a simple linear regression model with one independent variable, OLS minimizes the sum of squared residuals: \\[ \\min \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\min \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\] Weighted Least Squares (WLS) corrects for heteroscedasticity by minimizing the weighted sum of squared residuals: \\[ \\min \\sum_{i=1}^{n} w_i (y_i - \\hat{y}_i)^2 = \\min \\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i)^2 \\] Step 1: Define the weighted residual sum of squares The objective function is: \\[Q_w(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i)^2\\] We aim to minimize \\(Q(\\beta_0, \\beta_1)\\) with respect to \\(\\beta_0\\) and \\(\\beta_1\\). Step 2: Take the partial derivatives with respect to $ _0 $ and $ _1 $: \\[\\frac{\\partial Q_w}{\\partial \\beta_0} = -2 \\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i)\\] Setting the derivative equal to zero: \\[\\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i) = 0 \\] \\[\\sum_{i=1}^{n} w_i y_i = \\sum_{i=1}^{n} w_i (\\beta_0 + \\beta_1 x_i)\\] \\[\\beta_0 \\sum_{i=1}^{n} w_i + \\beta_1 \\sum_{i=1}^{n} w_i x_i = \\sum_{i=1}^{n} w_i y_i \\tag{3}\\] Now with respect to \\(\\beta_1\\): \\[\\frac{\\partial Q_w}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i) x_i\\] Setting the derivative equal to zero: \\[-2 \\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i) x_i = 0\\] \\[\\sum_{i=1}^{n} w_i y_i x_i = \\sum_{i=1}^{n} w_i (\\beta_0 + \\beta_1 x_i) x_i\\] \\[\\beta_0 \\sum_{i=1}^{n} w_i x_i + \\beta_1 \\sum_{i=1}^{n} w_i x_i^2 = \\sum_{i=1}^{n} w_i y_i x_i \\tag{4}\\] Now we solve the system of two linear equations: \\(\\beta_0 \\sum_{i=1}^{n} w_i + \\beta_1 \\sum_{i=1}^{n} w_i x_i = \\sum_{i=1}^{n} w_i y_i\\) \\(\\beta_0 \\sum_{i=1}^{n} w_i x_i + \\beta_1 \\sum_{i=1}^{n} w_i x_i^2 = \\sum_{i=1}^{n} w_i y_i x_i\\) Solve for \\(\\beta_1\\) Multiply equation (3) by \\(\\frac{\\sum_{i=1}^{n} w_i x_i}{\\sum_{i=1}^n w_i}\\) and subtract it from equation (4) to eliminate \\(\\beta_0\\): \\[\\beta_1 \\left( \\sum_{i=1}^{n} w_i x_i^2 - \\frac{\\left( \\sum_{i=1}^{n} w_i x_i \\right)^2}{\\sum_{i=1}^{n} w_i} \\right) = \\sum_{i=1}^{n} w_i y_i x_i - \\frac{\\sum_{i=1}^{n} w_i x_i \\sum_{i=1}^{n} w_i y_i}{\\sum_{i=1}^{n} w_i}\\] Thus, \\(\\beta_1\\) is: \\[\\beta_1 = \\frac{\\sum_{i=1}^{n} w_i x_i y_i - \\frac{\\sum_{i=1}^{n} w_i x_i \\sum_{i=1}^{n} w_i y_i}{\\sum_{i=1}^{n} w_i}}{\\sum_{i=1}^{n} w_i x_i^2 - \\frac{\\left( \\sum_{i=1}^{n} w_i x_i \\right)^2}{\\sum_{i=1}^{n} w_i}}\\] Then the weighted estimators for \\(\\beta_0\\) and \\(\\beta_1\\) are: \\[\\hat{\\beta}_{w,1} = \\frac{\\sum_{i=1}^{n} w_i x_i y_i - \\frac{\\sum_{i=1}^{n} w_i x_i \\sum_{i=1}^{n} w_i y_i}{\\sum_{i=1}^{n} w_i}}{\\sum_{i=1}^{n} w_i x_i^2 - \\frac{\\left( \\sum_{i=1}^{n} w_i x_i \\right)^2}{\\sum_{i=1}^{n} w_i}},\\] \\[\\hat{\\beta}_{w,0} = \\frac{\\sum_{i=1}^{n} w_i y_i - \\beta_1 \\sum_{i=1}^{n} w_i x_i}{\\sum_{i=1}^{n} w_i}.\\] 4.9 Model in Matrix Form We can specify the same model as in Model, in matrix form as follows: \\[\\mathbf{y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}\\] where: \\(\\mathbf{y}={y_1,\\ldots,y_n}&#39;\\) \\(\\mathbf{e}={e_1,\\ldots,e_n}&#39;\\) \\(\\boldsymbol{\\beta}={\\beta_0,\\beta_1}&#39;\\) \\(\\mathbf{X}=[\\mathbb{1}_n \\mathbf{x}]\\) \\(\\mathbb{1}_n={1,\\ldots,1}&#39;\\) (1 \\(n\\)-times). In this way we can re-write our minimization problem as follows: \\[ \\min \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\min \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 = \\min \\sum_{i=1}^{n} (\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})\\] equivalent to: \\[\\min \\mathbf{y}&#39;\\mathbf{y}- \\mathbf{y}&#39;\\mathbf{X}\\boldsymbol{\\beta}- \\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{y}+ \\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{X}\\boldsymbol{\\beta}\\] The procedure to find the estimators is very similar, working with the gradient instead of the partial derivatives: \\[\\nabla_\\beta Q = \\mathbf{0} \\] then: \\[-\\mathbf{X}&#39;\\mathbf{y}-\\mathbf{X}&#39;\\mathbf{y}+ 2 \\mathbf{X}&#39; \\mathbf{X}\\boldsymbol{\\beta}= \\mathbf{0}\\] \\[2 \\mathbf{X}&#39; \\mathbf{X}\\boldsymbol{\\beta}= 2 \\mathbf{X}&#39;\\mathbf{y}\\] \\[\\mathbf{X}&#39; \\mathbf{X}\\boldsymbol{\\beta}= \\mathbf{X}&#39;\\mathbf{y}\\] \\[\\boldsymbol{\\beta}= \\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}\\mathbf{X}&#39;\\mathbf{y}\\] Notice how we find the estimates much easier. It is also easy to show that a minimum is attained by computing the Hessian Matrix of \\(Q\\) \\[H_Q = 2 \\mathbf{X}&#39; \\mathbf{X}\\] and noticing that: \\[\\mathbf{X}&#39; \\mathbf{X}\\] is positive-definite if \\(\\mathbf{x}\\) is not a constant vector. We can verify that indeed the estimate \\(\\hat{\\boldsymbol{\\beta}}\\) is the same as before by manually doing the computations for this case: We have: \\[ \\mathbf{X} = \\begin{bmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{bmatrix} \\] \\[ \\mathbf{X}&#39; = \\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1 \\\\ x_1 &amp; x_2 &amp; \\cdots &amp; x_n \\end{bmatrix} \\] Multiplying \\(\\mathbf{X}&#39;\\) by \\(\\mathbf{X}\\): \\[ \\mathbf{X}&#39; \\mathbf{X} = \\begin{bmatrix} n &amp; \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i &amp; \\sum_{i=1}^n x_i^2 \\end{bmatrix} \\] Similarly, we have: \\[ \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\] Multiplying \\(\\mathbf{X}&#39;\\) by \\(\\mathbf{y}\\): \\[ \\mathbf{X}&#39; \\mathbf{y} = \\begin{bmatrix} \\sum_{i=1}^n y_i \\\\ \\sum_{i=1}^n x_i y_i \\end{bmatrix} \\] To compute the inverse of \\(\\mathbf{X}&#39;\\mathbf{X}\\) we have that the determinant is: \\[ |\\mathbf{X}&#39; \\mathbf{X}|=\\text{det}(\\mathbf{X}^\\top \\mathbf{X}) = n \\sum_{i=1}^n x_i^2 - \\left( \\sum_{i=1}^n x_i \\right)^2 \\] Then, the inverse is: \\[ (\\mathbf{X}&#39; \\mathbf{X})^{-1} = \\frac{1}{n \\sum_{i=1}^n x_i^2 - \\left( \\sum_{i=1}^n x_i \\right)^2} \\begin{bmatrix} \\sum_{i=1}^n x_i^2 &amp; - \\sum_{i=1}^n x_i \\\\ - \\sum_{i=1}^n x_i &amp; n \\end{bmatrix} \\] Then, we have that: \\[\\begin{align*} (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} &amp;= \\frac{1}{n \\sum_{i=1}^n x_i^2 - \\left( \\sum_{i=1}^n x_i \\right)^2} \\begin{bmatrix} \\sum_{i=1}^n x_i^2 &amp; - \\sum_{i=1}^n x_i \\\\ - \\sum_{i=1}^n x_i &amp; n \\end{bmatrix} \\begin{bmatrix} \\sum_{i=1}^n y_i \\\\ \\sum_{i=1}^n x_i y_i \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} \\frac{1}{n \\sum_{i=1}^n x_i^2 - \\left( \\sum_{i=1}^n x_i \\right)^2} \\left( \\sum_{i=1}^n x_i^2 \\sum_{i=1}^n y_i - \\sum_{i=1}^n x_i \\sum_{i=1}^n x_i y_i \\right) \\\\ \\frac{1}{n \\sum_{i=1}^n x_i^2 - \\left( \\sum_{i=1}^n x_i \\right)^2} \\left( - \\sum_{i=1}^n x_i \\sum_{i=1}^n y_i + n \\sum_{i=1}^n x_i y_i \\right) \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} \\frac{1}{n \\sum_{i=1}^n x_i^2 - (n \\bar{x})^2} \\left( n \\bar{y} \\sum_{i=1}^n x_i^2 - n \\bar{x} \\sum_{i=1}^n x_i y_i \\right) \\\\ \\frac{1}{n \\sum_{i=1}^n x_i^2 - (n \\bar{x})^2} \\left( - n \\bar{x}n \\bar{y} + n \\sum_{i=1}^n x_i y_i \\right) \\end{bmatrix} \\\\ \\end{align*}\\] Now simplifying, the first row, we have that: \\[\\begin{align*} \\frac{n \\bar{y} \\sum_{i=1}^n x_i^2 - n \\bar{x} \\sum_{i=1}^n x_i y_i}{n \\sum_{i=1}^n x_i^2 - (n \\bar{x})^2} &amp;= \\frac{1}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2} \\left( \\bar{y} \\sum_{i=1}^n x_i^2 - \\bar{x} \\sum_{i=1}^n x_i y_i \\right) \\\\ &amp;= \\frac{1}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2} \\left( \\bar{y} \\sum_{i=1}^n x_i^2 - \\bar{y} n\\bar{x}^2 + \\bar{y} n\\bar{x}^2 - \\bar{x} \\sum_{i=1}^n x_i y_i \\right) \\\\ &amp;= \\frac{1}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2} \\left( \\bar{y} \\sum_{i=1}^n x_i^2 - \\bar{y} n\\bar{x}^2 + \\bar{y} n\\bar{x}^2 - \\bar{x} \\sum_{i=1}^n x_i y_i \\right) \\\\ &amp;= \\frac{1}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2} \\left( \\bar{y} \\left(\\sum_{i=1}^n x_i^2 - n\\bar{x}^2 \\right) + \\bar{x} \\left(n\\bar{y}\\bar{x} - \\sum_{i=1}^n x_i y_i \\right) \\right) \\\\ &amp;= \\bar{y} + \\bar{x} \\frac{n\\bar{y}\\bar{x} - \\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2} \\\\ &amp;= \\bar{y} + \\bar{x} \\frac{-(n-1)S_{xy}}{(n-1)S_{xx}} \\\\ &amp;= \\bar{y} - \\beta_1 \\bar{x} \\end{align*}\\] And simplifying the second row, we have: \\[\\frac{1}{n \\sum_{i=1}^n x_i^2 - (n \\bar{x})^2} \\left( - n \\bar{x}n \\bar{y} + n \\sum_{i=1}^n x_i y_i \\right) = \\frac{1}{\\sum_{i=1}^n x_i^2 - n \\bar{x}^2} \\left( - n\\bar{x}\\bar{y} + \\sum_{i=1}^n x_i y_i \\right) = \\frac{S_{xy}}{S_{xx}} = \\beta_1\\] So both expressions are equivalent. 4.9.1 Weighted Least Squares in Matrix Form In a similar way we can solve the weighted least squares problem in matrix form as follows: \\[Q_w = \\sum_{i=1}^{n} w_i (y_i - \\beta_0 - \\beta_1 x_i)^2 = (\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;\\mathbf{W}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{y}&#39;\\mathbf{W}\\mathbf{y}-\\mathbf{y}&#39;\\mathbf{W}\\mathbf{X}\\boldsymbol{\\beta}- \\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{W}\\mathbf{y}+ \\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{W}\\mathbf{X}\\boldsymbol{\\beta}\\] where, \\(\\mathbf{W}\\) is a diagonal matrix with diagonal entries \\(w_1,\\ldots,w_n\\). then the gradient is: \\[ \\nabla_\\beta Q_w = -2 \\mathbf{X}&#39;\\mathbf{W}\\mathbf{y}+ 2\\mathbf{X}&#39;\\mathbf{W}\\mathbf{X}\\boldsymbol{\\beta}\\] then, making it equal to zero, we have that: \\[ 2\\mathbf{X}&#39;\\mathbf{W}\\mathbf{X}\\boldsymbol{\\beta}= 2 \\mathbf{X}&#39;\\mathbf{W}\\mathbf{y}\\] \\[\\mathbf{X}&#39;\\mathbf{W}\\mathbf{X}\\boldsymbol{\\beta}= \\mathbf{X}&#39;\\mathbf{W}\\mathbf{y}\\] \\[\\boldsymbol{\\beta}= \\left(\\mathbf{X}&#39;\\mathbf{W}\\mathbf{X}\\right)^{-1}\\mathbf{X}&#39;\\mathbf{W}\\mathbf{y}\\] And the Hessian Matrix is given by: \\[H_{Q_w} = 2 \\mathbf{X}&#39;\\mathbf{W}\\mathbf{X}\\] which is positive-definite under the same conditions that before and if the entries of \\(\\mathbf{W}\\) are positive. "],["polynomial-regression.html", "5 Polynomial Regression", " 5 Polynomial Regression Polynomial regression is an extension of linear regression where the relationship between the independent variable \\(x\\) and the dependent variable \\(y\\) is modeled as an \\(n\\)-th degree polynomial. Instead of fitting a straight line, it fits a curve that can capture more complex patterns in the data. In polynomial regression, the model takes the form: \\[ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_k x^k + \\epsilon \\] Where: - \\(y\\) is the dependent variable, - \\(x\\) is the independent variable, - \\(\\beta_0, \\beta_1, \\dots, \\beta_n\\) are the coefficients, - \\(\\epsilon\\) is the error term. Polynomial regression is useful when the data exhibits a nonlinear trend that cannot be well approximated by a straight line. By increasing the degree of the polynomial, the model can fit more complex patterns, though caution is needed to avoid overfitting. There are two main scenarios for using polynomial regression: The underlying process is inherently polynomial. A polynomial provides a good approximation for the relationship. If you know that the phenomenon you’re modeling follows a polynomial function, there’s no need to worry about overfitting. You can confidently use the polynomial degree that best fits the known relationship. However, if you’re using polynomial regression to approximate a nonlinear relationship between the independent and dependent variables, be cautious. Overfitting can become a significant issue, particularly outside the range of the data used for model fitting, where predictions can be unreliable. In the second scenario, it’s common to limit the polynomial degree to 2nd or 3rd order to prevent overfitting while still capturing key nonlinear patterns. We use our Burger Data set, to see an example of using polynomial regression to approximate a non-linear relationship. We will use a second degree polynomial to fit the data. dat &lt;- read.csv(&quot;Burger Data.csv&quot;) # Polynomial Regression Fit outRegPol &lt;- lm(Burgers ~ Price + I(Price^2), data = dat) # Data Limits xmin &lt;- min(dat$Price) xmax &lt;- max(dat$Price) ymin &lt;- min(dat$Burgers) ymax &lt;- max(dat$Burgers) # Scatter plot plot(x = dat$Price, y = dat$Burgers, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers Sold&quot;) par(new = TRUE) # Plots real non-linear relationship curve(expr = 100 - 30 * log(x), from = xmin, to = xmax, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;&quot;, ylab = &quot;&quot;, col = &#39;red&#39;, lwd = 2) par(new = TRUE) # Plots approximated polynomial relationship curve(expr = outRegPol$coefficients[1] + outRegPol$coefficients[2] * x + outRegPol$coefficients[3] * x^2, from = xmin, to = xmax, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;&quot;, ylab = &quot;&quot;, col = &#39;blue&#39;, lwd = 2) The second-degree polynomial approximation provides a reasonably good fit to the data. The red line represents the true nonlinear relationship, while the blue line shows the polynomial approximation. However, if we restrict the dataset to prices below $10, the fit noticeably worsens. This demonstrates a limitation of the polynomial model: while it might fit well within certain ranges, it can lead to unrealistic predictions outside those bounds. For instance, the model might predict that burger sales will eventually rise again as prices increase, which contradicts typical market expectations. dat &lt;- read.csv(&quot;Burger Data.csv&quot;) # Select only Prices bellow 10 dollars sel &lt;- dat$Price &lt; 10 datRes &lt;- dat[sel, ] # Polynomial Regression Fit outRegPolRes &lt;- lm(Burgers ~ Price + I(Price^2), data = datRes) # Data Limits xmin &lt;- min(dat$Price) xmax &lt;- max(dat$Price) ymin &lt;- min(dat$Burgers) ymax &lt;- max(dat$Burgers) # Scatter plot plot(x = dat$Price, y = dat$Burgers, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers Sold&quot;, main = &quot;Polynomial fit with Restricted data (Price &lt; 10)&quot;) par(new = TRUE) # Plots real non-linear relationship curve(expr = 100 - 30 * log(x), from = xmin, to = xmax, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;&quot;, ylab = &quot;&quot;, col = &#39;red&#39;, lwd = 2) par(new = TRUE) # Plots approximated polynomial relationship curve(expr = outRegPolRes$coefficients[1] + outRegPolRes$coefficients[2] * x + outRegPolRes$coefficients[3] * x^2, from = xmin, to = xmax, ylim = c(ymin, ymax), xlim = c(xmin, xmax), xlab = &quot;&quot;, ylab = &quot;&quot;, col = &#39;blue&#39;, lwd = 2) We can also check the \\(R^2\\) of the restricted fit. summary(outRegPolRes) ## ## Call: ## lm(formula = Burgers ~ Price + I(Price^2), data = datRes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.0809 -3.5462 -0.0635 3.0957 13.8374 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 116.0882 3.2929 35.254 &lt; 2e-16 *** ## Price -18.3910 1.2519 -14.691 &lt; 2e-16 *** ## I(Price^2) 1.0709 0.1105 9.695 4.13e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.062 on 63 degrees of freedom ## Multiple R-squared: 0.9131, Adjusted R-squared: 0.9104 ## F-statistic: 331.2 on 2 and 63 DF, p-value: &lt; 2.2e-16 And notice that, the \\(R^2\\) with the restricted data is pretty good, however we see that outside the restricted range the fit is pretty bad. Although polynomial regression is technically a form of multivariate linear regression—since it involves multiple polynomial terms of a single independent variable—it can still be analyzed using the tools discussed in the next chapter. Despite having multiple polynomial terms, the model has only one original independent variable that has been transformed. Consequently, the relationship between the transformed variable and the dependent variable can be effectively visualized using a scatter plot. "],["multiple-linear-regression.html", "6 Multiple Linear Regression 6.1 Introduction 6.2 Example 6.3 Least Squares Estimation 6.4 Properties of the Estimates 6.5 Multiple \\(R^2\\) 6.6 Geometric Interpretation of Multiple Linear Regression 6.7 Centered and Standarized Variables 6.8 Variable Cross-Effects 6.9 Outliers and Leverage 6.10 Stability of the Solution", " 6 Multiple Linear Regression 6.1 Introduction Multiple regression is a statistical technique used to model the relationship between a dependent variable and two or more independent variables. It extends simple linear regression by allowing for a more complex analysis of how various factors impact an outcome. The general form of the multiple regression equation is: \\[ y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\ldots + \\beta_p x_{p,i} + e_i \\quad i=\\{1,\\ldots,n\\} \\] Where: - \\(y\\) is the dependent variable. - \\(\\beta_0\\) is the intercept. - \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are the coefficients of the independent variables \\(X_1, X_2, \\ldots, X_p\\). - $e represents the error term. This technique is widely used across various fields, including economics, social sciences, and healthcare, to control for multiple factors and enhance prediction accuracy. However, it requires careful attention to assumptions. Beyond linearity, now we have to consider issues like independence, multicollinearity, and variable selection which can affect the results. We already have seen an example of Multiple linear regression when we worked with Polynomial regression. However, multiple linear regression is more general. 6.2 Example Consider, for example, the task of explaining a country’s GDP using other economic variables such as inflation, unemployment, reference interest rate, government spending (as a percentage of GDP), and exports (as a percentage of GDP). In this case, visualization is not as straightforward, and visually inspecting these relationships is much less practical. Nevertheless, with this number of variables, we can visually explore the relationships between them as follows: # Reads Data dat &lt;- read.csv(file = &quot;Gdp Data.csv&quot;) # Plot the scatterplots for each pair of variables pairs(dat) Here we can see, that some independent variables are more related to GDP and some independent variables are more related between themselves. This is valuable information that will help us to develop the right linear model with this variables. We can also observe the correlation between these variables as follows: # Computes the correlation between variables cor(dat) ## gdp inf une int gov exp ## gdp 1.0000000 0.875131082 -0.74874795 0.6964256 0.22172279 0.173602651 ## inf 0.8751311 1.000000000 -0.78173033 0.8292061 0.31103644 0.005685918 ## une -0.7487479 -0.781730327 1.00000000 -0.3642453 -0.16674407 0.010553855 ## int 0.6964256 0.829206121 -0.36424525 1.0000000 0.21389456 0.015699798 ## gov 0.2217228 0.311036436 -0.16674407 0.2138946 1.00000000 0.018475446 ## exp 0.1736027 0.005685918 0.01055386 0.0156998 0.01847545 1.000000000 We can also fit simple linear regression with each one of the independent variables. Inflation Rate # Fits with Inflation outRegInf &lt;- lm(gdp ~ inf, data = dat) varVal &lt;- dat$inf out &lt;- outRegInf varNam &lt;- &quot;Inflation Rate&quot; # Plots Regression Line and Scatterplot and residuals plot par(mfrow = c(1, 2)) plot(x = varVal, y = dat$gd, xlab = varNam, ylab = &quot;GDP&quot;) abline(a = out$coefficients[1], b = out$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = varVal, y = out$residuals, xlab = varNam, ylab = &quot;Residuals&quot;) abline(h = 0, lwd = 2) Unemployment Rate # Fits with Inflation outRegUne &lt;- lm(gdp ~ une, data = dat) varVal &lt;- dat$une out &lt;- outRegUne varNam &lt;- &quot;Unemplyment Rate&quot; # Plots Regression Line and Scatterplot and residuals plot par(mfrow = c(1, 2)) plot(x = varVal, y = dat$gd, xlab = varNam, ylab = &quot;GDP&quot;) abline(a = out$coefficients[1], b = out$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = varVal, y = out$residuals, xlab = varNam, ylab = &quot;Residuals&quot;) abline(h = 0, lwd = 2) Interest Rate # Fits with Inflation outRegInt &lt;- lm(gdp ~ int, data = dat) varVal &lt;- dat$int out &lt;- outRegInt varNam &lt;- &quot;Interest Rate&quot; # Plots Regression Line and Scatterplot and residuals plot par(mfrow = c(1, 2)) plot(x = varVal, y = dat$gd, xlab = varNam, ylab = &quot;GDP&quot;) abline(a = out$coefficients[1], b = out$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = varVal, y = out$residuals, xlab = varNam, ylab = &quot;Residuals&quot;) abline(h = 0, lwd = 2) Goverment Spending # Fits with Inflation outRegGov &lt;- lm(gdp ~ gov, data = dat) varVal &lt;- dat$gov out &lt;- outRegGov varNam &lt;- &quot;Goverment Spending&quot; # Plots Regression Line and Scatterplot and residuals plot par(mfrow = c(1, 2)) plot(x = varVal, y = dat$gd, xlab = varNam, ylab = &quot;GDP&quot;) abline(a = out$coefficients[1], b = out$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = varVal, y = out$residuals, xlab = varNam, ylab = &quot;Residuals&quot;) abline(h = 0, lwd = 2) Exports # Fits with Inflation outRegExp &lt;- lm(gdp ~ exp, data = dat) varVal &lt;- dat$exp out &lt;- outRegExp varNam &lt;- &quot;Exports&quot; # Plots Regression Line and Scatterplot and residuals plot par(mfrow = c(1, 2)) plot(x = varVal, y = dat$gd, xlab = varNam, ylab = &quot;GDP&quot;) abline(a = out$coefficients[1], b = out$coefficients[2], col = &#39;red&#39;, lwd = 2) plot(x = varVal, y = out$residuals, xlab = varNam, ylab = &quot;Residuals&quot;) abline(h = 0, lwd = 2) All of them seem like good candidates for a linear relationship with the GDP, however when we use them all together, a more careful analysis should be made. We can see the summary reports for the individual regressions and the regression with all independent variables as follows: outRegAll &lt;- lm(gdp ~ inf + une + int + gov + exp, data = dat) # Summary All print(&quot;All Independent Variables&quot;) ## [1] &quot;All Independent Variables&quot; summary(outRegAll) ## ## Call: ## lm(formula = gdp ~ inf + une + int + gov + exp, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.56610 -0.38300 -0.00634 0.36630 1.22542 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.555301 0.380440 4.088 6.41e-05 *** ## inf 0.312218 0.090012 3.469 0.000647 *** ## une -0.377334 0.129275 -2.919 0.003938 ** ## int 0.177827 0.128312 1.386 0.167403 ## gov -0.008483 0.010361 -0.819 0.413950 ## exp 0.064657 0.011930 5.420 1.80e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5024 on 190 degrees of freedom ## Multiple R-squared: 0.8096, Adjusted R-squared: 0.8046 ## F-statistic: 161.6 on 5 and 190 DF, p-value: &lt; 2.2e-16 print(&quot;Only Inflation Rate&quot;) ## [1] &quot;Only Inflation Rate&quot; summary(outRegInf) ## ## Call: ## lm(formula = gdp ~ inf, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.40960 -0.38896 0.03562 0.37998 1.33364 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.02551 0.08705 11.78 &lt;2e-16 *** ## inf 0.49489 0.01965 25.19 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5514 on 194 degrees of freedom ## Multiple R-squared: 0.7659, Adjusted R-squared: 0.7646 ## F-statistic: 634.5 on 1 and 194 DF, p-value: &lt; 2.2e-16 print(&quot;Only Unemployment Rate&quot;) ## [1] &quot;Only Unemployment Rate&quot; summary(outRegUne) ## ## Call: ## lm(formula = gdp ~ une, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.42276 -0.49693 0.02667 0.49525 2.79562 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.0868 0.2046 29.74 &lt;2e-16 *** ## une -1.0400 0.0661 -15.73 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7554 on 194 degrees of freedom ## Multiple R-squared: 0.5606, Adjusted R-squared: 0.5584 ## F-statistic: 247.5 on 1 and 194 DF, p-value: &lt; 2.2e-16 print(&quot;Only Interest Rate&quot;) ## [1] &quot;Only Interest Rate&quot; summary(outRegInt) ## ## Call: ## lm(formula = gdp ~ int, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.27807 -0.50801 -0.00257 0.50336 2.69719 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.31600 0.32323 -4.071 6.8e-05 *** ## int 0.86483 0.06398 13.517 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8178 on 194 degrees of freedom ## Multiple R-squared: 0.485, Adjusted R-squared: 0.4824 ## F-statistic: 182.7 on 1 and 194 DF, p-value: &lt; 2.2e-16 print(&quot;Only Government Spending&quot;) ## [1] &quot;Only Government Spending&quot; summary(outRegGov) ## ## Call: ## lm(formula = gdp ~ gov, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4370 -0.6442 -0.1258 0.7429 3.1839 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.37117 0.51450 2.665 0.00835 ** ## gov 0.06464 0.02041 3.167 0.00179 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.111 on 194 degrees of freedom ## Multiple R-squared: 0.04916, Adjusted R-squared: 0.04426 ## F-statistic: 10.03 on 1 and 194 DF, p-value: 0.001789 print(&quot;Only Exports&quot;) ## [1] &quot;Only Exports&quot; summary(outRegExp) ## ## Call: ## lm(formula = gdp ~ exp, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0084 -0.6679 -0.1133 0.6581 3.0810 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.32709 0.27818 8.365 1.16e-14 *** ## exp 0.06540 0.02664 2.455 0.015 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.122 on 194 degrees of freedom ## Multiple R-squared: 0.03014, Adjusted R-squared: 0.02514 ## F-statistic: 6.028 on 1 and 194 DF, p-value: 0.01496 As we can see, the values for the coefficients can change when doing simple linear regression and multiple linear regression. If the changes are very dramatic (like change in the sign of the coefficient) further inspection is necessary for that variable. 6.3 Least Squares Estimation For least squares estimation, we need to solve the problem: \\[ \\min_\\boldsymbol{\\beta}Q(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - \\hat{y}(\\boldsymbol{\\beta}))^2 = (\\mathbf{y}- \\hat{\\mathbf{y}})&#39;(\\mathbf{y}- \\hat{\\mathbf{y}}) = (\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\] The representation in matrix notation of the problem, allows us to use the same expression to solve this problem as with simple linear regression. The solution is obtained in the exact same way, and is given by: \\[ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39; \\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y} \\] however in this case: \\[ \\hat{\\boldsymbol{\\beta}} = \\left(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2,\\ldots,\\hat{\\beta}_p\\right)&#39; \\] this is the reason, working in matrix form is very useful. 6.4 Properties of the Estimates As with simple linear regression, we can consider several estimates: \\(\\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\beta}\\) the estimates of the observations, \\(\\hat{\\mathbf{e}} = \\mathbf{y}- \\hat{\\mathbf{y}} = \\mathbf{y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) the estimates of the errors. We also note that: \\[ \\hat{\\mathbf{y}} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{y}= \\mathbf{H}y \\] where \\(\\mathbf{H}\\) is called the hat matrix, because it transforms \\(\\mathbf{y}\\) into \\(\\hat{\\mathbf{y}}\\), or the projection matrix. We will see that: \\(\\hat{\\boldsymbol{\\beta}}\\) is a linear combination of \\(y\\). The sum of the estimated errors is equal to zero, \\(\\sum_{i=1}^n \\hat{e_i} = 0\\). \\(\\hat{\\mathbf{e}}\\) and \\(\\hat{\\mathbf{x}_j}\\) are orthogonal for \\(j=\\{1,\\ldots,p\\}\\). \\(\\hat{\\mathbf{e}}\\) and \\(\\hat{\\mathbf{y}}\\) are orthogonal. \\(\\bar{y} = \\hat{\\bar{y}}\\). To see that \\(\\hat{\\boldsymbol{\\beta}}\\) is a linear combination of \\(y\\), we need to express \\(\\hat{\\boldsymbol{\\beta}}\\) as follows: \\[ \\hat{\\boldsymbol{\\beta}} = \\mathbf{A}\\mathbf{y} \\] for some matrix \\(\\mathbf{A}\\). This is very easy to do, we just let \\(\\mathbf{A}= (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\), so: \\[ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}= \\mathbf{A}\\mathbf{y} \\] Now to see that the sum of the estimated errors is equal to zero, \\(\\sum_{i=1}^n \\hat{e_i} = 0\\), we notice that we need to show that: \\[ \\hat{\\mathbf{e}}&#39; \\mathbf{1}= 0 \\] To do so we notice that: \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y} &amp;\\implies (\\mathbf{X}&#39;\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}&#39;\\mathbf{y}\\\\ &amp;\\implies \\mathbf{X}&#39;\\mathbf{y}- \\mathbf{X}&#39;\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}\\\\ &amp;\\implies \\mathbf{X}&#39;\\left(\\mathbf{y}- \\hat{\\mathbf{y}}\\right) = \\mathbf{0}\\\\ &amp;\\implies \\mathbf{X}&#39;\\hat{\\mathbf{e}} = \\mathbf{0} \\end{align*}\\] Now focusing on the product \\(\\mathbf{X}&#39;\\hat{\\mathbf{e}}\\) we have that: \\[ \\mathbf{X}&#39;\\hat{\\mathbf{e}} = \\left[\\begin{matrix} \\mathbf{1}&#39; \\\\ \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\vdots \\\\ \\mathbf{x}_p \\end{matrix}\\right] \\hat{\\mathbf{e}} = \\left[\\begin{matrix} \\mathbf{1}&#39; \\hat{\\mathbf{e}} \\\\ \\mathbf{x}_1 \\hat{\\mathbf{e}} \\\\ \\mathbf{x}_2 \\hat{\\mathbf{e}} \\\\ \\vdots \\\\ \\mathbf{x}_p \\hat{\\mathbf{e}} \\end{matrix}\\right] \\] So we have that: \\[ \\left[\\begin{matrix} \\mathbf{1}&#39; \\hat{\\mathbf{e}} \\\\ \\mathbf{x}_1 \\hat{\\mathbf{e}} \\\\ \\mathbf{x}_2 \\hat{\\mathbf{e}} \\\\ \\vdots \\\\ \\mathbf{x}_p \\hat{\\mathbf{e}} \\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{matrix}\\right] \\] So from the first line of this result, we have that: \\[ \\mathbf{1}&#39; \\hat{\\mathbf{e}} = 0 \\] which is the result we wanted to proof. Now, to show that \\(\\hat{\\mathbf{e}}\\) and \\(\\hat{\\mathbf{x}_j}\\) are orthogonal for \\(j=\\{1,\\ldots,p\\}\\), we use again on: \\[ \\left[\\begin{matrix} \\mathbf{1}&#39; \\hat{\\mathbf{e}} \\\\ \\mathbf{x}_1 \\hat{\\mathbf{e}} \\\\ \\mathbf{x}_2 \\hat{\\mathbf{e}} \\\\ \\vdots \\\\ \\mathbf{x}_p \\hat{\\mathbf{e}} \\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{matrix}\\right] \\] And notice that lines 2 to \\(p+1\\) proof this results, that is \\[ \\mathbf{x}_i &#39; \\hat{\\mathbf{e}} = 0 \\quad i=\\{1,\\ldots,p\\} \\] Now to show that \\(\\hat{\\mathbf{e}}\\) and \\(\\hat{\\mathbf{y}}\\) are orthogonal, we show that: \\[ \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{y}} = 0 \\] Now \\[\\begin{align*} \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{y}} &amp;= (\\mathbf{y}- \\hat{\\mathbf{y}})&#39;\\hat{\\mathbf{y}} \\\\ &amp;= \\left(\\mathbf{y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\right)&#39;\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\ &amp;= \\left(\\mathbf{y}- \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}\\right)&#39;\\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}\\\\ &amp;= \\mathbf{y}&#39; \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}- \\mathbf{y}&#39; \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{y}\\\\ &amp;= \\mathbf{y}&#39; \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}- \\mathbf{y}&#39; \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{y}\\\\ &amp;= 0 \\end{align*}\\] Finally, to show that \\(\\bar{y} = \\hat{\\bar{y}}\\), we use: \\[ \\hat{\\mathbf{e}}&#39;\\mathbf{1}= (\\mathbf{y}- \\hat{\\mathbf{y}})&#39;\\mathbf{1}= \\mathbf{y}&#39;\\mathbf{1}- \\hat{\\mathbf{y}}&#39;\\mathbf{1}= \\sum_{i=1}^ny_i - \\sum_{i=1}^n\\hat{y}_i = n\\bar{y} - n\\hat{\\bar{y}} \\] since \\(\\hat{\\mathbf{e}}&#39;\\mathbf{1}= 0\\), then we have that \\[ n\\bar{y} - n\\hat{\\bar{y}} = 0 \\implies n\\bar{y} = n\\hat{\\bar{y}} \\implies \\bar{y} = \\hat{\\bar{y}} \\] 6.5 Multiple \\(R^2\\) As with simple linear regression we can explain the total variability, by decomposing the variability in two parts, the regression variability and the error variability. First, we define this concepts: Total Sum of Squares \\(SS_{tot}\\): The total sum of squares measures the total variability in \\(\\mathbf{y}\\): \\[ SS_{tot} = (\\mathbf{y} - \\bar{y} \\mathbf{1})&#39; (\\mathbf{y} - \\bar{y} \\mathbf{1}) \\] Residual Sum of Squares \\(SS_{res}\\): The residual sum of squares measures the unexplained variability in the regression model: \\[ SS_{res} = (\\mathbf{y} - \\hat{\\mathbf{y}})&#39; (\\mathbf{y} - \\hat{\\mathbf{y}}) \\] Explained Sum of Squares \\(SS_{reg}\\) The explained sum of squares measures how much of the total variability is explained by the regression model. It is the difference between the predicted values and the mean of \\(\\mathbf{y}\\): \\[ SS_{reg} = (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1})&#39; (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) \\] As with simple linear regression, it can be shown that: \\[ SS_{tot} = SS_{reg} + SS_{res} \\] To see this, we start form \\(SS_{tot}\\), and do the adding and subtracting trick: \\[\\begin{align*} SS_{tot} &amp;= (\\mathbf{y} - \\bar{y} \\mathbf{1})&#39; (\\mathbf{y} - \\bar{y} \\mathbf{1}) \\\\ &amp;= (\\mathbf{y} - \\hat{\\mathbf{y}} + \\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1})&#39; (\\mathbf{y} - \\hat{\\mathbf{y}} + \\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) \\\\ &amp;= (\\mathbf{y} - \\hat{\\mathbf{y}})&#39; (\\mathbf{y} - \\hat{\\mathbf{y}}) + (\\mathbf{y} - \\hat{\\mathbf{y}})&#39; (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) + (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1})&#39; (\\mathbf{y} - \\hat{\\mathbf{y}}\\mathbf{1}) + (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1})&#39; (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) \\end{align*}\\] Now, notice that: \\[ (\\mathbf{y} - \\hat{\\mathbf{y}})&#39; (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) = \\hat{\\mathbf{e}}&#39; (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) = \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{y}} - \\bar{y}\\hat{\\mathbf{e}}&#39; \\mathbf{1} = 0 - \\bar{y}0 = 0 \\] And similarly for \\((\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1})&#39; (\\mathbf{y} - \\hat{\\mathbf{y}}\\mathbf{1}) = 0\\), then: \\[ SS_{tot} = (\\mathbf{y} - \\hat{\\mathbf{y}})&#39; (\\mathbf{y} - \\hat{\\mathbf{y}}) + (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1})&#39; (\\hat{\\mathbf{y}} - \\bar{y} \\mathbf{1}) = SS_{reg} + SS_{res} \\] The multiple \\(R^2\\) is the variability explained by the regression with respect to the total variability and can be expressed as: \\[ R^2 = \\frac{SS_{reg}}{SS_{tot}} \\] or using the previous expression \\[ 1 = \\frac{SS_{tot}}{SS_{tot}} = \\frac{SS_{reg}}{SS_{tot}} + \\frac{SS_{res}}{SS_{tot}} = R^2 + \\frac{SS_{res}}{SS_{tot}} \\implies R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\] Finally, we work on the expressions of \\(SS_{res}\\) and \\(SS_{tot}\\), to express them in terms of projection matrices. First note that: \\[ \\mathbf{y}- \\hat{\\mathbf{y}} = \\mathbf{y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{y}- \\mathbf{H}\\mathbf{y}= (\\mathbf{I}- \\mathbf{H})\\mathbf{y} \\] and also notice that \\((\\mathbf{I}- \\mathbf{H})\\) is symmetric and: \\[ (\\mathbf{I}- \\mathbf{H})(\\mathbf{I}- \\mathbf{H}) = \\mathbf{I}-\\mathbf{H}- \\mathbf{H}+ \\mathbf{H}\\mathbf{H} \\] and \\[ \\mathbf{H}\\mathbf{H}= \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; = \\mathbf{H} \\] this means \\(\\mathbf{H}\\) is idempotent. In fact, all projection matrices are idempotent. Then, we have that: \\[ (\\mathbf{I}- \\mathbf{H})(\\mathbf{I}- \\mathbf{H}) = \\mathbf{I}-\\mathbf{H}- \\mathbf{H}+ \\mathbf{H}= \\mathbf{I}-\\mathbf{H}- \\mathbf{H} \\] which makes \\(\\mathbf{I}- \\mathbf{H}\\) also idempotent. Therefore: \\[ SS_{res} = (\\mathbf{y}- \\hat{\\mathbf{y}})&#39;(\\mathbf{y}- \\hat{\\mathbf{y}}) = ((\\mathbf{I}- \\mathbf{H})\\mathbf{y})&#39;((\\mathbf{I}- \\mathbf{H})\\mathbf{y}) = \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}= \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y} \\] And we can do a similar trick for the \\(SS_{tot}\\) by writing \\(\\bar{y} \\mathbf{1}\\) as a result of projecting \\(\\mathbf{y}\\) with a design matrix \\(\\mathbf{1}\\): \\[ \\bar{y} \\mathbf{1}= \\mathbf{1}\\bar{y} = \\mathbf{1}\\frac{1}{n} \\sum_{i=1}^n y_i = \\mathbf{1}\\frac{1}{n}\\mathbf{1}&#39; \\mathbf{y}= \\mathbf{1}(\\mathbf{1}&#39;\\mathbf{1})^{-1}\\mathbf{1}&#39; \\mathbf{y} \\] where we use the fact that \\(\\mathbf{1}&#39;\\mathbf{1}= n\\). We call \\(\\mathbf{H}_0 = \\mathbf{1}(\\mathbf{1}&#39;\\mathbf{1})^{-1}\\mathbf{1}&#39;\\), since \\(\\mathbf{1}(\\mathbf{1}&#39;\\mathbf{1})^{-1}\\mathbf{1}&#39;\\) is a projection matrix. And since it is a projection matrix it is idempotent (it is also not difficult to check this manually) and \\(\\mathbf{I}- \\mathbf{H}_0\\) is also idempotent. So we can do: \\[ \\mathbf{y}- \\hat{y}\\mathbf{1}= \\mathbf{y}- \\mathbf{H}_0 \\mathbf{y}= (\\mathbf{I}-\\mathbf{H}_0)\\mathbf{y} \\] \\[ SS_{tot} = (\\mathbf{y}- \\bar{y}\\mathbf{1})&#39;(\\mathbf{y}- \\bar{y}\\mathbf{1}) = ((\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y})&#39;((\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}) = \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}= \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y} \\] so the \\(R^2\\) can be expressed as follows: \\[ R^2 = 1 - \\frac{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}}{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}} \\] When written like this, it is easy to see that: \\[ \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}= \\min_\\boldsymbol{\\beta}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\] the solution to this minimization problem, since we are using the optimal value \\(\\hat{\\boldsymbol{\\beta}}\\). And \\[ \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}= \\min_{\\beta_0} (\\mathbf{y}- \\mathbf{X}_0 \\beta_0)&#39;(\\mathbf{y}- \\mathbf{X}_0 \\beta_0) \\] where \\(\\mathbf{X}_0\\) is just a matrix with one column \\(\\mathbf{1}\\). Now, we also have that: \\[ \\min_\\boldsymbol{\\beta}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\leq \\min_{\\beta_0} (\\mathbf{y}- \\mathbf{X}_0 \\beta_0)&#39;(\\mathbf{y}- \\mathbf{X}_0 \\beta_0) \\] therefore \\[ \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}\\leq \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y} \\] and since both of them are quadratic forms, we have that: \\(\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}, \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}\\geq 0\\) then: \\[0 \\leq \\frac{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}}{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}} \\leq 0\\] then: \\[0 \\leq R^2 \\leq 0\\]. Where we use the fact that all symmetric idempotent matrices are symmetric positive semi-definite. Another interpretation of \\(R^2\\) is the percentage of the variability explained by multiple regression of a “poor man’s regression” in which you don’t have independent variables (that is you are independent variable poor). In this way, we can define \\[ \\bar{y} \\mathbf{1}= \\hat{\\mathbf{y}}_0 \\] the “poor man’s prediction”, of which \\(\\mathbf{H}_0\\) is it’s projection matrix (or hat matrix). 6.6 Geometric Interpretation of Multiple Linear Regression Multiple linear regression can be thought as projecting \\(\\mathbf{y}\\) in the column space of the design matrix \\(\\mathbf{X}\\). The following diagram pictures multiple linear regression. Here we can see several components: \\(\\mathbf{y}\\) is the vector of observations. Is a vector in \\(\\mathbb{R}^n\\). The grey hyper-plane is the column space generated by \\(\\mathbf{X}\\), a sub-space of \\(\\mathbb{R}^n\\). The multiple regression prediction \\(\\hat{\\mathbf{y}}\\) of \\(\\mathbf{y}\\) is the projection of \\(\\mathbf{y}\\) on the space generated by the column of \\(\\mathbf{X}\\). The poor man’s prediction \\(\\hat{\\mathbf{y}}_0\\), in the column space of \\(\\mathbf{X}\\) (since, one of the columns is \\(\\mathbf{1}\\)), but in most cases it is different to \\(\\hat{\\mathbf{y}}\\) (the closest vector in the column space of \\(\\mathbf{X}\\) to \\(\\mathbf{y}\\)). We notice that the differences: \\(\\mathbf{y}- \\hat{\\mathbf{y}}\\). \\(\\mathbf{y}- \\hat{\\mathbf{y}}_0\\) \\(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_0\\) form a right triangle, then it must be that: \\[ ||\\mathbf{y}- \\hat{\\mathbf{y}}_0||^2 = ||\\mathbf{y}- \\hat{\\mathbf{y}}||^2 + ||\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_0||^2 \\] which is the same as \\[ (\\mathbf{y}- \\hat{\\mathbf{y}})&#39;(\\mathbf{y}- \\hat{\\mathbf{y}}_0) = (\\mathbf{y}- \\hat{\\mathbf{y}})&#39;(\\mathbf{y}- \\hat{\\mathbf{y}}) + (\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_0)&#39;(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_0) \\] that can be expressed as: \\[ SS_{tot} = SS_{res} + SS_{reg} \\] 6.7 Centered and Standarized Variables 6.7.1 Centered Variables Like with simple linear regression we can center and standardize our variables. For this section, let us use the notation: \\[\\mathbf{X}= \\left[\\mathbf{x}_1, \\mathbf{x}_2, \\ldots \\mathbf{x}_p \\right]\\] a matrix with \\(p\\) variables in which each column is a variable \\(\\mathbf{x}_i\\). In the context of linear regression you can this is similar to the design matrix except that it doesn’t have the column of ones. In this way, we will rename the design matrix as \\[\\mathbf{X}_{*} = \\left[\\mathbf{1}\\mathbf{X}\\right] \\] we introduce this notation, since we don’t want to center or standardize the column of ones. To center matrix \\(\\mathbf{X}\\) we need to remove the mean of every column. Notice that the vector of means is given by: \\[\\bar{\\mathbf{x}} = \\left[\\begin{matrix} \\bar{\\mathbf{x}}_1 \\\\ \\bar{\\mathbf{x}}_2 \\\\ \\vdots \\\\ \\bar{\\mathbf{x}}_p \\end{matrix}\\right] = \\left[\\begin{matrix} \\frac{1}{n} \\mathbf{x}_1&#39;\\mathbf{1}\\\\ \\frac{1}{n} \\mathbf{x}_2&#39;\\mathbf{1}\\\\ \\vdots \\\\ \\frac{1}{n} \\mathbf{x}_p&#39;\\mathbf{1} \\end{matrix}\\right] = \\frac{1}{n} \\left[\\begin{matrix} \\mathbf{x}_1&#39;\\mathbf{1}\\\\ \\mathbf{x}_2&#39;\\mathbf{1}\\\\ \\vdots \\\\ \\mathbf{x}_p&#39;\\mathbf{1} \\end{matrix}\\right] = \\frac{1}{n} \\mathbf{X}&#39; \\mathbf{1}\\] then the centered data \\(\\mathbf{X}_c\\) is given by: \\[\\mathbf{X}_c = \\mathbf{X}- \\left[\\begin{matrix} \\bar{\\mathbf{x}}_1 &amp; \\bar{\\mathbf{x}}_2 &amp; \\dots &amp; \\bar{\\mathbf{x}}_p\\\\ \\bar{\\mathbf{x}}_1 &amp; \\bar{\\mathbf{x}}_2 &amp; \\dots &amp; \\bar{\\mathbf{x}}_p\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\bar{\\mathbf{x}}_1 &amp; \\bar{\\mathbf{x}}_2 &amp; \\dots &amp; \\bar{\\mathbf{x}}_p \\end{matrix}\\right] = \\mathbf{X}- \\mathbf{1}\\bar{\\mathbf{x}}&#39; = \\mathbf{X}- \\mathbf{1}\\left(\\frac{1}{n} \\mathbf{X}&#39; \\mathbf{1}\\right)&#39; = \\mathbf{X}- \\frac{1}{n}\\mathbf{1}\\mathbf{1}&#39; \\mathbf{X}= \\left(\\mathbf{I}- \\frac{1}{n}\\mathbf{1}\\mathbf{1}&#39; \\right)\\mathbf{X}\\] We call \\[\\mathbf{C}= \\left(\\mathbf{I}- \\frac{1}{n}\\mathbf{1}\\mathbf{1}&#39; \\right) = \\mathbf{I}- \\mathbf{H}_0 \\] the centering matrix, since it centers the variables of matrix \\(\\mathbf{X}\\). Note also, that \\(\\mathbf{C}\\) also centers any matrix with \\(n\\) rows, in particular a vector of size \\(n\\) is also centered by \\(\\mathbf{C}\\). So we can center \\(y\\) the dependent variable, the same way: \\[\\mathbf{y}_c = \\mathbf{C}\\mathbf{y}\\] 6.7.2 Sample Covariance Having defined, the centered matrix \\(\\mathbf{X}_c\\) we can define the sample covariance of \\(\\mathbf{X}\\), \\(\\mathbf{S}_{XX} \\in \\mathbb{R}^{p \\times p}\\) as follows: \\[ \\mathbf{S}_{XX} = \\frac{1}{n-1} \\left(\\mathbf{X}- \\mathbf{1}\\bar{\\mathbf{x}}&#39;\\right)&#39;\\left(\\mathbf{X}- \\mathbf{1}\\bar{\\mathbf{x}}&#39;\\right) = \\frac{1}{n-1} \\mathbf{X}_c&#39;\\mathbf{X}_c = \\frac{1}{n-1} \\mathbf{X}&#39;\\mathbf{C}&#39; \\mathbf{C}\\mathbf{X}\\] Now, since \\(\\mathbf{C}= \\mathbf{I}- \\mathbf{H}_0\\) is idempotent and symmetric we have that: \\[ \\mathbf{S}_{XX} = \\frac{1}{n-1} \\mathbf{X}&#39;\\mathbf{C}&#39; \\mathbf{C}\\mathbf{X}= \\frac{1}{n-1} \\mathbf{X}&#39; \\mathbf{C}\\mathbf{X}= \\frac{1}{n-1} \\mathbf{X}_c&#39;\\mathbf{X}= \\frac{1}{n-1} \\mathbf{X}&#39;\\mathbf{X}_c \\] So the sample covariance, is the same for the original variables and the centered variables. We can also define the covariance vector between variables \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots \\mathbf{x}_p\\) and variable \\(\\mathbf{y}\\), \\(\\mathbf{S}_{Xy} \\in \\mathbb{R}^{p \\times 1}\\), as follows: \\[ \\mathbf{S}_{Xy} = \\frac{1}{n-1} \\left(\\mathbf{X}- \\mathbf{1}\\bar{\\mathbf{x}}&#39;\\right)&#39;\\left(\\mathbf{y}- \\mathbf{1}\\bar{y}\\right) = \\frac{1}{n-1} \\mathbf{X}_c&#39;\\mathbf{y}_c = \\frac{1}{n-1} \\mathbf{X}&#39;\\mathbf{C}&#39; \\mathbf{C}\\mathbf{y}\\] and, in the same way than before, we have that: \\[\\mathbf{S}_{Xy} = \\frac{1}{n-1} \\mathbf{X}&#39; \\mathbf{C}&#39; \\mathbf{C}\\mathbf{y}= \\frac{1}{n-1} \\mathbf{X}&#39; \\mathbf{C}\\mathbf{y}= \\frac{1}{n-1} \\mathbf{X}_c&#39;\\mathbf{y}= \\frac{1}{n-1} \\mathbf{X}&#39;\\mathbf{y}_c\\] so, you don’t need to center both variables. As long as you center one of them the result will be the same. With this measures, we can focus on splitting the vector of estimated coefficients \\(\\hat{\\boldsymbol{\\beta}}\\), into the estimate for the intercept and the estimates for the independent variables, as follows: \\[\\hat{\\boldsymbol{\\beta}} = \\left[\\begin{matrix} \\hat{\\beta}_0 \\\\ \\hat{\\boldsymbol{\\beta}}_{-0} \\end{matrix}\\right]\\] with \\(\\hat{\\beta}_0\\) the estimate of the intercept and \\(\\hat{\\boldsymbol{\\beta}}_{-0}\\) the coefficients for all independent variables. That is: \\[\\hat{\\boldsymbol{\\beta}}_{-0} = \\left[\\begin{matrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\\\ \\vdots \\\\ \\hat{\\beta}_p \\end{matrix}\\right] \\in \\mathbb{R}^{p \\times 1}\\] Under the new notation, \\(\\mathbf{X}_{*}\\) for the design matrix, we have that: \\[ \\hat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}_{*}&#39;\\mathbf{X}_{*}\\right)^{-1}\\mathbf{X}_{*}&#39;\\mathbf{y}\\] so: \\[ \\hat{\\boldsymbol{\\beta}} = \\left[\\begin{matrix} \\hat{\\beta}_0 \\\\ \\hat{\\boldsymbol{\\beta}}_{-0} \\end{matrix}\\right] = \\left( \\left[\\mathbf{1}\\mathbf{X}\\right]&#39; \\left[\\mathbf{1}\\mathbf{X}\\right] \\right)^{-1} \\left[\\mathbf{1}\\mathbf{X}\\right]&#39; \\mathbf{y}\\] so we need to compute \\(\\left( \\left[\\mathbf{1}\\mathbf{X}\\right]&#39; \\left[\\mathbf{1}\\mathbf{X}\\right] \\right)^{-1}\\). We start by computing: \\[\\left[\\mathbf{1}\\mathbf{X}\\right]&#39; \\left[\\mathbf{1}\\mathbf{X}\\right] = \\left[\\begin{matrix} \\mathbf{1}&#39; \\\\ \\mathbf{X}&#39; \\end{matrix}\\right] \\left[\\mathbf{1}\\mathbf{X}\\right] = \\left[\\begin{matrix} \\mathbf{1}&#39; \\mathbf{1}&amp; \\mathbf{1}&#39; \\mathbf{X}\\\\ \\mathbf{X}&#39;\\mathbf{1}&amp; \\mathbf{X}&#39; \\mathbf{X}&#39; \\end{matrix}\\right] = \\left[\\begin{matrix} n &amp; n\\bar{\\mathbf{x}}&#39; \\\\ n\\bar{\\mathbf{x}} &amp; \\mathbf{X}&#39; \\mathbf{X}&#39; \\end{matrix}\\right] \\] Now, we need to invert a 2 by 2 block matrix (luckily there is a formula for this). The formula is in the prerequisites section, however note that in this case one of the blocks is of height 1, since the first value we are looking for \\(\\hat{\\beta}_0\\) is a scalar. After applying the formula, we have: \\[\\begin{align*} \\left( \\left[\\mathbf{1}\\mathbf{X}\\right]&#39; \\left[\\mathbf{1}\\mathbf{X}\\right] \\right)^{-1} &amp;= \\left[\\begin{matrix} n^{-1} + n^{-1} n\\bar{\\mathbf{x}}&#39; \\left(\\mathbf{X}&#39; \\mathbf{X}&#39; - n\\bar{\\mathbf{x}} n^{-1} n\\bar{\\mathbf{x}}&#39; \\right)^{-1}n\\bar{\\mathbf{x}} n^{-1} &amp; -n^{-1} n\\bar{\\mathbf{x}}&#39; \\left(\\mathbf{X}&#39; \\mathbf{X}&#39; - n\\bar{\\mathbf{x}} n^{-1} n\\bar{\\mathbf{x}}&#39; \\right)^{-1} \\\\ -\\left(\\mathbf{X}&#39; \\mathbf{X}&#39; - n\\bar{\\mathbf{x}} n^{-1} n\\bar{\\mathbf{x}}&#39; \\right)^{-1}n\\bar{\\mathbf{x}} n^{-1} &amp; \\left(\\mathbf{X}&#39; \\mathbf{X}&#39; - n\\bar{\\mathbf{x}} n^{-1} n\\bar{\\mathbf{x}}&#39; \\right)^{-1} \\end{matrix}\\right] \\\\ &amp;= \\left[\\begin{matrix} n^{-1} + \\bar{\\mathbf{x}}&#39; \\left(\\mathbf{X}&#39; \\mathbf{X}- n\\bar{\\mathbf{x}}\\bar{\\mathbf{x}}&#39; \\right)^{-1}\\bar{\\mathbf{x}} &amp; \\bar{\\mathbf{x}}&#39; \\left(\\mathbf{X}&#39; \\mathbf{X}- n\\bar{\\mathbf{x}} \\bar{\\mathbf{x}}&#39; \\right)^{-1} \\\\ -\\left(\\mathbf{X}&#39; \\mathbf{X}- n\\bar{\\mathbf{x}} \\bar{\\mathbf{x}}&#39; \\right)^{-1}\\bar{\\mathbf{x}} &amp; \\left(\\mathbf{X}&#39; \\mathbf{X}- n\\bar{\\mathbf{x}} \\bar{\\mathbf{x}}&#39; \\right)^{-1} \\end{matrix}\\right] \\end{align*}\\] Now, notice that: \\[\\begin{align*} \\mathbf{X}&#39; \\mathbf{X}- n\\bar{\\mathbf{x}} \\bar{\\mathbf{x}}&#39; &amp;= \\mathbf{X}&#39; \\mathbf{X}- n\\left(\\frac{\\mathbf{X}&#39;\\mathbf{1}}{n}\\right)\\left(\\frac{\\mathbf{X}&#39;\\mathbf{1}}{n}\\right)&#39; \\\\ &amp;= \\mathbf{X}&#39; \\mathbf{X}- \\frac{1}{n}\\mathbf{X}&#39;\\mathbf{1}\\mathbf{1}&#39; \\mathbf{X}\\\\ &amp;= \\mathbf{X}&#39;\\left(\\mathbf{I}- \\frac{1}{n}\\mathbf{1}\\mathbf{1}&#39;\\right) \\mathbf{X}\\\\ &amp;= \\mathbf{X}&#39;\\mathbf{C}\\mathbf{X}\\\\ &amp;= (n-1)\\mathbf{S}_{XX} \\end{align*}\\] then: \\[\\begin{align*} \\left( \\left[\\mathbf{1}\\mathbf{X}\\right]&#39; \\left[\\mathbf{1}\\mathbf{X}\\right] \\right)^{-1} &amp;= \\left[\\begin{matrix} n^{-1} + \\bar{\\mathbf{x}}&#39; \\left((n-1)\\mathbf{S}_{XX} \\right)^{-1}\\bar{\\mathbf{x}} &amp; -\\bar{\\mathbf{x}}&#39; \\left((n-1)\\mathbf{S}_{XX} \\right)^{-1} \\\\ -\\left((n-1)\\mathbf{S}_{XX} \\right)^{-1}\\bar{\\mathbf{x}} &amp; \\left((n-1)\\mathbf{S}_{XX} \\right)^{-1} \\end{matrix}\\right] \\\\ &amp;= \\left[\\begin{matrix} n^{-1} + \\frac{1}{n-1}\\bar{\\mathbf{x}}&#39; \\mathbf{S}_{XX}^{-1} \\bar{\\mathbf{x}} &amp; -\\frac{1}{n-1}\\bar{\\mathbf{x}}&#39; \\mathbf{S}_{XX}^{-1} \\\\ -\\frac{1}{n-1}\\mathbf{S}_{XX}^{-1} \\bar{\\mathbf{x}} &amp; \\frac{1}{n-1}\\mathbf{S}_{XX}^{-1} \\end{matrix}\\right] \\end{align*}\\] In a similar way we can easily compute: \\[\\left[\\mathbf{1}\\mathbf{X}\\right]&#39; \\mathbf{y}= \\left[\\begin{matrix} \\mathbf{1}&#39; \\mathbf{y}\\\\ \\mathbf{X}&#39;\\mathbf{y} \\end{matrix}\\right] = \\left[\\begin{matrix} n\\bar{y} \\\\ \\mathbf{X}&#39;\\mathbf{y} \\end{matrix}\\right]\\] Then, we have that: \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}} &amp;= \\left[\\begin{matrix} \\\\ \\hat{\\beta}_0 \\\\ \\hat{\\boldsymbol{\\beta}}_{-0} \\end{matrix}\\right] \\\\ &amp;= \\left[\\begin{matrix} n^{-1} + \\frac{1}{n-1}\\bar{\\mathbf{x}}&#39; \\mathbf{S}_{XX}^{-1} \\bar{\\mathbf{x}} &amp; -\\frac{1}{n-1}\\bar{\\mathbf{x}}&#39; \\mathbf{S}_{XX}^{-1} \\\\ -\\frac{1}{n-1}\\mathbf{S}_{XX}^{-1} \\bar{\\mathbf{x}} &amp; \\frac{1}{n-1}\\mathbf{S}_{XX}^{-1} \\end{matrix}\\right] \\left[\\begin{matrix} n\\bar{y} \\\\ \\mathbf{X}&#39;\\mathbf{y} \\end{matrix}\\right] \\\\ &amp;= \\left[\\begin{matrix} n^{-1}n\\bar{y} + \\frac{n\\bar{y}}{n-1}\\bar{\\mathbf{x}}&#39; \\mathbf{S}_{XX}^{-1} \\bar{\\mathbf{x}} - \\frac{1}{n-1}\\bar{\\mathbf{x}}&#39; \\mathbf{S}_{XX}^{-1}\\mathbf{X}&#39;\\mathbf{y}\\\\ -\\frac{n\\bar{y}}{n-1}\\mathbf{S}_{XX}^{-1} \\bar{\\mathbf{x}} + \\frac{1}{n-1}\\mathbf{S}_{XX}^{-1}\\mathbf{X}&#39;\\mathbf{y} \\end{matrix}\\right] \\end{align*}\\] Then, working first with the second row-block: \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}}_{-0} &amp;= -\\frac{n\\bar{y}}{n-1}\\mathbf{S}_{XX}^{-1} \\bar{\\mathbf{x}} + \\frac{1}{n-1}\\mathbf{S}_{XX}^{-1}\\mathbf{X}&#39;y \\\\ &amp;= \\frac{1}{n-1}\\mathbf{S}_{XX}^{-1}\\mathbf{X}&#39;y -\\frac{1}{n-1}\\mathbf{S}_{XX}^{-1} \\bar{\\mathbf{x}}n\\bar{y} \\\\ &amp;= \\frac{1}{n-1}\\mathbf{S}_{XX}^{-1}\\left(\\mathbf{X}&#39;y - n\\bar{\\mathbf{x}}\\bar{y}\\right) \\end{align*}\\] Now, we note that: \\[\\begin{align*} \\mathbf{X}&#39;\\mathbf{y}- n\\bar{\\mathbf{x}}\\bar{y} &amp;= \\mathbf{X}&#39;\\mathbf{y}- n\\bar{\\mathbf{x}}\\bar{y} \\\\ &amp;= \\mathbf{X}&#39;\\mathbf{y}- n\\left(\\frac{\\mathbf{X}&#39;\\mathbf{1}}{n}\\right)\\left(\\frac{\\mathbf{1}&#39;\\mathbf{y}}{n}\\right) \\\\ &amp;= \\mathbf{X}&#39;\\mathbf{y}- \\frac{1}{n}\\mathbf{X}&#39;\\mathbf{1}\\mathbf{1}&#39;\\mathbf{y}\\\\ &amp;= \\mathbf{X}&#39; \\left(\\mathbf{I}- \\frac{1}{n} \\mathbf{1}\\mathbf{1}&#39; \\right) \\mathbf{y}\\\\ &amp;= \\mathbf{X}&#39; \\mathbf{C}\\mathbf{y}\\\\ &amp;= (n-1)\\mathbf{S}_{Xy} \\end{align*}\\] Then, we have that: \\[\\hat{\\boldsymbol{\\beta}}_{-0} = \\frac{1}{n-1}\\mathbf{S}_{XX}^{-1}(n-1)\\mathbf{S}_{Xy} = \\mathbf{S}_{XX}^{-1}\\mathbf{S}_{Xy}\\] An equivalent result to that of simple linear regression, both using the covariance matrix of the independent variables and the covariance vector of the independent variables and the dependent variable. This way of writing the coefficients shows the influence of each component: \\(\\mathbf{S}_{XX}^{-1}\\): The relationship between the independent variables. \\(\\mathbf{S}_{Xy}\\): The relationship between the independent variables and the dependent variables. Also notice that, since centralizing doesn’t change the values of \\(\\mathbf{S}_{X_cX_c}^{-1}\\) and \\(\\mathbf{S}_{X_cy_c}\\), centralizing the independent variables or the dependent variable (or both), doesn’t change the value of the coefficients of the independent variables. Now, we can work more easily with the intercept estimate: \\[\\begin{align*} \\hat{\\beta}_0 &amp;= n^{-1}n\\bar{y} + \\frac{n\\bar{y}}{n-1}\\bar{\\mathbf{x}}&#39; \\mathbf{S}_{XX}^{-1} \\bar{\\mathbf{x}} - \\frac{1}{n-1}\\bar{\\mathbf{x}}&#39; \\mathbf{S}_{XX}^{-1}\\mathbf{X}&#39;\\mathbf{y}\\\\ &amp;= \\bar{y} + \\frac{1}{n-1}\\bar{\\mathbf{x}}&#39;\\mathbf{S}_{XX}^{-1} n\\bar{\\mathbf{x}}\\bar{y} - \\frac{1}{n-1}\\bar{\\mathbf{x}}&#39;\\mathbf{S}_{XX}^{-1}\\mathbf{X}&#39;\\mathbf{y}\\\\ &amp;= \\bar{y} + \\frac{1}{n-1}\\bar{\\mathbf{x}}&#39;\\mathbf{S}_{XX}^{-1} \\left(n\\bar{\\mathbf{x}}\\bar{y} - \\mathbf{X}&#39;\\mathbf{y}\\right) \\\\ &amp;= \\bar{y} - \\frac{1}{n-1}\\bar{\\mathbf{x}}&#39;\\mathbf{S}_{XX}^{-1} \\left(\\mathbf{X}&#39;\\mathbf{y}- n\\bar{\\mathbf{x}}\\bar{y} \\right) \\\\ &amp;= \\bar{y} - \\frac{1}{n-1}\\bar{\\mathbf{x}}&#39;\\mathbf{S}_{XX}^{-1} (n-1)\\mathbf{S}_{Xy} \\\\ &amp;= \\bar{y} - \\bar{\\mathbf{x}}&#39;\\mathbf{S}_{XX}^{-1} \\mathbf{S}_{Xy} \\\\ &amp;= \\bar{y} - \\bar{\\mathbf{x}}&#39; \\hat{\\boldsymbol{\\beta}}_{-0} \\end{align*}\\] Again, an equivalent result to that of simple linear regression. Like in simple linear regression, centering the independent variables does affect the intercept estimate, since \\(\\bar{\\mathbf{x}}=\\mathbf{0}\\), we have that the coefficient after centering the independent variables is \\(\\bar{y}\\) the mean of the dependent variable. And if we also center the dependent variable, then \\(\\bar{y}=0\\) so the estimate of the intercept is \\(0\\) also. Therefore, if you are centering all variables, it is not necessary to add the column of ones in the design matrix, since the estimate of the intercept is \\(0\\). 6.7.3 Satandard Variables In the same way we worked with centered variables, we can work with standard variables to define the sample correlations. The standardization of \\(\\mathbf{X}\\), \\(\\mathbf{X}_s\\), is given by: \\[\\begin{align*} \\mathbf{X}_s &amp;= \\left[\\begin{matrix} \\frac{x_{11} - \\bar{\\mathbf{x}}_1}{S_{x_1x_1}^{1/2}} &amp; \\frac{x_{12} - \\bar{\\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \\dots &amp; \\frac{x_{1p} - \\bar{\\mathbf{x}}_p}{S_{x_px_p}^{1/2}} \\\\ \\frac{x_{21} - \\bar{\\mathbf{x}}_1}{S_{x_1x_1}^{1/2}} &amp; \\frac{x_{22} - \\bar{\\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \\dots &amp; \\frac{x_{2p} - \\bar{\\mathbf{x}}_p}{S_{x_px_p}^{1/2}} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{x_{n1} - \\bar{\\mathbf{x}}_1}{S_{x_1x_1}^{1/2}} &amp; \\frac{x_{n2} - \\bar{\\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \\dots &amp; \\frac{x_{np} - \\bar{\\mathbf{x}}_p}{S_{x_px_p}^{1/2}} \\end{matrix}\\right] \\\\ &amp;= \\left[\\begin{matrix} x_{11} - \\bar{\\mathbf{x}}_1 &amp; x_{12} - \\bar{\\mathbf{x}}_2 &amp; \\dots &amp; x_{1p} - \\bar{\\mathbf{x}}_p \\\\ x_{21} - \\bar{\\mathbf{x}}_1 &amp; x_{22} - \\bar{\\mathbf{x}}_2 &amp; \\dots &amp; x_{2p} - \\bar{\\mathbf{x}}_p \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} - \\bar{\\mathbf{x}}_1 &amp; x_{n2} - \\bar{\\mathbf{x}}_2 &amp; \\dots &amp; x_{np} - \\bar{\\mathbf{x}}_p \\end{matrix}\\right] \\left[\\begin{matrix} \\frac{1}{S_{x_1x_1}^{1/2}} &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\frac{x_{22} - \\bar{\\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\frac{x_{np} - \\bar{\\mathbf{x}}_p}{S_{x_px_p}^{1/2}} \\end{matrix}\\right] &amp;= \\mathbf{X}_c \\mathbf{D}_X &amp;= \\mathbf{C}\\mathbf{X}\\mathbf{D}_X \\end{align*}\\] where \\[ \\mathbf{D}_X = \\left[\\begin{matrix} \\frac{1}{S_{x_1x_1}^{1/2}} &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\frac{x_{22} - \\bar{\\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\frac{x_{np} - \\bar{\\mathbf{x}}_p}{S_{x_px_p}^{1/2}} \\end{matrix}\\right]\\] is the matrix that standardizes \\(\\mathbf{X}\\). Notice that unlike \\(\\mathbf{C}\\), that centers any matrix with the appropriate number of rows, \\(\\mathbf{D}_X\\) only standardizes \\(\\mathbf{X}\\). 6.7.4 Sample Correlation Matrix We define the sample correlation matrix as: \\[ r_{XX} = \\frac{\\mathbf{X}_s&#39;\\mathbf{X}_s}{n-1} \\] where we break a little our notation convention of using bold capital letters for matrices. Entry at row \\(i\\) and column \\(j\\) of \\(r_{XX}\\) is given by: \\[ \\left[r_{XX}\\right]_{ij} = \\frac{S_{x_i x_j}}{S_{x_ix_i}^{1/2}S_{x_jx_j}^{1/2}} \\] We can also define the sample correlation between \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) as follows: \\[r_{Xy} = \\frac{\\mathbf{X}_s&#39; \\mathbf{y}_s}{n-1} \\] where \\(\\mathbf{y}_s\\) is the standardization of \\(\\mathbf{y}\\). With this definitions in hand, we can see how the coefficients look like with standardized variables. Since, standardized variables are also centered, it is not necessary to include the column of ones in the design matrix, as the intercept estimate is always 0. Then the estimate of the coefficients of the independent variables is given by: \\[\\hat{\\boldsymbol{\\beta}_s} = \\left(\\mathbf{X}_s&#39; \\mathbf{X}_s \\right)^{-1} \\mathbf{X}_s&#39; \\mathbf{y}_s = \\frac{1}{n-1}r_{XX}^{-1}(n-1)r_{Xy}=r_{XX}^{-1}r_{Xy}\\] Notice that the coefficients estimates do change for standardized variables, since in general \\(r_{XX}\\neq\\mathbf{S}_{XX}\\) and \\(r_{Xy}\\neq\\mathbf{S}_{Xy}\\). In the case of standardized variables the coefficient estimates depend in part from the correlation between independent variables \\(r_{XX}\\) and the correlations between independent and dependent variables \\(r_{Xy}\\). Working with standardized variables is useful, since standardized variables are unit-less, so the estimated coefficients magnitudes are comparable. We can see these results in practice with our GDP data: # Read Data dat &lt;- read.csv(&quot;Gdp data.csv&quot;) # Design Matrix Independent Variables X &lt;- as.matrix(dat[, -1]) # Dependent Variable y &lt;- dat$gdp # Number of Observations n &lt;- nrow(X) # Design Matrix with the column of Ones Z &lt;- cbind(rep(1, n), X) # Centering # Vector of Ones v1 &lt;- rep(1, n) # Centering Matrix C &lt;- diag(n) - (1/n) * v1 %*% t(v1) # Independent Variables Centered Xc &lt;- C %*% X # Dependent Variable Centered yc &lt;- C %*% y # Design Matrix with Independent Variables Centered Zc &lt;- cbind(rep(1, n), Xc) # Checks that the Variables are actually centered print(round(colMeans(Xc), 8)) ## inf une int gov exp ## 0 0 0 0 0 print(round(mean(yc), 8)) ## [1] 0 # Shows the Mean of y print(round(mean(y), 8)) ## [1] 2.981131 # Compute the Estimates of the Coefficients with Original Variables b &lt;- solve(t(Z) %*% Z, t(Z) %*% y) # Compute the Estimates of the Coefficients with Centered Independent Variables Only bs1 &lt;- solve(t(Zc) %*% Zc, t(Zc) %*% y) # Compute the Estimates of the Coefficients with All Variables Centered bs2 &lt;- solve(t(Zc) %*% Zc, t(Zc) %*% yc) # Compute the Estimates of the Coefficients with All variables Centered and no column of ones bs3 &lt;- solve(t(Xc) %*% Xc, t(Xc) %*% yc) # Shows the Estimated Coefficients Side-by-Side print(round(cbind(b, bs1, bs2, c(0, bs3)), 8)) ## [,1] [,2] [,3] [,4] ## 1.55530096 2.98113131 0.00000000 0.00000000 ## inf 0.31221813 0.31221813 0.31221813 0.31221813 ## une -0.37733403 -0.37733403 -0.37733403 -0.37733403 ## int 0.17782709 0.17782709 0.17782709 0.17782709 ## gov -0.00848329 -0.00848329 -0.00848329 -0.00848329 ## exp 0.06465692 0.06465692 0.06465692 0.06465692 Here we can appreciate that the estimated coefficients for the independent variables do not change, however the estimate for the intercept changes depending on if the independent variables are centered or the dependent variable is centered of both. Also, notice that this are the that we had using the lm function of R. We can also check that computing the sample covariance matrix using our formula results in the same quantities that using the cov function in R. # Sample Covariance of X SXX &lt;- t(Xc) %*% Xc / (n-1) # Sample Covariance between X and y SXy &lt;- t(Xc) %*% yc / (n-1) # Shows the comparison in covariance matrices print(SXX) ## inf une int gov exp ## inf 4.03991957 -1.28576595 1.52551017 2.4374124 0.03447976 ## une -1.28576595 0.66963355 -0.27282160 -0.5319862 0.02605596 ## int 1.52551017 -0.27282160 0.83778484 0.7633037 0.04335484 ## gov 2.43741235 -0.53198621 0.76330368 15.2006711 0.21732195 ## exp 0.03447976 0.02605596 0.04335484 0.2173220 9.10237220 print(cov(X)) ## inf une int gov exp ## inf 4.03991957 -1.28576595 1.52551017 2.4374124 0.03447976 ## une -1.28576595 0.66963355 -0.27282160 -0.5319862 0.02605596 ## int 1.52551017 -0.27282160 0.83778484 0.7633037 0.04335484 ## gov 2.43741235 -0.53198621 0.76330368 15.2006711 0.21732195 ## exp 0.03447976 0.02605596 0.04335484 0.2173220 9.10237220 # Shows the comparison in covariance vectors print(SXy) ## [,1] ## inf 1.9993285 ## une -0.6964324 ## int 0.7245455 ## gov 0.9825766 ## exp 0.5953308 print(cov(X, y)) ## [,1] ## inf 1.9993285 ## une -0.6964324 ## int 0.7245455 ## gov 0.9825766 ## exp 0.5953308 Finally, we can test our new formulas for the estimates: # Computes the estimates of the coefficients using the covariance matrices b1 &lt;- solve(SXX, SXy) b0 &lt;- mean(y) - t(colMeans(X)) %*% b1 # Shows the Estimates Side by Side print(round(cbind(b, c(b0, b1)), 8)) ## [,1] [,2] ## 1.55530096 1.55530096 ## inf 0.31221813 0.31221813 ## une -0.37733403 -0.37733403 ## int 0.17782709 0.17782709 ## gov -0.00848329 -0.00848329 ## exp 0.06465692 0.06465692 In the same way, we can work with the sample correlations # Standardizing matrix of X DX &lt;- diag(1/sqrt(diag(SXX))) # Standardize X Xs &lt;- Xc %*% DX # Shows that Xs is indeed standardize print(round(colMeans(Xs), 8)) ## [1] 0 0 0 0 0 print(apply(X = Xs, MARGIN = 2, FUN = sd)) ## [1] 1 1 1 1 1 # Standardizes y ys &lt;- yc / sd(y) # Shows that ys is standardized print(round(mean(ys), 8)) ## [1] 0 print(round(sd(ys), 8)) ## [1] 1 # Sample Correlation of X rXX &lt;- t(Xs) %*% Xs / (n-1) # Sample Correlation between X and y rXy &lt;- t(Xs) %*% ys / (n-1) # Shows the comparison in correlation matrices print(rXX) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1.000000000 -0.78173033 0.8292061 0.31103644 0.005685918 ## [2,] -0.781730327 1.00000000 -0.3642453 -0.16674407 0.010553855 ## [3,] 0.829206121 -0.36424525 1.0000000 0.21389456 0.015699798 ## [4,] 0.311036436 -0.16674407 0.2138946 1.00000000 0.018475446 ## [5,] 0.005685918 0.01055386 0.0156998 0.01847545 1.000000000 print(cor(X)) ## inf une int gov exp ## inf 1.000000000 -0.78173033 0.8292061 0.31103644 0.005685918 ## une -0.781730327 1.00000000 -0.3642453 -0.16674407 0.010553855 ## int 0.829206121 -0.36424525 1.0000000 0.21389456 0.015699798 ## gov 0.311036436 -0.16674407 0.2138946 1.00000000 0.018475446 ## exp 0.005685918 0.01055386 0.0156998 0.01847545 1.000000000 # Shows the comparison in correlation vectors print(rXy) ## [,1] ## [1,] 0.8751311 ## [2,] -0.7487479 ## [3,] 0.6964256 ## [4,] 0.2217228 ## [5,] 0.1736027 print(cor(X, y)) ## [,1] ## inf 0.8751311 ## une -0.7487479 ## int 0.6964256 ## gov 0.2217228 ## exp 0.1736027 and can test that: \\[\\hat{\\boldsymbol{\\beta}_s} = r_{XX}^{-1}r_{Xy}\\] # Computes the estimates of the coefficients using the covariance matrices bs1 &lt;- solve(t(Xs) %*% Xs, t(Xs) %*% ys) bs2 &lt;- solve(rXX, rXy) # Shows the Estimates Side by Side print(round(cbind(bs1, bs2), 8)) ## [,1] [,2] ## [1,] 0.55210259 0.55210259 ## [2,] -0.27165637 -0.27165637 ## [3,] 0.14319884 0.14319884 ## [4,] -0.02909852 -0.02909852 ## [5,] 0.17161988 0.17161988 We can also contrast the effects of the standarization on the coefficients # Computes the estimates of the coefficients using the covariance matrices bs1 &lt;- lm(y ~ X)$coefficients bs2 &lt;- lm(ys ~ Xs)$coefficients # Shows the Estimates Side by Side print(round(cbind(bs1, bs2), 8)) ## bs1 bs2 ## (Intercept) 1.55530096 0.00000000 ## Xinf 0.31221813 0.55210259 ## Xune -0.37733403 -0.27165637 ## Xint 0.17782709 0.14319884 ## Xgov -0.00848329 -0.02909852 ## Xexp 0.06465692 0.17161988 First, we can observe the 0 intercept estimate on the standardize values, so it is not necessary to estimate it, we could have done so by using lm(ys ~ Xs - 1) instead of lm(ys ~ Xs). Next we observe, the change in magnitudes for the estimated coefficients of the independent variables. 6.8 Variable Cross-Effects For this sub-section we will work with standardized values so there is no need to estimate the intercept. Since all variables are standardized, we will not use the \\(\\mathbf{X}_s\\) notation, but instead only \\(\\mathbf{X}\\), to make notation less confusing. The same goes with \\(\\hat{\\boldsymbol{\\beta}}_s\\), it will be only \\(\\hat{\\boldsymbol{\\beta}}\\). The idea is to analyze the estimated coefficients when you divide the independent variables in to groups \\(1\\) and \\(2\\). So we can divide the design matrix in two: \\[\\mathbf{X}= [\\mathbf{X}_1 \\mathbf{X}_2]\\] Then, we can compute the coefficient estimates: \\[\\hat{\\boldsymbol{\\beta}} = \\left[\\begin{matrix} \\hat{\\boldsymbol{\\beta}}_1 \\\\ \\hat{\\boldsymbol{\\beta}}_2 \\end{matrix}\\right] = \\left([\\mathbf{X}_1 \\mathbf{X}_2]&#39;[\\mathbf{X}_1 \\mathbf{X}_2]\\right)^{-1}[\\mathbf{X}_1 \\mathbf{X}_2]&#39; \\mathbf{y}\\] So, we can work with these computations in the same way we did before: \\[\\begin{align*} [\\mathbf{X}_1 \\mathbf{X}_2]&#39;[\\mathbf{X}_1 \\mathbf{X}_2] &amp;= \\left[\\begin{matrix} \\mathbf{X}_1&#39; \\\\ \\mathbf{X}_2&#39; \\end{matrix}\\right] [\\mathbf{X}_1 \\mathbf{X}_2] \\\\ &amp;= \\left[\\begin{matrix} \\mathbf{X}_1&#39;\\mathbf{X}_1 &amp; \\mathbf{X}_1&#39;\\mathbf{X}_2 \\\\ \\mathbf{X}_2&#39;\\mathbf{X}_1 &amp; \\mathbf{X}_2&#39;\\mathbf{X}_2 \\end{matrix}\\right] \\\\ &amp;= (n-1) \\left[\\begin{matrix} \\frac{\\mathbf{X}_1&#39;\\mathbf{X}_1}{n-1} &amp; \\frac{\\mathbf{X}_1&#39;\\mathbf{X}_2}{n-1} \\\\ \\frac{\\mathbf{X}_2&#39;\\mathbf{X}_1}{n-1} &amp; \\frac{\\mathbf{X}_2&#39;\\mathbf{X}_2}{n-1} \\end{matrix}\\right] \\\\ &amp;= (n-1) \\left[\\begin{matrix} r_{X_1X_1} &amp; r_{X_1X_2} \\\\ r_{X_2X_1} &amp; r_{X_2X_2} \\end{matrix}\\right] \\end{align*}\\] In the same way, we have that: \\[\\begin{align*} [\\mathbf{X}_1 \\mathbf{X}_2]&#39;\\mathbf{y} &amp;= \\left[\\begin{matrix} \\mathbf{X}_1&#39; \\\\ \\mathbf{X}_2&#39; \\end{matrix}\\right] \\mathbf{y}\\\\ &amp;= \\left[\\begin{matrix} \\mathbf{X}_1&#39;\\mathbf{y}\\\\ \\mathbf{X}_2&#39;\\mathbf{y} \\end{matrix}\\right] \\\\ &amp;= (n-1) \\left[\\begin{matrix} \\frac{\\mathbf{X}_1&#39;\\mathbf{y}}{n-1} \\\\ \\frac{\\mathbf{X}_2&#39;\\mathbf{y}}{n-1} \\end{matrix}\\right] \\\\ &amp;= (n-1) \\left[\\begin{matrix} r_{X_1y} \\\\ r_{X_2y} \\end{matrix}\\right] \\end{align*}\\] Then we have that: \\[ \\hat{\\boldsymbol{\\beta}} = \\left[\\begin{matrix} \\hat{\\boldsymbol{\\beta}}_1 \\\\ \\hat{\\boldsymbol{\\beta}}_2 \\end{matrix}\\right] = \\left((n-1) \\left[\\begin{matrix} r_{X_1X_1} &amp; r_{X_1X_2} \\\\ r_{X_2X_1} &amp; r_{X_2X_2} \\end{matrix}\\right]\\right)^{-1} (n-1) \\left[\\begin{matrix} r_{Xy,1} \\\\ r_{Xy,2} \\end{matrix}\\right] = \\left[\\begin{matrix} r_{X_1X_1} &amp; r_{X_1X_2} \\\\ r_{X_2X_1} &amp; r_{X_2X_2} \\end{matrix}\\right]^{-1} \\left[\\begin{matrix} r_{X_1y} \\\\ r_{X_2y} \\end{matrix}\\right] \\] Now we can compute the inverse of the 2 by 2 block matrix as before with, but first we define: \\[r_{X_1|X_2} = r_{X_1X_1} - r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2X_1}\\] this is also, the Schur component. This can also be seen as the sample correlation matrix of \\(X_1\\) after accounting by the relationships with \\(X_2\\). \\[ \\left[\\begin{matrix} r_{XX,11} &amp; r_{XX,12} \\\\ r_{XX,12} &amp; r_{XX,22} \\end{matrix}\\right]^{-1} = \\left[\\begin{matrix} r_{X_1|X_2}^{-1} &amp; -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1} \\\\ -r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1} &amp; r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1} \\end{matrix}\\right] \\] Then, we have that: \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}} &amp;= \\left[\\begin{matrix} \\hat{\\boldsymbol{\\beta}}_1 \\\\ \\hat{\\boldsymbol{\\beta}}_2 \\end{matrix}\\right] \\\\ &amp;= \\left[\\begin{matrix} r_{X_1|X_2}^{-1} &amp; -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1} \\\\ -r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1} &amp; r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1} \\end{matrix}\\right] \\left[\\begin{matrix} r_{X_1y} \\\\ r_{X_2y} \\end{matrix}\\right] \\\\ &amp;= \\left[\\begin{matrix} r_{X_1|X_2}^{-1}r_{X_1y} -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y} \\\\ -r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1y} + r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y} \\end{matrix}\\right] \\end{align*}\\] Since the variables in the groupings 1 and 2 can be switched and have no special characteristics, we only need to analyze the structure of \\(\\hat{\\boldsymbol{\\beta}}_1\\) in relation to the variables of group 2, the results would be analogous for \\(\\hat{\\boldsymbol{\\beta}}_1\\). Then we have that: \\[ \\hat{\\boldsymbol{\\beta}}_1 = r_{X_1|X_2}^{-1}r_{X_1y} -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y} = r_{X_1|X_2}^{-1}\\left(r_{X_1y} -r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y}\\right) \\] Now, suppose that we want to fit the following linear models: \\[\\mathbf{y}= \\mathbf{X}_1 \\boldsymbol{\\beta}_1 + \\mathbf{e}\\quad \\text{and} \\quad \\mathbf{y}= \\mathbf{X}_2 \\boldsymbol{\\beta}_2 + \\mathbf{e}\\] that is a linear model of \\(\\mathbf{y}\\) using only the variables in group 1 for one model and only variables from the group 2 in the second model. Then, the coefficient estimates will be: \\[\\tilde{\\boldsymbol{\\beta}}_1 = r_{X_1X_1}^{-1}r_{X_1y} \\quad \\text{and} \\quad \\tilde{\\boldsymbol{\\beta}}_2 = r_{X_2X_2}^{-1}r_{X_2y}\\] Note that, in general, this estimates will be different to the estimates using all the variables, that is: \\[\\tilde{\\boldsymbol{\\beta}}_1 \\neq \\hat{\\boldsymbol{\\beta}}_1 \\quad \\text{and} \\quad \\tilde{\\boldsymbol{\\beta}}_2 \\neq \\hat{\\boldsymbol{\\beta}}_2\\] Then, we can re-write, our coefficient estimate for \\(\\boldsymbol{\\beta}_1\\) as follows: \\[ \\hat{\\boldsymbol{\\beta}}_1 = r_{X_1|X_2}^{-1}\\left(r_{X_1y} -r_{X_1X_2}\\tilde{\\boldsymbol{\\beta}}_2\\right) \\] Now, if \\(r_{X_1X_2} = \\mathbf{0}\\), that is the variables in group 1 and group 2 are uncorrelated, then \\[r_{X_1|X_2} = r_{X_1X_1} - r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2X_1} = r_{X_1X_1}\\] so: \\[\\hat{\\boldsymbol{\\beta}}_1 = r_{X_1|X_2}^{-1}\\left(r_{X_1y} -r_{X_1X_2}\\tilde{\\boldsymbol{\\beta}}_2\\right) = r_{X_1X_1}^{-1}r_{X_1y} = \\tilde{\\boldsymbol{\\beta}}_1\\] So, when the variables in 1 group are uncorrelated with the other the coefficient estimates are the same for the full and partial model. We can also deduce that even when the sample correlation between \\(X_1\\) and \\(y\\) is \\(\\mathbf{0}\\), that is \\(r_{X_1y} = \\mathbf{0}\\), the estimated coefficients will not be \\(\\mathbf{0}\\) in general. In fact, we have that: \\[ \\hat{\\boldsymbol{\\beta}}_1 = -r_{X_1|X_2}^{-1}r_{X_1X_2}\\tilde{\\boldsymbol{\\beta}}_2 \\] 6.8.1 Single Variable Cross-Effects In the special case where group 1 consists of only 1 variable, we have that: \\(\\hat{\\beta}_1, \\quad r_{X_1X_1} = r_{x_1x_1}=1, \\quad r_{X_1|X_2}=r_{x_1|X_2}, \\quad r_{X_1y}=r_{x_1y}=\\tilde{\\beta}_1\\) are scalars, while \\[r_{X_1X_2} = r_{x_1X_2}=r_{X_2X_1}&#39; = r_{X_2x_1}&#39;, \\quad r_{X_2y}, \\quad \\tilde{\\boldsymbol{\\beta}}_2\\] are vectors of size \\(p-1\\). Now, consider the following linear model: \\[\\mathbf{x}_1 = \\mathbf{X}_2 \\boldsymbol{\\alpha}_2 + \\mathbf{e}\\] that is, fitting the single variable in group 1, as the dependent variable, and the variables in group 2, as the independent variables. Then we have that the estimated coefficients are: \\[ \\hat{\\boldsymbol{\\alpha}}_2 = r_{X_2X_2}^{-1}r_{X_2x_1}\\] then, we notice the following \\[r_{x_1|X_2} = r_{X_1X_1} - r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2X_1} = r_{x_1X_1} - r_{x_1X_2}r_{X_2X_2}^{-1}r_{X_2x_1} = 1 - r_{x_1X_2}\\hat{\\boldsymbol{\\alpha}}_2 = 1 - r_{x_1X_2}r_{X_2X_2}^{-1}r_{X_2X_2}\\hat{\\boldsymbol{\\alpha}}_2 = 1 - \\hat{\\boldsymbol{\\alpha}}_2&#39;r_{X_2X_2}\\hat{\\boldsymbol{\\alpha}}_2 = 1 - R^2_1 \\] where, \\(R^2_1\\) is the multiple coefficient of determination for model: \\[\\mathbf{x}_1 = \\mathbf{X}_2 \\boldsymbol{\\alpha}_2 + \\mathbf{e}\\] that is, how much of \\(\\mathbf{x}_1\\) is explained by \\(\\mathbf{X}_2\\). Then, we have that: \\[\\hat{\\beta}_1 = r_{1_1|X_2}^{-1}\\left(r_{x_1y} -r_{x_1X_2}\\tilde{\\boldsymbol{\\beta}}_2\\right) = \\frac{1}{1-R^2_1} \\left(\\tilde{\\beta}_1 -r_{x_1X_2}\\tilde{\\boldsymbol{\\beta}}_2\\right)\\] We name \\(\\frac{1}{1 - R^2_1}\\) as: \\[VIF_1 = \\frac{1}{1 - R^2_1} \\] the bigger, the more distorted is the value of \\(\\hat{\\beta}_1\\). This number increases as \\(R^2_1\\) in creases. If \\(R^2_1 = 0\\), that is \\(\\mathbf{X}_2\\) explains nothing of \\(\\mathbf{x}_1\\), then \\[ VIF_1 = 1 \\] that represents no distortion. If \\(R^2_1 \\to 1\\), then: \\[ VIF_1 \\to 1 \\] which represents infinite distortion. The name \\(VIF\\) stands for variance inflation factor. We will see that later, how it relates to the variance of the estimate \\(\\hat{\\beta}_1\\). Of course, variable 1, didn’t have any particular property, so we can generalize this to any variable \\(k\\). On the other hand, we also have an effect \\(r_{x_1X_2}\\tilde{\\boldsymbol{\\beta}}_2\\) on the coefficient 6.8.1.1 Single Variable Cross-effects example We can see the effects the other variables have in changing the coefficients in our GDP data set, by computing \\(R^2_k\\) for each variable. # Re-names Standardized Variables y &lt;- ys X &lt;- Xs # Coefficients for the Full Model b &lt;- lm(y ~ X -1)$coefficients # Individual Coefficients # Number of Variables P &lt;- ncol(X) # Vector to Save Individual Coefficients bi &lt;- numeric(length = P) for(k in 1:P){ bi[k] &lt;- lm(y ~ X[, k] - 1)$coefficients } # Coefficients without variable k bt &lt;- matrix(data = NA, nrow = P-1, ncol = P) for(k in 1:P){ bt[, k] &lt;- lm(y ~ X[, -k] - 1)$coefficients } # Coefficient of Determination of X_k ~ X_{-k} models and coefficients R2k &lt;- numeric(length = P) for(k in 1:P){ outReg &lt;- lm(X[, k] ~ X[, -k] - 1) R2k[k] &lt;- summary(outReg)$r.squared } # Cross product of correlation and coefficients rbt &lt;- numeric(length = P) rXX &lt;- cor(X) for(k in 1:P){ rbt[k] &lt;- rXX[k, -k] %*% bt[, k] } # Formats and Show Info Table tab &lt;- cbind(b, bi, R2k, 1/(1-R2k), rbt) colnames(tab) &lt;- c(&quot;b_full&quot;, &quot;b_ind&quot;, &quot;R2k&quot;, &quot;VIF&quot;, &quot;rb_ind&quot;) rownames(tab) &lt;- colnames(dat)[2:6] knitr::kable(tab) b_full b_ind R2k VIF rb_ind inf 0.5521026 0.8751311 0.9604527 25.286192 0.8532969 une -0.2716564 -0.7487479 0.8843302 8.645299 -0.7173255 int 0.1431988 0.6964256 0.9061532 10.655663 0.6829869 gov -0.0290985 0.2217228 0.2067576 1.260649 0.2448050 exp 0.1716199 0.1736027 0.0008505 1.000851 0.0021287 6.9 Outliers and Leverage It is harder to identify outliers visually in multiple linear regression, specially if the number of variables is big. The effect of an observation \\(i\\) on the linear regression fit will depend on 2 things: The size of the residual for observation \\(i\\), that is \\(\\hat{e}_i^2\\). How “close” is observation \\(i\\) to other observations. Note: For this section, we are going to use \\(X_i\\in\\mathbb{R}^{1 \\times p}\\) as the \\(i-th\\) row of the design matrix. That is the \\(i\\)-th observation of the independent varaibles. 6.9.1 Leverage In multiple linear regression, leverage refers to the influence that a particular data point has on the estimation of the regression model. Specifically, it measures how far an observation’s values for the independent variables deviate from the mean of those variables. Leverage identifies data points that could potentially have a significant effect on the regression line, particularly those that are outliers in terms of the input variables. Leverage values are calculated from the hat matrix \\(\\mathbf{H}\\), which is derived from the design matrix \\(\\mathbf{X}\\) used in the regression. The diagonal elements of the hat matrix, \\(h_{ii}\\), represent the leverage of each data point. These values range from 0 to 1, where: A leverage value close to 0 indicates that the point is not influential in determining the regression line. A leverage value closer to 1 suggests that the point has a higher influence on the fitted model. High-leverage points are those whose independent variable values are far from the average, and they may disproportionately affect the slope and intercept of the regression line. These points can distort the regression model, leading to biased estimates if not handled properly. A high-leverage point may not necessarily be an outlier in the response variable (dependent variable), but it could have a significant impact on the overall model fit, making leverage a critical diagnostic in regression analysis. 6.9.1.1 Properties of Leverages Lets analyze the structure of the hat matrix. First, let us see that \\(\\mathbf{H}\\) is symmetric semi-positive definite. Pick \\(z \\in \\mathbb{R}^n\\). Recall, that \\(\\mathbf{X}&#39; \\mathbf{X}\\) is positive definite, then \\[(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\] is positive definite. Then \\[\\begin{align*} z&#39;Hz &amp;= \\mathbf{z}&#39;\\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;z \\\\ &amp;= (\\mathbf{X}&#39;\\mathbf{z})&#39;(\\mathbf{X}&#39;\\mathbf{X})^{-1}(\\mathbf{X}&#39;\\mathbf{z}) \\\\ &amp;= \\mathbf{w}&#39; (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\mathbf{w}&amp; \\mathbf{w}= \\mathbf{X}&#39;\\mathbf{z}\\\\ &amp;\\geq 0 \\end{align*}\\] Therefore, \\(\\mathbf{H}\\) is semi-positive definite. This implies that all the eigenvalues of \\(\\mathbf{H}\\) are non-negative, that is positive or \\(0\\). Next, let us see that the diagonal elements are between 0 and 1. To see this, consider leverage \\(i\\) (\\(h_{ii}\\)), and the basis vectors \\(v_j \\in \\mathbb{R}^n\\) such that are full of zeros except for entry \\(j\\). Then: \\[h_{ij} = v_i &#39;\\mathbf{H}v_j\\] in particular, leverage \\(i\\) is given by: \\[h_{ii} = v_i &#39;\\mathbf{H}v_i\\] Now, consider \\(\\mathbf{H}v_i\\) and notice that: \\[\\begin{align*} ||v_i - \\mathbf{H}v_i||_2^2 &amp;= (v_i - \\mathbf{H}v_i)&#39;(v_i - \\mathbf{H}v_i) \\\\ &amp;= v_i&#39;v_i - v_i&#39;\\mathbf{H}v_i - v_i&#39;\\mathbf{H}v_i + v_i&#39; \\mathbf{H}\\mathbf{H}v_i \\\\ &amp;= v_i&#39;v_i - v_i&#39;\\mathbf{H}\\mathbf{H}v_i - v_i&#39;\\mathbf{H}\\mathbf{H}v_i + v_i&#39; \\mathbf{H}\\mathbf{H}v_i \\\\ &amp;= v_i&#39;v_i - v_i&#39;\\mathbf{H}\\mathbf{H}v_i \\\\ &amp;= ||v_i||_2^2 - ||\\mathbf{H}v_i||_2^2 \\\\ &amp;= 1 - ||\\mathbf{H}v_i||_2^2 \\end{align*}\\] Then \\[\\begin{align*} ||v_i - \\mathbf{H}v_i||_2^2 = 1 - ||\\mathbf{H}v_i||_2^2 &amp; \\implies ||v_i - \\mathbf{H}v_i||_2^2 + ||\\mathbf{H}v_i||_2^2 = 1 \\\\ &amp; \\implies ||\\mathbf{H}v_i||_2^2 \\leq 1 \\\\ &amp; \\implies v_i&#39; \\mathbf{H}\\mathbf{H}v_i \\leq 1 \\\\ &amp; \\implies v_i&#39; \\mathbf{H}v_i \\leq 1 \\\\ &amp; \\implies h_{ii} \\leq 1 \\\\ \\end{align*}\\] Since \\(h_{ii}=||\\mathbf{H}v_i||_2^2\\) is a norm then \\(h_{ii} \\geq 0\\), then: \\[ 0 \\leq h_{ii} \\leq 1 \\] So, leverages are bounded. Furthermore, we have that: \\[ \\sum_{i=1}^p h_{ii} = p \\] To see this, we will need the following properties: \\(\\text{rank}(\\mathbf{H}) = p.\\) All the eigenvalues of \\(\\mathbf{H}\\) are either 0 or 1. As usual, we assume that \\(\\text{rank}(\\mathbf{X}) = p\\) and \\(n&gt;p\\), that is there are more observations than independent variables. Then \\[\\begin{align*} \\text{rank}(\\mathbf{X}) = p &amp;\\implies \\text{rank}(\\mathbf{X}&#39;\\mathbf{X}) = p \\\\ &amp;\\implies \\text{rank}\\left((\\mathbf{X}&#39;\\mathbf{X})^{-1}\\right) = p \\\\ &amp;\\implies \\text{rank}\\left(\\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\right) = p \\\\ &amp;\\implies \\text{rank}\\left(\\mathbf{H}\\right) = p \\\\ \\end{align*}\\] Now, let \\(\\lambda\\) be an eigenvalue of \\(\\mathbf{H}\\), with respective \\(\\mathbf{v}\\) as eigenvector. Then: \\[\\begin{align*} \\mathbf{H}\\mathbf{v}= \\lambda \\mathbf{v}\\\\ &amp;\\implies \\mathbf{H}\\mathbf{H}\\mathbf{v}= \\mathbf{H}\\lambda \\mathbf{v}= \\lambda \\mathbf{H}\\mathbf{v}\\\\ &amp;\\implies \\mathbf{H}\\mathbf{v}= \\lambda \\lambda \\mathbf{v}= \\lambda^2 \\mathbf{v}\\\\ &amp;\\implies \\lambda = 1 \\land \\lambda = 0 \\end{align*}\\] Now since, \\(\\mathbf{H}\\) is of rank \\(p\\), then it must have \\(p\\) non-zero eigenvalues. And, since all non-zero eigenvalues are 1, then we have that: \\(\\sum_{i=1}^n \\lambda_i = p\\) that is the sum of the eigenvalues is \\(p\\). We also know that for any square matrix, the sum of the eigenvalues is equal to the trace of the matrix. Then: \\[ \\text{trace}(\\mathbf{H}) = p \\implies \\sum_{i=1}^n h_{ii} = p\\] This is telling us important information about the leverages. You can compute a proportional leverage as: \\[ \\frac{h_{ii}}{p}\\] the proportion of the leverage \\(i\\) out of “all” the leverage. Or in terms of “observations”, you can compute: \\[\\frac{n h_{ii}}{p}\\] if this number is several times bigger than \\(1\\), it will be an observation with considerable leverage. Another way to characterize the leverages can be found as follows: Recall that the hat matrix is given by: \\[\\mathbf{H}= \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\] and we can express the design matrix as follows: \\[ \\mathbf{X}= \\left[\\begin{matrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{matrix}\\right] \\] and \\[ \\mathbf{X}&#39; = [X_1&#39; X_2&#39; \\ldots X_n&#39;] \\] Then, the hat matrix is given by: \\[\\begin{align*} \\mathbf{H} &amp;= \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\\\ &amp;= \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1} [X_1&#39; X_2&#39; \\ldots X_n&#39;] \\\\ &amp;= \\mathbf{X}[(\\mathbf{X}&#39;\\mathbf{X})^{-1}X_1&#39; (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_2&#39; \\ldots (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_n&#39;] \\\\ &amp;= \\left[\\begin{matrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{matrix}\\right] [(\\mathbf{X}&#39;\\mathbf{X})^{-1}X_1&#39; (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_2&#39; \\ldots (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_n&#39;] \\\\ &amp;= \\left[\\begin{matrix} X_1 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_1&#39; &amp; X_1 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_2&#39; &amp; \\ldots &amp; X_1 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_n&#39; \\\\ X_2 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_1&#39; &amp; X_2 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_2&#39; &amp; \\ldots &amp; X_2 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_n&#39; \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ X_n (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_1&#39; &amp; X_n (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_2&#39; &amp; \\ldots &amp; X_n (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_n&#39; \\\\ \\end{matrix}\\right] \\end{align*}\\] we are going to use this expression latter, but we can also notice that: \\[h_{ij} = X_i (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_j&#39;\\] So we can characterize the leverages as: \\[h_{ij} = X_i (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_j&#39;\\] 6.9.2 Outliers identification Identifying outliers in multiple regression is hard. One way to measure the influence of an observation is to see how different are the predicted values if an observation is removed. We will call: \\(\\mathbf{X}_{(i)}\\) the design matrix, when the \\(i\\)-th observation is removed. \\(\\mathbf{y}_{(i)}\\) the dependent variables without the observation \\(i\\). \\(\\hat{\\mathbf{y}}_{(i)}\\) the predicted values when the \\(i\\)-th observation is removed. \\(\\mathbf{H}_{(i)}\\) the hat matrix when the \\(i\\)-th observation is removed. Then a measure of influence can be: \\[||\\mathbf{y}- \\hat{\\mathbf{y}}_{(i)}||^2_2 = (\\mathbf{y}- \\hat{\\mathbf{y}}_{(i)})&#39;(\\mathbf{y}- \\hat{\\mathbf{y}}_{(i)}) = \\sum_{j=1}^n (\\mathbf{y}_j - \\hat{\\mathbf{y}}_{(i),j})^2\\] Notice that: \\[\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}\\] and, similarly \\[\\hat{\\mathbf{y}}_{(i)} = \\mathbf{H}_{(i)} \\mathbf{y}_{(i)}\\] Now, let us compute: \\[\\begin{align*} \\hat{\\mathbf{y}} &amp;= \\mathbf{H}\\mathbf{y}\\\\ &amp;= \\left[\\begin{matrix} X_1 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_1&#39; &amp; X_1 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_2&#39; &amp; \\ldots &amp; X_1 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_n&#39; \\\\ X_2 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_1&#39; &amp; X_2 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_2&#39; &amp; \\ldots &amp; X_2 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_n&#39; \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ X_n (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_1&#39; &amp; X_n (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_2&#39; &amp; \\ldots &amp; X_n (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_n&#39; \\\\ \\end{matrix}\\right] \\left[\\begin{matrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{matrix}\\right] \\\\ &amp;= \\left[\\begin{matrix} \\sum_{k=1}^n X_1 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_k&#39;y_k \\\\ \\sum_{k=1}^n X_2 (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_k&#39;y_k \\\\ \\vdots \\\\ \\sum_{k=1}^n X_n (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_k&#39;y_k \\end{matrix}\\right] \\end{align*}\\] \\[ \\] Then \\[\\hat{y}_j = \\sum_{k=1}^n X_j (\\mathbf{X}&#39;\\mathbf{X})^{-1}X_k&#39;y_k = X_j (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\sum_{k=1}^n X_k&#39;y_k\\] in a similar way, observation : \\[\\hat{y}_{(i)j} = \\sum_{k=1}^n X_j (\\mathbf{X}_{(i)}&#39;\\mathbf{X}_{(i)})^{-1}X_k&#39;y_k = X_j (\\mathbf{X}_{(i)}&#39;\\mathbf{X}_{(i)})^{-1} \\sum_{k=1}^n X_k&#39;y_k\\] Notice that it is not necessary to use the subscript \\((i)\\) when we are dealing with single observations. Now, we will relate \\((\\mathbf{X}&#39;\\mathbf{X})^{-1}\\) to \\((\\mathbf{X}_{(i)}&#39;\\mathbf{X}_{(i)})^{-1}\\) using the Sherman–Morrison formula. First notice that: \\[\\mathbf{X}&#39; \\mathbf{X}= \\mathbf{X}_{(i)}&#39;\\mathbf{X}_{(i)} + X_i&#39;X_i\\] then \\[\\mathbf{X}_{(i)}&#39;\\mathbf{X}_{(i)} = \\mathbf{X}&#39; \\mathbf{X}- X_i&#39;X_i\\] Now we apply the Sherman–Morrison formula. \\[\\begin{align*} \\left(\\mathbf{X}_{(i)}&#39;\\mathbf{X}_{(i)}\\right)^{-1} = \\left(\\mathbf{X}&#39; \\mathbf{X}- X_i&#39;X_i\\right)^{-1} &amp;= \\left(\\mathbf{X}&#39; \\mathbf{X}+ (-X_i)&#39;X_i\\right)^{-1} \\\\ &amp;= \\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1} - \\frac{\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}(-X_i)&#39;X_i\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}}{1 + (-X_i)&#39;\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}X_i} \\\\ &amp;= \\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1} + \\frac{\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}X_i&#39;X_i\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}}{1 - X_i&#39;\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}X_i} \\\\ &amp;= \\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1} + \\frac{\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}X_i&#39;X_i\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}}{1 - h_{ii}} \\end{align*}\\] Then, \\[\\hat{y}_{(i)j} = X_j \\left(\\mathbf{X}_{(i)}&#39;\\mathbf{X}_{(i)}\\right)^{-1} \\sum_{k \\neq i}X_k&#39;y_k = X_j\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}\\sum_{k \\neq i}X_k&#39;y_k + \\frac{X_j\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}X_i&#39;X_i\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}\\sum_{k \\neq i}X_k&#39;y_k}{1 - h_{ii}}\\] Now \\[\\begin{align*} X_j\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}\\sum_{k \\neq i}X_k&#39;y_k &amp;= X_j\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}\\left(\\sum_{k \\neq i}X_k&#39;y_k + X_i&#39;y_i - X_i&#39;y_i\\right) \\\\ &amp;= X_j\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}\\left(\\sum_{k = 1}^nX_k&#39;y_k - X_i&#39;y_i\\right) \\\\ &amp;= X_j\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}\\sum_{k = 1}^nX_k&#39;y_k - X_j\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}X_i&#39;y_i \\\\ &amp;= \\hat{y}_j - h_{ij}y_i \\end{align*}\\] and \\[\\begin{align*} X_j\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}X_i&#39;X_i\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}\\sum_{k \\neq i}X_k&#39;y_k &amp;= h_{ij}X_i\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}\\sum_{k \\neq i}X_k&#39;y_k \\\\ &amp;= h_{ij}\\sum_{k \\neq i}X_i\\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}X_k&#39;y_k \\\\ &amp;= h_{ij}\\sum_{k \\neq i}h_{ik}y_k \\end{align*}\\] Then \\[\\begin{align*} \\hat{y}_{(i)j} = \\hat{y}_j - h_{ij}y_i + \\frac{h_{ij}\\sum_{k \\neq i}h_{ik}y_k}{1 - h_{ii}} &amp;= \\hat{y}_j - h_{ij}\\left(y_i - \\frac{\\sum_{k \\neq i}h_{ik}y_k}{1 - h_{ii}} \\right) \\\\ &amp;= \\hat{y}_j - h_{ij}\\left(\\frac{y_i - y_ih_{ii}}{1 - h_{ii}} - \\frac{\\sum_{k \\neq i}h_{ik}y_k}{1 - h_{ii}} \\right) \\\\ &amp;= \\hat{y}_j - h_{ij}\\left(\\frac{y_i - y_ih_{ii} - \\sum_{k \\neq i}h_{ik}y_k}{1 - h_{ii}} \\right) \\\\ &amp;= \\hat{y}_j - h_{ij}\\left(\\frac{y_i - \\sum_{k=1}^n h_{ik}y_k}{1 - h_{ii}} \\right) \\\\ &amp;= \\hat{y}_j - h_{ij}\\left(\\frac{y_i - \\hat{y}_i}{1 - h_{ii}} \\right) \\\\ &amp;= \\hat{y}_j - h_{ij}\\left(\\frac{\\hat{e}_i}{1 - h_{ii}} \\right) \\end{align*}\\] Then \\[\\begin{align*} \\hat{y}_{(i)j} - \\hat{y}_j = - h_{ij}\\left(\\frac{\\hat{e}_i}{1 - h_{ii}} \\right) &amp;\\implies \\left(\\hat{y}_{(i)j} - \\hat{y}_j \\right)^2= \\left(h_{ij}\\left(\\frac{\\hat{e}_i}{1 - h_{ii}} \\right) \\right)^2 = \\frac{\\hat{e}_i^2}{(1 - h_{ii})^2}h_{ij}^2 \\\\ &amp;\\implies \\left(\\hat{\\mathbf{y}}_{(i)} - \\hat{\\mathbf{y}} \\right)&#39;\\left(\\hat{\\mathbf{y}}_{(i)} - \\hat{\\mathbf{y}} \\right) = \\sum_{j=1}^n \\frac{\\hat{e}_i^2}{(1 - h_{ii})^2}h_{ij}^2 \\\\ &amp;\\implies ||\\hat{\\mathbf{y}}_{(i)} - \\hat{\\mathbf{y}}||_2^2 = \\frac{\\hat{e}_i^2}{(1 - h_{ii})^2} \\sum_{j=1}^n h_{ij}^2 \\end{align*}\\] Now for any symmetric and idempotent matrix \\(\\mathbf{M}\\in \\mathbb{R}^{n \\times n}\\) we have that: \\[m_{ii} = \\sum_{j = 1}^n m_{ij}^2\\] Then \\[||\\hat{\\mathbf{y}}_{(i)} - \\hat{\\mathbf{y}}||_2^2 = \\frac{\\hat{e}_i^2}{(1 - h_{ii})^2} h_{ii}^2 = \\frac{h_{ii}}{(1 - h_{ii})^2}\\hat{e}_i^2 \\] That is the change in the predictions is the result of a function of the leverages and a function of the error of observation \\(i\\). We can see this with our GDP dataset. We generate new observations, in which # Reads Data dat &lt;- read.csv(file = &quot;Gdp data.csv&quot;) # Extracts the Data y &lt;- as.numeric(dat[, 1]) X &lt;- as.matrix(dat[, -1]) n &lt;- nrow(X) X &lt;- cbind(rep(1, n), X) # Obtains the Estimates b &lt;- solve(t(X) %*% X, t(X) %*% y) # Predicted Values yh &lt;- X %*% b # Errors eh &lt;- abs(y - yh) # Obtains the Minimum, Median and Maximum Errors err &lt;- quantile(eh, probs = c(0, 0.5, 1)) # Generates New Observations # An observation at one extreme of the Observed Independent Variables X1 &lt;- apply(X = X, MARGIN = 2, FUN = max) # An observation at the center of the Observed Independent Variables X2 &lt;- apply(X = X, MARGIN = 2, FUN = mean) # Dependent Variables with different Error levels y1 &lt;- c(t(X1) %*% b) + c(0, err, err[3] * 2) y2 &lt;- c(t(X2) %*% b) + c(0, err, err[3] * 2) # Outlier Level m &lt;- length(y1) # Predictions without the Observations yhi &lt;- yh bi &lt;- b yi &lt;- y Xi &lt;- X # Table with Changes tab &lt;- matrix(data = NA, nrow = 0, ncol = 4) for(i in 1:m){ # Complete Data Set X &lt;- rbind(Xi, X2) y &lt;- c(yi, y2[i]) # Coefficients b &lt;- solve(t(X) %*% X, t(X) %*% y) # Predictions yh &lt;- c(X %*% b) yhi &lt;- c(rbind(Xi, X2) %*% bi) # Hat matrix H &lt;- X %*% solve(t(X) %*% X, t(X)) # Leverage New Observation h &lt;- H[n+1, n+1] # Change in Predictions preCha &lt;- sum((yh - yhi)^2) # Leverage Component levCom &lt;- h / ((1 - h)^2) # Error Component errCom &lt;- (y[n+1] - yh[n+1])^2 # Saves Values tab &lt;- rbind(tab, c(preCha, levCom, errCom, levCom * errCom)) } for(i in 1:m){ # Complete Data Set X &lt;- rbind(Xi, X1) y &lt;- c(yi, y1[i]) # Coefficients b &lt;- solve(t(X) %*% X, t(X) %*% y) # Predictions yh &lt;- c(X %*% b) yhi &lt;- c(rbind(Xi, X1) %*% bi) # Hat matrix H &lt;- X %*% solve(t(X) %*% X, t(X)) # Leverage New Observation h &lt;- H[n+1, n+1] # Change in Predictions preCha &lt;- sum((yh - yhi)^2) # Leverage Component levCom &lt;- h / ((1 - h)^2) # Error Component errCom &lt;- (y[n+1] - yh[n+1])^2 # Saves Values tab &lt;- rbind(tab, c(preCha, levCom, errCom, levCom * errCom)) } tab &lt;- data.frame(tab) tab &lt;- cbind(c(rep(&quot;Low&quot;, 5), rep(&quot;High&quot;, 5)), tab) tab &lt;- cbind(rep(c(&quot;Minimum&quot;, &quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;, &quot;Very High&quot;), 2), tab) colnames(tab) &lt;- c(&quot;Error Level&quot;, &quot;Leverage Level&quot;, &quot;Sum of Sq. Diff.&quot;, &quot;Lev. Comp.&quot;, &quot;Err. Comp.&quot;, &quot;Using Formula&quot;) knitr::kable(tab, digits = 5) Error Level Leverage Level Sum of Sq. Diff. Lev. Comp. Err. Comp. Using Formula Minimum Low 0.00000 0.00513 0.00000 0.00000 Low Low 0.00000 0.00513 0.00002 0.00000 Medium Low 0.00070 0.00513 0.13617 0.00070 High Low 0.01245 0.00513 2.42785 0.01245 Very High Low 0.04980 0.00513 9.71139 0.04980 Minimum High 0.00000 0.86603 0.00000 0.00000 Low High 0.00001 0.86603 0.00001 0.00001 Medium High 0.04918 0.86603 0.05679 0.04918 High High 0.87684 0.86603 1.01248 0.87684 Very High High 3.50735 0.86603 4.04991 3.50735 6.10 Stability of the Solution The results of the regression can be subject to numerical problems or the impact of changes in the data. In general, the results would depend on the stability of \\(\\mathbf{X}&#39;\\mathbf{X}\\). One way to measure the worst case scenario is the condition number of the matrix \\(\\mathbf{X}&#39; \\mathbf{X}\\), denoted \\(\\kappa(\\mathbf{X}&#39; \\mathbf{X})\\), and is equal to: \\[ \\kappa(\\mathbf{X}&#39; \\mathbf{X}) = \\frac{\\lambda_{max}}{\\lambda_{min}} \\] where, \\(\\lambda_{max}\\) and \\(\\lambda_{min}\\) are the maximum and minimum eigenvalues of \\(\\mathbf{X}&#39; \\mathbf{X}\\). There are some considerations, about the condition number: The condition number is a measure of the worst case scenario, a bigger condition number of a matrix, does not necessarily will be more unstable than a matrix with a lower condition number. \\(\\lambda_{max} = \\max_i \\sigma_i^2\\) and \\(\\lambda_{max} = \\min_i \\sigma_i^2\\), where \\(\\sigma_i\\) are the singular values of \\(\\mathbf{X}\\). The condition number will be affected by: Disparities in the magnitude of the variables. The level of multicoliniarity. Influential observations. Since, the condition number is affected by the magnitude of the variables, it is common to work with centered and standardized variables when inspecting this issues. Another approach is to check the determinant of a matrix. A matrix would be in general more unstable the close is it’s determinant closer to 0. Indeed if the determinant is 0, then \\(\\mathbf{X}&#39;\\mathbf{X}\\) will not be invertible and the solution \\[ \\hat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\mathbf{X}&#39; \\mathbf{y}\\] is not applicable. However, just by looking at the determinant is hard to know what is close to 0. Consider for example the determinant of a multiple of the identity, then: \\(\\det(c\\mathbf{I}_n) = c^n \\mathbf{I}\\) if \\(c&lt;1\\) this will make the determinant closer to zero. However the matrix \\(c \\mathbf{I}\\) has condition number of \\(1\\). It is better to evaluate the change in the determinant after some normalization to avoid this scenarios. Consider, for a moment, the standardized case, with estimated coefficients: \\[ \\hat{\\boldsymbol{\\beta}}=r_{XX}^{-1}r_{Xy}\\] then we can see how the determinant the correlation matrix, changes when we remove a variable. Without loss of generality, we can assume we remove variable 1. Then, the correlation matrix will be given by: \\[r_{XX} = \\left[ \\begin{matrix} r_{X_2X_2} &amp; r_{X_2x_{1}} \\\\ r_{x_{1}X} &amp; r_{x_{1}x_{1}} \\\\ \\end{matrix} \\right]\\] Then, using the properties of the determinant of a block matrix, and the results of the Single Variable Cross-Effects subsection, we have that: \\[\\begin{align*} \\det(r_{XX}) &amp;= \\det \\left( \\left[ \\begin{matrix} r_{X_2X_2} &amp; r_{X_2x_{1}} \\\\ r_{x_{1}X} &amp; r_{x_{1}x_{1}} \\\\ \\end{matrix} \\right]\\right) \\\\ &amp;= \\det(r_{X_2X_2})\\det(r_{x_{1}x_{1}} - r_{X_2x_{1}}r_{X_2X_2}^{-1}r_{x_{1}X_2}) \\\\ &amp;= \\det(r_{X_2X_2})\\det(r_{x_{1}|X_2}) \\\\ &amp;= \\det(r_{X_2X_2})\\det(1-R^2_{1}) \\\\ &amp;= \\det(r_{X_2X_2})(1-R^2_{1}) \\end{align*}\\] We can see that the determinant is closer to 0 the more correlated is the first variable to the rest of the variables, that is: \\[R^2_1 \\to 1 \\implies \\det(r_{XX}) \\to 0\\] In a similar way, assume that we are working with centered values, we can analyze what would happen if we remove an observation \\(X_{i}\\). Here we analyze \\(\\mathbf{X}&#39; \\mathbf{X}\\). Again, using properties of the determinant, and results of the Outliers Identification we have that: \\[\\det(\\mathbf{X}_{(i)}&#39;\\mathbf{X}_{(i)}) = \\det(\\mathbf{X}_{(i)}&#39;\\mathbf{X}_{(i)}) \\\\ =\\det(\\mathbf{X}&#39;\\mathbf{X}&#39; - X_iX_i&#39;) \\\\ =\\det(\\mathbf{X}&#39;\\mathbf{X}&#39;)\\det(1 - X_i(\\mathbf{X}&#39;\\mathbf{X})^{-1}X_i&#39;) \\\\ =\\det(\\mathbf{X}&#39;\\mathbf{X}&#39;)\\det(1 - h_{ii}) \\\\ =\\det(\\mathbf{X}&#39;\\mathbf{X}&#39;)(1 - h_{ii}) \\\\\\] So, the determinant deteriorates the more leverage observation \\(i\\) has. We can see examples of the previous operations with our GDP data. # Read Data dat &lt;- read.csv(&quot;Gdp data.csv&quot;) # Design Matrix Independent Variables X &lt;- as.matrix(dat[, -1]) # Centering # Vector of Ones v1 &lt;- rep(1, n) # Centering Matrix C &lt;- diag(n) - (1/n) * v1 %*% t(v1) # Independent Variables Centered Xc &lt;- C %*% X # Standardizing matrix of X DX &lt;- diag(1/sqrt(diag(SXX))) # Standardize X Xs &lt;- Xc %*% DX # Condition numbers print(paste0(&quot;Condition number of X&#39;X = &quot;, round(kappa(t(X) %*% X), 2))) ## [1] &quot;Condition number of X&#39;X = 28072.42&quot; print(paste0(&quot;Condition number of Xc&#39;Xc = &quot;, round(kappa(t(Xc) %*% Xc), 2))) ## [1] &quot;Condition number of Xc&#39;Xc = 1186.14&quot; print(paste0(&quot;Condition number of Xs&#39;Xs = &quot;, round(kappa(t(Xs) %*% Xs), 2))) ## [1] &quot;Condition number of Xs&#39;Xs = 105.68&quot; as we can see the condition number improves as with the centering and the standardization. However, this is not necessarily so. Now we evaluate the effect of each variable on the determinant. P &lt;- ncol(X) vecDet &lt;- numeric(length = P) vecR2 &lt;- numeric(length = P) # Computes for(i in 1:P){ vecDet[i] &lt;- det(cor(X[, -i])) vecR2[i] &lt;- summary(lm(Xs[, i] ~ Xs[, -i] - 1))$r.squared } # Formats and Show Info Table tab &lt;- cbind(rep(det(cor(X)), P), vecDet, vecR2) colnames(tab) &lt;- c(&quot;r_{X&#39;X}&quot;, &quot;r_{X_(i)&#39;X_(i)}&quot;, &quot;R2&quot;) rownames(tab) &lt;- colnames(dat)[2:6] knitr::kable(tab) r_{X’X} r_{X_(i)’X_(i)} R2 inf 0.0323914 0.8190564 0.9604527 une 0.0323914 0.2800338 0.8843302 int 0.0323914 0.3451523 0.9061532 gov 0.0323914 0.0408342 0.2067576 exp 0.0323914 0.0324190 0.0008505 Finally we can evaluate the effects of a new observations depending on the leverage: # Read Data dat &lt;- read.csv(&quot;Gdp data.csv&quot;) # Design Matrix Independent Variables X &lt;- as.matrix(dat[, -1]) P &lt;- ncol(X) n &lt;- nrow(X) # Centering # Vector of Ones v1 &lt;- rep(1, n) # Centering Matrix C &lt;- diag(n) - (1/n) * v1 %*% t(v1) # Independent Variables Centered Xc &lt;- C %*% X # Standardizing matrix of X DX &lt;- diag(1/sqrt(diag(SXX))) # Standardize X Xs &lt;- Xc %*% DX # New Observations # Very Leveraged Observation Xn &lt;- t(rep(3, P)) # Mildly Leveraged Observation Xn &lt;- rbind(Xn, t(rep(1, P))) # No Leverage Xn &lt;- rbind(Xn, t(rep(0, P))) # Number of New Observations m &lt;- nrow(Xn) # Computes the Effects of the New Observation vecDet &lt;- numeric(length = m) vecLev &lt;- numeric(length = m) for(i in 1:m){ Z &lt;- rbind(Xs, Xn[i, ]) vecDet[i] &lt;- det(t(Z) %*% Z) vecLev[i] &lt;- t(Xn[i, ]) %*% solve(t(Z) %*% Z, Xn[i, ]) } # Formats and Show Info Table tab &lt;- cbind(rep(det(t(Xs) %*% Xs), 3), vecDet, vecLev, vecLev * n / P) colnames(tab) &lt;- c(&quot;det(Xs_(i)&#39;Xs_(i))&quot;,&quot;det(Xs&#39;Xs)&quot;, &quot;Leverage&quot;, &quot;Obs. Equivalent&quot;) rownames(tab) &lt;- c(&quot;Big Leverage&quot;, &quot;Medium Leverage&quot;, &quot;No Leverage&quot;) knitr::kable(tab) det(Xs_(i)’Xs_(i)) det(Xs’Xs) Leverage Obs. Equivalent Big Leverage 9132788636 18778457801 0.5136561 20.135319 Medium Leverage 9132788636 10204529654 0.1050260 4.117019 No Leverage 9132788636 9132788636 0.0000000 0.000000 "],["bootstrapping.html", "7 Bootstrapping 7.1 Introduction 7.2 Bootstrapping Example", " 7 Bootstrapping 7.1 Introduction Bootstrapping is a statistical technique that estimates the variability of a statistic by resampling data with replacement. It creates multiple simulated datasets (“bootstrap samples”) from the original data, allowing the construction of empirical distributions for estimates like means, medians, or regression coefficients. This method does not rely on assumptions about the data’s distribution, making it highly flexible and robust, especially with small sample sizes. Key Points: Resampling with Replacement: Each bootstrap sample allows the same data point to appear multiple times. No Distribution Assumptions: Unlike traditional methods, bootstrapping is non-parametric. Estimates Uncertainty: Useful for computing confidence intervals and standard errors without strict assumptions. Bootstrapping is a simple and powerful tool for inference, often applied when data is scarce or distributional assumptions are unclear. 7.2 Bootstrapping Example So far we have been able to compute several estimates of interest: \\[ \\hat{\\boldsymbol{\\beta}} \\] \\[ \\hat{\\mathbf{y}} \\] \\[ \\hat{\\mathbf{e}} \\] But they are point estimates. When we talked about influential observations and outliers, we saw what were some of the effects on the estimates when we removed observations. But this changes were very limited. With bootstrapping we can add uncertainty to our estimates or any quantity of interest. Let’s see this with our GDP data. # Data Load dat &lt;- read.csv(file = &quot;Gdp data.csv&quot;) # Assigns Values X &lt;- as.matrix(dat[, 2:6]) y &lt;- as.numeric(dat$gdp) # Number of Observations n &lt;- nrow(X) # Design Matrix X &lt;- cbind(rep(1, n), X) # Number of Variables p &lt;- ncol(X) # Number of Bootstrapping Samples m &lt;- 1000 # Bootstrapping Coefficient Estimates booCoe &lt;- matrix(data = NA, nrow = m, ncol = p) for(i in 1:m){ # Sample Selection booSel &lt;- sample(x = 1:n, size = n, replace = TRUE) # Bootstrap Samples Xb &lt;- X[booSel, ] yb &lt;- y[booSel] # Coefficient Estimation booCoe[i, ] &lt;- solve(t(Xb) %*% Xb, t(Xb) %*% yb) } # Plots the distribution for each coefficient boxplot(booCoe, col = rgb(0, 0, 1, 0.5), xaxt = &quot;n&quot;) axis(side = 1, at = 1:6, labels = c(&quot;Intercept&quot;, &quot;Inf&quot;, &quot;Une&quot;, &quot;Int&quot;, &quot;Gov&quot;, &quot;Exp&quot;)) abline(h = 0, col = rgb(1, 0, 0), lwd = 2) Here we can see that the intercept has a lot of uncertainty. While the coefficients corresponding to the inflation rate, unemployment rate and the interest rate have less variability and the government expenditure and exports have very little variability. We see that even though the intercept is very variable is always positive. While the coefficients for inflation and unemployment have less variability but a couple of times change signs. The interest rate coefficient, while positive most of the time is negative sometimes. The government expenditure coefficients lands almost exactly at 0, with very little variability, while the coefficient for exports is small but always positive. We can observe how the coefficient estimates are related: # Data Frame booCoeDf &lt;- data.frame(booCoe) colnames(booCoeDf) &lt;- c(&quot;Intercept&quot;, &quot;Inf&quot;, &quot;Une&quot;, &quot;Int&quot;, &quot;Gov&quot;, &quot;Exp&quot;) pairs(booCoeDf) Here we can observe that the coefficients for inflation, unemployment and interest rate are very correlated. While government expenditure and exports are almost independent of the other variables. This type of analysis is not limited to the coefficients, it can also be applied to the predictions (in and out of sample). For example, we can compare the estimates and the observed values for GDP. We can compute 95% bootstrap confidence intervals for the estimates and analyze for which observations the linear model is a good fit, and for which it isn’t. # Mean Estimates yh &lt;- booCoe %*% t(X) # Bootstrap Prediction Quantiles 2.5% and 97.5% booConQua &lt;- apply(X = yh, MARGIN = 2, FUN = quantile, probs = c(0.025, 0.975)) # Plots the Intervals and Observations # Number of Plots numPlo &lt;- 2 ymin &lt;- min(y, booConQua) ymax &lt;- max(y, booConQua) xmin &lt;- 0 xmax &lt;- n / numPlo xSet &lt;- matrix(data = 1:n, nrow = numPlo, ncol = n / numPlo) # Bootstrap Confidence Intervals Plot par(mfrow = c(numPlo, 1)) par(mar = c(0, 3, 0, 0)) for(i in 1:numPlo){ plot(NULL, xlim = c(xmin, xmax), ylim = c(ymin, ymax), ylab = &quot;GDP % Growth&quot;, xlab = &quot;Countries&quot;, xaxt = &quot;n&quot;) segments(x0 = xSet[i, ], x1 = xSet[i, ], y0 = booConQua[1, xSet[i, ]], y1 = booConQua[2, xSet[i, ]], lwd = 3) points(x = xSet[i, ], y = y[xSet[i, ]], pch = 16, col = rgb(1, 0, 0, 0.75)) } # Coverage of the Bootstrap Confidence Intervals print(paste0(&quot;Bootstrap Confidence Intervals Coverage = &quot;, round(mean((y &gt; booConQua[1, ]) &amp; (y &lt; booConQua[2, ])) * 100, 2) )) ## [1] &quot;Bootstrap Confidence Intervals Coverage = 27.04&quot; As we can see, the mean estimate confidence intervals of the observations do not cover 95% of the observed values. This is because we are only estimating the mean value (the place where the line is) and not where the observations could be, that is it is not accounting for possible errors. We can include this uncertainty component by adding the bootstrapping errors. Since, we have made no assumptions about the errors, we can assume now that the errors will behave the same way for all observations, so we can create bootstrap errors and add them to our bootstrap mean estimates to create prediction intervals. ### Error Bootstrapping Samples eh &lt;- t(t(yh) - y) ### Then we re-sample the errors eh &lt;- eh[sample(x = 1:(m * n), size = m * n, replace = FALSE)] # Adds the resampled errors to the bootstrap observations sample yp &lt;- yh + eh # Bootstrap Prediction Quantiles 2.5% and 97.5% booPreQua &lt;- apply(X = yp, MARGIN = 2, FUN = quantile, probs = c(0.025, 0.975)) # Plots the Intervals and Observations # Number of Plots numPlo &lt;- 2 ymin &lt;- min(y, booPreQua) ymax &lt;- max(y, booPreQua) xmin &lt;- 0 xmax &lt;- n / numPlo xSet &lt;- matrix(data = 1:n, nrow = numPlo, ncol = n / numPlo) # Non-Covered Observations nonCov &lt;- ((y &gt; booPreQua[1, ]) &amp; (y &lt; booPreQua[2, ])) # Bootstrap Confidence Intervals Plot par(mfrow = c(numPlo, 1)) par(mar = c(0, 3, 0, 0)) for(i in 1:numPlo){ plot(NULL, xlim = c(xmin, xmax), ylim = c(ymin, ymax), ylab = &quot;GDP % Growth&quot;, xlab = &quot;Countries&quot;, xaxt = &quot;n&quot;) segments(x0 = xSet[i, ] - (i - 1), x1 = xSet[i, ] - (i - 1), y0 = booConQua[1, xSet[i, ]], y1 = booConQua[2, xSet[i, ]], lwd = 3) segments(x0 = xSet[i, ] - (i - 1), x1 = xSet[i, ] - (i - 1), y0 = booPreQua[1, xSet[i, ]], y1 = booPreQua[2, xSet[i, ]], lwd = 1) points(x = xSet[i, ] - (i - 1), y = y[xSet[i, ]], pch = 16, col = rgb(1, 0, 0, 0.75)) points(x = xSet[i, ][!nonCov[xSet[i, ]]] - (i - 1), y = y[xSet[i, ][!nonCov[xSet[i, ]]]], pch = 1, col = rgb(0, 0, 1, 0.75)) } # Coverage of the Bootstrap Confidence Intervals print(paste0(&quot;Bootstrap Predictive Intervals Coverage = &quot;, round(mean((y &gt; booPreQua[1, ]) &amp; (y &lt; booPreQua[2, ])) * 100, 2) )) ## [1] &quot;Bootstrap Predictive Intervals Coverage = 95.41&quot; In the figure the thin intervals represent the 95% predictive intervals and the thick intervals the 95% mean confidence intervals. Notice here that the coverage of the Predictive Intervals is much closer to 95%. Also, notice that if you run this code in your computer the results will be slightly different, since this results depend on the random sampling used to create the bootstrap samples. "],["mean-and-varaince-assumptions.html", "8 Mean and Varaince Assumptions 8.1 Mean Assumptions 8.2 Variance Assumptions 8.3 Cross-Covariances 8.4 Gauss-Markov Theorem 8.5 Estimate of \\(\\sigma^2\\)", " 8 Mean and Varaince Assumptions So far we have not made any probabilistic assumptions about the different elements in our model. So the model errors are unknown but are not random. Now we will assume that the random errors are random variables. However, we will not specify the complete distribution of the errors and will limit ourselves to make assumptions about the mean and variance of the errors. 8.1 Mean Assumptions We will begin by making the assumptions about the mean of the errors. Specifically, we will assume that: \\[ \\mathbb{E}[e_i] = 0 \\in \\mathbb{R}\\quad \\forall i\\in\\{1,\\ldots,n\\}\\] This can be expressed in vector form as follows: \\[ \\mathbb{E}[\\mathbf{e}] = \\mathbf{0}\\in \\mathbb{R}^n\\] Note that \\(\\mathbf{X}\\) is a known constant and \\(\\boldsymbol{\\beta}\\) is an unknown constant (an unknown parameter). This implies that + \\(\\mathbf{y}\\) is a random vector. And therefor, any function of \\(\\mathbf{y}\\) will be a random varaible. In particular, our estimates: \\[ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{y}\\] \\[ \\hat{\\mathbf{y}} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{y}= \\mathbf{H}\\mathbf{y}\\] \\[ \\hat{\\mathbf{e}} = \\mathbf{y}- \\hat{\\mathbf{y}} = (\\mathbf{I}- \\mathbf{H})\\mathbf{y}\\] are random vectors. Then we can try to compute the mean of these values. This should be possible since all 3 estimates are linear combinations of \\(\\mathbf{y}\\) and we can compute the mean of \\(\\mathbf{y}\\). 8.1.1 Expectation of \\(\\mathbf{y}\\): \\[\\begin{align*} \\mathbb{E}[\\mathbf{y}] &amp;= \\mathbb{E}[\\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}] \\\\ &amp;= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbb{E}[\\mathbf{e}] \\\\ &amp;= \\mathbf{X}\\boldsymbol{\\beta} \\end{align*}\\] 8.1.2 Expectation of \\(\\hat{\\boldsymbol{\\beta}}\\) \\[\\begin{align*} \\mathbb{E}[\\hat{\\boldsymbol{\\beta}}] &amp;= \\mathbb{E}[(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{y}] \\\\ &amp;= (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbb{E}[\\mathbf{y}] \\\\ &amp;= (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{X}\\boldsymbol{\\beta}\\\\ &amp;= \\boldsymbol{\\beta}\\\\ \\end{align*}\\] So, \\(\\hat{\\boldsymbol{\\beta}}\\) is an unbiased estimator of \\(\\boldsymbol{\\beta}\\). 8.1.3 Expectation of \\(\\hat{\\mathbf{y}}\\) \\[\\begin{align*} \\mathbb{E}[\\hat{\\mathbf{y}}] &amp;= \\mathbb{E}[\\mathbf{X}\\hat{\\boldsymbol{\\beta}}] \\\\ &amp;= \\mathbf{X}\\mathbb{E}[\\hat{\\boldsymbol{\\beta}}] \\\\ &amp;= \\mathbf{X}\\boldsymbol{\\beta}\\\\ &amp;= \\mathbb{E}[\\mathbf{y}] \\\\ \\end{align*}\\] So \\(\\mathbf{y}\\) and \\(\\hat{\\mathbf{y}}\\) have the same mean. 8.1.4 Expectation of \\(\\hat{\\mathbf{e}}\\) \\[\\begin{align*} \\mathbb{E}[\\hat{\\mathbf{e}}] &amp;= \\mathbb{E}[\\mathbf{y}- \\hat{\\mathbf{y}}] \\\\ &amp;= \\mathbb{E}[\\mathbf{y}] - \\mathbb{E}[\\hat{\\mathbf{y}}] \\\\ &amp;= \\mathbf{0} \\end{align*}\\] So \\(\\mathbf{y}\\) and \\(\\hat{\\mathbf{y}}\\) have the same mean. Without any further assumptions, we can get more additional results. Next, we move to assumptions on the variance of the errors. 8.2 Variance Assumptions While the assumptions on the mean are assumptions about the first moment of the errors, now we will make assumptions about the second moment of the errors. In particular, we will assume that all the errors have the same (finite) variance and are uncorrelated. That is: \\[ \\mathbb{V}[e_i] = \\sigma^2 &lt; \\infty \\quad \\forall i \\in \\{1,\\ldots,n\\}, \\quad \\text{and} \\quad \\mathbb{C}[e_i, e_j] = 0 \\quad \\forall i \\neq j\\] We can express this assumption in vector form as: \\[ \\mathbb{V}[\\mathbf{e}] = \\sigma^2 \\mathbf{I}\\quad \\sigma^2 &lt; \\infty \\] As we did before, we can try to compute the variance of all the random quantities we have. 8.2.1 Variance of \\(\\mathbf{y}\\) \\[\\begin{align*} \\mathbb{V}[\\mathbf{y}] &amp;= \\mathbb{V}[\\mathbf{X}+ \\mathbf{e}] \\\\ &amp;= \\mathbb{V}[\\mathbf{e}] \\\\ &amp;= \\sigma^2 \\mathbf{I} \\end{align*}\\] So, the observations \\(\\mathbf{y}\\) and the errors \\(\\mathbf{e}\\) have the same variance. This makes sense since they only differ by a non-random element \\(\\mathbf{X}\\boldsymbol{\\beta}\\). 8.2.2 Variance of \\(\\hat{\\boldsymbol{\\beta}}\\) \\[\\begin{align*} \\mathbb{V}[\\hat{\\boldsymbol{\\beta}}] &amp;= \\mathbb{V}[(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbf{y}] \\\\ &amp;= (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\mathbb{V}[\\mathbf{y}] \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1} \\\\ &amp;= (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; (\\sigma^2 \\mathbf{I}) \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1} \\\\ &amp;= \\sigma^2 (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1} \\\\ &amp;= \\sigma^2 (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\\\ \\end{align*}\\] 8.2.3 Variance of \\(\\hat{\\mathbf{y}}\\) \\[\\begin{align*} \\mathbb{V}[\\hat{\\mathbf{y}}] &amp;= \\mathbb{V}[\\mathbf{H}\\mathbf{y}] \\\\ &amp;= \\mathbf{H}\\mathbb{V}[\\mathbf{y}] \\mathbf{H}&#39; \\\\ &amp;= \\mathbf{H}(\\sigma^2 \\mathbf{I}) \\mathbf{H}&amp; \\text{since $\\mathbf{H}$ is symmetric} \\\\ &amp;= \\sigma^2 \\mathbf{H}\\mathbf{H}\\\\ &amp;= \\sigma^2 \\mathbf{H}&amp; \\text{since $\\mathbf{H}$ is idempotent} \\\\ \\end{align*}\\] So, while \\(\\mathbf{y}\\) and \\(\\hat{\\mathbf{y}}\\) have the same mean, they do not have the same variance. We will see that the variance of \\(\\hat{\\mathbf{y}}\\) is “smaller” than the variance of \\(\\mathbf{y}\\). I say “smaller” since for matrices it is not clear what does it mean to be smaller or bigger. Not for now, that the diagonal elements of these matrices satisfy the following: \\[[\\mathbb{V}[\\mathbf{y}]]_{ii} = \\sigma^2 \\leq \\sigma^2 h_{ii} = [\\mathbb{V}[\\hat{\\mathbf{y}}]]_{ii}\\] since the leverages \\(h_{ii}\\) satisfy \\(0 \\leq h_{ii} \\leq 1\\). 8.2.4 Variance of \\(\\hat{\\mathbf{e}}\\) \\[\\begin{align*} \\mathbb{V}[\\hat{\\mathbf{e}}] &amp;= \\mathbb{V}[(\\mathbf{I}- \\mathbf{H}) \\mathbf{y}] \\\\ &amp;= (\\mathbf{I}- \\mathbf{H}) \\mathbb{V}[\\mathbf{y}] (\\mathbf{I}- \\mathbf{H})&#39; \\\\ &amp;= (\\mathbf{I}- \\mathbf{H}) (\\sigma^2 \\mathbf{I}) (\\mathbf{I}- \\mathbf{H}) &amp; \\text{since $(\\mathbf{I}- \\mathbf{H})$ is symmetric} \\\\ &amp;= \\sigma^2 (\\mathbf{I}- \\mathbf{H})(\\mathbf{I}- \\mathbf{H}) \\\\ &amp;= \\sigma^2 (\\mathbf{I}- \\mathbf{H}) &amp; \\text{since $(\\mathbf{I}- \\mathbf{H})$ is idempotent} \\\\ \\end{align*}\\] Then we can see that \\(\\mathbf{y}\\), \\(\\hat{\\mathbf{y}}\\) and \\(\\hat{\\mathbf{e}}\\) have variances that are idempotent matrices multiplied by the scalar \\(\\sigma^2\\). 8.3 Cross-Covariances When we introduce the assumption of the variance, we can check the cross-covariances of our estimators. This will help us later in the course. We can check several cross-covariances but for now I will only check 2. 8.3.1 Cross-covaraince of \\(\\hat{y}\\) and \\(\\hat{e}\\) \\[\\begin{align*} \\mathbb{C}[\\hat{\\mathbf{y}}, \\hat{\\mathbf{e}}] &amp;=\\mathbb{C}[(\\mathbf{I}- \\mathbf{H}) \\mathbf{y}, \\mathbf{H}\\mathbf{y}] \\\\ &amp;=(\\mathbf{I}- \\mathbf{H})\\mathbb{C}[\\mathbf{y},\\mathbf{y}] \\mathbf{H}&#39; \\\\ &amp;=(\\mathbf{I}- \\mathbf{H})\\mathbb{V}[\\mathbf{y}] \\mathbf{H}&amp; \\text{since $\\mathbf{H}$ is symmetric and $\\mathbb{C}[\\mathbf{y},\\mathbf{y}] = \\mathbb{V}[\\mathbf{y}]$} \\\\ &amp;=(\\mathbf{I}- \\mathbf{H})(\\sigma^2 \\mathbf{I}) \\mathbf{H}&amp; \\text{since $\\mathbb{V}[\\mathbf{y}] = \\sigma^2 \\mathbf{I}$} \\\\ &amp;=\\sigma^2 (\\mathbf{I}- \\mathbf{H}) \\mathbf{H}\\\\ &amp;=\\sigma^2 (\\mathbf{H}- \\mathbf{H}\\mathbf{H}) \\\\ &amp;=\\sigma^2 (\\mathbf{H}- \\mathbf{H}) &amp; \\text{since $\\mathbf{H}$ is idempotent} \\\\ &amp;=\\mathbf{0}\\in \\mathbb{R}^{n \\times n} \\\\ \\end{align*}\\] Then, the residuals \\(\\hat{\\mathbf{e}}\\) and the estimates of the observations \\(\\hat{\\mathbf{y}}\\) are uncorrelated. We had a similar result before, that however is not the same (we couldn’t have even talked before expectation and covariance since we didn’t have random variables). That result was: \\[ \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{y}} = 0 \\in \\mathbb{R}\\] Notice that, the dimension of the zero’s. 8.3.2 Cross-covaraince of \\(\\hat{y}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) \\[\\begin{align*} \\mathbb{C}[\\hat{\\mathbf{e}}, \\hat{\\boldsymbol{\\beta}}] &amp;= \\mathbb{C}[(\\mathbf{I}- \\mathbf{H}) \\mathbf{y}, (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{y}] \\\\ &amp;= (\\mathbf{I}- \\mathbf{H}) \\mathbb{C}[\\mathbf{y}, \\mathbf{y}] \\mathbf{X}(\\mathbf{X}&#39; \\mathbf{X})^{-1} \\\\ &amp;= (\\mathbf{I}- \\mathbf{H}) \\mathbb{V}[\\mathbf{y}] \\mathbf{X}(\\mathbf{X}&#39; \\mathbf{X})^{-1} \\\\ &amp;= (\\mathbf{I}- \\mathbf{H}) (\\sigma^2 \\mathbf{I}) \\mathbf{X}(\\mathbf{X}&#39; \\mathbf{X})^{-1} \\\\ &amp;= \\sigma^2 (\\mathbf{I}- \\mathbf{H}) \\mathbf{X}(\\mathbf{X}&#39; \\mathbf{X})^{-1} \\\\ &amp;= \\sigma^2 (\\mathbf{I}- \\mathbf{X}(\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39;) \\mathbf{X}(\\mathbf{X}&#39; \\mathbf{X})^{-1} \\\\ &amp;= \\sigma^2 (\\mathbf{X}- \\mathbf{X}(\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39;\\mathbf{X})(\\mathbf{X}&#39; \\mathbf{X})^{-1} \\\\ &amp;= \\sigma^2 (\\mathbf{X}- \\mathbf{X})(\\mathbf{X}&#39; \\mathbf{X})^{-1} \\\\ &amp;= \\mathbf{0}\\in \\mathbb{R}^{n \\times p} \\\\ \\end{align*}\\] So the residuals and the estimate of \\(\\boldsymbol{\\beta}\\) are uncorrelated. We will use this result in the next chapter. 8.4 Gauss-Markov Theorem This theorem justifies the use of the Ordinary Least Squares (OLS) estimator, since it is the “best” estimator in a way. 8.4.1 Assumptions The theorem assumes that: The errors have the same finite variance. The errors are uncorrelated. This assumption is equivalent to our assumption of: \\[ \\mathbb{V}[\\mathbf{e}] = \\sigma^2 \\mathbf{I}\\quad \\sigma^2 &lt; \\infty \\] 8.4.2 Statement In the linear regression model, the OLS estimator \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{y}\\) has the smallest variance among all unbiased linear estimators. That is, if \\(\\tilde{\\boldsymbol{\\beta}}\\) is a linear estimator, then: \\[ \\mathbb{V}[\\tilde{\\boldsymbol{\\beta}}] - \\mathbb{V}[\\hat{\\boldsymbol{\\beta}}] \\] is semi-positive definite. Because of this, the OLS estimator is called the Best Linear Unbiased Estimator (BLUE), where best means smaller variance. 8.4.3 Proof Let \\(\\tilde{\\boldsymbol{\\beta}}\\) be an unbiased linear estimator of \\(\\boldsymbol{\\beta}\\). Then, we can write \\(\\tilde{\\boldsymbol{\\beta}}\\) as follows: \\[\\tilde{\\boldsymbol{\\beta}} = \\mathbf{A}\\mathbf{y}\\] where \\(\\mathbf{A}\\) is a constant matrix of the appropriate dimensions. Then we can write \\(\\tilde{\\boldsymbol{\\beta}}\\) as follows: \\[\\begin{align*} \\tilde{\\boldsymbol{\\beta}} &amp;=(\\mathbf{A}- (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39; + (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39;) \\mathbf{y}\\\\ &amp;=(\\mathbf{A}- (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39;)\\mathbf{y}+ (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{y}\\\\ &amp;=(\\mathbf{D}) + \\hat{\\boldsymbol{\\beta}} &amp; \\text{with $\\mathbf{D}= \\mathbf{A}- (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39;$} \\\\ \\end{align*}\\] Now, since \\(\\tilde{\\boldsymbol{\\beta}}\\) is unbiased, we have that: \\[\\mathbb{E}[\\tilde{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p \\] And \\[\\begin{align*} \\mathbb{E}[\\tilde{\\boldsymbol{\\beta}}] &amp;= \\mathbb{E}[\\mathbf{D}\\mathbf{y}+ \\hat{\\boldsymbol{\\beta}}] \\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p \\\\ &amp;= \\mathbf{D}\\mathbb{E}[\\mathbf{y}] + \\mathbb{E}[\\hat{\\boldsymbol{\\beta}}] \\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p \\\\ &amp;= \\mathbf{D}\\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p &amp; \\text{since $\\mathbb{E}[\\mathbf{y}] = \\mathbf{X}\\boldsymbol{\\beta}$ and $\\mathbb{E}[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}$} \\\\ \\end{align*}\\] Then \\[\\begin{align*} \\mathbb{E}[\\tilde{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p \\\\ &amp;\\implies \\mathbf{D}\\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\beta}= \\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p \\\\ &amp;\\implies \\mathbf{D}\\mathbf{X}\\boldsymbol{\\beta}= \\mathbf{0}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p \\\\ &amp;\\implies \\mathbf{D}\\mathbf{X}= \\mathbf{0}\\\\ \\end{align*}\\] Now, we can analyze the variance of \\(\\tilde{\\boldsymbol{\\beta}}\\). \\[\\begin{align*} \\mathbb{V}[\\tilde{\\boldsymbol{\\beta}}] &amp;= \\mathbb{V}[\\mathbf{D}\\mathbf{y}+ \\hat{\\boldsymbol{\\beta}}] \\\\ &amp;= \\mathbb{V}[\\mathbf{D}\\mathbf{y}+ (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{y}] &amp;&amp; \\text{since $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{y}$} \\\\ &amp;= \\mathbb{V}[(\\mathbf{D}+ (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39;) \\mathbf{y}] \\\\ &amp;= (\\mathbf{D}+ (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39;) \\mathbb{V}[\\mathbf{y}] (\\mathbf{D}+ (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39;)&#39; \\\\ &amp;= (\\mathbf{D}+ (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39;) \\mathbb{V}[\\mathbf{y}] (\\mathbf{D}&#39; + \\mathbf{X}(\\mathbf{X}&#39; \\mathbf{X})^{-1}) \\\\ &amp;= (\\mathbf{D}+ (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39;) (\\sigma^2 \\mathbf{I}) (\\mathbf{D}&#39; + \\mathbf{X}(\\mathbf{X}&#39; \\mathbf{X})^{-1}) &amp;&amp; \\text{since $\\mathbb{V}[\\mathbf{y}] = \\sigma^2 \\mathbf{I}$} \\\\ &amp;= \\sigma^2 (\\mathbf{D}+ (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39;) (\\mathbf{D}&#39; + \\mathbf{X}(\\mathbf{X}&#39; \\mathbf{X})^{-1}) \\\\ &amp;= \\sigma^2 (\\mathbf{D}\\mathbf{D}&#39; + (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{D}&#39; \\\\ &amp;\\quad + \\mathbf{D}\\mathbf{X}(\\mathbf{X}&#39; \\mathbf{X})^{-1} + (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{X}(\\mathbf{X}&#39; \\mathbf{X})^{-1}) \\\\ &amp;= \\sigma^2 (\\mathbf{D}\\mathbf{D}&#39; + (\\mathbf{X}&#39; \\mathbf{X})^{-1}) &amp;&amp; \\text{since $\\mathbf{D}\\mathbf{X}= \\mathbf{0}$} \\\\ &amp;= \\sigma^2 (\\mathbf{X}&#39; \\mathbf{X})^{-1} + \\sigma^2 (\\mathbf{D}\\mathbf{D}&#39;) \\\\ &amp;= \\mathbb{V}[\\hat{\\boldsymbol{\\beta}}] + \\sigma^2 (\\mathbf{D}\\mathbf{D}&#39;) &amp;&amp; \\text{since $\\mathbb{V}[\\hat{\\boldsymbol{\\beta}}] = \\sigma^2 (\\mathbf{X}&#39; \\mathbf{X})^{-1}$} \\\\ \\end{align*}\\] Since \\(\\mathbf{D}\\mathbf{D}&#39;\\) is semi-positive definite, we have that: \\[ \\mathbb{V}[\\tilde{\\boldsymbol{\\beta}}] - \\mathbb{V}[\\hat{\\boldsymbol{\\beta}}] = \\sigma^2 (\\mathbf{D}\\mathbf{D}&#39;) \\] is semi-positive definite. This concludes the end of the proof. So, as it turns out, the OLS estimator that came out of minimizing the least squares, and made no assumptions about the expectation and variances is a very good estimator, since it is: Unbiased Among the unbiased and linear estimators, it has the smallest variance. Another common measure of performance is the Mean Square Error (MSE) of an estimator with respect to a parameter. Using the Gauss-Markov Theorem we can conclude that, among the unbiased and linear estimators, the OLS estimator has the smallest MSE. Since, both estimators are unbiased, we have that: \\[ \\mathbb{M}(\\hat{\\mathbf{\\boldsymbol{\\beta}}}) = \\text{tr}(\\text{Var}(\\hat{\\mathbf{\\boldsymbol{\\beta}}})) + \\|\\text{Bias}(\\hat{\\mathbf{\\boldsymbol{\\beta}}})\\|^2 = \\text{tr}(\\text{Var}(\\hat{\\mathbf{\\boldsymbol{\\beta}}})) \\] \\[ \\mathbb{M}(\\tilde{\\mathbf{\\boldsymbol{\\beta}}}) = \\text{tr}(\\text{Var}(\\tilde{\\mathbf{\\boldsymbol{\\beta}}})) + \\|\\text{Bias}(\\tilde{\\mathbf{\\boldsymbol{\\beta}}})\\|^2 = \\text{tr}(\\text{Var}(\\tilde{\\mathbf{\\boldsymbol{\\beta}}})) \\] Then, by the Gauss-Markov theorem: \\[\\begin{align*} \\mathbb{M}[\\tilde{\\boldsymbol{\\beta}}] - \\mathbb{M}[\\hat{\\boldsymbol{\\beta}}] &amp;= \\text{tr}(\\text{Var}(\\tilde{\\mathbf{\\boldsymbol{\\beta}}})) - \\text{tr}(\\text{Var}(\\hat{\\mathbf{\\boldsymbol{\\beta}}})) \\\\ &amp;= \\text{tr}(\\text{Var}(\\tilde{\\mathbf{\\boldsymbol{\\beta}}})) - \\text{Var}(\\hat{\\mathbf{\\boldsymbol{\\beta}}})) &amp;&amp; \\text{since the trace operator is linear.} \\\\ &amp;\\geq 0 \\\\ \\end{align*}\\] Where we use the fact that \\(\\text{Var}(\\tilde{\\mathbf{\\boldsymbol{\\beta}}})) - \\text{Var}(\\hat{\\mathbf{\\boldsymbol{\\beta}}})\\) is positive semi-definite by the Gauss-Markov theorem and the trace of a positive semi-definite matrix is non-negative. 8.5 Estimate of \\(\\sigma^2\\) Since we have introduced a new parameter \\(\\sigma^2\\), we might be interested in estimate it. We will propose an estimate of \\(\\sigma^2\\) of the following shape: \\[\\hat{\\sigma}^2 = a \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}\\] where we will choose the scalar \\(a\\) later. In particular, we want to take the expectation of this estimate. To do so, we will take a roundabout way. First we will show that: \\[(\\mathbf{y}- \\mathbb{E}[\\mathbf{y}])&#39;(\\mathbf{y}- \\mathbb{E}[\\mathbf{y}]) = \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}} + (\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])&#39;(\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])\\] and we will use this fact to compute the expectation of \\(\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}\\). \\[\\begin{align*} (\\mathbf{y}- \\mathbb{E}[\\mathbf{y}])&#39;&amp; (\\mathbf{y}- \\mathbb{E}[\\mathbf{y}]) \\\\ &amp;= (\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) &amp;&amp; \\text{since $\\mathbb{E}[\\mathbf{y}] = \\mathbf{X}\\boldsymbol{\\beta}$} \\\\ &amp;= (\\mathbf{y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}} + \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}} + \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\ &amp;= (\\mathbf{y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}})&#39;(\\mathbf{y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) + 2(\\mathbf{y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}})&#39;(\\mathbf{X}\\hat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\ &amp;\\quad + (\\mathbf{X}\\hat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{X}\\hat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\ &amp;= (\\mathbf{y}- \\hat{\\mathbf{y}})&#39;(\\mathbf{y}- \\hat{\\mathbf{y}}) + 2(\\mathbf{y}- \\hat{\\mathbf{y}})&#39;(\\mathbf{X}\\hat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\ &amp;\\quad + (\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])&#39;(\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}]) \\\\ &amp;= \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}} + 2\\hat{\\mathbf{e}}&#39;(\\mathbf{X}\\hat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta}) + (\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])&#39;(\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}]) \\\\ &amp;= \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}} + 2\\hat{\\mathbf{e}}&#39;\\mathbf{X}(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}) + (\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])&#39;(\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}]) \\\\ &amp;= \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}} + (\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])&#39;(\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}]) &amp;&amp; \\text{since $\\hat{\\mathbf{e}}&#39;\\mathbf{X}=\\mathbf{0}$} \\\\ \\end{align*}\\] Since \\[\\mathbb{E}[(\\mathbf{y}- \\mathbb{E}[\\mathbf{y}])&#39;(\\mathbf{y}- \\mathbb{E}[\\mathbf{y}])] = \\text{tr}(\\mathbb{V}[\\mathbf{y}]) = \\text{tr}(\\sigma^2 \\mathbf{I}) = \\sigma^2 \\text{tr}(\\mathbf{I}) = \\sigma^2 n\\] \\[\\mathbb{E}[(\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])&#39;(\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])] = \\text{tr}(\\mathbb{V}[\\hat{\\mathbf{y}}]) = \\text{tr}(\\sigma^2 \\mathbf{H}) = \\sigma^2 \\text{tr}(\\mathbf{H}) = \\sigma^2 p\\] where we use the fact that the trace of an idempotent matrix is equal to the rank of the matrix. Then we get that: \\[\\begin{align*} (\\mathbf{y}- \\mathbb{E}[\\mathbf{y}])&#39;&amp; (\\mathbf{y}- \\mathbb{E}[\\mathbf{y}]) = \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}} + (\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])&#39;(\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}]) \\\\ &amp;\\implies \\mathbb{E}[(\\mathbf{y}- \\mathbb{E}[\\mathbf{y}])&#39;(\\mathbf{y}- \\mathbb{E}[\\mathbf{y}])] = \\mathbb{E}[\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}] + \\mathbb{E}[(\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])&#39;(\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])] \\\\ &amp;\\implies \\text{tr}(\\mathbb{V}[\\mathbf{y}]) = \\mathbb{E}[\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}] + \\text{tr}(\\mathbb{V}[\\hat{\\mathbf{y}}]) \\\\ &amp;\\implies \\sigma^2 n = \\mathbb{E}[\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}] + \\sigma^2 p \\\\ &amp;\\implies \\mathbb{E}[\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}] = \\sigma^2 n - \\sigma^2 p \\\\ &amp;\\implies \\mathbb{E}[\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}] = \\sigma^2 (n - p) \\\\ \\end{align*}\\] Then, if we set \\(a=(\\frac{1}{n-p})\\) we have that our proposed estimator \\(\\hat{\\sigma}^2\\) is unbiased. Since: \\[\\begin{align*} \\mathbb{E}[a \\hat{\\sigma}^2] &amp;= \\mathbb{E}[a \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}] \\\\ &amp;= a \\mathbb{E}[\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}] \\\\ &amp;= a \\sigma^2 (n-p) \\\\ &amp;= \\frac{1}{n-p} \\sigma^2 (n-p) \\\\ &amp;= \\sigma^2 \\\\ \\end{align*}\\] We can choose or obtain other values for \\(a\\), however the estimator will not be unbiased. "],["normality-assumption.html", "9 Normality Assumption 9.1 Introduction 9.2 Maximum Likelihood Estimation 9.3 Distribution of Estimates 9.4 Interval Estimation 9.5 Hypothesis Testing", " 9 Normality Assumption 9.1 Introduction To the mean and covariance assumptions, we can add the normality assumption. This is a very strong and powerful assumption, that will enable us to obtain the distribution of our data and several of our estimates. We will assume: \\[\\mathbf{e}\\sim N(\\mathbf{0}, \\sigma^2 \\mathbf{I})\\] Also, note that the Normal distribution is completely characterized by its mean and variance, so if the distribution of our estimates is normal, we will already have the information to completely characterize their distribution since we have computed the mean and variance of the estimates in the previous chapter. Another thing, that assuming normality allows is to obtain the likelihood of our data, and in this way obtain Maximum Likelihood Estimates (MLE) of the parameters we have introduced \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\). 9.2 Maximum Likelihood Estimation In order to perform the maximum likelihood estimates of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\), we need the distribution of \\(\\mathbf{y}\\). This is very easy to obtain, since: \\[\\mathbf{y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}\\] is a linear combination of \\(\\mathbf{e}\\) (in fact is just a translation of \\(\\mathbf{e}\\)), and therefore it is normally distributed. Since we have already computed its mean and variance in the previous chapter, using those computations, we can conclude that: \\[\\mathbf{y}\\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I})\\] This means that the likelihood of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is given by: \\[ \\mathcal{L}(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{y}) = N(\\mathbf{y}| \\mathbf{X}\\mathbf{y}, \\sigma^2 \\mathbf{I}) \\] Then this means that: \\[\\begin{align*} \\mathcal{L}(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{y}) &amp;= (2 \\pi)^{-\\frac{n}{2}} |\\sigma^2 \\mathbf{I}|^{-\\frac{1}{2}} \\exp\\left\\{ -\\frac{1}{2}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\sigma^2 \\mathbf{I})^{-1}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\right\\} \\\\ &amp;= (2 \\pi)^{-\\frac{n}{2}} (\\sigma^2)^{-\\frac{n}{2}} |\\mathbf{I}| \\exp\\left\\{ -\\frac{1}{2}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;\\frac{\\mathbf{I}}{\\sigma^2}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\right\\} \\\\ &amp;= (2 \\pi \\sigma^2)^{-\\frac{n}{2}} \\exp\\left\\{ -\\frac{1}{2 \\sigma^2}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\right\\} \\\\ &amp;= (2 \\pi \\sigma^2)^{-\\frac{n}{2}} \\exp\\left\\{ -\\frac{1}{2 \\sigma^2}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\right\\} \\\\ &amp;= (2 \\pi \\sigma^2)^{-\\frac{n}{2}} \\exp\\left\\{ -\\frac{1}{2 \\sigma^2}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\right\\} \\\\ \\end{align*}\\] Now recalling from: \\[(\\mathbf{y}- \\mathbb{E}[\\mathbf{y}])&#39;(\\mathbf{y}- \\mathbb{E}[\\mathbf{y}]) = \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}} + (\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])&#39;(\\hat{\\mathbf{y}} - \\mathbb{E}[\\hat{\\mathbf{y}}])\\] that is: \\[(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) = \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}} + (\\mathbf{X}\\hat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{X}\\hat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta}) = \\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}} + (\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})&#39;\\mathbf{X}\\mathbf{X}( \\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})\\] then, the likelihood can be written as: \\[\\begin{align*} \\mathcal{L}(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{y}) &amp;= (2 \\pi \\sigma^2)^{-\\frac{n}{2}} \\exp\\left\\{ -\\frac{1}{2 \\sigma^2}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\right\\} \\\\ &amp;= (2 \\pi \\sigma^2)^{-\\frac{n}{2}} \\exp\\left\\{ -\\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{2 \\sigma^2} -\\frac{(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})&#39;\\mathbf{X}\\mathbf{X}( \\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})}{2 \\sigma^2} \\right\\} \\\\ \\end{align*}\\] This is a useful way to write the likelihood, since it is easy to optimize with respect to \\(\\boldsymbol{\\beta}\\). Since, independently of the value of \\(\\sigma^2\\), the value of \\(\\boldsymbol{\\beta}\\) that maximizes the likelihood is the OLS estimator \\(\\hat{\\boldsymbol{\\beta}}\\), since it makes zero the following term: \\[ -\\frac{(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})&#39;\\mathbf{X}\\mathbf{X}( \\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})}{2 \\sigma^2} \\] In this way, we only need to maximize the likelihood with respect to \\(\\sigma^2\\), as the following marginal likelihood: \\[ \\mathcal{L}(\\sigma^2 | \\mathbf{y}, \\boldsymbol{\\beta}= \\hat{\\boldsymbol{\\beta}} ) = (2 \\pi \\sigma^2)^{-\\frac{n}{2}} \\exp\\left\\{ -\\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{2 \\sigma^2} \\right\\}\\] Instead of maximizing the marginal likelihood directly, we will maximize the marginal log-likelihood: \\[ \\ell(\\sigma^2 | \\mathbf{y}, \\boldsymbol{\\beta}= \\hat{\\boldsymbol{\\beta}}) = -\\frac{n}{2} \\log(2 \\pi) -\\frac{n}{2} \\log(\\sigma^2) -\\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{2 \\sigma^2}\\] We can do this maximization, by taking the derivative: \\[\\begin{align*} \\frac{d \\ell}{d \\sigma^2} &amp;= -\\frac{n}{2 \\sigma^2} + \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{2 (\\sigma^2)^2} \\end{align*}\\] Then \\[\\begin{align*} \\frac{d \\ell}{d \\sigma^2} =0 &amp;\\implies -\\frac{n}{2 \\sigma^2} + \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{2 (\\sigma^2)^2} = 0 \\\\ &amp;\\implies -n + \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2} = 0 \\\\ &amp;\\implies \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2} = n \\\\ &amp;\\implies \\sigma^2 = \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{n} \\\\ \\end{align*}\\] So we have, that the Maximum Likelihood Estimate of \\(\\sigma^2\\) is given by: \\[\\tilde{\\sigma}^2 = \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{n}\\] And, taking the second derivative to confirm it is a maximum we have that: \\[\\begin{align*} \\frac{d^2 \\ell}{d (\\sigma^2)^2} \\bigg|_{\\sigma^2 = \\tilde{\\sigma}^2} &amp;= \\frac{n}{2 (\\tilde{\\sigma}^2)^2} - \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{(\\tilde{\\sigma}^2)^3} \\\\ &amp;= \\frac{n}{2 (\\tilde{\\sigma}^2)^2} - \\frac{n}{(\\tilde{\\sigma}^2)^2} \\\\ &amp;= - \\frac{n}{(2 \\tilde{\\sigma}^2)^2} \\\\ &amp;\\leq 0 \\end{align*}\\] So \\(\\tilde{\\sigma}^2\\) is indeed maximizing the marginal likelihood. Note that, unlike with the \\(\\boldsymbol{\\beta}\\) parameter, our OLS estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\) is different to the MLE estimator \\(\\tilde{\\sigma}^2\\). In particular \\(\\tilde{\\sigma}^2\\) is biased. This doesn’t mean that one estimate is better than the other, they just have different properties. We will work more with \\(\\hat{\\sigma}^2\\) than \\(\\tilde{\\sigma}^2\\), since it is more useful to build certain statistics. 9.3 Distribution of Estimates In the same way that in the last chapters we computed the mean and the variance of our estimates, we can obtain the distribution of the estimates: 9.3.1 Distribution of \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\hat{\\mathbf{y}}\\) and \\(\\hat{\\mathbf{e}}\\) Most of our estimates are linear combinations of \\(\\mathbf{y}\\), so they are normally distributed and we can use the mean and variances computed in the last chapter to fully characterize their distributions. This is the case for: \\[ \\hat{\\boldsymbol{\\beta}}, \\quad \\hat{\\mathbf{y}}, \\quad \\hat{\\mathbf{e}} \\] Their distributions are: \\[ \\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2 (\\mathbf{X}&#39;\\mathbf{X})^{-1}) \\] \\[ \\hat{\\mathbf{y}} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{H}) \\] \\[ \\hat{\\mathbf{e}} \\sim N(\\mathbf{0}, \\sigma^2 (\\mathbf{I}-\\mathbf{H})) \\] This is not the case for the estimate \\(\\hat{\\sigma}^2\\), since it is not a linear transformation of \\(\\mathbf{y}\\). 9.3.2 Distribution of \\(\\hat{\\sigma}^2\\) Obtaining the distribution of \\(\\hat{\\sigma}^2\\) is not as straight forward. We will use 3 steps: Express \\(\\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2}\\) as quadratic form of standard normal with an idempotent matrix. Show that the quadratic form of standard normal with an idempotent matrix is distributed as a chi squared. Relate the distribution of \\(\\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2}\\) to the distribution of \\(\\hat{\\sigma}^2\\). 9.3.2.1 Distribution of \\(\\hat{\\sigma}^2\\) Step 1 First, note that: \\[\\begin{align*} (\\mathbf{I}- \\mathbf{H}) \\mathbf{y} &amp;= (\\mathbf{I}- \\mathbf{H}) (\\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}) &amp;&amp; \\text{since $\\mathbf{y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}$} \\\\ &amp;= \\mathbf{I}\\mathbf{X}\\boldsymbol{\\beta}- \\mathbf{H}\\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{I}\\mathbf{e}- \\mathbf{H}\\mathbf{e}\\\\ &amp;= \\mathbf{X}\\boldsymbol{\\beta}- \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}- \\mathbf{H}\\mathbf{e}&amp;&amp; \\text{since $\\mathbf{H}\\mathbf{X}= \\mathbf{X}$} \\\\ &amp;= \\mathbf{e}- \\mathbf{H}\\mathbf{e}\\\\ &amp;= (\\mathbf{I}- \\mathbf{H}) \\mathbf{e}\\\\ \\end{align*}\\] Then: \\[\\begin{align*} \\mathbf{y}&#39; (\\mathbf{I}- \\mathbf{H}) \\mathbf{y} &amp;= \\mathbf{y}&#39; (\\mathbf{I}- \\mathbf{H}) (\\mathbf{I}- \\mathbf{H}) \\mathbf{y}&amp;&amp; \\text{since $(\\mathbf{I}- \\mathbf{H})$ is idempotent} \\\\ &amp;= \\mathbf{e}&#39; (\\mathbf{I}- \\mathbf{H}) (\\mathbf{I}- \\mathbf{H}) \\mathbf{e}&amp;&amp; \\text{since $(\\mathbf{I}- \\mathbf{H}) \\mathbf{y}= (\\mathbf{I}- \\mathbf{H}) \\mathbf{e}$} \\\\ &amp;= \\mathbf{e}&#39; (\\mathbf{I}- \\mathbf{H}) \\mathbf{e}&amp;&amp; \\text{since $(\\mathbf{I}- \\mathbf{H})$ is idempotent} \\\\ \\end{align*}\\] Then, \\[\\begin{align*} \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2} &amp;= \\frac{\\mathbf{y}&#39; (\\mathbf{I}- \\mathbf{H}) \\mathbf{y}}{\\sigma^2} &amp;&amp; \\text{since $\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}} = \\mathbf{y}&#39; (\\mathbf{I}- \\mathbf{H}) \\mathbf{y}$} \\\\ &amp;= \\frac{\\mathbf{e}&#39; (\\mathbf{I}- \\mathbf{H}) \\mathbf{e}}{\\sigma^2} &amp;&amp; \\text{since $\\mathbf{e}&#39; (\\mathbf{I}- \\mathbf{H}) \\mathbf{e}= \\mathbf{y}&#39; (\\mathbf{I}- \\mathbf{H}) \\mathbf{y}$} \\\\ &amp;= \\frac{\\mathbf{e}}{\\sqrt{\\sigma^2}}&#39; (\\mathbf{I}- \\mathbf{H}) \\frac{\\mathbf{e}}{\\sqrt{\\sigma^2}} \\\\ \\end{align*}\\] Finally, note that \\(\\frac{\\mathbf{e}}{\\sqrt{\\sigma^2}}\\) is a linear function of \\(\\mathbf{e}\\) that is normal, therefore it is normal also, with mean: \\[ \\mathbb{E}\\left[\\frac{\\mathbf{e}}{\\sqrt{\\sigma^2}}\\right] = \\frac{1}{\\sqrt{\\sigma^2}} \\mathbb{E}[\\mathbf{e}] = \\frac{1}{\\sqrt{\\sigma^2}} \\mathbf{0}= \\mathbf{0}\\] \\[ \\mathbb{V}\\left[\\frac{\\mathbf{e}}{\\sqrt{\\sigma^2}}\\right] = \\left(\\frac{1}{\\sqrt{\\sigma^2}}\\right)^2 \\mathbb{V}[\\mathbf{e}] = \\frac{1}{\\sigma^2} \\sigma^2 \\mathbf{I}= \\mathbf{I}\\] So \\[ \\frac{\\mathbf{e}}{\\sqrt{\\sigma^2}} \\sim N(\\mathbf{0}, \\mathbf{I}) \\] is a multivariate standard normal. This concludes step 1. 9.3.2.2 Distribution of \\(\\hat{\\sigma}^2\\) Step 2 Let \\(\\mathbf{z}\\in \\mathbb{R}^{n}\\) a multivariate standard normal and \\(\\mathbf{M}\\in \\mathbb{R}^{n \\times n}\\) and idempotent matrix of rank \\(m\\). Then, we will show that: \\[ \\mathbf{z}&#39; \\mathbf{M}\\mathbf{z}\\sim \\chi^2_{m} \\] To show this result, we will use the spectral decomposition of \\(\\mathbf{M}\\), that is: \\[ \\mathbf{M}= \\mathbf{V}\\boldsymbol{\\Sigma}\\mathbf{V}&#39; \\] with \\(\\mathbf{V}\\) orthonormal and \\(\\boldsymbol{\\Sigma}\\) diagonal. Since \\(\\mathbf{V}= [\\mathbf{v}_1,\\ldots,\\mathbf{v}_n]\\) is orthonormal, then we have that: \\[ \\mathbf{v}_i&#39;\\mathbf{v}_j = 0 \\quad \\forall i \\neq j \\quad \\text{and} \\quad ||\\mathbf{v}_i||^2_2 = 1 \\quad \\forall i\\] and, since \\(\\mathbf{M}\\) is idempotent, then \\(\\boldsymbol{\\Sigma}\\) is diagonal with exactly \\(m\\) entries equal to \\(1\\) and the rest equal to \\(0\\). Without loss of generality, we can assume that the first \\(m\\) entries of the diagonal are equal to \\(1\\) and the next entries equal to \\(0\\). Then, first note that \\(\\mathbf{V}&#39; \\mathbf{z}\\) is a linear combination of a normal distribution. We will show, that \\(\\mathbf{V}&#39; \\mathbf{z}\\in \\mathbb{R}^{n}\\) is also standard normal. Note that: \\[ \\mathbb{E}[\\mathbf{V}&#39; \\mathbf{z}] = \\mathbf{V}&#39; \\mathbb{E}[\\mathbf{z}] = \\mathbf{V}&#39; \\mathbf{0}= \\mathbf{0}\\] \\[ \\mathbb{V}[\\mathbf{V}&#39; \\mathbf{z}] = \\mathbf{V}&#39; \\mathbb{V}[\\mathbf{z}] \\mathbf{V}= \\mathbf{V}&#39; \\mathbf{I}\\mathbf{V}= \\mathbf{V}&#39; \\mathbf{V}= \\mathbf{I}\\] Then \\(\\mathbf{V}&#39; \\mathbf{z}\\) is also standard normal. Let’s name \\(\\mathbf{w}= \\mathbf{V}&#39; \\mathbf{z}\\), then each of the components \\(w_1,\\ldots,w_n\\) of \\(\\mathbf{w}\\) are independent univariate standard normally distributed. Then \\[\\begin{align*} \\mathbf{z}&#39; \\mathbf{M}\\mathbf{z} &amp;= \\mathbf{z}&#39; (\\mathbf{V}\\boldsymbol{\\Sigma}\\mathbf{V}&#39;) \\mathbf{z}&amp;&amp; \\text{using the spectral decomposition of $\\mathbf{M}$} \\\\ &amp;= (\\mathbf{V}&#39; \\mathbf{z})&#39; \\boldsymbol{\\Sigma}(\\mathbf{V}&#39; \\mathbf{z}) \\\\ &amp;= \\mathbf{w}&#39; \\boldsymbol{\\Sigma}\\mathbf{w}&amp;&amp; \\text{since $\\mathbf{w}= \\mathbf{V}&#39; \\mathbf{z}$} \\\\ &amp;= \\sum_{i=1}^n [\\boldsymbol{\\Sigma}]_{ii} w_i^2 \\\\ &amp;= \\sum_{i=1}^{m} w_i^2 &amp;&amp; \\text{since only the first $m$ entries are equal to $1$} \\\\ &amp;\\sim \\chi^2_m &amp;&amp; \\text{by definition of the $\\chi^2$ distribution} \\\\ \\end{align*}\\] 9.3.2.3 Distribution of \\(\\hat{\\sigma}^2\\) Step 3 Using step 1 and 2, we can conclude that: \\[ \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2} \\sim \\chi^2_{n-p} \\] since the rank of the idempotent matrix \\((\\mathbf{I}- \\mathbf{H})\\) is \\(n-p\\). Since: \\[ \\hat{\\sigma}^2 = \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{n-p} = \\frac{\\sigma^2}{n-p}\\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2} \\sim \\frac{\\sigma^2}{n-p}\\chi^2_{n-p} \\] 9.3.3 Independence of \\(\\hat{\\mathbf{e}}\\) and \\(\\hat{\\mathbf{y}}\\) We have seen that \\(\\hat{\\mathbf{e}}\\) and \\(\\hat{\\mathbf{y}}\\) are uncorrelated, that is \\[ \\mathbb{C}[\\hat{\\mathbf{e}}, \\hat{\\mathbf{y}}] = \\mathbf{0}\\] however, this doesn’t necessarily mean they are independent. Independence is a much more stronger property. For example, if two random variables are independent then any function of this random variables will be independent. However, if two random variables are uncorrelated then not necessarily any function of this random variables will be uncorrelated. Now, after our assumption of normality, then \\(\\hat{\\mathbf{e}}\\) and \\(\\hat{\\mathbf{y}}\\) are independent, because uncorrelated random variables implies independence for normally distributed random variables, as is the case of \\(\\hat{\\mathbf{e}}\\) and \\(\\hat{\\mathbf{y}}\\). Then, any variable that is a function of \\(\\hat{\\mathbf{e}}\\) will be independent of any random variable that is a function of \\(\\hat{\\mathbf{y}}\\), even if these new random variables are not normal themselves. 9.4 Interval Estimation So far, we have obtained point estimates of different quantities of interest, \\(\\boldsymbol{\\beta}\\), \\(\\mathbf{e}\\) and \\(\\hat{\\sigma}^2\\), however the fact we have the distribution of estimates of this quantities will allow us to obtain interval estimators. 9.4.1 Confidence Intervals for Coefficients We know that the OLS estimate for the coefficients, has the following distribution: \\[ \\hat{\\boldsymbol{\\beta}} \\sim N(\\mathbf{0}, \\sigma^2 (\\mathbf{X}&#39;\\mathbf{X})^{-1}) \\] then, this means that each of the entries of \\(\\hat{\\boldsymbol{\\beta}} = (\\hat{\\beta}_0, \\hat{\\beta}_1,\\ldots,\\hat{\\beta}_{p-1})\\) is normally distributed, as follows: \\[ \\hat{\\beta}_i \\sim N(\\beta_i, \\sigma^2 [(\\mathbf{X}&#39;\\mathbf{X})^{-1}]_{ii}) \\] we will call \\[ \\sigma^2_{\\beta_i} = \\sigma^2 [(\\mathbf{X}&#39;\\mathbf{X})^{-1}]_{ii} \\] then, we can re-write the distribution as follows: \\[ \\hat{\\beta}_i \\sim N \\left(\\beta_i, \\sigma^2_{\\beta_i} \\right) \\] unfortunately this distribution depends on \\(\\sigma^2\\) which is unknown, so we can’t use it to build a confidence interval. So we transform the statistic first by removing the mean \\(\\beta_i\\) and by diving by the standard deviation: \\[ t^0_{\\beta_i}=\\frac{\\hat{\\beta}_i - \\beta_i}{\\sqrt{\\sigma^2_{\\beta_i}}} \\] Then, this new quantity is normally distributed, since it a linear transformation of a random variable that is normally distributed, and has mean as variance as follows: \\[ \\mathbb{E}[t^0_{\\beta_i}] = \\mathbb{E}\\left[\\frac{\\hat{\\beta}_i - \\beta_i}{\\sqrt{\\sigma^2_{\\beta_i}}}\\right]= \\frac{\\hat{\\mathbb{E}[\\beta}_i] - \\beta_i}{\\sqrt{\\sigma^2_{\\beta_i}}} = \\frac{\\beta_i - \\beta_i}{\\sqrt{\\sigma^2_{\\beta_i}}} = 0 \\] \\[ \\mathbb{V}[t^0_{\\beta_i}] = \\mathbb{V}\\left[\\frac{\\hat{\\beta}_i - \\beta_i}{\\sqrt{\\sigma^2_{\\beta_i}}}\\right]= \\left(\\frac{1}{\\sqrt{\\sigma^2_{\\beta_i}}}\\right)^2 \\mathbb{V}[\\hat{\\beta}_i - \\beta_i] = \\frac{1}{\\sigma^2_{\\beta_i}} \\mathbb{V}[\\hat{\\beta}_i] = \\frac{1}{\\sigma^2_{\\beta_i}} \\sigma^2_{\\beta_i} = 1 \\] then: \\[ t^0_{\\beta_i} \\sim N(0, 1) \\] that is \\(t^0_{\\beta_i}\\) is distributed like a standard normal. Now the distribution doesn’t depend on any unknown parameter (on top of \\(\\beta_i\\), that is the parameter of interest), but the quantity itself depends on \\(\\sigma^2\\) through \\(\\sigma^2_{\\beta_i}\\), so we can’t use it to build a confidence interval. We consider a new quantity \\[ t_{\\beta_i} = \\frac{\\hat{\\beta}_i - \\beta_i}{\\sqrt{\\hat{\\sigma}^2_{\\beta_i}}} \\] where \\(\\hat{\\sigma}^2_{\\beta_i} = \\hat{\\sigma}^2 [(\\mathbf{X}&#39;\\mathbf{X})^{-1}]_{ii}\\), so \\(t_{\\beta_i}\\) doesn’t depend on \\(\\sigma^2\\). Let’s compute the distribution of this quantity, first lets re-write the statistic as follows: \\[ t_{\\beta_i} = \\frac{\\hat{\\beta}_i - \\beta_i}{\\sqrt{\\hat{\\sigma}^2_{\\beta_i}}} = \\frac{\\sqrt{\\frac{1}{\\sigma^2}}}{\\sqrt{\\frac{1}{\\sigma^2}}}\\frac{\\hat{\\beta}_i - \\beta_i}{\\sqrt{\\hat{\\sigma}^2[(\\mathbf{X}&#39;\\mathbf{X})^{-1}]_{ii}}} = \\frac{\\frac{\\left(\\hat{\\beta}_i - \\beta_i\\right)}{\\sqrt{\\sigma^2[(\\mathbf{X}&#39;\\mathbf{X})^{-1}]_{ii}}}}{ \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sigma^2}}} = \\frac{\\frac{\\left(\\hat{\\beta}_i - \\beta_i\\right)}{\\sqrt{\\sigma^2_{\\beta_i}}}}{ \\sqrt{\\frac{(n-p)\\frac{\\hat{\\sigma}^2}{\\sigma^2}}{n-p}}} = \\frac{t^0_{\\beta_i}}{ \\sqrt{\\frac{\\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2}}{n-p}}}\\] Now, we know \\(t^0_{\\beta_i}\\) is standard normal distributed, and from the distribution of \\(\\hat{\\sigma}^2\\) we have that: \\[ \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2} \\sim \\chi^2_{n-p} \\] and from the independence of \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\mathbf{e}}\\) we have that any function of both variables is independent, in particular \\[ t^0_{\\beta_i} \\quad \\text{and} \\quad \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2} \\] are independent. Therefore \\[ t_{\\beta_i} \\sim t_{n-p} \\] Now, let \\(t \\sim t_m\\) a random variable with a \\(t\\) distribution with \\(m\\) degrees of freedom. Then call: \\[ t_m\\left(a\\right) \\quad \\text{such that} \\quad \\mathbb{P}\\left(t\\leq t_m\\left(a\\right) \\right) = a\\] for any \\(a\\in[0,1]\\) Then, we have that: \\[\\begin{align*} \\mathbb{P} &amp;\\left( -t_{n-p}\\left(\\frac{\\alpha}{2}\\right) \\leq t_{\\beta_i} \\leq t_{n-p}\\left(\\frac{\\alpha}{2}\\right) \\right) = \\alpha &amp;&amp; \\text{since the $t$ distribution is symmetric} \\\\ &amp;\\implies \\mathbb{P}\\left( -t_{n-p}\\left(\\frac{\\alpha}{2}\\right) \\leq \\frac{\\hat{\\beta}_i - \\beta_i}{\\sqrt{\\hat{\\sigma}^2_{\\beta_i}}} \\leq t_{n-p}\\left(\\frac{\\alpha}{2}\\right) \\right) = \\alpha &amp;&amp; \\text{since the $t_{\\beta_i} = \\frac{\\hat{\\beta}_i - \\beta_i}{\\sqrt{\\hat{\\sigma}^2_{\\beta_i}}}$} \\\\ &amp;\\implies \\mathbb{P}\\left( -t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\beta_i}} \\leq \\hat{\\beta}_i - \\beta_i \\leq t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\beta_i}} \\right) = \\alpha \\\\ &amp;\\implies \\mathbb{P}\\left( -t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\beta_i}} \\leq \\beta_i - \\hat{\\beta}_i \\leq t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\beta_i}} \\right) = \\alpha \\\\ &amp;\\implies \\mathbb{P}\\left( \\hat{\\beta}_i - t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\beta_i}} \\leq \\beta_i \\leq \\hat{\\beta}_i + t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\beta_i}} \\right) = \\alpha \\\\ \\end{align*}\\] So \\[ \\left(\\hat{\\beta}_i - t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\beta_i}}, \\hat{\\beta}_i + t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\beta_i}} \\right) \\] is a random interval, that is an interval that is a function of a random variables, in this case the random variables \\(\\hat{\\beta}_i\\) and \\(\\hat{\\sigma}^2_{\\beta_i}\\). This random interval will capture the true parameter \\(\\beta_i\\) with probability \\(\\alpha\\). However, when data is observed and the interval is fixed (at the observed values), the interval either captures the true parameter or not (something we don’t know in general). 9.4.2 Confidence intervals for the expected mean of a new observation \\(\\mathbf{x}_{new}\\) Note that the expected mean of a new observation \\(\\mathbf{x}_{new}\\) is given by: \\[ \\mathbb{E}[y_{new}] = \\mathbf{x}_{new}&#39; \\boldsymbol{\\beta}\\] then we can consider, an estimate of this parameter, as: \\[ \\mathbf{x}_{new}&#39; \\hat{\\boldsymbol{\\beta}} \\] this estimate is a linear combination of \\(\\hat{\\boldsymbol{\\beta}}\\), therefore it has a normal distribution with mean and variance as follows: \\[\\mathbb{E}[\\mathbf{x}_{new} \\hat{\\boldsymbol{\\beta}}] = \\mathbf{x}_{new} \\mathbb{E}[\\hat{\\boldsymbol{\\beta}}] = \\mathbf{x}_{new}&#39; \\boldsymbol{\\beta}\\] \\[\\mathbb{V}[\\mathbf{x}_{new}&#39; \\hat{\\boldsymbol{\\beta}}] = \\mathbf{x}_{new}&#39; \\mathbb{V}[\\hat{\\boldsymbol{\\beta}}] \\mathbf{x}_{new} = \\mathbf{x}_{new}&#39; \\sigma^2 (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{x}_{new} = \\sigma^2 \\mathbf{x}_{new}&#39; (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{x}_{new} \\] that is: \\[ \\mathbf{x}_{new}&#39; \\hat{\\boldsymbol{\\beta}} \\sim N \\left(\\mathbf{x}_{new}&#39; \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{x}_{new}&#39; (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{x}_{new} \\right) \\] so, similarly, we can consider \\[ t_{\\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}} = \\frac{\\mathbf{x}_{new}&#39;\\hat{\\boldsymbol{\\beta}} - \\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}}{\\sqrt{\\hat{\\sigma}^2_{\\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}}}}=\\frac{\\frac{\\mathbf{x}_{new}&#39;\\hat{\\boldsymbol{\\beta}} - \\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}}{\\sqrt{\\sigma^2_{\\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}}}}}{\\sqrt{\\frac{\\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2}}{n-p}}} \\sim \\chi^2_{n-p} \\] where \\(\\hat{\\sigma}^2_{\\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}} = \\hat{\\sigma}^2 \\mathbf{x}_{new}&#39; (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{x}_{new}\\). This quantity is distributed as a \\(t\\) with \\(n-p\\) degrees of freedom since: \\[\\frac{\\mathbf{x}_{new}&#39;\\hat{\\boldsymbol{\\beta}} - \\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}}{\\sqrt{\\sigma^2_{\\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}}}} \\sim N(0, 1)\\] \\[ \\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2} \\sim \\chi^2_{n-p}\\] and this random variables are independent since one is a function of \\(\\hat{\\boldsymbol{\\beta}}\\) and the other a function of \\(\\hat{\\mathbf{e}}\\). Then we can conclude that: \\[\\begin{align*} \\mathbb{P} &amp;\\left( -t_{n-p}\\left(\\frac{\\alpha}{2}\\right) \\leq t_{\\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}} \\leq t_{n-p}\\left(\\frac{\\alpha}{2}\\right) \\right) = \\alpha \\\\ &amp;\\implies \\mathbb{P}\\left( \\mathbf{x}_{new}&#39;\\hat{\\boldsymbol{\\beta}} - t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}}} \\leq \\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}\\leq \\mathbf{x}_{new}&#39;\\hat{\\boldsymbol{\\beta}} + t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}}} \\right) = \\alpha \\\\ \\end{align*}\\] so, the random interval is given by: \\[ \\left( \\mathbf{x}_{new}&#39;\\hat{\\boldsymbol{\\beta}} - t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}}} , \\mathbf{x}_{new}&#39;\\hat{\\boldsymbol{\\beta}} + t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}}} \\right) \\] is a random interval that captures \\(\\mathbf{x}_{new}&#39;\\boldsymbol{\\beta}\\) with probability \\(\\alpha\\). 9.4.3 Confidence intervals for linear combinations of \\(\\boldsymbol{\\beta}\\) Note that if we consider the parameter \\[ \\mathbf{a}&#39; \\hat{\\boldsymbol{\\beta}}\\] then we note that \\(\\beta_i\\) and \\(\\mathbf{x}_{new} \\boldsymbol{\\beta}\\) are particular cases, where the value of \\(\\mathbf{a}\\) is a s follows: \\[ \\mathbf{a}= (0,\\ldots,0,1,0,\\ldots,0) \\quad \\text{for} \\quad \\mathbf{a}\\boldsymbol{\\beta}= \\beta_i\\] \\[ \\mathbf{a}= \\mathbf{x}_{new} \\quad \\text{for} \\quad \\mathbf{a}\\boldsymbol{\\beta}= \\mathbf{x}_{new}&#39; \\boldsymbol{\\beta}\\] then, performing similar operations as before, we can create random intervals to estimate \\(\\mathbf{a}&#39; \\boldsymbol{\\beta}\\) as follows: \\[ \\left( \\mathbf{a}&#39;\\hat{\\boldsymbol{\\beta}} - t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\mathbf{a}&#39;\\boldsymbol{\\beta}}} , \\mathbf{a}&#39;\\hat{\\boldsymbol{\\beta}} + t_{n-p}\\left(\\frac{\\alpha}{2}\\right)\\sqrt{\\hat{\\sigma}^2_{\\mathbf{a}&#39;\\boldsymbol{\\beta}}} \\right) \\] that captures \\(\\mathbf{a}&#39; \\boldsymbol{\\beta}\\) with probability \\(\\alpha\\). Where \\(\\hat{\\sigma}^2_{\\mathbf{a}&#39;\\boldsymbol{\\beta}} = \\hat{\\sigma}^2 \\mathbf{a}&#39; (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{a}\\). 9.5 Hypothesis Testing We will approach hypothesis testing using an implausibility framework. This involves formulating a null hypothesis, \\(H_0\\), and assuming it to be true. Next, we calculate a test statistic that follows a specific distribution under the null hypothesis. By comparing the observed value of the statistic to this distribution, we assess how plausible it is to observe such a value if \\(H_0\\) is true. 9.5.1 Testing for the Overall Regression For this hypothesis, we will use the notation of: \\[ \\mathbf{X}^* = [\\mathbf{1}\\mathbf{X}] \\quad \\text{and} \\quad \\boldsymbol{\\beta}^* = [\\beta_0, \\boldsymbol{\\beta}]&#39; \\in \\mathbb{R}^{p}\\] that is, the \\(*\\) indicates all the independent variables. With \\(\\mathbf{X}\\) of full rank. Our first test is to see if the Linear Regression framework is useful at all. That is, we want to test \\(\\mathcal{H}_0: \\boldsymbol{\\beta}= \\mathbf{0}\\). Before designing our test statistic we will show the following auxiliary results: \\(SS_{reg} = \\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}\\). \\(\\mathbf{H}\\mathbf{H}_0 = \\mathbf{H}_0 \\mathbf{H}= \\mathbf{H}_0\\). \\((\\mathbf{H}- \\mathbf{H}_0)\\) is idempotent. \\(\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}\\) and \\(\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}\\) are independent. Under the null hypothesis \\(\\mathcal{H}_0: \\boldsymbol{\\beta}= \\mathbf{0}\\), \\(\\frac{\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}}{\\sigma^2}\\) is distributed like a \\(\\chi^2_{p-1}\\). For auxiliary result 1, we have that: \\[\\begin{align*} SS_{tot} &amp;= SS_{reg} + SS_{res} \\\\ &amp;\\implies \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}= SS_{reg} + \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}&amp;&amp; \\text{since $SS_{tot} = \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}$ and $SS_{res} = \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}$.} \\\\ &amp;\\implies SS_{reg} = \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0)\\mathbf{y}- \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}&amp;&amp; \\\\ &amp;\\implies SS_{reg} = \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_0 - \\mathbf{I}+ \\mathbf{H})\\mathbf{y}&amp;&amp; \\\\ &amp;\\implies SS_{reg} = \\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}&amp;&amp; \\\\ \\end{align*}\\] For auxiliary result 2, we have that: Both \\(\\mathbf{H}\\) and \\(\\mathbf{H}0\\) are symmetric, then \\(\\mathbf{H}\\mathbf{H}_0 = \\mathbf{H}_0 \\mathbf{H}\\), and: \\[\\begin{align*} \\mathbf{H}\\mathbf{H}_0 = \\mathbf{H}\\mathbf{1}(\\mathbf{1}&#39; \\mathbf{1})^{-1} \\mathbf{1}&#39; &amp;&amp; \\\\ \\mathbf{H}\\mathbf{H}_0 = \\mathbf{1}(\\mathbf{1}&#39; \\mathbf{1})^{-1} \\mathbf{1}&#39; &amp;&amp; \\text{since $\\mathbf{H}\\mathbf{1}= \\mathbf{1}$.} \\\\ \\mathbf{H}\\mathbf{H}_0 = \\mathbf{H}_0 &amp;&amp; \\\\ \\end{align*}\\] For auxiliary result 3, we have that: \\[\\begin{align*} (\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{H}- \\mathbf{H}_0) &amp;= \\mathbf{H}\\mathbf{H}- \\mathbf{H}\\mathbf{H}_0 - \\mathbf{H}_0 \\mathbf{H}+ \\mathbf{H}_0 \\mathbf{H}_0 &amp;&amp; \\\\ &amp;= \\mathbf{H}- \\mathbf{H}\\mathbf{H}_0 - \\mathbf{H}_0 \\mathbf{H}+ \\mathbf{H}_0 &amp;&amp; \\text{since $\\mathbf{H}_0$ and $\\mathbf{H}$ are idempotent.} \\\\ &amp;= \\mathbf{H}- \\mathbf{H}_0 - \\mathbf{H}_0 + \\mathbf{H}_0 &amp;&amp; \\text{since $\\mathbf{H}\\mathbf{H}_0 = \\mathbf{H}_0 \\mathbf{H}= \\mathbf{H}_0$.} \\\\ &amp;= \\mathbf{H}- \\mathbf{H}_0 &amp;&amp; \\\\ \\end{align*}\\] so, \\((\\mathbf{H}- \\mathbf{H}_0)\\) is idempotent. For auxiliary result 4, first we have that: \\[\\begin{align*} \\mathbb{C}[(\\mathbf{H}- \\mathbf{H}_0) \\mathbf{y}, (\\mathbf{I}- \\mathbf{H}) \\mathbf{y}] &amp;= (\\mathbf{H}- \\mathbf{H}_0) \\mathbb{C}[\\mathbf{y},\\mathbf{y}] (\\mathbf{I}- \\mathbf{H}) \\\\ &amp;= (\\mathbf{H}- \\mathbf{H}_0) \\mathbb{V}[\\mathbf{y}] (\\mathbf{I}- \\mathbf{H}) \\\\ &amp;= \\sigma^2 (\\mathbf{H}- \\mathbf{H}_0) (\\mathbf{I}- \\mathbf{H}) \\\\ &amp;= \\sigma^2 (\\mathbf{H}- \\mathbf{H}_0 - \\mathbf{H}\\mathbf{H}+ \\mathbf{H}_0 \\mathbf{H}) \\\\ &amp;= \\sigma^2 (\\mathbf{H}- \\mathbf{H}_0 - \\mathbf{H}+ \\mathbf{H}_0 \\mathbf{H}) &amp;&amp; \\text{since $\\mathbf{H}$ is idempotent.} \\\\ &amp;= \\sigma^2 (\\mathbf{H}- \\mathbf{H}_0 - \\mathbf{H}+ \\mathbf{H}_0) &amp;&amp; \\text{since $\\mathbf{H}_0 \\mathbf{H}= \\mathbf{H}_0$.} \\\\ &amp;= \\sigma^2 \\mathbf{0}&amp;&amp; \\\\ &amp;= \\mathbf{0}&amp;&amp; \\\\ \\end{align*}\\] This tells us that \\((\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}\\) and \\((\\mathbf{I}- \\mathbf{H})\\mathbf{y}\\) are uncorrelated. Now, since \\((\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}\\) and \\((\\mathbf{I}- \\mathbf{H})\\mathbf{y}\\) are normally distributed, then zero correlation implies independence. Then, any function of this 2 quantities are independent. Note that: \\[\\begin{align*} \\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y} &amp;= \\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}&amp;&amp; \\text{since $(\\mathbf{H}- \\mathbf{H}_0)$ is idempotent.} \\end{align*}\\] Then, \\(\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}\\) is a quadratic function of \\((\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}\\). Similarly, \\(\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H})\\mathbf{y}\\) is a quadratic function of \\((\\mathbf{H}- \\mathbf{H})\\mathbf{y}\\). Therefore, \\(\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}\\) and \\(\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H})\\mathbf{y}\\) are independent. For result 5, we have that: \\[\\begin{align*} (\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y} &amp;= (\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{X}^* \\boldsymbol{\\beta}^* + \\mathbf{e}) &amp;&amp; \\text{since $\\mathbf{y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}$.} \\\\ &amp;= (\\mathbf{H}- \\mathbf{H}_0)([\\mathbf{1}\\mathbf{X}] [\\beta_0, \\boldsymbol{\\beta}]&#39; + \\mathbf{e}) &amp;&amp; \\text{since $\\mathbf{X}^* = [\\mathbf{1}\\mathbf{X}] \\quad \\text{and} \\quad \\boldsymbol{\\beta}^* = [\\beta_0, \\boldsymbol{\\beta}]&#39;$.} \\\\ &amp;= (\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{1}\\beta_0 + \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}) &amp;&amp; \\\\ &amp;= (\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{1}\\beta_0) + (\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}) &amp;&amp; \\\\ &amp;= (\\mathbf{H}\\mathbf{1}- \\mathbf{H}_0 \\mathbf{1})\\beta_0 + (\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}) &amp;&amp; \\\\ &amp;= (\\mathbf{1}- \\mathbf{1})\\beta_0 + (\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}) &amp;&amp; \\\\ &amp;= (\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}) &amp;&amp; \\\\ &amp;= (\\mathbf{H}- \\mathbf{H}_0)\\mathbf{e}&amp;&amp; \\text{iff $\\mathcal{H}_0: \\boldsymbol{\\beta}= \\mathbf{0}$ for any full rank $\\mathbf{X}$.} \\end{align*}\\] That is, for any full rank \\(\\mathbf{X}\\), we have that: \\[ (\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}= (\\mathbf{H}- \\mathbf{H}_0)\\mathbf{e}\\iff \\mathcal{H}_0: \\boldsymbol{\\beta}= \\mathbf{0}\\] Then: \\[\\begin{align*} \\mathbf{e}\\sim N(0, \\sigma^2 \\mathbf{I}) &amp;\\implies \\mathbf{e}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{e}\\sim \\sigma^2 \\chi^2_{p-1} &amp;&amp; \\text{since $(\\mathbf{H}- \\mathbf{H}_0)$ is idempotent of rank $p-1$}. \\\\ &amp;\\implies \\frac{\\mathbf{e}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{e}}{\\sigma^2} \\sim \\chi^2_{p-1} &amp;&amp; \\\\ &amp;\\implies \\frac{\\mathbf{e}&#39;(\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{e}}{\\sigma^2} \\sim \\chi^2_{p-1} &amp;&amp; \\text{since $(\\mathbf{H}- \\mathbf{H}_0)$ is idempotent}. \\\\ &amp;\\implies \\frac{\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{H}- \\mathbf{H}_0)(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}}{\\sigma^2} \\sim \\chi^2_{p-1} &amp;&amp; \\text{iff the null hypothesis holds}. \\\\ &amp;\\implies \\frac{\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}}{\\sigma^2} \\sim \\chi^2_{p-1} &amp;&amp; \\text{since $(\\mathbf{H}- \\mathbf{H}_0)$ is idempotent}. \\\\ \\end{align*}\\] That is: \\[ \\frac{\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}}{\\sigma^2} \\sim \\chi^2_{p-1} \\iff \\mathcal{H}_0: \\boldsymbol{\\beta}= \\mathbf{0}\\] With this results, we propose the following statistic: \\[ F_{\\boldsymbol{\\beta}= 0} = \\frac{\\frac{SS_{reg}}{p-1}}{\\frac{SS_{res}}{n-p}} \\] and we will show that, this statistic is distributed like an \\(F_{p-1,n-p}\\) only under the null hypothesis. \\[\\begin{align*} F_{\\boldsymbol{\\beta}= 0} &amp;= \\frac{\\frac{SS_{reg}}{p-1}}{\\frac{SS_{res}}{n-p}} &amp;&amp; \\\\ &amp;= \\frac{\\frac{\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}}{p-1}}{\\frac{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}}{n-p}} &amp;&amp; \\text{since $SS_{reg} = \\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}$ and $SS_{res}=\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}$} \\\\ &amp;= \\frac{\\frac{\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}}{\\sigma^2}\\frac{1}{p-1}}{\\frac{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}}{\\sigma^2}\\frac{1}{n-p}} &amp;&amp; \\\\ &amp;\\sim \\frac{\\frac{\\chi^2_{p-1}}{p-1}}{\\frac{\\chi^2_{n-p}}{n-p}} &amp;&amp; \\text{since $\\frac{\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}}{\\sigma^2} \\sim \\chi^2_{p-1}$ under the null hypothesis.} \\\\ &amp;\\sim F_{p-1,n-p} &amp;&amp; \\text{since $\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_0)\\mathbf{y}$ and $\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H})\\mathbf{y}$ are independent.} \\end{align*}\\] So, once we observe the value of this statistic, we can contrast it with respect respect to this distribution. Call \\(F^*_{\\boldsymbol{\\beta}= 0}\\) the observed value, and consider a random variable \\(F \\sim F_{p-1,n-p}\\), then we can see what would be the probability of observing the value of the statistic (or a more extreme value). \\[ \\mathbb{P}(F \\geq F^*_{\\boldsymbol{\\beta}= 0}) \\] depending on how small or big is this probability, we can reject or not reject the null hypothesis. This value is a called a p-value. 9.5.2 Testing if one variable is not relevant We can test if a particular variable is not relevant for the regression. That is, \\(\\mathcal{H}_0: \\beta_i = 0\\). We will use the same strategy, that is, we will build a test statistic that has a certain distribution only under the null hypothesis. For this hypothesis we propose the following test statistic: \\[ t_{\\beta_i = 0} = \\frac{\\hat{\\beta}_i}{\\sqrt{\\hat{\\sigma}^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}}} \\] First note that: \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}} \\sim N \\left(\\boldsymbol{\\beta}, \\sigma^2(\\mathbf{X}\\mathbf{X})^{-1}\\right) &amp;\\implies \\hat{\\beta}_i \\sim H(\\beta_i, \\sigma^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}) \\\\ &amp;\\implies \\frac{\\hat{\\beta}_i}{\\sqrt{\\sigma^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}}} \\sim N \\left(\\frac{\\beta_i}{\\sqrt{\\sigma^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}}}, 1 \\right) \\\\ &amp;\\implies \\frac{\\hat{\\beta}_i}{\\sqrt{\\sigma^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}}} \\sim N \\left( 0, 1 \\right) &amp;&amp; \\iff \\mathcal{H}_0: \\beta_i = 0 \\\\ \\end{align*}\\] Then we have: $$$$ \\[\\begin{align*} t_{\\beta_i = 0} &amp;= \\frac{\\hat{\\beta}_i}{\\sqrt{\\hat{\\sigma}^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}}} \\\\ &amp;= \\frac{\\frac{\\hat{\\beta}_i }{\\sqrt{\\sigma^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}}}}{\\frac{\\sqrt{\\hat{\\sigma}^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}}}{\\sqrt{\\sigma^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}}}} \\\\ &amp;= \\frac{\\frac{\\hat{\\beta}_i }{\\sqrt{\\sigma^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}}}}{\\sqrt{\\frac{\\hat{\\sigma}^2}{\\sigma^2}}} \\\\ &amp;= \\frac{\\frac{\\hat{\\beta}_i }{\\sqrt{\\sigma^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}}}}{\\sqrt{\\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2}\\frac{1}{n-p}}} &amp;&amp; \\text{since $\\hat{\\sigma}^2 = \\frac{\\hat{\\mathbf{e}}\\hat{\\mathbf{e}}}{n-p}$} \\\\ &amp;\\sim \\frac{N \\left(\\frac{\\beta_i}{\\sqrt{\\sigma^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}}}, 1 \\right)}{\\sqrt{\\frac{\\chi^2_{n-p}}{n-p}}} &amp;&amp; \\text{since $\\frac{\\hat{\\beta}_i}{\\sqrt{\\sigma^2 [(\\mathbf{X}\\mathbf{X})^{-1}]_{ii}}} \\sim N \\left( 0, 1 \\right)$ and $\\frac{\\hat{\\mathbf{e}}&#39;\\hat{\\mathbf{e}}}{\\sigma^2} \\sim \\chi^2_{n-p}$} \\\\ &amp;\\sim \\frac{N \\left(0, 1 \\right)}{\\sqrt{\\frac{\\chi^2_{n-p}}{n-p}}} &amp;&amp; \\iff \\mathcal{H}_0: \\beta_i = 0 \\\\ &amp;\\sim t_{n-p} &amp;&amp; \\text{since $\\hat{\\beta}_i$ and $\\hat{\\sigma}^2$ are independent}. \\\\ \\end{align*}\\] Then, under the null hypothesis we have that: \\[ t_{\\beta_i = 0} \\sim t_{n-p}\\] So, if we call \\(t_{\\beta_i = 0}^*\\) the observed value of \\(t_{\\beta_i = 0}\\), and if we let \\(t\\) be distributed as \\(t_{n-p}\\), we can compute: \\[ \\mathbb{P}(t \\geq t_{\\beta_i = 0}^*) \\] and depending on the value, we can reject or accept the null hypothesis. 9.5.3 Testing if a Subgroup of the Variables is Relevant For this test, we can assume without loss of generality, that the variables we want to see if it is relevant are the first \\(k\\). So we can divide the design matrix as: \\[ \\mathbf{X}= [\\mathbf{X}_1 \\mathbf{X}_2] \\] where the variables to test are in \\(\\mathbf{X}_1\\) and the rest of the variables are in \\(\\mathbf{X}_2\\) (including possibly the intercept). And similarly we have \\(\\boldsymbol{\\beta}= [\\boldsymbol{\\beta}_1 \\boldsymbol{\\beta}_2]&#39;\\). This test is similar to the first test once we express it accordingly. We will consider two linear regressions. One including all variables and one excluding the variables to be tested indexed by \\(2\\). With this we can build the following test statistics: \\[ F_{\\boldsymbol{\\beta}_1=\\mathbf{0}} = \\frac{\\frac{SS_{res,2} - SS_{res}}{k}}{\\frac{SS_{res}}{n-p}} \\] Then note the following: \\[\\begin{align*} SS_{res,2} - SS_{res} &amp;= \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_2)\\mathbf{y}- \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}) \\mathbf{y}\\\\ &amp;= \\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H}_2 - \\mathbf{I}+ \\mathbf{H}) \\mathbf{y}\\\\ &amp;= \\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_2) \\mathbf{y}\\\\ \\end{align*}\\] Again, we will see that \\((\\mathbf{H}- \\mathbf{H}_2)\\) is idempotent and \\((\\mathbf{H}- \\mathbf{H}_2)\\mathbf{y}= (\\mathbf{H}- \\mathbf{H}_2)\\mathbf{e}\\) only under the null hypothesis. First, let us see that \\((\\mathbf{H}- \\mathbf{H}_2)\\) is idempotent. First note that: \\[ \\mathbf{H}\\mathbf{H}_2 = \\mathbf{H}_2 \\mathbf{H}= \\mathbf{H}_2\\] since \\(\\mathbf{H}_2\\) is the projection matrix of the columns of \\(\\mathbf{X}_2\\) a subspace of the columns of \\(\\mathbf{X}\\). Then: \\[\\begin{align*} (\\mathbf{H}- \\mathbf{H}_2)(\\mathbf{H}- \\mathbf{H}_2) &amp;= \\mathbf{H}\\mathbf{H}- \\mathbf{H}_2 \\mathbf{H}- \\mathbf{H}\\mathbf{H}_2 + \\mathbf{H}_2 \\mathbf{H}_2 \\\\ &amp;= \\mathbf{H}- \\mathbf{H}_2 \\mathbf{H}- \\mathbf{H}\\mathbf{H}_2 + \\mathbf{H}_2 &amp;&amp; \\text{since $\\mathbf{H}_2$ and $\\mathbf{H}$ are idempotent}. \\\\ &amp;= \\mathbf{H}- \\mathbf{H}_2 - \\mathbf{H}_2 + \\mathbf{H}_2 &amp;&amp; \\text{since $\\mathbf{H}\\mathbf{H}_2 = \\mathbf{H}_2 \\mathbf{H}= \\mathbf{H}_2$}. \\\\ &amp;= \\mathbf{H}- \\mathbf{H}_2 &amp;&amp; \\\\ \\end{align*}\\] then \\((\\mathbf{H}- \\mathbf{H}_2)\\) is idempotent. Now let us see that \\((\\mathbf{H}- \\mathbf{H}_R)\\mathbf{y}= (\\mathbf{H}- \\mathbf{H}_R)\\mathbf{e}\\) under the null hypothesis. First, let us note that: \\[ \\mathbf{H}\\mathbf{X}_2 = \\mathbf{X}_2 \\] since space generated by \\(\\mathbf{X}_2\\) is a subspace of the space generated by \\(\\mathbf{X}\\), since \\(\\mathbf{X}\\) contains the columns of \\(\\mathbf{X}_2\\). And we also note that: \\[ \\mathbf{H}_2 \\mathbf{X}_2 = \\mathbf{X}_2 \\] since \\(\\mathbf{H}_2\\) is the projection matrix of the space generated by the columns of \\(\\mathbf{X}_2\\). We note that this results can be proven algebraically. Then: \\[\\begin{align*} (\\mathbf{H}- \\mathbf{H}_2)\\mathbf{y} &amp;= (\\mathbf{H}- \\mathbf{H}_2)(\\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}) \\\\ &amp;= (\\mathbf{H}- \\mathbf{H}_2)([\\mathbf{X}_1 \\mathbf{X}_2] [\\boldsymbol{\\beta}_1&#39; \\boldsymbol{\\beta}_2&#39;]&#39; + \\mathbf{e}) \\\\ &amp;= (\\mathbf{H}- \\mathbf{H}_2)(\\mathbf{X}_1 \\boldsymbol{\\beta}_1 \\mathbf{X}_2 \\boldsymbol{\\beta}_2 + \\mathbf{e}) \\\\ &amp;= (\\mathbf{H}- \\mathbf{H}_2)(\\mathbf{X}_2 \\boldsymbol{\\beta}_2) + (\\mathbf{H}- \\mathbf{H}_1)(\\mathbf{X}_1 \\boldsymbol{\\beta}_1 + \\mathbf{e}) \\\\ &amp;= (\\mathbf{H}\\mathbf{X}_2 - \\mathbf{H}_2\\mathbf{X}_2)\\boldsymbol{\\beta}_2 + (\\mathbf{H}- \\mathbf{H}_1)(\\mathbf{X}_1 \\boldsymbol{\\beta}_1 + \\mathbf{e}) \\\\ &amp;= (\\mathbf{X}_2 - \\mathbf{X}_2)\\boldsymbol{\\beta}_2 + (\\mathbf{H}- \\mathbf{H}_1)(\\mathbf{X}_1 \\boldsymbol{\\beta}_1 + \\mathbf{e}) &amp;&amp; \\text{since $\\mathbf{H}\\mathbf{X}_1 = \\mathbf{X}_1$ and $\\mathbf{H}_1 \\mathbf{X}_1 = \\mathbf{X}_1$} \\\\ &amp;= (\\mathbf{H}- \\mathbf{H}_R)(\\mathbf{X}_1 \\boldsymbol{\\beta}_1 + \\mathbf{e}) &amp;&amp; \\\\ &amp;= (\\mathbf{H}- \\mathbf{H}_R)\\mathbf{e}&amp;&amp; \\iff \\mathcal{H}_0: \\boldsymbol{\\beta}_1 = \\mathbf{0}\\\\ \\end{align*}\\] So, if \\(\\mathbf{X}_1\\) is full rank, then we have that: \\[ (\\mathbf{H}- \\mathbf{H}_2)\\mathbf{y}= (\\mathbf{H}- \\mathbf{H}_R)\\mathbf{e}\\iff \\mathcal{H}_0: \\boldsymbol{\\beta}_1 = \\mathbf{0}\\] Then we can proceed to see what is the distribution of our test statistic under the null hypothesis. \\[\\begin{align*} F_{\\boldsymbol{\\beta}_1=\\mathbf{0}} &amp;= \\frac{\\frac{SS_{res,2} - SS_{res}}{k}}{\\frac{SS_{res}}{n-p}} &amp;&amp; \\\\ &amp;= \\frac{\\frac{\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_2)\\mathbf{y}}{k}}{\\frac{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}}{n-p}} &amp;&amp; \\text{since $SS_{res,2} - SS_{res} = \\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_2)\\mathbf{y}$ and $SS_{res}=\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}$} \\\\ &amp;= \\frac{\\frac{\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_2)\\mathbf{y}}{\\sigma^2}\\frac{1}{k}}{\\frac{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}}{\\sigma^2}\\frac{1}{n-p}} &amp;&amp; \\\\ &amp;= \\frac{\\frac{\\mathbf{y}&#39;(\\mathbf{H}- \\mathbf{H}_2)(\\mathbf{H}- \\mathbf{H}_2)(\\mathbf{H}- \\mathbf{H}_2)\\mathbf{y}}{\\sigma^2}\\frac{1}{k}}{\\frac{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}}{\\sigma^2}\\frac{1}{n-p}} &amp;&amp; \\text{snce $(\\mathbf{H}- \\mathbf{H}_2)$ is idempotent}. \\\\ &amp;= \\frac{\\frac{\\mathbf{e}&#39;(\\mathbf{H}- \\mathbf{H}_2)(\\mathbf{H}- \\mathbf{H}_2)(\\mathbf{H}- \\mathbf{H}_2)\\mathbf{e}}{\\sigma^2}\\frac{1}{k}}{\\frac{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}}{\\sigma^2}\\frac{1}{n-p}} &amp;&amp; \\iff \\mathcal{H}_0: \\boldsymbol{\\beta}_1 = \\mathbf{0}\\\\ &amp;= \\frac{\\frac{\\mathbf{e}&#39;(\\mathbf{H}- \\mathbf{H}_2)\\mathbf{e}}{\\sigma^2}\\frac{1}{k}}{\\frac{\\mathbf{y}&#39;(\\mathbf{I}- \\mathbf{H})\\mathbf{y}}{\\sigma^2}\\frac{1}{n-p}} &amp;&amp; \\text{since $(\\mathbf{H}- \\mathbf{H}_2)$ is idempotent}. \\\\ &amp;\\sim \\frac{\\frac{\\chi^2_{k}}{k}}{\\frac{\\chi^2_{n-p}}{n-p}} &amp;&amp; \\text{since $(\\mathbf{H}- \\mathbf{H}_2)$ is idempotent and $\\frac{\\mathbf{e}}{\\sqrt{\\sigma^2}} \\sim N(0, \\mathbf{I})$}. \\\\ &amp;\\sim F_{k,n-p} &amp;&amp; \\end{align*}\\] So, before we observe the data, \\(F_{\\boldsymbol{\\beta}_1=\\mathbf{0}}\\) has a \\(F_{k,n-p}\\) distribution. Then, once we observe the data, call \\(F_{\\boldsymbol{\\beta}_1=\\mathbf{0}}^*\\) the observed value of the statistic, and let \\(F\\) be distributed as an \\(F_{k,n-p}\\), we can compute: \\[ \\mathbb{P}(F \\geq F_{\\boldsymbol{\\beta}_1=\\mathbf{0}}^*) \\] and reject the null hypothesis if this probability is small and not reject if this probability is small. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
