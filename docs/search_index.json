[["index.html", "1 Stat 5385/6385", " 1 Stat 5385/6385 This is the website for STAT5385. It will contain the relevant information for the course and lecture notes and code seen in class. Find the syllabus here: Syllabus "],["prerequisites.html", "2 Prerequisites 2.1 General Math 2.2 Linear Algebra 2.3 Probability 2.4 Statistics 2.5 Calculus", " 2 Prerequisites Before diving into the course, it’s important to have a solid understanding of the following foundational concepts. These are categorized into five key topics: General Math Linear Algebra Probability Statistics Calculus You can check some of the requirements on Chapter 1 of the textbook. 2.1 General Math You should be familiar with the summation operator \\(\\sum\\). This operator is defined as follows: \\[\\sum_{i=1}^n x_i = x_1 + x_2 + \\ldots + x_n \\] Key properties of the summation operator include: Linearity: \\[\\sum_{i=1}^N (a + b x_i) = aN + b \\sum_{i=1}^N x_i\\] Additivity: \\[\\sum_{i=1}^N (x_i + y_i) = \\sum_{i=1}^N x_i + \\sum_{i=1}^N y_i\\] 2.2 Linear Algebra You should be familiar with the following linear algebra concepts: Matrices Determinants Eigenvalues and Eigenvectors Diagonalization Vector Spaces Linear Transformations 2.3 Probability Key probability concepts to understand include: Expected Value Variance Covariance Correlation Joint, Marginal, and Conditional Distributions Independence Central Limit Theorem Distributions: Normal Chi-Squared (\\(\\chi^2\\)) t-distribution F-distribution 2.4 Statistics Essential statistical concepts include: Point Estimation: Maximum Likelihood Least Squares Estimation Properties of Point Estimators: Unbiased Consistent Minimum Variance Interval Estimation Hypothesis Testing 2.5 Calculus Key calculus topics include: Gradients Optimization "],["introduction.html", "3 Introduction 3.1 Examples", " 3 Introduction Linear regression is a statistical method used to analyze the relationship between at least two variables: one dependent variable and at least one independent variable. This technique is closely tied to the correlation coefficient, which measures the strength and direction of a linear relationship between variables. In this document, we will explore how these concepts are connected. Linear regression is typically applied for three main purposes: Description: To describe the relationship between the variables under analysis. Control: To predict how changes in the independent variables will affect the dependent variable. Prediction: To forecast the value of the dependent variable based on new observations of the independent variables. 3.1 Examples 3.1.1 Ad Spending Imagine you are a newly hired data scientist at a mattress company. Your manager asks you to analyze the relationship between Google ad spending and mattress sales revenue. To illustrate this scenario, I have simulated a dataset: # Ad Spending Example # Set Seed set.seed(8272024) # Data Simulation x &lt;- rnorm(n = 100, mean = 70, sd = 30) y &lt;- 1000 + 5 * x + rnorm(n = 100, mean = 0, sd = 100) # Creates the Data Frame datAd &lt;- data.frame(cbind(y,x)) # Names the Variables colnames(datAd) &lt;- c(&quot;Revenue&quot;, &quot;Ad Spending&quot;) # Saves to csv write.csv(x = datAd[, c(1, 2)], file = &quot;Ad spending Data.csv&quot;, row.names = FALSE) You can load the dataset as follows: dat &lt;- read.csv(file = &quot;Ad spending Data.csv&quot;) To visualize the data: dat &lt;- read.csv(file = &quot;Ad spending Data.csv&quot;) plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) Next, you can perform a linear regression analysis: outReg &lt;- lm(Revenue ~ Ad.Spending, data = dat) The most important results from the regression analysis can be summarized as follows: summary(outReg) ## ## Call: ## lm(formula = Revenue ~ Ad.Spending, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -248.394 -58.805 3.782 63.577 196.745 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 997.3894 28.8185 34.61 &lt;2e-16 *** ## Ad.Spending 5.0247 0.3818 13.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 89.01 on 98 degrees of freedom ## Multiple R-squared: 0.6386, Adjusted R-squared: 0.6349 ## F-statistic: 173.2 on 1 and 98 DF, p-value: &lt; 2.2e-16 For now, let’s focus on the estimate for the intercept and the Ad.Spending coefficient. The intercept indicates the expected revenue when ad spending is zero. Based on your analysis, you observe that even without an ad campaign, mattress sales generate \\(r outReg\\)coefficients[1]. The coefficient for Ad.Spending shows that each dollar spent on ads increases revenue by \\(r outReg\\)coefficients[2]. To visualize the regression line: plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) The red regression line represents the “best fit” for these variables. In this context, “best fit” means the line that minimizes the sum of the squared distances from each point to the line. For comparison, here are examples of other lines that do not fit as well: Different slope (coefficient for the Ad.Spending variable): plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(a = outReg$coefficients[1], b = 6, col = &#39;blue&#39;, lwd = 2) Different intercept: plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(a = 1100, b = outReg$coefficients[2], col = &#39;blue&#39;, lwd = 2) We can expand the range for ad spending and revenue on the plot, as follows: plot(x = dat$Ad.Spending, y = dat$Revenue, xlab = &quot;Ad Spending ($)&quot;, ylab = &quot;Revenue ($)&quot;, ylim = c(800, 2500), xlim = c(0, 200)) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) abline(h = 0, v = 0) Within the range of observed ad spending values (r round(min(dat\\(Ad.Spending)) to r round(max(dat\\)Ad.Spending))), we can be reasonably confident in the relationship between ad spending and revenue. However, what happens when we consider values outside this range? While we can predict and control revenue to some extent by adjusting ad spending within the observed range, this confidence may not extend to values beyond it. For instance, it’s plausible that after reaching a certain level of ad spending, the market could become saturated, resulting in diminishing or even no additional revenue despite increased ad spending. Therefore, we should be cautious when extrapolating beyond the observed data, as the relationship may not hold under different conditions. 3.1.2 Wine and Life Expectancy In the previous example, we were able to manipulate the independent variable to influence the outcome. However, there are situations where our primary goal is simply to describe the relationship between two variables, without aiming to control them. In the following example, I generate a dataset that illustrates the relationship between wine consumption and life expectancy in years for an imaginary country. Here’s how the data is simulated: # Wine and Life Expectancy # Set Seed set.seed(8272024) # Data Simulation x &lt;- rbinom(n = 20, size = 20, prob = 0.1) y &lt;- 75 + 1.5 * x + rnorm(n = 20, mean = 0, sd = 3) # Creates the Data Frame datWin &lt;- data.frame(cbind(y,x)) # Names the Variables colnames(datWin) &lt;- c(&quot;Years&quot;, &quot;Glasses&quot;) # Saves to csv write.csv(datWin[, c(1, 2)], file = &quot;Wine Data.csv&quot;, row.names = FALSE) Here’s an improved version of your text: You should be able to generate the exact same data if you copy and paste the code and run it on your computer. Although the data is randomly simulated, I’ve set a seed to ensure that the simulation can be replicated consistently. We can read and plot the data in the as follows: # Reads the Data dat &lt;- read.csv(file = &quot;Wine Data.csv&quot;) # Plots the Data plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) Once again, we can perform a linear regression to examine the relationship between the variables and visualize it with a regression line: # Performs Linear Regression outReg &lt;- lm(Years ~ Glasses, data = dat) # Plots plot(x = dat$Glasses, y = dat$Years, xlab = &quot;Avg. Glasses of Wine per Week&quot;, ylab = &quot;Life Expectancy (Years)&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) Given the regression line, can we confidently conclude that increasing wine consumption leads to a longer life expectancy? 3.1.3 Burger Demand Imagine you work at a burger franchise where prices change daily. As the franchise manager, you want to predict the demand for burgers at a given price. Over the past 100 days, you’ve collected data on the number of burgers sold at each price set by the franchise’s main office. Here’s what your data looks like: # Reads the Data dat &lt;- read.csv(file = &quot;Burger Data.csv&quot;) # Plots the Data plot(x = dat$Price, y = dat$Burgers, xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers Sold&quot;) Is this dataset a good candidate for linear regression? While it’s true that we can always fit a line to any dataset, the real question is whether that line meaningfully represents the relationship between the variables. Here’s what happens when we apply linear regression to this data: # Performs Linear Regression outReg &lt;- lm(Burgers ~ Price, data = dat) # Plots plot(x = dat$Price, y = dat$Burgers, xlab = &quot;Price ($)&quot;, ylab = &quot;Burgers Sold&quot;) abline(a = outReg$coefficients[1], b = outReg$coefficients[2], col = &#39;red&#39;, lwd = 2) Do you think this is a good fit for the data? The regression line appears to be inadequate for both low and high price points. Additionally, since the number of burgers sold must be positive, the regression line should not intersect the x-axis. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
