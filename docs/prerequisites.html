<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Prerequisites | _main.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Prerequisites | _main.knit" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Prerequisites | _main.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="introduction.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Stat 5385/6385</a></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#general-math"><i class="fa fa-check"></i><b>2.1</b> General Math</a></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.2.3</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.2.4</b> Positive Definite Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.3</b> Probability</a></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.4</b> Statistics</a></li>
<li class="chapter" data-level="2.5" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.5</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.5.1</b> Gradient</a></li>
<li class="chapter" data-level="2.5.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.5.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.5.3</b> Applications:</a></li>
<li class="chapter" data-level="2.5.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.5.4</b> Matrix Calculus</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#examples"><i class="fa fa-check"></i><b>3.1</b> Examples</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction.html"><a href="introduction.html#ad-spending"><i class="fa fa-check"></i><b>3.1.1</b> Ad Spending</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction.html"><a href="introduction.html#winw-example"><i class="fa fa-check"></i><b>3.1.2</b> Wine and Life Expectancy</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction.html"><a href="introduction.html#burger-demand"><i class="fa fa-check"></i><b>3.1.3</b> Burger Demand</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model"><i class="fa fa-check"></i><b>4.1</b> Model</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>4.2</b> Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#other-estimated-quantites"><i class="fa fa-check"></i><b>4.2.1</b> Other estimated quantites</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#properties-of-the-estimates"><i class="fa fa-check"></i><b>4.3</b> Properties of the Estimates</a></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-and-standarizing-the-data"><i class="fa fa-check"></i><b>4.4</b> Centering and Standarizing the Data</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independent-variable-centered"><i class="fa fa-check"></i><b>4.4.1</b> Independent variable centered</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#both-variables-centered"><i class="fa fa-check"></i><b>4.4.2</b> Both Variables centered</a></li>
<li class="chapter" data-level="4.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independent-and-dependent-variable-standardized"><i class="fa fa-check"></i><b>4.4.3</b> Independent and dependent variable standardized</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>4.5</b> Coefficient of Determination</a></li>
<li class="chapter" data-level="4.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residual-analysis"><i class="fa fa-check"></i><b>4.6</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#non-linear-regression-function"><i class="fa fa-check"></i><b>4.6.1</b> Non-linear regression function</a></li>
<li class="chapter" data-level="4.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#heteroscedasticity"><i class="fa fa-check"></i><b>4.6.2</b> Heteroscedasticity</a></li>
<li class="chapter" data-level="4.6.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outliers"><i class="fa fa-check"></i><b>4.6.3</b> Outliers</a></li>
<li class="chapter" data-level="4.6.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#variables-ommited"><i class="fa fa-check"></i><b>4.6.4</b> Variables Ommited</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#cross-validation"><i class="fa fa-check"></i><b>4.7</b> Cross-Validation</a></li>
<li class="chapter" data-level="4.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>4.8</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="4.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-in-matrix-form"><i class="fa fa-check"></i><b>4.9</b> Model in Matrix Form</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares-in-matrix-form"><i class="fa fa-check"></i><b>4.9.1</b> Weighted Least Squares in Matrix Form</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="polynomial-regression.html"><a href="polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Polynomial Regression</a></li>
<li class="chapter" data-level="6" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="multiple-regression.html"><a href="multiple-regression.html#example-5"><i class="fa fa-check"></i><b>6.2</b> Example</a></li>
<li class="chapter" data-level="6.3" data-path="multiple-regression.html"><a href="multiple-regression.html#least-squares-estimation-1"><i class="fa fa-check"></i><b>6.3</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.4" data-path="multiple-regression.html"><a href="multiple-regression.html#properties-of-the-estimates-1"><i class="fa fa-check"></i><b>6.4</b> Properties of the Estimates</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="prerequisites" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Prerequisites<a href="prerequisites.html#prerequisites" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Before diving into the course, it’s important to have a solid understanding of the following foundational concepts. These are categorized into five key topics:</p>
<ul>
<li><a href="prerequisites.html#general-math">General Math</a></li>
<li><a href="prerequisites.html#linear-algebra">Linear Algebra</a></li>
<li><a href="prerequisites.html#probability">Probability</a></li>
<li><a href="prerequisites.html#statistics">Statistics</a></li>
<li><a href="prerequisites.html#calculus">Calculus</a></li>
</ul>
<p>You can check some of the requirements on Chapter 1 of the textbook.</p>
<div id="general-math" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> General Math<a href="prerequisites.html#general-math" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You should be familiar with the <strong>summation operator</strong> <span class="math inline">\(\sum\)</span>. This operator is defined as follows:</p>
<p><span class="math display">\[\sum_{i=1}^n x_i = x_1 + x_2 + \ldots + x_n \]</span></p>
<p>Key properties of the summation operator include:</p>
<ul>
<li><p><strong>Linearity</strong>:
<span class="math display">\[\sum_{i=1}^N (a + b x_i) = aN + b \sum_{i=1}^N x_i\]</span></p></li>
<li><p><strong>Additivity</strong>:
<span class="math display">\[\sum_{i=1}^N (x_i + y_i) = \sum_{i=1}^N x_i + \sum_{i=1}^N y_i\]</span></p></li>
</ul>
</div>
<div id="linear-algebra" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Linear Algebra<a href="prerequisites.html#linear-algebra" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You should be familiar with the following linear algebra concepts:</p>
<ul>
<li><a href="prerequisites.html#linear-independence">Linear Independence</a></li>
<li><a href="prerequisites.html#full-rank-matrix">Full Rank Matrix</a></li>
<li><a href="prerequisites.html#inverse-matrix">Inverse Matrix</a></li>
<li><a href="prerequisites.html#positive-definite-matrix">Positive Definite Matrix</a></li>
<li>Determinants</li>
<li>Eigenvalues and Eigenvectors</li>
<li>Diagonalization</li>
<li>Vector Spaces</li>
<li>Linear Transformations</li>
</ul>
<div id="linear-independence" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Linear Independence<a href="prerequisites.html#linear-independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Linear independence</strong> is a fundamental concept in linear algebra that describes a set of vectors where no vector can be written as a linear combination of the others. In other words, the vectors are not “redundant,” meaning none of the vectors depends on any other in the set.</p>
<div id="definition" class="section level4 hasAnchor" number="2.2.1.1">
<h4><span class="header-section-number">2.2.1.1</span> Definition:<a href="prerequisites.html#definition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A set of vectors <span class="math inline">\(\{ \mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n \}\)</span> in a vector space is <strong>linearly independent</strong> if the only solution to the equation:</p>
<p><span class="math display">\[
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n = \mathbf{0}
\]</span></p>
<p>is when all the scalar coefficients <span class="math inline">\(c_1, c_2, \ldots, c_n\)</span> are zero, i.e., <span class="math inline">\(c_1 = c_2 = \cdots = c_n = 0\)</span>.</p>
<p>If any of the coefficients can be non-zero while still satisfying this equation, then the vectors are <strong>linearly dependent</strong>.</p>
</div>
<div id="example" class="section level4 hasAnchor" number="2.2.1.2">
<h4><span class="header-section-number">2.2.1.2</span> Example:<a href="prerequisites.html#example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consider two vectors in <span class="math inline">\(\mathbb{R}^2\)</span>:</p>
<p><span class="math display">\[
\mathbf{v}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad \mathbf{v}_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}
\]</span></p>
<p>These vectors are <strong>linearly independent</strong> because there is no way to express one as a multiple of the other. The only solution to:</p>
<p><span class="math display">\[
c_1 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + c_2 \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
\]</span></p>
<p>is <span class="math inline">\(c_1 = 0\)</span> and <span class="math inline">\(c_2 = 0\)</span>.</p>
<p>In contrast, if:</p>
<p><span class="math display">\[
\mathbf{v}_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad \mathbf{v}_2 = \begin{bmatrix} 2 \\ 4 \end{bmatrix}
\]</span></p>
<p>These vectors are <strong>linearly dependent</strong>, because <span class="math inline">\(\mathbf{v}_2 = 2 \mathbf{v}_1\)</span>. Therefore, you can express <span class="math inline">\(mathbf{v}_2\)</span> as a linear combination of <span class="math inline">\(\mathbf{v}_1\)</span>.</p>
</div>
<div id="key-points" class="section level4 hasAnchor" number="2.2.1.3">
<h4><span class="header-section-number">2.2.1.3</span> Key Points:<a href="prerequisites.html#key-points" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Linearly independent</strong> vectors carry distinct information and cannot be derived from each other.</li>
<li><strong>Linearly dependent</strong> vectors are redundant because one or more can be expressed as a combination of others.</li>
<li>In a set of linearly independent vectors, removing any vector would reduce the span of the vector space they cover.</li>
</ul>
</div>
<div id="importance" class="section level4 hasAnchor" number="2.2.1.4">
<h4><span class="header-section-number">2.2.1.4</span> Importance:<a href="prerequisites.html#importance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Linear independence is crucial in determining the <strong>rank</strong> of a matrix.</li>
<li>In systems of equations, linear independence of the rows or columns determines if the system has a unique solution.</li>
<li>In vector spaces, the <strong>dimension</strong> of the space is the maximum number of linearly independent vectors.</li>
</ul>
</div>
</div>
<div id="full-rank-matrix" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Full Rank Matrix<a href="prerequisites.html#full-rank-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <strong>full rank matrix</strong> is a matrix in which the rank is equal to the largest possible value for that matrix, meaning:</p>
<ul>
<li>For an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span>, the rank is the maximum number of linearly independent rows or columns.
<ul>
<li>If the rank is equal to <span class="math inline">\(m\)</span> (the number of rows), the matrix has <strong>full row rank</strong>.</li>
<li>If the rank is equal to <span class="math inline">\(n\)</span> (the number of columns), the matrix has <strong>full column rank</strong>.</li>
</ul></li>
</ul>
<div id="for-a-square-matrix-m-n" class="section level4 hasAnchor" number="2.2.2.1">
<h4><span class="header-section-number">2.2.2.1</span> For a square matrix (<span class="math inline">\(m = n\)</span>):<a href="prerequisites.html#for-a-square-matrix-m-n" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>A square matrix is <strong>full rank</strong> if its rank is equal to its dimension, i.e., if the matrix is invertible.</li>
<li>In this case, <span class="math inline">\(\text{rank}(\mathbf{A}) = n\)</span>, meaning all rows and columns are linearly independent, and the matrix has an inverse.</li>
</ul>
</div>
<div id="for-a-rectangular-matrix-m-neq-n" class="section level4 hasAnchor" number="2.2.2.2">
<h4><span class="header-section-number">2.2.2.2</span> For a rectangular matrix (<span class="math inline">\(m \neq n\)</span>):<a href="prerequisites.html#for-a-rectangular-matrix-m-neq-n" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>A matrix is <strong>full rank</strong> if the rank equals the smaller of the number of rows or columns. For an <span class="math inline">\(m \times n\)</span> matrix, the rank is at most <span class="math inline">\(\min(m, n)\)</span>.
<ul>
<li>If the matrix has full row rank, all rows are linearly independent.</li>
<li>If the matrix has full column rank, all columns are linearly independent.</li>
</ul></li>
</ul>
</div>
<div id="example-1" class="section level4 hasAnchor" number="2.2.2.3">
<h4><span class="header-section-number">2.2.2.3</span> Example:<a href="prerequisites.html#example-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consider the matrix:</p>
<p><span class="math display">\[
\mathbf{A}= \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{bmatrix}
\]</span></p>
<p>This is a <span class="math inline">\(2 \times 3\)</span> matrix. Since its two rows are linearly independent, it has <strong>full row rank</strong>, with rank = 2 (the number of rows). However, it does not have full column rank because it has only two independent rows for three columns.</p>
</div>
<div id="key-properties" class="section level4 hasAnchor" number="2.2.2.4">
<h4><span class="header-section-number">2.2.2.4</span> Key Properties:<a href="prerequisites.html#key-properties" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>A full rank matrix has <strong>no redundant rows or columns</strong> (no row or column can be written as a linear combination of others).</li>
<li>A square matrix with full rank is <strong>invertible</strong> (non-singular).</li>
<li>For a rectangular matrix, full rank implies the matrix has <strong>maximal independent information</strong> in terms of its rows or columns.</li>
</ul>
</div>
<div id="importance-1" class="section level4 hasAnchor" number="2.2.2.5">
<h4><span class="header-section-number">2.2.2.5</span> Importance:<a href="prerequisites.html#importance-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Full rank matrices are crucial in solving systems of linear equations. A system <span class="math inline">\(\mathbf{A}\mathbf{x} = \mathbf{b}\)</span> has a unique solution if <span class="math inline">\(\mathbf{A}\)</span> is a square, full rank matrix.</li>
<li>In linear algebra and machine learning, the rank provides insight into the dimensionality and the independence of the data or transformation matrix.</li>
</ul>
</div>
</div>
<div id="inverse-matrix" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Inverse Matrix<a href="prerequisites.html#inverse-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An <strong>inverse matrix</strong> of a square matrix <span class="math inline">\(\mathbf{A}\)</span>, denoted as <span class="math inline">\(\mathbf{A}^{-1}\)</span>, is a matrix that, when multiplied by <span class="math inline">\(\mathbf{A}\)</span>, results in the identity matrix <span class="math inline">\(I\)</span>. This relationship is expressed as:</p>
<p><span class="math display">\[
\mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1} \mathbf{A}= \mathbf{I}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{I}\)</span> is the identity matrix, and its diagonal elements are 1, with all off-diagonal elements being 0.</p>
<div id="conditions-for-a-matrix-to-have-an-inverse" class="section level4 hasAnchor" number="2.2.3.1">
<h4><span class="header-section-number">2.2.3.1</span> Conditions for a Matrix to Have an Inverse:<a href="prerequisites.html#conditions-for-a-matrix-to-have-an-inverse" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The matrix <span class="math inline">\(\mathbf{A}\)</span> must be <strong>square</strong>, meaning it has the same number of rows and columns.</li>
<li>The matrix <span class="math inline">\(\mathbf{A}\)</span> must be <strong>non-singular</strong>, meaning its <strong>determinant</strong> is non-zero (<span class="math inline">\(|\mathbf{A}| \neq 0\)</span>).</li>
</ul>
<div id="properties-of-the-inverse-matrix" class="section level5 hasAnchor" number="2.2.3.1.1">
<h5><span class="header-section-number">2.2.3.1.1</span> Properties of the Inverse Matrix:<a href="prerequisites.html#properties-of-the-inverse-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ol style="list-style-type: decimal">
<li><strong>Uniqueness:</strong> If a matrix has an inverse, it is unique.</li>
<li><strong>Inverse of a Product:</strong> The inverse of the product of two matrices <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> is given by <span class="math inline">\((\mathbf{A}\mathbf{B})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}\)</span>.</li>
<li><strong>Inverse of the Inverse:</strong> <span class="math inline">\((\mathbf{A}^{-1})^{-1} = \mathbf{A}\)</span>.</li>
<li><strong>Transpose of the Inverse:</strong> <span class="math inline">\((\mathbf{A}^{-1})&#39; = (\mathbf{A}&#39;)^{-1}\)</span>.</li>
</ol>
</div>
</div>
<div id="special-case" class="section level4 hasAnchor" number="2.2.3.2">
<h4><span class="header-section-number">2.2.3.2</span> Special Case:<a href="prerequisites.html#special-case" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a <span class="math inline">\(2 \times 2\)</span> matrix:</p>
<p><span class="math display">\[
\mathbf{A}= \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}
\]</span></p>
<p>The inverse of <span class="math inline">\(\mathbf{A}\)</span> (if <span class="math inline">\(|\mathbf{A}|=\det(\mathbf{A}) \neq 0\)</span>) is:</p>
<p><span class="math display">\[
A^{-1} = \frac{1}{ad - bc} \begin{bmatrix} d &amp; -b \\ -c &amp; a \end{bmatrix}
\]</span></p>
<p>where <span class="math inline">\(ad - bc\)</span> is the <strong>determinant</strong> of the matrix <span class="math inline">\(\mathbf{A}\)</span>.</p>
</div>
</div>
<div id="positive-definite-matrix" class="section level3 hasAnchor" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Positive Definite Matrix<a href="prerequisites.html#positive-definite-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <strong>positive definite matrix</strong> is a symmetric matrix <span class="math inline">\(\mathbf{A}\)</span> where, for any non-zero vector <span class="math inline">\(\mathbf{x}\)</span>, the following condition holds:</p>
<p><span class="math display">\[
\mathbf{x}&#39; \mathbf{A}\mathbf{x} &gt; 0
\]</span></p>
<div id="key-properties-1" class="section level4 hasAnchor" number="2.2.4.1">
<h4><span class="header-section-number">2.2.4.1</span> Key Properties:<a href="prerequisites.html#key-properties-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><strong>Symmetry:</strong> The matrix <span class="math inline">\(\mathbf{A}\)</span> must be symmetric, meaning <span class="math inline">\(\mathbf{A}= \mathbf{A}&#39;\)</span>.</li>
<li><strong>Positive quadratic form:</strong> For any non-zero vector <span class="math inline">\(\mathbf{x}\)</span>, the quadratic form <span class="math inline">\(\mathbf{x}&#39; \mathbf{A}\mathbf{x}\)</span> must yield a positive value.</li>
</ol>
</div>
<div id="characteristics-of-a-positive-definite-matrix" class="section level4 hasAnchor" number="2.2.4.2">
<h4><span class="header-section-number">2.2.4.2</span> Characteristics of a Positive Definite Matrix:<a href="prerequisites.html#characteristics-of-a-positive-definite-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>All the <strong>eigenvalues</strong> of a positive definite matrix are <strong>positive</strong>.</li>
<li>The <strong>determinants</strong> of the leading principal minors (submatrices) of the matrix are positive.</li>
<li>The <strong>diagonal elements</strong> of a positive definite matrix are positive.</li>
</ul>
</div>
<div id="example-2" class="section level4 hasAnchor" number="2.2.4.3">
<h4><span class="header-section-number">2.2.4.3</span> Example:<a href="prerequisites.html#example-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The matrix:
<span class="math display">\[
\mathbf{A}= \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}
\]</span>
is positive definite, because for any non-zero vector <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(\mathbf{x}&#39; \mathbf{A}\mathbf{x} &gt; 0\)</span>. For instance, if <span class="math inline">\(\mathbf{x} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span>, then:</p>
<p><span class="math display">\[
\mathbf{x}&#39; \mathbf{A}\mathbf{x} = \begin{bmatrix} 1 &amp; 1 \end{bmatrix} \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = 6 &gt; 0
\]</span></p>
</div>
</div>
</div>
<div id="probability" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Probability<a href="prerequisites.html#probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Key probability concepts to understand include:</p>
<ul>
<li>Expected Value</li>
<li>Variance</li>
<li>Covariance</li>
<li>Correlation</li>
<li>Joint, Marginal, and Conditional Distributions</li>
<li>Independence</li>
<li>Central Limit Theorem</li>
<li>Distributions:
<ul>
<li>Normal</li>
<li>Chi-Squared (<span class="math inline">\(\chi^2\)</span>)</li>
<li>t-distribution</li>
<li>F-distribution</li>
</ul></li>
</ul>
</div>
<div id="statistics" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Statistics<a href="prerequisites.html#statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Essential statistical concepts include:</p>
<ul>
<li><strong>Point Estimation</strong>:
<ul>
<li>Maximum Likelihood</li>
<li>Least Squares Estimation</li>
</ul></li>
<li><strong>Properties of Point Estimators</strong>:
<ul>
<li>Unbiased</li>
<li>Consistent</li>
<li>Minimum Variance</li>
</ul></li>
<li><strong>Interval Estimation</strong></li>
<li><strong>Hypothesis Testing</strong></li>
</ul>
</div>
<div id="calculus" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Calculus<a href="prerequisites.html#calculus" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Key calculus topics include:</p>
<ul>
<li><a href="prerequisites.html#gradient">Gradient</a></li>
<li><a href="#hessian">Hessian</a></li>
<li><a href="prerequisites.html#matrix-calculus">Matrix Calculus</a></li>
<li>Optimization</li>
</ul>
<div id="gradient" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Gradient<a href="prerequisites.html#gradient" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>gradient</strong> of a function is a vector that contains the partial derivatives of the function with respect to each of its variables. It points in the direction of the steepest ascent of the function, and its magnitude indicates the rate of change in that direction.</p>
<p>For a scalar function <span class="math inline">\(f(x_1, x_2, \ldots, x_n)\)</span>, where <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> are the variables, the gradient is defined as:</p>
<p><span class="math display">\[
\nabla f = \frac{d}{d \mathbf{x}} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}
\]</span></p>
<div id="key-points-1" class="section level4 hasAnchor" number="2.5.1.1">
<h4><span class="header-section-number">2.5.1.1</span> Key Points:<a href="prerequisites.html#key-points-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Direction:</strong> The gradient points in the direction of the greatest increase of the function.</li>
<li><strong>Magnitude:</strong> The magnitude of the gradient represents how fast the function increases in that direction.</li>
<li><strong>Zero Gradient:</strong> If <span class="math inline">\(\nabla f = 0\)</span>, it indicates that the function has a critical point, which could be a local minimum, maximum, or saddle point.</li>
</ul>
</div>
<div id="example-3" class="section level4 hasAnchor" number="2.5.1.2">
<h4><span class="header-section-number">2.5.1.2</span> Example:<a href="prerequisites.html#example-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a function <span class="math inline">\(f(x, y) = x^2 + y^2\)</span>, the gradient is:</p>
<p><span class="math display">\[
\nabla f = \begin{bmatrix} \frac{\partial}{\partial x} (x^2 + y^2) \\ \frac{\partial}{\partial y} (x^2 + y^2) \end{bmatrix} = \begin{bmatrix} 2x \\ 2y \end{bmatrix}
\]</span></p>
<p>This shows that the gradient points outward from the origin, and its magnitude increases as <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> increase.</p>
</div>
<div id="applications" class="section level4 hasAnchor" number="2.5.1.3">
<h4><span class="header-section-number">2.5.1.3</span> Applications:<a href="prerequisites.html#applications" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>In <strong>optimization</strong>, the gradient is used to find the minimum or maximum of a function (e.g., in gradient descent, a common optimization algorithm).</li>
<li>In <strong>vector calculus</strong>, the gradient is used to describe the slope or rate of change of scalar fields (such as temperature, pressure, or altitude in physical applications).</li>
</ul>
</div>
</div>
<div id="hessian-matrix" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Hessian Matrix<a href="prerequisites.html#hessian-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Hessian matrix</strong> is a square matrix of second-order partial derivatives of a scalar-valued function. It describes the local curvature of a multivariable function and is used to assess the nature of critical points (i.e., whether they are minima, maxima, or saddle points).</p>
<p>For a scalar function <span class="math inline">\(f(x_1, x_2, \ldots, x_n)\)</span>, the Hessian matrix <span class="math inline">\(\mathbf{H}\)</span> is defined as:</p>
<p><span class="math display">\[
\mathbf{H}(f) = \frac{d}{d \mathbf{x} d\mathbf{x}&#39;} f(\mathbf{x}) =\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
\]</span></p>
<div id="key-properties-2" class="section level4 hasAnchor" number="2.5.2.1">
<h4><span class="header-section-number">2.5.2.1</span> Key Properties:<a href="prerequisites.html#key-properties-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The Hessian is <strong>symmetric</strong> if the second-order partial derivatives are continuous (by Clairaut’s theorem, also called Schwarz’s theorem).</li>
<li>It provides important information about the local behavior of the function, particularly around critical points where the gradient is zero.</li>
<li><strong>Eigenvalues</strong> of the Hessian matrix determine the type of critical points:
<ul>
<li>If all eigenvalues are positive, the function has a <strong>local minimum</strong>.</li>
<li>If all eigenvalues are negative, the function has a <strong>local maximum</strong>.</li>
<li>If some eigenvalues are positive and others are negative, the function has a <strong>saddle point</strong>.</li>
</ul></li>
</ul>
</div>
<div id="example-4" class="section level4 hasAnchor" number="2.5.2.2">
<h4><span class="header-section-number">2.5.2.2</span> Example:<a href="prerequisites.html#example-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a function <span class="math inline">\(f(x, y) = x^2 + xy + y^2\)</span>, the Hessian matrix is:</p>
<p><span class="math display">\[
\mathbf{H}(f) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x^2} &amp; \frac{\partial^2 f}{\partial x \partial y} \\
\frac{\partial^2 f}{\partial y \partial x} &amp; \frac{\partial^2 f}{\partial y^2}
\end{bmatrix}
= \begin{bmatrix}
2 &amp; 1 \\
1 &amp; 2
\end{bmatrix}
\]</span></p>
</div>
</div>
<div id="applications-1" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Applications:<a href="prerequisites.html#applications-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>In <strong>optimization</strong>, the Hessian is used to assess the convexity or concavity of a function, which helps in identifying the nature of critical points.</li>
<li>In <strong>machine learning</strong>, it is used to optimize loss functions and can be part of second-order optimization methods like Newton’s method.</li>
<li>In <strong>economics</strong> and <strong>engineering</strong>, the Hessian helps in analyzing systems involving multiple variables and understanding how they interact with each other.</li>
</ul>
</div>
<div id="matrix-calculus" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> Matrix Calculus<a href="prerequisites.html#matrix-calculus" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You need to know the following matrix calculus operations:</p>
<p><span class="math display">\[
\frac{d}{d \mathbf{x}} \left(\mathbf{c}&#39;\mathbf{x}\right)
\]</span>
<span class="math display">\[
\frac{d}{d \mathbf{x}} \left(\mathbf{x}&#39;\mathbf{A}\mathbf{x}\right)
\]</span>
<span class="math display">\[
\frac{d}{d \mathbf{x} d\mathbf{x}&#39;} \left(\mathbf{x}&#39;\mathbf{A}\mathbf{x}\right)
\]</span></p>
<p>Let <span class="math inline">\(\mathbf{c}\)</span> be a constant vector and <span class="math inline">\(\mathbf{x}\)</span> be a variable vector, both of size <span class="math inline">\(n \times 1\)</span>. We want to compute the derivative of the product:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \mathbf{c}&#39; \mathbf{x}
\]</span>
Where:
<span class="math display">\[
\mathbf{c}&#39; \mathbf{x} = \sum_{i=1}^{n} c_i x_i
\]</span></p>
<p>To differentiate <span class="math inline">\(f(\mathbf{x}) = \mathbf{c}&#39; \mathbf{x}\)</span> with respect to the variable vector <span class="math inline">\(\mathbf{x}\)</span>, we take the derivative of each component separately:</p>
<p><span class="math display">\[
\nabla f = \frac{d}{d \mathbf{x}} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix} = \begin{bmatrix} \frac{\partial }{\partial x_1} (\mathbf{c}&#39; \mathbf{x})  \\ \frac{\partial }{\partial x_2} (\mathbf{c}&#39; \mathbf{x}) \\ \vdots \\ \frac{\partial }{\partial x_n} (\mathbf{c}&#39; \mathbf{x}) \end{bmatrix} = \begin{bmatrix} \frac{\partial }{\partial x_1} \left(\sum_{i=1}^{n} c_i x_i\right)  \\ \frac{\partial }{\partial x_2} \left(\sum_{i=1}^{n} c_i x_i\right) \\ \vdots \\ \frac{\partial }{\partial x_n} \left(\sum_{i=1}^{n} c_i x_i\right) \end{bmatrix}
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{c}\)</span> is a constant vector, the derivative of each term <span class="math inline">\(c_i x_i\)</span> is simply <span class="math inline">\(c_i\)</span>, that is:</p>
<p><span class="math display">\[
\frac{d}{d x_j} \left(\sum_{i=1}^{n} c_i x_i\right) = c_j
\]</span></p>
<p>Thus, the derivative of the entire sum is the vector:</p>
<p><span class="math display">\[
\frac{d}{d \mathbf{x}} \left( \mathbf{c}&#39; \mathbf{x} \right) = \begin{bmatrix} c_1  \\ c_2 \\ \vdots \\ c_n \end{bmatrix} = \mathbf{c}
\]</span></p>
<p>Now, let’s go through the derivative of the quadratic form <span class="math inline">\(f(\mathbf{x}) = \mathbf{x}&#39; \mathbf{A}\mathbf{x}\)</span>, where:</p>
<ul>
<li><span class="math inline">\(\mathbf{x}\)</span> is a variable vector of size <span class="math inline">\(n \times 1\)</span>,</li>
<li><span class="math inline">\(\mathbf{A}\)</span> is a constant, symmetric matrix of size <span class="math inline">\(n \times n\)</span>.</li>
</ul>
<p><span class="math display">\[
f(\mathbf{x}) = \mathbf{x}&#39; \mathbf{A}\mathbf{x}
\]</span>
First, expand the quadratic form:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j
\]</span>
Then</p>
<p><span class="math display">\[
\nabla f = \frac{d}{d \mathbf{x}} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix} = \begin{bmatrix} \frac{\partial }{\partial x_1} (\mathbf{x}&#39; \mathbf{A}\mathbf{x})  \\ \frac{\partial }{\partial x_2} (\mathbf{x}&#39; \mathbf{A}\mathbf{x}) \\ \vdots \\ \frac{\partial }{\partial x_n} (\mathbf{x}&#39; \mathbf{A}\mathbf{x}) \end{bmatrix} = \begin{bmatrix} \frac{\partial }{\partial x_1} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right)  \\ \frac{\partial }{\partial x_2} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right) \\ \vdots \\ \frac{\partial }{\partial x_n} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right) \end{bmatrix}
\]</span>
For each component <span class="math inline">\(x_k\)</span> in the vector <span class="math inline">\(\mathbf{x}\)</span>, the derivative of <span class="math inline">\(f(\mathbf{x})\)</span> is:</p>
<p><span class="math display">\[
\frac{\partial}{\partial x_k} f(\mathbf{x}) = \frac{\partial}{\partial x_k} \left( \sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j \right) = \sum_{i=1}^{n} \sum_{j=1}^{n} \frac{\partial}{\partial x_k}  x_i a_{ij} x_j
\]</span></p>
<p>Each term <span class="math inline">\(x_i a_{ij} x_j\)</span> has two components that depend on <span class="math inline">\(\mathbf{x}\)</span>:</p>
<ul>
<li>If <span class="math inline">\(i = j = k\)</span>, the derivative with respect to <span class="math inline">\(x_k\)</span> is:</li>
</ul>
<p><span class="math display">\[
\frac{\partial}{\partial x_k} (x_i a_{ij} x_j) = 2 a_{kk} x_k
\]</span></p>
<ul>
<li>If <span class="math inline">\(i \neq j\)</span> and <span class="math inline">\(i = k\)</span>, the derivative with respect to <span class="math inline">\(x_k\)</span> is:</li>
</ul>
<p><span class="math display">\[
\frac{\partial}{\partial x_k} (x_i a_{ij} x_j) = a_{kj} x_j
\]</span>
- Similarly, if <span class="math inline">\(i \neq j\)</span> and <span class="math inline">\(j = k\)</span>, the derivative with respect to <span class="math inline">\(x_k\)</span> is:</p>
<p><span class="math display">\[
\frac{\partial}{\partial x_k} (x_i a_{ij} x_j) = a_{ik} x_i
\]</span>
- Finally, if <span class="math inline">\(i \neq k\)</span> and <span class="math inline">\(j \neq k\)</span>, then:</p>
<p><span class="math display">\[
\frac{\partial}{\partial x_k} (x_i a_{ij} x_j) = 0
\]</span>
Then</p>
<p><span class="math display">\[
\frac{\partial}{\partial x_k} f(\mathbf{x}) = 2 a_{kk} x_k + \sum_{i \neq k} a_{ik} x_i + \sum_{j \neq k} a_{kj} x_j
\]</span></p>
<p>Now since <span class="math inline">\(\mathbf{A}\)</span> is symmetric (<span class="math inline">\(a_{ij} = a_{ji}\)</span>), then:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial}{\partial x_k} f(\mathbf{x})
  &amp;= 2 a_{kk} x_k + \sum_{i \neq k} a_{ik} x_i + \sum_{i \neq k} a_{ik} x_i \\
  &amp;= 2 a_{kk} x_k + 2\sum_{i \neq k} a_{ik} x_i \\
  &amp;= 2 \left(\sum_{i \neq k} a_{ik} x_i + a_{kk}x_k \right) \\
  &amp;= 2 \left(\sum_{i = 1}^n a_{ki} x_i\right)   
\end{align*}\]</span></p>
<p>Then:</p>
<p><span class="math display">\[
\nabla f = \begin{bmatrix} \frac{\partial }{\partial x_1} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right)  \\ \frac{\partial }{\partial x_2} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right) \\ \vdots \\ \frac{\partial }{\partial x_n} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right) \end{bmatrix} = \begin{bmatrix} 2 \sum_{i = 1}^n a_{1i} x_i  \\ 2 \sum_{i = 1}^n a_{2i} x_i \\ \vdots \\ 2 \sum_{i = 1}^n a_{ni} x_i \end{bmatrix} = 2 \mathbf{A}\mathbf{x}
\]</span></p>
<p>Finally for the second derivative we have that:</p>
<p>In general, the Hessian matrix of a scalar function <span class="math inline">\(f(\mathbf{x})\)</span>, where <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> is a vector of variables, is a matrix that contains all the second-order partial derivatives of the function. It is defined as:</p>
<p><span class="math display">\[
\mathbf{H}(f) = \frac{d^2 f}{d\mathbf{x}d\mathbf{x}&#39;} =  \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
= \begin{bmatrix}
\frac{\partial }{\partial x_1}\left(\frac{d f}{d\mathbf{x}}\right)&#39; \\
\frac{\partial }{\partial x_2}\left(\frac{d f}{d\mathbf{x}}\right)&#39; \\
\vdots  \\
\frac{\partial }{\partial x_n}\left(\frac{d f}{d\mathbf{x}}\right)&#39;
\end{bmatrix}
= \begin{bmatrix}
\frac{\partial }{\partial x_1}\left(2\mathbf{A}\mathbf{x}\right)&#39; \\
\frac{\partial }{\partial x_2}\left(2\mathbf{A}\mathbf{x}\right)&#39; \\
\vdots  \\
\frac{\partial }{\partial x_n}\left(2\mathbf{A}\mathbf{x}\right)&#39;
\end{bmatrix}
\]</span></p>
<p>Now</p>
<p><span class="math display">\[
\frac{\partial }{\partial x_k}\left(2\mathbf{A}\mathbf{x}\right)
= 2\frac{\partial }{\partial x_k}\begin{bmatrix}
\sum_{i = 1}^n a_{1i} x_i \\
\sum_{i = 1}^n a_{2i} x_i \\
\vdots  \\
\sum_{i = 1}^n a_{ni} x_i
\end{bmatrix}
= 2 \begin{bmatrix}
\frac{\partial }{\partial x_k} \left(\sum_{i = 1}^n a_{1i} x_i \right)\\
\frac{\partial }{\partial x_k} \left(\sum_{i = 1}^n a_{2i} x_i \right)\\
\vdots  \\
\frac{\partial }{\partial x_k} \left(\sum_{i = 1}^n a_{ni} x_i \right)
\end{bmatrix}
= 2 \begin{bmatrix}
a_{k1} \\
a_{k2} \\
\vdots \\
a_{kn}
\end{bmatrix}
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\mathbf{H}(f) = \frac{d^2 f}{d\mathbf{x}d\mathbf{x}&#39;}
= \begin{bmatrix}
2\begin{bmatrix}
a_{11} \\
a_{12} \\
\vdots \\
a_{1n}
\end{bmatrix}&#39; \\
2\begin{bmatrix}
a_{21} \\
a_{22} \\
\vdots \\
a_{2n}
\end{bmatrix}&#39; \\
\vdots \\
2\begin{bmatrix}
a_{n1} \\
a_{n2} \\
\vdots \\
a_{nn}
\end{bmatrix}&#39;
\end{bmatrix}
=  2\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn}
\end{bmatrix}
= 2 \mathbf{A}
\]</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
