<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Prerequisites | _main.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Prerequisites | _main.knit" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Prerequisites | _main.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="introduction.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Stat 5385/6385 Fall 2024</a></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#general-math"><i class="fa fa-check"></i><b>2.1</b> General Math</a></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.2.3</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.2.4</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.2.5" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.2.5</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.2.6" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.2.6</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.2.7" data-path="prerequisites.html"><a href="prerequisites.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.2.7</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="2.2.8" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.2.8</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.2.9" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.2.9</b> Idempotent Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.3</b> Probability</a></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.4</b> Statistics</a></li>
<li class="chapter" data-level="2.5" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.5</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.5.1</b> Gradient</a></li>
<li class="chapter" data-level="2.5.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.5.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.5.3</b> Applications:</a></li>
<li class="chapter" data-level="2.5.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.5.4</b> Matrix Calculus</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#examples-1"><i class="fa fa-check"></i><b>3.1</b> Examples</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction.html"><a href="introduction.html#ad-spending"><i class="fa fa-check"></i><b>3.1.1</b> Ad Spending</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction.html"><a href="introduction.html#winw-example"><i class="fa fa-check"></i><b>3.1.2</b> Wine and Life Expectancy</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction.html"><a href="introduction.html#burger-demand"><i class="fa fa-check"></i><b>3.1.3</b> Burger Demand</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model"><i class="fa fa-check"></i><b>4.1</b> Model</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>4.2</b> Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#other-estimated-quantites"><i class="fa fa-check"></i><b>4.2.1</b> Other estimated quantites</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#properties-of-the-estimates"><i class="fa fa-check"></i><b>4.3</b> Properties of the Estimates</a></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-and-standarizing-the-data"><i class="fa fa-check"></i><b>4.4</b> Centering and Standarizing the Data</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independent-variable-centered"><i class="fa fa-check"></i><b>4.4.1</b> Independent variable centered</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#both-variables-centered"><i class="fa fa-check"></i><b>4.4.2</b> Both Variables centered</a></li>
<li class="chapter" data-level="4.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independent-and-dependent-variable-standardized"><i class="fa fa-check"></i><b>4.4.3</b> Independent and dependent variable standardized</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>4.5</b> Coefficient of Determination</a></li>
<li class="chapter" data-level="4.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residual-analysis"><i class="fa fa-check"></i><b>4.6</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#non-linear-regression-function"><i class="fa fa-check"></i><b>4.6.1</b> Non-linear regression function</a></li>
<li class="chapter" data-level="4.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#heteroscedasticity"><i class="fa fa-check"></i><b>4.6.2</b> Heteroscedasticity</a></li>
<li class="chapter" data-level="4.6.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outliers"><i class="fa fa-check"></i><b>4.6.3</b> Outliers</a></li>
<li class="chapter" data-level="4.6.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#variables-ommited"><i class="fa fa-check"></i><b>4.6.4</b> Variables Ommited</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#cross-validation"><i class="fa fa-check"></i><b>4.7</b> Cross-Validation</a></li>
<li class="chapter" data-level="4.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>4.8</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="4.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-in-matrix-form"><i class="fa fa-check"></i><b>4.9</b> Model in Matrix Form</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares-in-matrix-form"><i class="fa fa-check"></i><b>4.9.1</b> Weighted Least Squares in Matrix Form</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="polynomial-regression.html"><a href="polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Polynomial Regression</a></li>
<li class="chapter" data-level="6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#example-8"><i class="fa fa-check"></i><b>6.2</b> Example</a></li>
<li class="chapter" data-level="6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#least-squares-estimation-1"><i class="fa fa-check"></i><b>6.3</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#properties-of-the-estimates-1"><i class="fa fa-check"></i><b>6.4</b> Properties of the Estimates</a></li>
<li class="chapter" data-level="6.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-r2"><i class="fa fa-check"></i><b>6.5</b> Multiple <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="6.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#geometric-interpretation-of-multiple-linear-regression"><i class="fa fa-check"></i><b>6.6</b> Geometric Interpretation of Multiple Linear Regression</a></li>
<li class="chapter" data-level="6.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#centered-and-standarized-variables"><i class="fa fa-check"></i><b>6.7</b> Centered and Standarized Variables</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#centered-variables"><i class="fa fa-check"></i><b>6.7.1</b> Centered Variables</a></li>
<li class="chapter" data-level="6.7.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sample-covariance"><i class="fa fa-check"></i><b>6.7.2</b> Sample Covariance</a></li>
<li class="chapter" data-level="6.7.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#satandard-variables"><i class="fa fa-check"></i><b>6.7.3</b> Satandard Variables</a></li>
<li class="chapter" data-level="6.7.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sample-correlation-matrix"><i class="fa fa-check"></i><b>6.7.4</b> Sample Correlation Matrix</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#variable-cross-effects"><i class="fa fa-check"></i><b>6.8</b> Variable Cross-Effects</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#single-variable-cross-effects"><i class="fa fa-check"></i><b>6.8.1</b> Single Variable Cross-Effects</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#outliers-and-leverage"><i class="fa fa-check"></i><b>6.9</b> Outliers and Leverage</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#leverage"><i class="fa fa-check"></i><b>6.9.1</b> Leverage</a></li>
<li class="chapter" data-level="6.9.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#outliers-identification"><i class="fa fa-check"></i><b>6.9.2</b> Outliers identification</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="prerequisites" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Prerequisites<a href="prerequisites.html#prerequisites" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Before diving into the course, it’s important to have a solid understanding of the following foundational concepts. These are categorized into five key topics:</p>
<ul>
<li><a href="prerequisites.html#general-math">General Math</a></li>
<li><a href="prerequisites.html#linear-algebra">Linear Algebra</a></li>
<li><a href="prerequisites.html#probability">Probability</a></li>
<li><a href="prerequisites.html#statistics">Statistics</a></li>
<li><a href="prerequisites.html#calculus">Calculus</a></li>
</ul>
<p>You can check some of the requirements on Chapter 1 of the textbook.</p>
<div id="general-math" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> General Math<a href="prerequisites.html#general-math" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You should be familiar with the <strong>summation operator</strong> <span class="math inline">\(\sum\)</span>. This operator is defined as follows:</p>
<p><span class="math display">\[\sum_{i=1}^n x_i = x_1 + x_2 + \ldots + x_n \]</span></p>
<p>Key properties of the summation operator include:</p>
<ul>
<li><p><strong>Linearity</strong>:
<span class="math display">\[\sum_{i=1}^N (a + b x_i) = aN + b \sum_{i=1}^N x_i\]</span></p></li>
<li><p><strong>Additivity</strong>:
<span class="math display">\[\sum_{i=1}^N (x_i + y_i) = \sum_{i=1}^N x_i + \sum_{i=1}^N y_i\]</span></p></li>
</ul>
</div>
<div id="linear-algebra" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Linear Algebra<a href="prerequisites.html#linear-algebra" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You should be familiar with the following linear algebra concepts:</p>
<ul>
<li><a href="prerequisites.html#linear-independence">Linear Independence</a></li>
<li><a href="prerequisites.html#column-space-of-a-matrix">Column Space of a Matrix</a></li>
<li><a href="prerequisites.html#rank-of-a-matrix">Rank of a Matrix</a></li>
<li><a href="prerequisites.html#full-rank-matrix">Full Rank Matrix</a></li>
<li><a href="prerequisites.html#inverse-matrix">Inverse Matrix</a></li>
<li><a href="prerequisites.html#positive-definite-matrix">Positive Definite Matrix</a></li>
<li><a href="prerequisites.html#singular-value-decomposition">Singular Value Decomposition</a></li>
<li><a href="prerequisites.html#eigendecomposition">Eigendecomposition</a></li>
<li><a href="prerequisites.html#idempotent-matrix">Idempotent Matrix</a></li>
<li>Determinants</li>
<li>Eigenvalues and Eigenvectors</li>
<li>Diagonalization</li>
<li>Vector Spaces</li>
<li>Linear Transformations</li>
</ul>
<div id="linear-independence" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Linear Independence<a href="prerequisites.html#linear-independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Linear independence</strong> is a fundamental concept in linear algebra that describes a set of vectors where no vector can be written as a linear combination of the others. In other words, the vectors are not “redundant,” meaning none of the vectors depends on any other in the set.</p>
<div id="definition" class="section level4 hasAnchor" number="2.2.1.1">
<h4><span class="header-section-number">2.2.1.1</span> Definition:<a href="prerequisites.html#definition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A set of vectors <span class="math inline">\(\{ \mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n \}\)</span> in a vector space is <strong>linearly independent</strong> if the only solution to the equation:</p>
<p><span class="math display">\[
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n = \mathbf{0}
\]</span></p>
<p>is when all the scalar coefficients <span class="math inline">\(c_1, c_2, \ldots, c_n\)</span> are zero, i.e., <span class="math inline">\(c_1 = c_2 = \cdots = c_n = 0\)</span>.</p>
<p>If any of the coefficients can be non-zero while still satisfying this equation, then the vectors are <strong>linearly dependent</strong>.</p>
</div>
<div id="example" class="section level4 hasAnchor" number="2.2.1.2">
<h4><span class="header-section-number">2.2.1.2</span> Example:<a href="prerequisites.html#example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consider two vectors in <span class="math inline">\(\mathbb{R}^2\)</span>:</p>
<p><span class="math display">\[
\mathbf{v}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad \mathbf{v}_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}
\]</span></p>
<p>These vectors are <strong>linearly independent</strong> because there is no way to express one as a multiple of the other. The only solution to:</p>
<p><span class="math display">\[
c_1 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + c_2 \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
\]</span></p>
<p>is <span class="math inline">\(c_1 = 0\)</span> and <span class="math inline">\(c_2 = 0\)</span>.</p>
<p>In contrast, if:</p>
<p><span class="math display">\[
\mathbf{v}_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad \mathbf{v}_2 = \begin{bmatrix} 2 \\ 4 \end{bmatrix}
\]</span></p>
<p>These vectors are <strong>linearly dependent</strong>, because <span class="math inline">\(\mathbf{v}_2 = 2 \mathbf{v}_1\)</span>. Therefore, you can express <span class="math inline">\(mathbf{v}_2\)</span> as a linear combination of <span class="math inline">\(\mathbf{v}_1\)</span>.</p>
</div>
<div id="key-points" class="section level4 hasAnchor" number="2.2.1.3">
<h4><span class="header-section-number">2.2.1.3</span> Key Points:<a href="prerequisites.html#key-points" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Linearly independent</strong> vectors carry distinct information and cannot be derived from each other.</li>
<li><strong>Linearly dependent</strong> vectors are redundant because one or more can be expressed as a combination of others.</li>
<li>In a set of linearly independent vectors, removing any vector would reduce the span of the vector space they cover.</li>
</ul>
</div>
<div id="importance" class="section level4 hasAnchor" number="2.2.1.4">
<h4><span class="header-section-number">2.2.1.4</span> Importance:<a href="prerequisites.html#importance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Linear independence is crucial in determining the <strong>rank</strong> of a matrix.</li>
<li>In systems of equations, linear independence of the rows or columns determines if the system has a unique solution.</li>
<li>In vector spaces, the <strong>dimension</strong> of the space is the maximum number of linearly independent vectors.</li>
</ul>
</div>
</div>
<div id="column-space-of-a-matrix" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Column Space of a Matrix<a href="prerequisites.html#column-space-of-a-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>column space</strong> of a matrix is the set of all possible linear combinations of its columns. If you have a matrix <span class="math inline">\(\mathbf{A}\)</span> with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(p\)</span> columns, the column space of <span class="math inline">\(\mathbf{A}\)</span>, denoted as <strong>Col(<span class="math inline">\(\mathbf{A}\)</span>)</strong>, consists of all vectors in <span class="math inline">\(\mathbb{R}^n\)</span> that can be expressed as a linear combination of the columns of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<div id="definition-1" class="section level4 hasAnchor" number="2.2.2.1">
<h4><span class="header-section-number">2.2.2.1</span> Definition:<a href="prerequisites.html#definition-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Given a matrix <span class="math inline">\(\mathbf{A}\)</span> with columns <span class="math inline">\(\mathbf{a}_1, \mathbf{a}_2, \dots, \mathbf{a}_p\)</span>, the column space of <span class="math inline">\(\mathbf{A}\)</span> is defined as:</p>
<p><span class="math display">\[
\text{Col}(\mathbf{A}) = \left\{ \mathbf{y} \in \mathbb{R}^n \mid \mathbf{y} = \mathbf{A} \mathbf{c} \text{ for some } \mathbf{c} \in \mathbb{R}^p \right\}
\]</span></p>
<p>This means the column space is the span of the columns of <span class="math inline">\(\mathbf{A}\)</span>, or equivalently, all vectors that can be written as <span class="math inline">\(\mathbf{y} = c_1 \mathbf{a}_1 + c_2 \mathbf{a}_2 + \dots + c_p \mathbf{a}_p\)</span>, where <span class="math inline">\(c_1, c_2, \dots, c_p\)</span> are scalars.</p>
</div>
<div id="properties" class="section level4 hasAnchor" number="2.2.2.2">
<h4><span class="header-section-number">2.2.2.2</span> Properties:<a href="prerequisites.html#properties" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The column space of <span class="math inline">\(\mathbf{A}\)</span> is a <strong>subspace</strong> of <span class="math inline">\(\mathbb{R}^n\)</span>.</li>
<li>The <strong>dimension</strong> of the column space of <span class="math inline">\(\mathbf{A}\)</span> is called the <strong>rank</strong> of the matrix and corresponds to the number of linearly independent columns in <span class="math inline">\(\mathbf{A}\)</span>.</li>
<li>The column space provides valuable information about the linear independence and span of the columns of a matrix.</li>
</ul>
</div>
<div id="geometric-interpretation" class="section level4 hasAnchor" number="2.2.2.3">
<h4><span class="header-section-number">2.2.2.3</span> Geometric Interpretation:<a href="prerequisites.html#geometric-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In geometric terms, the column space represents the set of all possible vectors that can be “reached” by linearly combining the columns of the matrix. For example:
- For a matrix with 2 columns in <span class="math inline">\(\mathbb{R}^3\)</span>, the column space will be a plane in <span class="math inline">\(\mathbb{R}^3\)</span> if the columns are linearly independent.
- For a matrix with 3 columns in <span class="math inline">\(\mathbb{R}^2\)</span>, the column space will span all of <span class="math inline">\(\mathbb{R}^2\)</span> (if the columns are linearly independent) or a line (if they are dependent).</p>
</div>
</div>
<div id="rank-of-a-matrix" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Rank of a Matrix<a href="prerequisites.html#rank-of-a-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>rank</strong> of a matrix is the dimension of its column space, which is the number of linearly independent columns in the matrix. Alternatively, it is also the dimension of the row space, which is the number of linearly independent rows.</p>
<div id="definition-2" class="section level4 hasAnchor" number="2.2.3.1">
<h4><span class="header-section-number">2.2.3.1</span> Definition:<a href="prerequisites.html#definition-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a matrix <span class="math inline">\(\mathbf{A}\)</span>, the rank is defined as:</p>
<p><span class="math display">\[
\text{rank}(\mathbf{A}) = \dim(\text{Col}(\mathbf{A})) = \dim(\text{Row}(\mathbf{A}))
\]</span></p>
<p>This is the maximum number of linearly independent rows or columns in the matrix. In other words, it tells you how many of the matrix’s columns (or rows) are not redundant and cannot be written as a linear combination of the others.</p>
</div>
<div id="key-points-1" class="section level4 hasAnchor" number="2.2.3.2">
<h4><span class="header-section-number">2.2.3.2</span> Key Points:<a href="prerequisites.html#key-points-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The rank of a matrix <span class="math inline">\(\mathbf{A}\)</span> is denoted as <strong>rank(<span class="math inline">\(\mathbf{A}\)</span>)</strong>.</li>
<li>It measures the number of independent directions in the column space or row space.</li>
<li><strong>Full rank</strong>: A matrix is said to have full rank if its rank is equal to the smaller of the number of rows or columns. For an <span class="math inline">\(m \times n\)</span> matrix:
<ul>
<li>If <span class="math inline">\(\text{rank}(\mathbf{A}) = m\)</span> (number of rows), it has full row rank.</li>
<li>If <span class="math inline">\(\text{rank}(\mathbf{A}) = n\)</span> (number of columns), it has full column rank.</li>
</ul></li>
<li><strong>Rank-deficient</strong>: If the rank of the matrix is less than the smaller of the number of rows or columns, the matrix is called rank-deficient, meaning that some of its rows or columns are linearly dependent.</li>
</ul>
</div>
<div id="example-1" class="section level4 hasAnchor" number="2.2.3.3">
<h4><span class="header-section-number">2.2.3.3</span> Example:<a href="prerequisites.html#example-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consider the matrix:
<span class="math display">\[
\mathbf{A} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{bmatrix}
\]</span></p>
<p>The rank of <span class="math inline">\(\mathbf{A}\)</span> is 2 because two of the rows (or columns) are linearly independent, but the third row (or column) is a linear combination of the others.</p>
</div>
<div id="properties-1" class="section level4 hasAnchor" number="2.2.3.4">
<h4><span class="header-section-number">2.2.3.4</span> Properties:<a href="prerequisites.html#properties-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The rank of a matrix is always less than or equal to the minimum of the number of rows and columns:
<span class="math display">\[ \text{rank}(\mathbf{A}) \leq \min(m, n) \]</span></li>
<li>The rank of a matrix is equal to the number of non-zero singular values in its singular value decomposition (SVD).</li>
<li>In square matrices, the rank gives insight into whether the matrix is <strong>invertible</strong>. A square matrix is invertible if and only if it has full rank.</li>
</ul>
</div>
</div>
<div id="full-rank-matrix" class="section level3 hasAnchor" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Full Rank Matrix<a href="prerequisites.html#full-rank-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <strong>full rank matrix</strong> is a matrix in which the rank is equal to the largest possible value for that matrix, meaning:</p>
<ul>
<li>For an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span>, the rank is the maximum number of linearly independent rows or columns.
<ul>
<li>If the rank is equal to <span class="math inline">\(m\)</span> (the number of rows), the matrix has <strong>full row rank</strong>.</li>
<li>If the rank is equal to <span class="math inline">\(n\)</span> (the number of columns), the matrix has <strong>full column rank</strong>.</li>
</ul></li>
</ul>
<div id="for-a-square-matrix-m-n" class="section level4 hasAnchor" number="2.2.4.1">
<h4><span class="header-section-number">2.2.4.1</span> For a square matrix (<span class="math inline">\(m = n\)</span>):<a href="prerequisites.html#for-a-square-matrix-m-n" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>A square matrix is <strong>full rank</strong> if its rank is equal to its dimension, i.e., if the matrix is invertible.</li>
<li>In this case, <span class="math inline">\(\text{rank}(\mathbf{A}) = n\)</span>, meaning all rows and columns are linearly independent, and the matrix has an inverse.</li>
</ul>
</div>
<div id="for-a-rectangular-matrix-m-neq-n" class="section level4 hasAnchor" number="2.2.4.2">
<h4><span class="header-section-number">2.2.4.2</span> For a rectangular matrix (<span class="math inline">\(m \neq n\)</span>):<a href="prerequisites.html#for-a-rectangular-matrix-m-neq-n" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>A matrix is <strong>full rank</strong> if the rank equals the smaller of the number of rows or columns. For an <span class="math inline">\(m \times n\)</span> matrix, the rank is at most <span class="math inline">\(\min(m, n)\)</span>.
<ul>
<li>If the matrix has full row rank, all rows are linearly independent.</li>
<li>If the matrix has full column rank, all columns are linearly independent.</li>
</ul></li>
</ul>
</div>
<div id="example-2" class="section level4 hasAnchor" number="2.2.4.3">
<h4><span class="header-section-number">2.2.4.3</span> Example:<a href="prerequisites.html#example-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consider the matrix:</p>
<p><span class="math display">\[
\mathbf{A}= \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{bmatrix}
\]</span></p>
<p>This is a <span class="math inline">\(2 \times 3\)</span> matrix. Since its two rows are linearly independent, it has <strong>full row rank</strong>, with rank = 2 (the number of rows). However, it does not have full column rank because it has only two independent rows for three columns.</p>
</div>
<div id="key-properties" class="section level4 hasAnchor" number="2.2.4.4">
<h4><span class="header-section-number">2.2.4.4</span> Key Properties:<a href="prerequisites.html#key-properties" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>A full rank matrix has <strong>no redundant rows or columns</strong> (no row or column can be written as a linear combination of others).</li>
<li>A square matrix with full rank is <strong>invertible</strong> (non-singular).</li>
<li>For a rectangular matrix, full rank implies the matrix has <strong>maximal independent information</strong> in terms of its rows or columns.</li>
</ul>
</div>
<div id="importance-1" class="section level4 hasAnchor" number="2.2.4.5">
<h4><span class="header-section-number">2.2.4.5</span> Importance:<a href="prerequisites.html#importance-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Full rank matrices are crucial in solving systems of linear equations. A system <span class="math inline">\(\mathbf{A}\mathbf{x} = \mathbf{b}\)</span> has a unique solution if <span class="math inline">\(\mathbf{A}\)</span> is a square, full rank matrix.</li>
<li>In linear algebra and machine learning, the rank provides insight into the dimensionality and the independence of the data or transformation matrix.</li>
</ul>
</div>
</div>
<div id="inverse-matrix" class="section level3 hasAnchor" number="2.2.5">
<h3><span class="header-section-number">2.2.5</span> Inverse Matrix<a href="prerequisites.html#inverse-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An <strong>inverse matrix</strong> of a square matrix <span class="math inline">\(\mathbf{A}\)</span>, denoted as <span class="math inline">\(\mathbf{A}^{-1}\)</span>, is a matrix that, when multiplied by <span class="math inline">\(\mathbf{A}\)</span>, results in the identity matrix <span class="math inline">\(I\)</span>. This relationship is expressed as:</p>
<p><span class="math display">\[
\mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1} \mathbf{A}= \mathbf{I}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{I}\)</span> is the identity matrix, and its diagonal elements are 1, with all off-diagonal elements being 0.</p>
<div id="conditions-for-a-matrix-to-have-an-inverse" class="section level4 hasAnchor" number="2.2.5.1">
<h4><span class="header-section-number">2.2.5.1</span> Conditions for a Matrix to Have an Inverse:<a href="prerequisites.html#conditions-for-a-matrix-to-have-an-inverse" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The matrix <span class="math inline">\(\mathbf{A}\)</span> must be <strong>square</strong>, meaning it has the same number of rows and columns.</li>
<li>The matrix <span class="math inline">\(\mathbf{A}\)</span> must be <strong>non-singular</strong>, meaning its <strong>determinant</strong> is non-zero (<span class="math inline">\(|\mathbf{A}| \neq 0\)</span>).</li>
</ul>
</div>
<div id="properties-of-the-inverse-matrix" class="section level4 hasAnchor" number="2.2.5.2">
<h4><span class="header-section-number">2.2.5.2</span> Properties of the Inverse Matrix:<a href="prerequisites.html#properties-of-the-inverse-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><strong>Uniqueness:</strong> If a matrix has an inverse, it is unique.</li>
<li><strong>Inverse of a Product:</strong> The inverse of the product of two matrices <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> is given by <span class="math inline">\((\mathbf{A}\mathbf{B})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}\)</span>.</li>
<li><strong>Inverse of the Inverse:</strong> <span class="math inline">\((\mathbf{A}^{-1})^{-1} = \mathbf{A}\)</span>.</li>
<li><strong>Transpose of the Inverse:</strong> <span class="math inline">\((\mathbf{A}^{-1})&#39; = (\mathbf{A}&#39;)^{-1}\)</span>.</li>
</ol>
</div>
<div id="special-case-2-by-2-matrix" class="section level4 hasAnchor" number="2.2.5.3">
<h4><span class="header-section-number">2.2.5.3</span> Special Case 2 by 2 Matrix<a href="prerequisites.html#special-case-2-by-2-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a <span class="math inline">\(2 \times 2\)</span> matrix:</p>
<p><span class="math display">\[
\mathbf{A}= \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}
\]</span></p>
<p>The inverse of <span class="math inline">\(\mathbf{A}\)</span> (if <span class="math inline">\(|\mathbf{A}|=\det(\mathbf{A}) \neq 0\)</span>) is:</p>
<p><span class="math display">\[
A^{-1} = \frac{1}{ad - bc} \begin{bmatrix} d &amp; -b \\ -c &amp; a \end{bmatrix}
\]</span></p>
<p>where <span class="math inline">\(ad - bc\)</span> is the <strong>determinant</strong> of the matrix <span class="math inline">\(\mathbf{A}\)</span>.</p>
</div>
<div id="special-case-2-by-2-block-matrix" class="section level4 hasAnchor" number="2.2.5.4">
<h4><span class="header-section-number">2.2.5.4</span> Special Case 2 by 2 Block Matrix<a href="prerequisites.html#special-case-2-by-2-block-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The inverse of a <span class="math inline">\(2 \times 2\)</span> <strong>block matrix</strong> can be expressed under certain conditions. Let’s consider a block matrix <span class="math inline">\(\mathbf{M}\)</span> of the form:</p>
<p><span class="math display">\[
\mathbf{M} =
\begin{bmatrix}
\mathbf{A} &amp; \mathbf{B} \\
\mathbf{C} &amp; \mathbf{D}
\end{bmatrix}
\]</span></p>
<p>where:
- <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{D}\)</span> are themselves square matrices, and <span class="math inline">\(\mathbf{B}\)</span> and <span class="math inline">\(\mathbf{C}\)</span> are matrices (not necessarily square).</p>
<p>Then the inverse of <span class="math inline">\(\mathbf{M}\)</span> is given by:</p>
<p><span class="math display">\[
\mathbf{M}^{-1} =
\begin{bmatrix}
\mathbf{A}^{-1} + \mathbf{A}^{-1} \mathbf{B} \mathbf{S}^{-1} \mathbf{C} \mathbf{A}^{-1} &amp; -\mathbf{A}^{-1} \mathbf{B} \mathbf{S}^{-1} \\
-\mathbf{S}^{-1} \mathbf{C} \mathbf{A}^{-1} &amp; \mathbf{S}^{-1}
\end{bmatrix}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{S}\)</span> is the <strong>Schur complement</strong> of <span class="math inline">\(\mathbf{A}\)</span> and is defined as:</p>
<p><span class="math display">\[
\mathbf{S} = \mathbf{D} - \mathbf{C} \mathbf{A}^{-1} \mathbf{B}
\]</span></p>
<div id="conditions-for-the-inverse-to-exist" class="section level5 hasAnchor" number="2.2.5.4.1">
<h5><span class="header-section-number">2.2.5.4.1</span> Conditions for the Inverse to Exist:<a href="prerequisites.html#conditions-for-the-inverse-to-exist" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><span class="math inline">\(\mathbf{A}\)</span> must be invertible,</li>
<li>The Schur complement <span class="math inline">\(\mathbf{S}\)</span> must also be invertible.</li>
</ul>
</div>
<div id="explanation-of-the-terms" class="section level5 hasAnchor" number="2.2.5.4.2">
<h5><span class="header-section-number">2.2.5.4.2</span> Explanation of the Terms:<a href="prerequisites.html#explanation-of-the-terms" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><span class="math inline">\(\mathbf{A}^{-1}\)</span>: The inverse of matrix <span class="math inline">\(\mathbf{A}\)</span>,</li>
<li><span class="math inline">\(\mathbf{S}^{-1}\)</span>: The inverse of the Schur complement <span class="math inline">\(\mathbf{S}\)</span>, which can be interpreted as the “effective” part of matrix <span class="math inline">\(\mathbf{D}\)</span> once the contribution of <span class="math inline">\(\mathbf{A}\)</span> has been removed.</li>
</ul>
<p>This formula generalizes the concept of inverting a matrix when it’s partitioned into blocks.</p>
</div>
</div>
<div id="sherman-morrison-formula" class="section level4 hasAnchor" number="2.2.5.5">
<h4><span class="header-section-number">2.2.5.5</span> Sherman-Morrison Formula<a href="prerequisites.html#sherman-morrison-formula" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <strong>Sherman-Morrison formula</strong> provides a way to compute the inverse of a matrix after it has been updated by a rank-one modification. Specifically, it addresses the situation where a matrix <strong>A</strong> has been updated by the outer product of two vectors <strong>u</strong> and <strong>v</strong>. The formula is:</p>
<p><span class="math display">\[
(\mathbf{A} + \mathbf{u} \mathbf{v}^T)^{-1} = \mathbf{A}^{-1} - \frac{\mathbf{A}^{-1} \mathbf{u} \mathbf{v}^T \mathbf{A}^{-1}}{1 + \mathbf{v}^T \mathbf{A}^{-1} \mathbf{u}}
\]</span></p>
<div id="requirements" class="section level5 hasAnchor" number="2.2.5.5.1">
<h5><span class="header-section-number">2.2.5.5.1</span> Requirements:<a href="prerequisites.html#requirements" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>A</strong> must be an invertible matrix.</li>
<li>The scalar <span class="math inline">\(1 + \mathbf{v}^T \mathbf{A}^{-1} \mathbf{u}\)</span> must not be zero.</li>
</ul>
</div>
<div id="explanation-of-the-terms-1" class="section level5 hasAnchor" number="2.2.5.5.2">
<h5><span class="header-section-number">2.2.5.5.2</span> Explanation of the terms:<a href="prerequisites.html#explanation-of-the-terms-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>A</strong> is an invertible <span class="math inline">\(n \times n\)</span> matrix.</li>
<li><strong>u</strong> and <strong>v</strong> are <span class="math inline">\(n \times 1\)</span> column vectors.</li>
<li>The outer product <span class="math inline">\(\mathbf{u} \mathbf{v}^T\)</span> is an <span class="math inline">\(n \times n\)</span> rank-one matrix.</li>
<li>The term <span class="math inline">\(1 + \mathbf{v}^T \mathbf{A}^{-1} \mathbf{u}\)</span> is a scalar.</li>
</ul>
<p>This formula is useful in situations where you need to efficiently update the inverse of a matrix after a low-rank modification, rather than recomputing the inverse from scratch.</p>
</div>
</div>
</div>
<div id="positive-definite-matrix" class="section level3 hasAnchor" number="2.2.6">
<h3><span class="header-section-number">2.2.6</span> Positive Definite Matrix<a href="prerequisites.html#positive-definite-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <strong>positive definite matrix</strong> is a symmetric matrix <span class="math inline">\(\mathbf{A}\)</span> where, for any non-zero vector <span class="math inline">\(\mathbf{x}\)</span>, the following condition holds:</p>
<p><span class="math display">\[
\mathbf{x}&#39; \mathbf{A}\mathbf{x} &gt; 0
\]</span></p>
<div id="key-properties-1" class="section level4 hasAnchor" number="2.2.6.1">
<h4><span class="header-section-number">2.2.6.1</span> Key Properties:<a href="prerequisites.html#key-properties-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><strong>Symmetry:</strong> The matrix <span class="math inline">\(\mathbf{A}\)</span> must be symmetric, meaning <span class="math inline">\(\mathbf{A}= \mathbf{A}&#39;\)</span>.</li>
<li><strong>Positive quadratic form:</strong> For any non-zero vector <span class="math inline">\(\mathbf{x}\)</span>, the quadratic form <span class="math inline">\(\mathbf{x}&#39; \mathbf{A}\mathbf{x}\)</span> must yield a positive value.</li>
</ol>
</div>
<div id="characteristics-of-a-positive-definite-matrix" class="section level4 hasAnchor" number="2.2.6.2">
<h4><span class="header-section-number">2.2.6.2</span> Characteristics of a Positive Definite Matrix:<a href="prerequisites.html#characteristics-of-a-positive-definite-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>All the <strong>eigenvalues</strong> of a positive definite matrix are <strong>positive</strong>.</li>
<li>The <strong>determinants</strong> of the leading principal minors (submatrices) of the matrix are positive.</li>
<li>The <strong>diagonal elements</strong> of a positive definite matrix are positive.</li>
</ul>
</div>
<div id="example-3" class="section level4 hasAnchor" number="2.2.6.3">
<h4><span class="header-section-number">2.2.6.3</span> Example:<a href="prerequisites.html#example-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The matrix:
<span class="math display">\[
\mathbf{A}= \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}
\]</span>
is positive definite, because for any non-zero vector <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(\mathbf{x}&#39; \mathbf{A}\mathbf{x} &gt; 0\)</span>. For instance, if <span class="math inline">\(\mathbf{x} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span>, then:</p>
<p><span class="math display">\[
\mathbf{x}&#39; \mathbf{A}\mathbf{x} = \begin{bmatrix} 1 &amp; 1 \end{bmatrix} \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = 6 &gt; 0
\]</span></p>
</div>
</div>
<div id="singular-value-decomposition" class="section level3 hasAnchor" number="2.2.7">
<h3><span class="header-section-number">2.2.7</span> Singular Value Decomposition<a href="prerequisites.html#singular-value-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Singular Value Decomposition (SVD)</strong> is a fundamental matrix factorization technique used in linear algebra to break down a matrix into three distinct components. It provides valuable insight into the structure of a matrix and is widely used in applications like data compression, signal processing, and dimensionality reduction.</p>
<div id="definition-3" class="section level4 hasAnchor" number="2.2.7.1">
<h4><span class="header-section-number">2.2.7.1</span> Definition:<a href="prerequisites.html#definition-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For any real (or complex) matrix <span class="math inline">\(\mathbf{A}\)</span> of size <span class="math inline">\(m \times n\)</span>, the Singular Value Decomposition is given by:</p>
<p><span class="math display">\[
\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;
\]</span></p>
<p>where:
- <span class="math inline">\(\mathbf{U}\)</span> is an <span class="math inline">\(m \times m\)</span> orthogonal matrix, whose columns are called the <strong>left singular vectors</strong>.
- <span class="math inline">\(\mathbf{\Sigma}\)</span> is an <span class="math inline">\(m \times n\)</span> diagonal matrix, where the diagonal entries are the <strong>singular values</strong> of <span class="math inline">\(\mathbf{A}\)</span>. The singular values are always non-negative and arranged in decreasing order.
- <span class="math inline">\(\mathbf{V}\)</span> is an <span class="math inline">\(n \times n\)</span> orthogonal matrix, whose columns are called the <strong>right singular vectors</strong>.</p>
</div>
<div id="interpretation-of-the-components" class="section level4 hasAnchor" number="2.2.7.2">
<h4><span class="header-section-number">2.2.7.2</span> Interpretation of the Components:<a href="prerequisites.html#interpretation-of-the-components" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><span class="math inline">\(\mathbf{U}\)</span> represents the orthonormal basis for the <strong>column space</strong> of <span class="math inline">\(\mathbf{A}\)</span>.</li>
<li><span class="math inline">\(\mathbf{V}\)</span> represents the orthonormal basis for the <strong>row space</strong> of <span class="math inline">\(\mathbf{A}\)</span>.</li>
<li><span class="math inline">\(\mathbf{\Sigma}\)</span> contains the singular values, which provide information about the importance or magnitude of the corresponding singular vectors. Large singular values indicate directions with significant data spread, while small or zero singular values correspond to directions with little or no data variation.</li>
</ul>
</div>
<div id="geometric-interpretation-1" class="section level4 hasAnchor" number="2.2.7.3">
<h4><span class="header-section-number">2.2.7.3</span> Geometric Interpretation:<a href="prerequisites.html#geometric-interpretation-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>SVD can be viewed geometrically as a transformation where:
1. <span class="math inline">\(\mathbf{V}\)</span> applies a rotation or reflection in the input space.
2. <span class="math inline">\(\mathbf{\Sigma}\)</span> stretches or compresses the data along certain axes.
3. <span class="math inline">\(\mathbf{U}\)</span> applies a final rotation or reflection in the output space.</p>
</div>
<div id="key-points-2" class="section level4 hasAnchor" number="2.2.7.4">
<h4><span class="header-section-number">2.2.7.4</span> Key Points:<a href="prerequisites.html#key-points-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Rank</strong>: The number of non-zero singular values in <span class="math inline">\(\mathbf{\Sigma}\)</span> equals the rank of the matrix <span class="math inline">\(\mathbf{A}\)</span>.</li>
<li><strong>Dimensionality Reduction</strong>: By truncating small singular values in <span class="math inline">\(\mathbf{\Sigma}\)</span>, we can approximate <span class="math inline">\(\mathbf{A}\)</span> with a lower-rank matrix, which is useful in compressing data while retaining most of its structure.</li>
<li><strong>Condition Number</strong>: The ratio of the largest to the smallest non-zero singular value gives the condition number of the matrix, which indicates how sensitive a matrix is to numerical errors or perturbations.</li>
</ul>
</div>
<div id="example-4" class="section level4 hasAnchor" number="2.2.7.5">
<h4><span class="header-section-number">2.2.7.5</span> Example:<a href="prerequisites.html#example-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a matrix <span class="math inline">\(\mathbf{A}\)</span> of size <span class="math inline">\(3 \times 2\)</span>, the SVD would look like:</p>
<p><span class="math display">\[
\mathbf{A} = \mathbf{U}
\begin{bmatrix}
\sigma_1 &amp; 0 \\
0 &amp; \sigma_2 \\
0 &amp; 0
\end{bmatrix}
\mathbf{V}&#39;
\]</span></p>
<p>where <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span> are the singular values, and <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are orthogonal matrices.</p>
</div>
<div id="applications-of-svd" class="section level4 hasAnchor" number="2.2.7.6">
<h4><span class="header-section-number">2.2.7.6</span> Applications of SVD:<a href="prerequisites.html#applications-of-svd" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Dimensionality Reduction</strong>: SVD is widely used in Principal Component Analysis (PCA) for reducing the dimensionality of large datasets.</li>
<li><strong>Low-Rank Approximations</strong>: In data compression, SVD helps to approximate matrices with fewer dimensions while maintaining the core structure.</li>
<li><strong>Solving Linear Systems</strong>: In cases where a matrix is close to singular, SVD can be used to solve linear systems more stably.</li>
<li><strong>Latent Semantic Analysis (LSA)</strong>: In natural language processing, SVD is used to reduce the dimensionality of word-document matrices to capture latent relationships between words and documents.</li>
</ul>
</div>
</div>
<div id="eigendecomposition" class="section level3 hasAnchor" number="2.2.8">
<h3><span class="header-section-number">2.2.8</span> Eigendecomposition<a href="prerequisites.html#eigendecomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Eigendecomposition</strong> is a matrix factorization technique used in linear algebra, where a square matrix is decomposed into its eigenvalues and eigenvectors. It is applicable to square matrices and provides deep insight into the matrix’s structure, particularly in understanding transformations, systems of linear equations, and differential equations.</p>
<div id="definition-4" class="section level4 hasAnchor" number="2.2.8.1">
<h4><span class="header-section-number">2.2.8.1</span> Definition:<a href="prerequisites.html#definition-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Given a square matrix <span class="math inline">\(\mathbf{A}\)</span> of size <span class="math inline">\(n \times n\)</span>, eigendecomposition is a factorization of the matrix into the following form:</p>
<p><span class="math display">\[
\mathbf{A} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^{-1}
\]</span></p>
<p>where:
- <span class="math inline">\(\mathbf{V}\)</span> is the matrix of <strong>eigenvectors</strong> of <span class="math inline">\(\mathbf{A}\)</span>, and each column of <span class="math inline">\(\mathbf{V}\)</span> is an eigenvector.
- <span class="math inline">\(\mathbf{\Lambda}\)</span> is a diagonal matrix of <strong>eigenvalues</strong> of <span class="math inline">\(\mathbf{A}\)</span>, with each diagonal element corresponding to an eigenvalue of <span class="math inline">\(\mathbf{A}\)</span>.
- <span class="math inline">\(\mathbf{V}^{-1}\)</span> is the inverse of the matrix of eigenvectors.</p>
</div>
<div id="eigenvalues-and-eigenvectors" class="section level4 hasAnchor" number="2.2.8.2">
<h4><span class="header-section-number">2.2.8.2</span> Eigenvalues and Eigenvectors:<a href="prerequisites.html#eigenvalues-and-eigenvectors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p><strong>Eigenvalue</strong> (<span class="math inline">\(\lambda\)</span>): A scalar <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(\mathbf{A}\)</span> if there exists a non-zero vector <span class="math inline">\(\mathbf{v}\)</span> such that:</p>
<p><span class="math display">\[
\mathbf{A} \mathbf{v} = \lambda \mathbf{v}
\]</span></p>
<p>In this case, <span class="math inline">\(\mathbf{v}\)</span> is called the eigenvector corresponding to the eigenvalue <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p><strong>Eigenvector</strong>: A non-zero vector <span class="math inline">\(\mathbf{v}\)</span> that remains parallel to itself (i.e., only scaled) when multiplied by <span class="math inline">\(\mathbf{A}\)</span> is called an eigenvector.</p></li>
</ul>
</div>
<div id="conditions-for-eigendecomposition" class="section level4 hasAnchor" number="2.2.8.3">
<h4><span class="header-section-number">2.2.8.3</span> Conditions for Eigendecomposition:<a href="prerequisites.html#conditions-for-eigendecomposition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>A matrix <span class="math inline">\(\mathbf{A}\)</span> is <strong>diagonalizable</strong> (i.e., it can be factored into <span class="math inline">\(\mathbf{A} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^{-1}\)</span>) if and only if it has <span class="math inline">\(n\)</span> linearly independent eigenvectors.</li>
<li>Not all matrices are diagonalizable. However, if <span class="math inline">\(\mathbf{A}\)</span> has <span class="math inline">\(n\)</span> distinct eigenvalues, then it is guaranteed to be diagonalizable.</li>
<li>Symmetric matrices are always diagonalizable.</li>
</ul>
</div>
<div id="geometric-interpretation-2" class="section level4 hasAnchor" number="2.2.8.4">
<h4><span class="header-section-number">2.2.8.4</span> Geometric Interpretation:<a href="prerequisites.html#geometric-interpretation-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Eigendecomposition reveals the directions (eigenvectors) along which the matrix transformation <span class="math inline">\(\mathbf{A}\)</span> acts as a simple scaling by the eigenvalues. Geometrically:
- Eigenvectors point in directions that remain invariant under the transformation by <span class="math inline">\(\mathbf{A}\)</span>.
- The corresponding eigenvalues tell us how much the matrix stretches or compresses vectors in the direction of those eigenvectors.</p>
</div>
<div id="example-5" class="section level4 hasAnchor" number="2.2.8.5">
<h4><span class="header-section-number">2.2.8.5</span> Example:<a href="prerequisites.html#example-5" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a matrix <span class="math inline">\(\mathbf{A}\)</span>:
<span class="math display">\[
\mathbf{A} = \begin{bmatrix} 4 &amp; 1 \\ 2 &amp; 3 \end{bmatrix}
\]</span>
The eigenvalues <span class="math inline">\(\lambda_1 = 5\)</span> and <span class="math inline">\(\lambda_2 = 2\)</span> can be found by solving the characteristic equation <span class="math inline">\(\det(\mathbf{A} - \lambda \mathbf{I}) = 0\)</span>. Corresponding eigenvectors can then be computed, allowing the matrix to be diagonalized as:</p>
<p><span class="math display">\[
\mathbf{A} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^{-1}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{\Lambda} = \text{diag}(5, 2)\)</span> and <span class="math inline">\(\mathbf{V}\)</span> is the matrix of eigenvectors.</p>
</div>
<div id="applications-of-eigendecomposition" class="section level4 hasAnchor" number="2.2.8.6">
<h4><span class="header-section-number">2.2.8.6</span> Applications of Eigendecomposition:<a href="prerequisites.html#applications-of-eigendecomposition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Diagonalization</strong>: Eigendecomposition allows matrices to be diagonalized, simplifying many computations (such as raising matrices to powers).</li>
<li><strong>Principal Component Analysis (PCA)</strong>: In data science, eigendecomposition is used in PCA to find directions of maximum variance in data.</li>
<li><strong>Solving Differential Equations</strong>: Eigenvalues and eigenvectors are useful in solving systems of linear differential equations.</li>
<li><strong>Quantum Mechanics</strong>: In physics, eigenvalues and eigenvectors describe the measurable properties (like energy levels) of systems.</li>
</ul>
<p>In summary, eigendecomposition is a powerful tool in linear algebra that provides insight into how a matrix transforms space, offering valuable properties through its eigenvalues and eigenvectors.</p>
</div>
</div>
<div id="idempotent-matrix" class="section level3 hasAnchor" number="2.2.9">
<h3><span class="header-section-number">2.2.9</span> Idempotent Matrix<a href="prerequisites.html#idempotent-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An <strong>idempotent matrix</strong> is a matrix that, when multiplied by itself, yields the same matrix. In other words, a matrix <span class="math inline">\(\mathbf{M}\)</span> is idempotent if it satisfies the condition:</p>
<p><span class="math display">\[
\mathbf{M}^2 = \mathbf{M}
\]</span></p>
<div id="key-properties-of-idempotent-matrices" class="section level4 hasAnchor" number="2.2.9.1">
<h4><span class="header-section-number">2.2.9.1</span> Key Properties of Idempotent Matrices:<a href="prerequisites.html#key-properties-of-idempotent-matrices" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><p><strong>Eigenvalues</strong>: The eigenvalues of an idempotent matrix are either 0 or 1. This is because for an eigenvector <span class="math inline">\(\mathbf{v}\)</span> with eigenvalue <span class="math inline">\(\lambda\)</span>, the equation <span class="math inline">\(\mathbf{M}^2 \mathbf{v} = \mathbf{M} \mathbf{v}\)</span> simplifies to <span class="math inline">\(\lambda^2 \mathbf{v} = \lambda \mathbf{v}\)</span>, meaning <span class="math inline">\(\lambda(\lambda - 1) = 0\)</span>, so <span class="math inline">\(\lambda = 0\)</span> or <span class="math inline">\(\lambda = 1\)</span>.</p></li>
<li><p><strong>Rank</strong>: The rank of an idempotent matrix <span class="math inline">\(\mathbf{M}\)</span> is equal to the trace of the matrix (the sum of the diagonal elements), which is also the number of eigenvalues equal to 1.</p></li>
<li><p><strong>Projection Interpretation</strong>: Idempotent matrices often represent projection matrices in linear algebra. A projection matrix projects vectors onto a subspace, and applying the projection multiple times doesn’t change the result beyond the first application, which is why it satisfies <span class="math inline">\(\mathbf{M}^2 = \mathbf{M}\)</span>.</p></li>
</ol>
</div>
<div id="examples" class="section level4 hasAnchor" number="2.2.9.2">
<h4><span class="header-section-number">2.2.9.2</span> Examples:<a href="prerequisites.html#examples" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><p><strong>Identity Matrix</strong>: The identity matrix <span class="math inline">\(\mathbf{I}\)</span> is idempotent because:
<span class="math display">\[
\mathbf{I}^2 = \mathbf{I}
\]</span></p></li>
<li><p><strong>Zero Matrix</strong>: The zero matrix <span class="math inline">\(\mathbf{0}\)</span> is also idempotent because:
<span class="math display">\[
\mathbf{0}^2 = \mathbf{0}
\]</span></p></li>
<li><p><strong>Projection Matrix</strong>: Consider a projection matrix onto the x-axis in 2D:
<span class="math display">\[
\mathbf{P} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}
\]</span>
This matrix is idempotent since:
<span class="math display">\[
\mathbf{P}^2 = \mathbf{P}
\]</span></p></li>
</ol>
</div>
<div id="use-in-statistics" class="section level4 hasAnchor" number="2.2.9.3">
<h4><span class="header-section-number">2.2.9.3</span> Use in Statistics:<a href="prerequisites.html#use-in-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Idempotent matrices are commonly used in statistics, particularly in the context of regression analysis. For example, the <strong>hat matrix</strong> <span class="math inline">\(\mathbf{H}\)</span> in linear regression, which transforms the observed values into the predicted values, is idempotent:</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top
\]</span>
where <span class="math inline">\(\mathbf{X}\)</span> is the design matrix.</p>
<p>In summary, idempotent matrices have unique properties and are frequently encountered in linear algebra, projections, and statistical applications.</p>
</div>
</div>
</div>
<div id="probability" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Probability<a href="prerequisites.html#probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Key probability concepts to understand include:</p>
<ul>
<li>Expected Value</li>
<li>Variance</li>
<li>Covariance</li>
<li>Correlation</li>
<li>Joint, Marginal, and Conditional Distributions</li>
<li>Independence</li>
<li>Central Limit Theorem</li>
<li>Distributions:
<ul>
<li>Normal</li>
<li>Chi-Squared (<span class="math inline">\(\chi^2\)</span>)</li>
<li>t-distribution</li>
<li>F-distribution</li>
</ul></li>
</ul>
</div>
<div id="statistics" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Statistics<a href="prerequisites.html#statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Essential statistical concepts include:</p>
<ul>
<li><strong>Point Estimation</strong>:
<ul>
<li>Maximum Likelihood</li>
<li>Least Squares Estimation</li>
</ul></li>
<li><strong>Properties of Point Estimators</strong>:
<ul>
<li>Unbiased</li>
<li>Consistent</li>
<li>Minimum Variance</li>
</ul></li>
<li><strong>Interval Estimation</strong></li>
<li><strong>Hypothesis Testing</strong></li>
</ul>
</div>
<div id="calculus" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Calculus<a href="prerequisites.html#calculus" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Key calculus topics include:</p>
<ul>
<li><a href="prerequisites.html#gradient">Gradient</a></li>
<li><a href="#hessian">Hessian</a></li>
<li><a href="prerequisites.html#matrix-calculus">Matrix Calculus</a></li>
<li>Optimization</li>
</ul>
<div id="gradient" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Gradient<a href="prerequisites.html#gradient" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>gradient</strong> of a function is a vector that contains the partial derivatives of the function with respect to each of its variables. It points in the direction of the steepest ascent of the function, and its magnitude indicates the rate of change in that direction.</p>
<p>For a scalar function <span class="math inline">\(f(x_1, x_2, \ldots, x_n)\)</span>, where <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> are the variables, the gradient is defined as:</p>
<p><span class="math display">\[
\nabla f = \frac{d}{d \mathbf{x}} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}
\]</span></p>
<div id="key-points-3" class="section level4 hasAnchor" number="2.5.1.1">
<h4><span class="header-section-number">2.5.1.1</span> Key Points:<a href="prerequisites.html#key-points-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Direction:</strong> The gradient points in the direction of the greatest increase of the function.</li>
<li><strong>Magnitude:</strong> The magnitude of the gradient represents how fast the function increases in that direction.</li>
<li><strong>Zero Gradient:</strong> If <span class="math inline">\(\nabla f = 0\)</span>, it indicates that the function has a critical point, which could be a local minimum, maximum, or saddle point.</li>
</ul>
</div>
<div id="example-6" class="section level4 hasAnchor" number="2.5.1.2">
<h4><span class="header-section-number">2.5.1.2</span> Example:<a href="prerequisites.html#example-6" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a function <span class="math inline">\(f(x, y) = x^2 + y^2\)</span>, the gradient is:</p>
<p><span class="math display">\[
\nabla f = \begin{bmatrix} \frac{\partial}{\partial x} (x^2 + y^2) \\ \frac{\partial}{\partial y} (x^2 + y^2) \end{bmatrix} = \begin{bmatrix} 2x \\ 2y \end{bmatrix}
\]</span></p>
<p>This shows that the gradient points outward from the origin, and its magnitude increases as <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> increase.</p>
</div>
<div id="applications" class="section level4 hasAnchor" number="2.5.1.3">
<h4><span class="header-section-number">2.5.1.3</span> Applications:<a href="prerequisites.html#applications" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>In <strong>optimization</strong>, the gradient is used to find the minimum or maximum of a function (e.g., in gradient descent, a common optimization algorithm).</li>
<li>In <strong>vector calculus</strong>, the gradient is used to describe the slope or rate of change of scalar fields (such as temperature, pressure, or altitude in physical applications).</li>
</ul>
</div>
</div>
<div id="hessian-matrix" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Hessian Matrix<a href="prerequisites.html#hessian-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Hessian matrix</strong> is a square matrix of second-order partial derivatives of a scalar-valued function. It describes the local curvature of a multivariable function and is used to assess the nature of critical points (i.e., whether they are minima, maxima, or saddle points).</p>
<p>For a scalar function <span class="math inline">\(f(x_1, x_2, \ldots, x_n)\)</span>, the Hessian matrix <span class="math inline">\(\mathbf{H}\)</span> is defined as:</p>
<p><span class="math display">\[
\mathbf{H}(f) = \frac{d}{d \mathbf{x} d\mathbf{x}&#39;} f(\mathbf{x}) =\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
\]</span></p>
<div id="key-properties-2" class="section level4 hasAnchor" number="2.5.2.1">
<h4><span class="header-section-number">2.5.2.1</span> Key Properties:<a href="prerequisites.html#key-properties-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The Hessian is <strong>symmetric</strong> if the second-order partial derivatives are continuous (by Clairaut’s theorem, also called Schwarz’s theorem).</li>
<li>It provides important information about the local behavior of the function, particularly around critical points where the gradient is zero.</li>
<li><strong>Eigenvalues</strong> of the Hessian matrix determine the type of critical points:
<ul>
<li>If all eigenvalues are positive, the function has a <strong>local minimum</strong>.</li>
<li>If all eigenvalues are negative, the function has a <strong>local maximum</strong>.</li>
<li>If some eigenvalues are positive and others are negative, the function has a <strong>saddle point</strong>.</li>
</ul></li>
</ul>
</div>
<div id="example-7" class="section level4 hasAnchor" number="2.5.2.2">
<h4><span class="header-section-number">2.5.2.2</span> Example:<a href="prerequisites.html#example-7" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a function <span class="math inline">\(f(x, y) = x^2 + xy + y^2\)</span>, the Hessian matrix is:</p>
<p><span class="math display">\[
\mathbf{H}(f) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x^2} &amp; \frac{\partial^2 f}{\partial x \partial y} \\
\frac{\partial^2 f}{\partial y \partial x} &amp; \frac{\partial^2 f}{\partial y^2}
\end{bmatrix}
= \begin{bmatrix}
2 &amp; 1 \\
1 &amp; 2
\end{bmatrix}
\]</span></p>
</div>
</div>
<div id="applications-1" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Applications:<a href="prerequisites.html#applications-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>In <strong>optimization</strong>, the Hessian is used to assess the convexity or concavity of a function, which helps in identifying the nature of critical points.</li>
<li>In <strong>machine learning</strong>, it is used to optimize loss functions and can be part of second-order optimization methods like Newton’s method.</li>
<li>In <strong>economics</strong> and <strong>engineering</strong>, the Hessian helps in analyzing systems involving multiple variables and understanding how they interact with each other.</li>
</ul>
</div>
<div id="matrix-calculus" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> Matrix Calculus<a href="prerequisites.html#matrix-calculus" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You need to know the following matrix calculus operations:</p>
<p><span class="math display">\[
\frac{d}{d \mathbf{x}} \left(\mathbf{c}&#39;\mathbf{x}\right)
\]</span>
<span class="math display">\[
\frac{d}{d \mathbf{x}} \left(\mathbf{x}&#39;\mathbf{A}\mathbf{x}\right)
\]</span>
<span class="math display">\[
\frac{d}{d \mathbf{x} d\mathbf{x}&#39;} \left(\mathbf{x}&#39;\mathbf{A}\mathbf{x}\right)
\]</span></p>
<p>Let <span class="math inline">\(\mathbf{c}\)</span> be a constant vector and <span class="math inline">\(\mathbf{x}\)</span> be a variable vector, both of size <span class="math inline">\(n \times 1\)</span>. We want to compute the derivative of the product:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \mathbf{c}&#39; \mathbf{x}
\]</span>
Where:
<span class="math display">\[
\mathbf{c}&#39; \mathbf{x} = \sum_{i=1}^{n} c_i x_i
\]</span></p>
<p>To differentiate <span class="math inline">\(f(\mathbf{x}) = \mathbf{c}&#39; \mathbf{x}\)</span> with respect to the variable vector <span class="math inline">\(\mathbf{x}\)</span>, we take the derivative of each component separately:</p>
<p><span class="math display">\[
\nabla f = \frac{d}{d \mathbf{x}} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix} = \begin{bmatrix} \frac{\partial }{\partial x_1} (\mathbf{c}&#39; \mathbf{x})  \\ \frac{\partial }{\partial x_2} (\mathbf{c}&#39; \mathbf{x}) \\ \vdots \\ \frac{\partial }{\partial x_n} (\mathbf{c}&#39; \mathbf{x}) \end{bmatrix} = \begin{bmatrix} \frac{\partial }{\partial x_1} \left(\sum_{i=1}^{n} c_i x_i\right)  \\ \frac{\partial }{\partial x_2} \left(\sum_{i=1}^{n} c_i x_i\right) \\ \vdots \\ \frac{\partial }{\partial x_n} \left(\sum_{i=1}^{n} c_i x_i\right) \end{bmatrix}
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{c}\)</span> is a constant vector, the derivative of each term <span class="math inline">\(c_i x_i\)</span> is simply <span class="math inline">\(c_i\)</span>, that is:</p>
<p><span class="math display">\[
\frac{d}{d x_j} \left(\sum_{i=1}^{n} c_i x_i\right) = c_j
\]</span></p>
<p>Thus, the derivative of the entire sum is the vector:</p>
<p><span class="math display">\[
\frac{d}{d \mathbf{x}} \left( \mathbf{c}&#39; \mathbf{x} \right) = \begin{bmatrix} c_1  \\ c_2 \\ \vdots \\ c_n \end{bmatrix} = \mathbf{c}
\]</span></p>
<p>Now, let’s go through the derivative of the quadratic form <span class="math inline">\(f(\mathbf{x}) = \mathbf{x}&#39; \mathbf{A}\mathbf{x}\)</span>, where:</p>
<ul>
<li><span class="math inline">\(\mathbf{x}\)</span> is a variable vector of size <span class="math inline">\(n \times 1\)</span>,</li>
<li><span class="math inline">\(\mathbf{A}\)</span> is a constant, symmetric matrix of size <span class="math inline">\(n \times n\)</span>.</li>
</ul>
<p><span class="math display">\[
f(\mathbf{x}) = \mathbf{x}&#39; \mathbf{A}\mathbf{x}
\]</span>
First, expand the quadratic form:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j
\]</span>
Then</p>
<p><span class="math display">\[
\nabla f = \frac{d}{d \mathbf{x}} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix} = \begin{bmatrix} \frac{\partial }{\partial x_1} (\mathbf{x}&#39; \mathbf{A}\mathbf{x})  \\ \frac{\partial }{\partial x_2} (\mathbf{x}&#39; \mathbf{A}\mathbf{x}) \\ \vdots \\ \frac{\partial }{\partial x_n} (\mathbf{x}&#39; \mathbf{A}\mathbf{x}) \end{bmatrix} = \begin{bmatrix} \frac{\partial }{\partial x_1} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right)  \\ \frac{\partial }{\partial x_2} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right) \\ \vdots \\ \frac{\partial }{\partial x_n} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right) \end{bmatrix}
\]</span>
For each component <span class="math inline">\(x_k\)</span> in the vector <span class="math inline">\(\mathbf{x}\)</span>, the derivative of <span class="math inline">\(f(\mathbf{x})\)</span> is:</p>
<p><span class="math display">\[
\frac{\partial}{\partial x_k} f(\mathbf{x}) = \frac{\partial}{\partial x_k} \left( \sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j \right) = \sum_{i=1}^{n} \sum_{j=1}^{n} \frac{\partial}{\partial x_k}  x_i a_{ij} x_j
\]</span></p>
<p>Each term <span class="math inline">\(x_i a_{ij} x_j\)</span> has two components that depend on <span class="math inline">\(\mathbf{x}\)</span>:</p>
<ul>
<li>If <span class="math inline">\(i = j = k\)</span>, the derivative with respect to <span class="math inline">\(x_k\)</span> is:</li>
</ul>
<p><span class="math display">\[
\frac{\partial}{\partial x_k} (x_i a_{ij} x_j) = 2 a_{kk} x_k
\]</span></p>
<ul>
<li>If <span class="math inline">\(i \neq j\)</span> and <span class="math inline">\(i = k\)</span>, the derivative with respect to <span class="math inline">\(x_k\)</span> is:</li>
</ul>
<p><span class="math display">\[
\frac{\partial}{\partial x_k} (x_i a_{ij} x_j) = a_{kj} x_j
\]</span>
- Similarly, if <span class="math inline">\(i \neq j\)</span> and <span class="math inline">\(j = k\)</span>, the derivative with respect to <span class="math inline">\(x_k\)</span> is:</p>
<p><span class="math display">\[
\frac{\partial}{\partial x_k} (x_i a_{ij} x_j) = a_{ik} x_i
\]</span>
- Finally, if <span class="math inline">\(i \neq k\)</span> and <span class="math inline">\(j \neq k\)</span>, then:</p>
<p><span class="math display">\[
\frac{\partial}{\partial x_k} (x_i a_{ij} x_j) = 0
\]</span>
Then</p>
<p><span class="math display">\[
\frac{\partial}{\partial x_k} f(\mathbf{x}) = 2 a_{kk} x_k + \sum_{i \neq k} a_{ik} x_i + \sum_{j \neq k} a_{kj} x_j
\]</span></p>
<p>Now since <span class="math inline">\(\mathbf{A}\)</span> is symmetric (<span class="math inline">\(a_{ij} = a_{ji}\)</span>), then:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial}{\partial x_k} f(\mathbf{x})
  &amp;= 2 a_{kk} x_k + \sum_{i \neq k} a_{ik} x_i + \sum_{i \neq k} a_{ik} x_i \\
  &amp;= 2 a_{kk} x_k + 2\sum_{i \neq k} a_{ik} x_i \\
  &amp;= 2 \left(\sum_{i \neq k} a_{ik} x_i + a_{kk}x_k \right) \\
  &amp;= 2 \left(\sum_{i = 1}^n a_{ki} x_i\right)   
\end{align*}\]</span></p>
<p>Then:</p>
<p><span class="math display">\[
\nabla f = \begin{bmatrix} \frac{\partial }{\partial x_1} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right)  \\ \frac{\partial }{\partial x_2} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right) \\ \vdots \\ \frac{\partial }{\partial x_n} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right) \end{bmatrix} = \begin{bmatrix} 2 \sum_{i = 1}^n a_{1i} x_i  \\ 2 \sum_{i = 1}^n a_{2i} x_i \\ \vdots \\ 2 \sum_{i = 1}^n a_{ni} x_i \end{bmatrix} = 2 \mathbf{A}\mathbf{x}
\]</span></p>
<p>Finally for the second derivative we have that:</p>
<p>In general, the Hessian matrix of a scalar function <span class="math inline">\(f(\mathbf{x})\)</span>, where <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> is a vector of variables, is a matrix that contains all the second-order partial derivatives of the function. It is defined as:</p>
<p><span class="math display">\[
\mathbf{H}(f) = \frac{d^2 f}{d\mathbf{x}d\mathbf{x}&#39;} =  \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
= \begin{bmatrix}
\frac{\partial }{\partial x_1}\left(\frac{d f}{d\mathbf{x}}\right)&#39; \\
\frac{\partial }{\partial x_2}\left(\frac{d f}{d\mathbf{x}}\right)&#39; \\
\vdots  \\
\frac{\partial }{\partial x_n}\left(\frac{d f}{d\mathbf{x}}\right)&#39;
\end{bmatrix}
= \begin{bmatrix}
\frac{\partial }{\partial x_1}\left(2\mathbf{A}\mathbf{x}\right)&#39; \\
\frac{\partial }{\partial x_2}\left(2\mathbf{A}\mathbf{x}\right)&#39; \\
\vdots  \\
\frac{\partial }{\partial x_n}\left(2\mathbf{A}\mathbf{x}\right)&#39;
\end{bmatrix}
\]</span></p>
<p>Now</p>
<p><span class="math display">\[
\frac{\partial }{\partial x_k}\left(2\mathbf{A}\mathbf{x}\right)
= 2\frac{\partial }{\partial x_k}\begin{bmatrix}
\sum_{i = 1}^n a_{1i} x_i \\
\sum_{i = 1}^n a_{2i} x_i \\
\vdots  \\
\sum_{i = 1}^n a_{ni} x_i
\end{bmatrix}
= 2 \begin{bmatrix}
\frac{\partial }{\partial x_k} \left(\sum_{i = 1}^n a_{1i} x_i \right)\\
\frac{\partial }{\partial x_k} \left(\sum_{i = 1}^n a_{2i} x_i \right)\\
\vdots  \\
\frac{\partial }{\partial x_k} \left(\sum_{i = 1}^n a_{ni} x_i \right)
\end{bmatrix}
= 2 \begin{bmatrix}
a_{k1} \\
a_{k2} \\
\vdots \\
a_{kn}
\end{bmatrix}
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\mathbf{H}(f) = \frac{d^2 f}{d\mathbf{x}d\mathbf{x}&#39;}
= \begin{bmatrix}
2\begin{bmatrix}
a_{11} \\
a_{12} \\
\vdots \\
a_{1n}
\end{bmatrix}&#39; \\
2\begin{bmatrix}
a_{21} \\
a_{22} \\
\vdots \\
a_{2n}
\end{bmatrix}&#39; \\
\vdots \\
2\begin{bmatrix}
a_{n1} \\
a_{n2} \\
\vdots \\
a_{nn}
\end{bmatrix}&#39;
\end{bmatrix}
=  2\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn}
\end{bmatrix}
= 2 \mathbf{A}
\]</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
