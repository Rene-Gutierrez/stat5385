<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 Normality Assumption | _main.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="9 Normality Assumption | _main.knit" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 Normality Assumption | _main.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mean-and-varaince-assumptions.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Stat 5385/6385 Fall 2024</a></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#general-math"><i class="fa fa-check"></i><b>2.1</b> General Math</a></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.2.3</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.2.4</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.2.5" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.2.5</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.2.6" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.2.6</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.2.7" data-path="prerequisites.html"><a href="prerequisites.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.2.7</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="2.2.8" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.2.8</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.2.9" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.2.9</b> Idempotent Matrix</a></li>
<li class="chapter" data-level="2.2.10" data-path="prerequisites.html"><a href="prerequisites.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>2.2.10</b> Determinant of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.3</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.3.1</b> Gradient</a></li>
<li class="chapter" data-level="2.3.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.3.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.3.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.3.3</b> Applications:</a></li>
<li class="chapter" data-level="2.3.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.3.4</b> Matrix Calculus</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.4</b> Probability</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="prerequisites.html"><a href="prerequisites.html#expected-value"><i class="fa fa-check"></i><b>2.4.1</b> Expected Value</a></li>
<li class="chapter" data-level="2.4.2" data-path="prerequisites.html"><a href="prerequisites.html#variance"><i class="fa fa-check"></i><b>2.4.2</b> Variance</a></li>
<li class="chapter" data-level="2.4.3" data-path="prerequisites.html"><a href="prerequisites.html#cross-covariance-matrix"><i class="fa fa-check"></i><b>2.4.3</b> Cross-Covariance Matrix</a></li>
<li class="chapter" data-level="2.4.4" data-path="prerequisites.html"><a href="prerequisites.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.4.4</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="2.4.5" data-path="prerequisites.html"><a href="prerequisites.html#chi2-distribution"><i class="fa fa-check"></i><b>2.4.5</b> <span class="math inline">\(\chi^2\)</span> Distribution</a></li>
<li class="chapter" data-level="2.4.6" data-path="prerequisites.html"><a href="prerequisites.html#t-distribution"><i class="fa fa-check"></i><b>2.4.6</b> <span class="math inline">\(t\)</span> Distribution</a></li>
<li class="chapter" data-level="2.4.7" data-path="prerequisites.html"><a href="prerequisites.html#f-distribution"><i class="fa fa-check"></i><b>2.4.7</b> <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.5</b> Statistics</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="prerequisites.html"><a href="prerequisites.html#bias-of-an-estimator"><i class="fa fa-check"></i><b>2.5.1</b> Bias of an Estimator</a></li>
<li class="chapter" data-level="2.5.2" data-path="prerequisites.html"><a href="prerequisites.html#unbiased-estimator"><i class="fa fa-check"></i><b>2.5.2</b> Unbiased Estimator</a></li>
<li class="chapter" data-level="2.5.3" data-path="prerequisites.html"><a href="prerequisites.html#mean-square-error-of-an-estimator"><i class="fa fa-check"></i><b>2.5.3</b> Mean Square Error of an Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#examples-1"><i class="fa fa-check"></i><b>3.1</b> Examples</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction.html"><a href="introduction.html#ad-spending"><i class="fa fa-check"></i><b>3.1.1</b> Ad Spending</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction.html"><a href="introduction.html#winw-example"><i class="fa fa-check"></i><b>3.1.2</b> Wine and Life Expectancy</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction.html"><a href="introduction.html#burger-demand"><i class="fa fa-check"></i><b>3.1.3</b> Burger Demand</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model"><i class="fa fa-check"></i><b>4.1</b> Model</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>4.2</b> Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#other-estimated-quantites"><i class="fa fa-check"></i><b>4.2.1</b> Other estimated quantites</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#properties-of-the-estimates"><i class="fa fa-check"></i><b>4.3</b> Properties of the Estimates</a></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-and-standarizing-the-data"><i class="fa fa-check"></i><b>4.4</b> Centering and Standarizing the Data</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independent-variable-centered"><i class="fa fa-check"></i><b>4.4.1</b> Independent variable centered</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#both-variables-centered"><i class="fa fa-check"></i><b>4.4.2</b> Both Variables centered</a></li>
<li class="chapter" data-level="4.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independent-and-dependent-variable-standardized"><i class="fa fa-check"></i><b>4.4.3</b> Independent and dependent variable standardized</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>4.5</b> Coefficient of Determination</a></li>
<li class="chapter" data-level="4.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residual-analysis"><i class="fa fa-check"></i><b>4.6</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#non-linear-regression-function"><i class="fa fa-check"></i><b>4.6.1</b> Non-linear regression function</a></li>
<li class="chapter" data-level="4.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#heteroscedasticity"><i class="fa fa-check"></i><b>4.6.2</b> Heteroscedasticity</a></li>
<li class="chapter" data-level="4.6.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outliers"><i class="fa fa-check"></i><b>4.6.3</b> Outliers</a></li>
<li class="chapter" data-level="4.6.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#variables-ommited"><i class="fa fa-check"></i><b>4.6.4</b> Variables Ommited</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#cross-validation"><i class="fa fa-check"></i><b>4.7</b> Cross-Validation</a></li>
<li class="chapter" data-level="4.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>4.8</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="4.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-in-matrix-form"><i class="fa fa-check"></i><b>4.9</b> Model in Matrix Form</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares-in-matrix-form"><i class="fa fa-check"></i><b>4.9.1</b> Weighted Least Squares in Matrix Form</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="polynomial-regression.html"><a href="polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Polynomial Regression</a></li>
<li class="chapter" data-level="6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#example-9"><i class="fa fa-check"></i><b>6.2</b> Example</a></li>
<li class="chapter" data-level="6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#least-squares-estimation-1"><i class="fa fa-check"></i><b>6.3</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#properties-of-the-estimates-1"><i class="fa fa-check"></i><b>6.4</b> Properties of the Estimates</a></li>
<li class="chapter" data-level="6.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-r2"><i class="fa fa-check"></i><b>6.5</b> Multiple <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="6.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#geometric-interpretation-of-multiple-linear-regression"><i class="fa fa-check"></i><b>6.6</b> Geometric Interpretation of Multiple Linear Regression</a></li>
<li class="chapter" data-level="6.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#centered-and-standarized-variables"><i class="fa fa-check"></i><b>6.7</b> Centered and Standarized Variables</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#centered-variables"><i class="fa fa-check"></i><b>6.7.1</b> Centered Variables</a></li>
<li class="chapter" data-level="6.7.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sample-covariance"><i class="fa fa-check"></i><b>6.7.2</b> Sample Covariance</a></li>
<li class="chapter" data-level="6.7.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#satandard-variables"><i class="fa fa-check"></i><b>6.7.3</b> Satandard Variables</a></li>
<li class="chapter" data-level="6.7.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sample-correlation-matrix"><i class="fa fa-check"></i><b>6.7.4</b> Sample Correlation Matrix</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#variable-cross-effects"><i class="fa fa-check"></i><b>6.8</b> Variable Cross-Effects</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#single-variable-cross-effects"><i class="fa fa-check"></i><b>6.8.1</b> Single Variable Cross-Effects</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#outliers-and-leverage"><i class="fa fa-check"></i><b>6.9</b> Outliers and Leverage</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#leverage"><i class="fa fa-check"></i><b>6.9.1</b> Leverage</a></li>
<li class="chapter" data-level="6.9.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#outliers-identification"><i class="fa fa-check"></i><b>6.9.2</b> Outliers identification</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#stability-of-the-solution"><i class="fa fa-check"></i><b>6.10</b> Stability of the Solution</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bootstrapping.html"><a href="bootstrapping.html"><i class="fa fa-check"></i><b>7</b> Bootstrapping</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bootstrapping.html"><a href="bootstrapping.html#introduction-2"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="bootstrapping.html"><a href="bootstrapping.html#bootstrapping-example"><i class="fa fa-check"></i><b>7.2</b> Bootstrapping Example</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html"><i class="fa fa-check"></i><b>8</b> Mean and Varaince Assumptions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#mean-assumptions"><i class="fa fa-check"></i><b>8.1</b> Mean Assumptions</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-mathbfy"><i class="fa fa-check"></i><b>8.1.1</b> Expectation of <span class="math inline">\(\mathbf{y}\)</span>:</a></li>
<li class="chapter" data-level="8.1.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-hatboldsymbolbeta"><i class="fa fa-check"></i><b>8.1.2</b> Expectation of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
<li class="chapter" data-level="8.1.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-hatmathbfy"><i class="fa fa-check"></i><b>8.1.3</b> Expectation of <span class="math inline">\(\hat{\mathbf{y}}\)</span></a></li>
<li class="chapter" data-level="8.1.4" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-hatmathbfe"><i class="fa fa-check"></i><b>8.1.4</b> Expectation of <span class="math inline">\(\hat{\mathbf{e}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-assumptions"><i class="fa fa-check"></i><b>8.2</b> Variance Assumptions</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-mathbfy"><i class="fa fa-check"></i><b>8.2.1</b> Variance of <span class="math inline">\(\mathbf{y}\)</span></a></li>
<li class="chapter" data-level="8.2.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-hatboldsymbolbeta"><i class="fa fa-check"></i><b>8.2.2</b> Variance of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
<li class="chapter" data-level="8.2.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-hatmathbfy"><i class="fa fa-check"></i><b>8.2.3</b> Variance of <span class="math inline">\(\hat{\mathbf{y}}\)</span></a></li>
<li class="chapter" data-level="8.2.4" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-hatmathbfe"><i class="fa fa-check"></i><b>8.2.4</b> Variance of <span class="math inline">\(\hat{\mathbf{e}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#cross-covariances"><i class="fa fa-check"></i><b>8.3</b> Cross-Covariances</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#cross-covaraince-of-haty-and-hate"><i class="fa fa-check"></i><b>8.3.1</b> Cross-covaraince of <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{e}\)</span></a></li>
<li class="chapter" data-level="8.3.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#cross-covaraince-of-haty-and-hatboldsymbolbeta"><i class="fa fa-check"></i><b>8.3.2</b> Cross-covaraince of <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#gauss-markov-theorem"><i class="fa fa-check"></i><b>8.4</b> Gauss-Markov Theorem</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#assumptions"><i class="fa fa-check"></i><b>8.4.1</b> Assumptions</a></li>
<li class="chapter" data-level="8.4.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#statement"><i class="fa fa-check"></i><b>8.4.2</b> Statement</a></li>
<li class="chapter" data-level="8.4.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#proof"><i class="fa fa-check"></i><b>8.4.3</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#estimate-of-sigma2"><i class="fa fa-check"></i><b>8.5</b> Estimate of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="normality-assumption.html"><a href="normality-assumption.html"><i class="fa fa-check"></i><b>9</b> Normality Assumption</a>
<ul>
<li class="chapter" data-level="9.1" data-path="normality-assumption.html"><a href="normality-assumption.html#introduction-3"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="normality-assumption.html"><a href="normality-assumption.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.2</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="normality-assumption.html"><a href="normality-assumption.html#distribution-of-estimates"><i class="fa fa-check"></i><b>9.3</b> Distribution of Estimates</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="normality-assumption.html"><a href="normality-assumption.html#distribution-of-hatboldsymbolbeta-hatmathbfy-and-hatmathbfe"><i class="fa fa-check"></i><b>9.3.1</b> Distribution of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, <span class="math inline">\(\hat{\mathbf{y}}\)</span> and <span class="math inline">\(\hat{\mathbf{e}}\)</span></a></li>
<li class="chapter" data-level="9.3.2" data-path="normality-assumption.html"><a href="normality-assumption.html#distribution-of-hatsigma2"><i class="fa fa-check"></i><b>9.3.2</b> Distribution of <span class="math inline">\(\hat{\sigma}^2\)</span></a></li>
<li class="chapter" data-level="9.3.3" data-path="normality-assumption.html"><a href="normality-assumption.html#independence-of-hatmathbfe-and-hatmathbfy"><i class="fa fa-check"></i><b>9.3.3</b> Independence of <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="normality-assumption.html"><a href="normality-assumption.html#interval-estimation"><i class="fa fa-check"></i><b>9.4</b> Interval Estimation</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="normality-assumption.html"><a href="normality-assumption.html#confidence-intervals-for-coefficients"><i class="fa fa-check"></i><b>9.4.1</b> Confidence Intervals for Coefficients</a></li>
<li class="chapter" data-level="9.4.2" data-path="normality-assumption.html"><a href="normality-assumption.html#confidence-intervals-for-the-expected-mean-of-a-new-observation-mathbfx_new"><i class="fa fa-check"></i><b>9.4.2</b> Confidence intervals for the expected mean of a new observation <span class="math inline">\(\mathbf{x}_{new}\)</span></a></li>
<li class="chapter" data-level="9.4.3" data-path="normality-assumption.html"><a href="normality-assumption.html#confidence-intervals-for-linear-combinations-of-boldsymbolbeta"><i class="fa fa-check"></i><b>9.4.3</b> Confidence intervals for linear combinations of <span class="math inline">\(\boldsymbol{\beta}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="normality-assumption.html"><a href="normality-assumption.html#hypothesis-testing"><i class="fa fa-check"></i><b>9.5</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="normality-assumption.html"><a href="normality-assumption.html#testing-for-the-overall-regression"><i class="fa fa-check"></i><b>9.5.1</b> Testing for the Overall Regression</a></li>
<li class="chapter" data-level="9.5.2" data-path="normality-assumption.html"><a href="normality-assumption.html#testing-if-one-variable-is-not-relevant"><i class="fa fa-check"></i><b>9.5.2</b> Testing if one variable is not relevant</a></li>
<li class="chapter" data-level="9.5.3" data-path="normality-assumption.html"><a href="normality-assumption.html#testing-if-a-subgroup-of-the-variables-is-relevant"><i class="fa fa-check"></i><b>9.5.3</b> Testing if a Subgroup of the Variables is Relevant</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="normality-assumption" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">9</span> Normality Assumption<a href="normality-assumption.html#normality-assumption" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-3" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Introduction<a href="normality-assumption.html#introduction-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To the mean and covariance assumptions, we can add the normality assumption. This
is a very strong and powerful assumption, that will enable us to obtain the
distribution of our data and several of our estimates.</p>
<p>We will assume:</p>
<p><span class="math display">\[\mathbf{e}\sim N(\mathbf{0}, \sigma^2 \mathbf{I})\]</span></p>
<p>Also, note that the Normal distribution is completely characterized by its
mean and variance, so if the distribution of our estimates is normal, we will
already have the information to completely characterize their distribution since
we have computed the mean and variance of the estimates in the previous chapter.</p>
<p>Another thing, that assuming normality allows is to obtain the likelihood of our
data, and in this way obtain Maximum Likelihood Estimates (MLE) of the parameters
we have introduced <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="maximum-likelihood-estimation" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Maximum Likelihood Estimation<a href="normality-assumption.html#maximum-likelihood-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In order to perform the maximum likelihood estimates of <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span>, we
need the distribution of <span class="math inline">\(\mathbf{y}\)</span>. This is very easy to obtain, since:</p>
<p><span class="math display">\[\mathbf{y}= \mathbf{X}\boldsymbol{\beta}+ \mathbf{e}\]</span>
is a linear combination of <span class="math inline">\(\mathbf{e}\)</span> (in fact is just a translation of <span class="math inline">\(\mathbf{e}\)</span>), and
therefore it is normally distributed. Since we have already computed its mean
and variance in the previous chapter, using those computations, we can conclude
that:</p>
<p><span class="math display">\[\mathbf{y}\sim N(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I})\]</span>
This means that the likelihood of <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span> is given by:</p>
<p><span class="math display">\[ \mathcal{L}(\boldsymbol{\beta}, \sigma^2 | \mathbf{y}) = N(\mathbf{y}| \mathbf{X}\mathbf{y}, \sigma^2 \mathbf{I}) \]</span>
Then this means that:</p>
<p><span class="math display">\[\begin{align*}
  \mathcal{L}(\boldsymbol{\beta}, \sigma^2 | \mathbf{y})
    &amp;= (2 \pi)^{-\frac{n}{2}} |\sigma^2 \mathbf{I}|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\sigma^2 \mathbf{I})^{-1}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) \right\}    \\
    &amp;= (2 \pi)^{-\frac{n}{2}} (\sigma^2)^{-\frac{n}{2}} |\mathbf{I}| \exp\left\{ -\frac{1}{2}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;\frac{\mathbf{I}}{\sigma^2}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) \right\} \\
    &amp;= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) \right\}                                   \\
    &amp;= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) \right\}                                   \\
    &amp;= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) \right\}                                   \\
\end{align*}\]</span></p>
<p>Now recalling from:</p>
<p><span class="math display">\[(\mathbf{y}- \mathbb{E}[\mathbf{y}])&#39;(\mathbf{y}- \mathbb{E}[\mathbf{y}]) = \hat{\mathbf{e}}&#39;\hat{\mathbf{e}} + (\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])&#39;(\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])\]</span>
that is:</p>
<p><span class="math display">\[(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) = \hat{\mathbf{e}}&#39;\hat{\mathbf{e}} + (\mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\beta}) = \hat{\mathbf{e}}&#39;\hat{\mathbf{e}} + (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})&#39;\mathbf{X}\mathbf{X}( \hat{\boldsymbol{\beta}} - \boldsymbol{\beta})\]</span></p>
<p>then, the likelihood can be written as:</p>
<p><span class="math display">\[\begin{align*}
  \mathcal{L}(\boldsymbol{\beta}, \sigma^2 | \mathbf{y})
    &amp;= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) \right\}                                                      \\
    &amp;= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{2 \sigma^2} -\frac{(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})&#39;\mathbf{X}\mathbf{X}( \hat{\boldsymbol{\beta}} - \boldsymbol{\beta})}{2 \sigma^2} \right\} \\
\end{align*}\]</span></p>
<p>This is a useful way to write the likelihood, since it is easy to optimize with
respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>. Since, independently of the value of <span class="math inline">\(\sigma^2\)</span>, the value of
<span class="math inline">\(\boldsymbol{\beta}\)</span> that maximizes the likelihood is the OLS estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, since it
makes zero the following term:</p>
<p><span class="math display">\[ -\frac{(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})&#39;\mathbf{X}\mathbf{X}( \hat{\boldsymbol{\beta}} - \boldsymbol{\beta})}{2 \sigma^2} \]</span>
In this way, we only need to maximize the likelihood with respect to <span class="math inline">\(\sigma^2\)</span>,
as the following marginal likelihood:</p>
<p><span class="math display">\[ \mathcal{L}(\sigma^2 | \mathbf{y}, \boldsymbol{\beta}= \hat{\boldsymbol{\beta}} ) =  (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{2 \sigma^2} \right\}\]</span>
Instead of maximizing the marginal likelihood directly, we will maximize the
marginal log-likelihood:</p>
<p><span class="math display">\[ \ell(\sigma^2 | \mathbf{y}, \boldsymbol{\beta}= \hat{\boldsymbol{\beta}}) = -\frac{n}{2} \log(2 \pi) -\frac{n}{2} \log(\sigma^2) -\frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{2 \sigma^2}\]</span>
We can do this maximization, by taking the derivative:</p>
<p><span class="math display">\[\begin{align*}
  \frac{d \ell}{d \sigma^2}
    &amp;=        -\frac{n}{2 \sigma^2} + \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{2 (\sigma^2)^2}
\end{align*}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{align*}
  \frac{d \ell}{d \sigma^2} =0
    &amp;\implies -\frac{n}{2 \sigma^2} + \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{2 (\sigma^2)^2} = 0 \\
    &amp;\implies -n + \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2} = 0                          \\
    &amp;\implies \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2} = n                               \\
    &amp;\implies \sigma^2 = \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{n}                               \\
\end{align*}\]</span></p>
<p>So we have, that the Maximum Likelihood Estimate of <span class="math inline">\(\sigma^2\)</span> is given by:</p>
<p><span class="math display">\[\tilde{\sigma}^2 = \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{n}\]</span></p>
<p>And, taking the second derivative to confirm it is a maximum we have that:</p>
<p><span class="math display">\[\begin{align*}
  \frac{d^2 \ell}{d (\sigma^2)^2} \bigg|_{\sigma^2 = \tilde{\sigma}^2}
    &amp;= \frac{n}{2 (\tilde{\sigma}^2)^2} - \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{(\tilde{\sigma}^2)^3} \\
    &amp;= \frac{n}{2 (\tilde{\sigma}^2)^2} - \frac{n}{(\tilde{\sigma}^2)^2}                   \\
    &amp;= - \frac{n}{(2 \tilde{\sigma}^2)^2}                                                  \\
    &amp;\leq 0                                                  
\end{align*}\]</span></p>
<p>So <span class="math inline">\(\tilde{\sigma}^2\)</span> is indeed maximizing the marginal likelihood.</p>
<p>Note that, unlike with the <span class="math inline">\(\boldsymbol{\beta}\)</span> parameter, our OLS estimate <span class="math inline">\(\hat{\sigma}^2\)</span>
of <span class="math inline">\(\sigma^2\)</span> is different to the MLE estimator <span class="math inline">\(\tilde{\sigma}^2\)</span>. In particular
<span class="math inline">\(\tilde{\sigma}^2\)</span> is biased.</p>
<p>This doesn’t mean that one estimate is better than the other, they just have
different properties.</p>
<p>We will work more with <span class="math inline">\(\hat{\sigma}^2\)</span> than <span class="math inline">\(\tilde{\sigma}^2\)</span>, since it is more
useful to build certain statistics.</p>
</div>
<div id="distribution-of-estimates" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Distribution of Estimates<a href="normality-assumption.html#distribution-of-estimates" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the same way that in the last chapters we computed the mean and the variance
of our estimates, we can obtain the distribution of the estimates:</p>
<div id="distribution-of-hatboldsymbolbeta-hatmathbfy-and-hatmathbfe" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> Distribution of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, <span class="math inline">\(\hat{\mathbf{y}}\)</span> and <span class="math inline">\(\hat{\mathbf{e}}\)</span><a href="normality-assumption.html#distribution-of-hatboldsymbolbeta-hatmathbfy-and-hatmathbfe" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Most of our estimates are linear combinations of <span class="math inline">\(\mathbf{y}\)</span>, so they are normally
distributed and we can use the mean and variances computed in the last chapter
to fully characterize their distributions. This is the case for:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\beta}}, \quad \hat{\mathbf{y}}, \quad \hat{\mathbf{e}} \]</span>
Their distributions are:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}&#39;\mathbf{X})^{-1}) \]</span>
<span class="math display">\[ \hat{\mathbf{y}} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{H}) \]</span>
<span class="math display">\[ \hat{\mathbf{e}} \sim N(\mathbf{0}, \sigma^2 (\mathbf{I}-\mathbf{H})) \]</span></p>
<p>This is not the case for the estimate <span class="math inline">\(\hat{\sigma}^2\)</span>, since it is not a linear
transformation of <span class="math inline">\(\mathbf{y}\)</span>.</p>
</div>
<div id="distribution-of-hatsigma2" class="section level3 hasAnchor" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> Distribution of <span class="math inline">\(\hat{\sigma}^2\)</span><a href="normality-assumption.html#distribution-of-hatsigma2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Obtaining the distribution of <span class="math inline">\(\hat{\sigma}^2\)</span> is not as straight forward. We
will use 3 steps:</p>
<ol style="list-style-type: decimal">
<li>Express <span class="math inline">\(\frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2}\)</span> as quadratic form of standard
normal with an idempotent matrix.</li>
<li>Show that the quadratic form of standard normal with an idempotent matrix is
distributed as a chi squared.</li>
<li>Relate the distribution of <span class="math inline">\(\frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2}\)</span> to the
distribution of <span class="math inline">\(\hat{\sigma}^2\)</span>.</li>
</ol>
<div id="distribution-of-hatsigma2-step-1" class="section level4 hasAnchor" number="9.3.2.1">
<h4><span class="header-section-number">9.3.2.1</span> Distribution of <span class="math inline">\(\hat{\sigma}^2\)</span> Step 1<a href="normality-assumption.html#distribution-of-hatsigma2-step-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>First, note that:</p>
<p><span class="math display">\[\begin{align*}
  (\mathbf{I}- \mathbf{H}) \mathbf{y}
    &amp;= (\mathbf{I}- \mathbf{H}) (\mathbf{X}\boldsymbol{\beta}+ \mathbf{e})                    &amp;&amp; \text{since $\mathbf{y}= \mathbf{X}\boldsymbol{\beta}+ \mathbf{e}$}  \\
    &amp;= \mathbf{I}\mathbf{X}\boldsymbol{\beta}- \mathbf{H}\mathbf{X}\boldsymbol{\beta}+ \mathbf{I}\mathbf{e}- \mathbf{H}\mathbf{e}\\
    &amp;= \mathbf{X}\boldsymbol{\beta}- \mathbf{X}\boldsymbol{\beta}+ \mathbf{e}- \mathbf{H}\mathbf{e}&amp;&amp; \text{since $\mathbf{H}\mathbf{X}= \mathbf{X}$}         \\
    &amp;= \mathbf{e}- \mathbf{H}\mathbf{e}\\
    &amp;= (\mathbf{I}- \mathbf{H}) \mathbf{e}\\
\end{align*}\]</span></p>
<p>Then:</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{y}&#39; (\mathbf{I}- \mathbf{H}) \mathbf{y}
    &amp;= \mathbf{y}&#39; (\mathbf{I}- \mathbf{H}) (\mathbf{I}- \mathbf{H}) \mathbf{y}&amp;&amp; \text{since $(\mathbf{I}- \mathbf{H})$ is idempotent}         \\
    &amp;= \mathbf{e}&#39; (\mathbf{I}- \mathbf{H}) (\mathbf{I}- \mathbf{H}) \mathbf{e}&amp;&amp; \text{since $(\mathbf{I}- \mathbf{H}) \mathbf{y}= (\mathbf{I}- \mathbf{H}) \mathbf{e}$} \\
    &amp;= \mathbf{e}&#39; (\mathbf{I}- \mathbf{H}) \mathbf{e}&amp;&amp; \text{since $(\mathbf{I}- \mathbf{H})$ is idempotent}         \\
\end{align*}\]</span></p>
<p>Then,</p>
<p><span class="math display">\[\begin{align*}
  \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2}
    &amp;= \frac{\mathbf{y}&#39; (\mathbf{I}- \mathbf{H}) \mathbf{y}}{\sigma^2}  &amp;&amp; \text{since $\hat{\mathbf{e}}&#39;\hat{\mathbf{e}} = \mathbf{y}&#39; (\mathbf{I}- \mathbf{H}) \mathbf{y}$}  \\
    &amp;= \frac{\mathbf{e}&#39; (\mathbf{I}- \mathbf{H}) \mathbf{e}}{\sigma^2}  &amp;&amp; \text{since $\mathbf{e}&#39; (\mathbf{I}- \mathbf{H}) \mathbf{e}= \mathbf{y}&#39; (\mathbf{I}- \mathbf{H}) \mathbf{y}$} \\
    &amp;= \frac{\mathbf{e}}{\sqrt{\sigma^2}}&#39; (\mathbf{I}- \mathbf{H}) \frac{\mathbf{e}}{\sqrt{\sigma^2}}                                 \\
\end{align*}\]</span></p>
<p>Finally, note that <span class="math inline">\(\frac{\mathbf{e}}{\sqrt{\sigma^2}}\)</span> is a linear function of <span class="math inline">\(\mathbf{e}\)</span>
that is normal, therefore it is normal also, with mean:</p>
<p><span class="math display">\[ \mathbb{E}\left[\frac{\mathbf{e}}{\sqrt{\sigma^2}}\right] =  \frac{1}{\sqrt{\sigma^2}} \mathbb{E}[\mathbf{e}] = \frac{1}{\sqrt{\sigma^2}} \mathbf{0}= \mathbf{0}\]</span>
<span class="math display">\[ \mathbb{V}\left[\frac{\mathbf{e}}{\sqrt{\sigma^2}}\right] =  \left(\frac{1}{\sqrt{\sigma^2}}\right)^2 \mathbb{V}[\mathbf{e}] = \frac{1}{\sigma^2} \sigma^2 \mathbf{I}= \mathbf{I}\]</span>
So</p>
<p><span class="math display">\[ \frac{\mathbf{e}}{\sqrt{\sigma^2}} \sim N(\mathbf{0}, \mathbf{I}) \]</span>
is a multivariate standard normal. This concludes step 1.</p>
</div>
<div id="distribution-of-hatsigma2-step-2" class="section level4 hasAnchor" number="9.3.2.2">
<h4><span class="header-section-number">9.3.2.2</span> Distribution of <span class="math inline">\(\hat{\sigma}^2\)</span> Step 2<a href="normality-assumption.html#distribution-of-hatsigma2-step-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(\mathbf{z}\in \mathbb{R}^{n}\)</span> a multivariate standard normal and <span class="math inline">\(\mathbf{M}\in \mathbb{R}^{n \times n}\)</span>
and idempotent matrix of rank <span class="math inline">\(m\)</span>. Then, we will show that:</p>
<p><span class="math display">\[ \mathbf{z}&#39; \mathbf{M}\mathbf{z}\sim \chi^2_{m} \]</span></p>
<p>To show this result, we will use the spectral decomposition of <span class="math inline">\(\mathbf{M}\)</span>, that is:</p>
<p><span class="math display">\[ \mathbf{M}= \mathbf{V}\boldsymbol{\Sigma}\mathbf{V}&#39; \]</span></p>
<p>with <span class="math inline">\(\mathbf{V}\)</span> orthonormal and <span class="math inline">\(\boldsymbol{\Sigma}\)</span> diagonal. Since <span class="math inline">\(\mathbf{V}= [\mathbf{v}_1,\ldots,\mathbf{v}_n]\)</span> is
orthonormal, then we have that:</p>
<p><span class="math display">\[ \mathbf{v}_i&#39;\mathbf{v}_j = 0 \quad \forall i \neq j \quad \text{and} \quad ||\mathbf{v}_i||^2_2 = 1 \quad \forall i\]</span>
and, since <span class="math inline">\(\mathbf{M}\)</span> is idempotent, then <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is diagonal with exactly <span class="math inline">\(m\)</span> entries
equal to <span class="math inline">\(1\)</span> and the rest equal to <span class="math inline">\(0\)</span>. Without loss of generality, we
can assume that the first <span class="math inline">\(m\)</span> entries of the diagonal are equal to <span class="math inline">\(1\)</span> and the
next entries equal to <span class="math inline">\(0\)</span>.</p>
<p>Then, first note that <span class="math inline">\(\mathbf{V}&#39; \mathbf{z}\)</span> is a linear combination of a normal distribution.
We will show, that <span class="math inline">\(\mathbf{V}&#39; \mathbf{z}\in \mathbb{R}^{n}\)</span> is also standard normal.</p>
<p>Note that:</p>
<p><span class="math display">\[ \mathbb{E}[\mathbf{V}&#39; \mathbf{z}] =  \mathbf{V}&#39; \mathbb{E}[\mathbf{z}] = \mathbf{V}&#39; \mathbf{0}= \mathbf{0}\]</span>
<span class="math display">\[ \mathbb{V}[\mathbf{V}&#39; \mathbf{z}] =  \mathbf{V}&#39; \mathbb{V}[\mathbf{z}] \mathbf{V}= \mathbf{V}&#39; \mathbf{I}\mathbf{V}= \mathbf{V}&#39; \mathbf{V}= \mathbf{I}\]</span></p>
<p>Then <span class="math inline">\(\mathbf{V}&#39; \mathbf{z}\)</span> is also standard normal. Let’s name <span class="math inline">\(\mathbf{w}= \mathbf{V}&#39; \mathbf{z}\)</span>, then each of
the components <span class="math inline">\(w_1,\ldots,w_n\)</span> of <span class="math inline">\(\mathbf{w}\)</span> are independent univariate standard normally
distributed.</p>
<p>Then</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{z}&#39; \mathbf{M}\mathbf{z}
    &amp;= \mathbf{z}&#39; (\mathbf{V}\boldsymbol{\Sigma}\mathbf{V}&#39;) \mathbf{z}&amp;&amp; \text{using the spectral decomposition of $\mathbf{M}$}         \\
    &amp;= (\mathbf{V}&#39; \mathbf{z})&#39; \boldsymbol{\Sigma}(\mathbf{V}&#39; \mathbf{z})                                                                \\
    &amp;= \mathbf{w}&#39; \boldsymbol{\Sigma}\mathbf{w}&amp;&amp; \text{since $\mathbf{w}= \mathbf{V}&#39; \mathbf{z}$}                            \\
    &amp;= \sum_{i=1}^n [\boldsymbol{\Sigma}]_{ii} w_i^2                                                             \\
    &amp;= \sum_{i=1}^{m} w_i^2           &amp;&amp; \text{since only the first $m$ entries are equal to $1$} \\
    &amp;\sim \chi^2_m                    &amp;&amp; \text{by definition of the $\chi^2$ distribution}        \\
\end{align*}\]</span></p>
</div>
<div id="distribution-of-hatsigma2-step-3" class="section level4 hasAnchor" number="9.3.2.3">
<h4><span class="header-section-number">9.3.2.3</span> Distribution of <span class="math inline">\(\hat{\sigma}^2\)</span> Step 3<a href="normality-assumption.html#distribution-of-hatsigma2-step-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Using step 1 and 2, we can conclude that:</p>
<p><span class="math display">\[ \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2} \sim \chi^2_{n-p} \]</span>
since the rank of the idempotent matrix <span class="math inline">\((\mathbf{I}- \mathbf{H})\)</span> is <span class="math inline">\(n-p\)</span>. Since:</p>
<p><span class="math display">\[ \hat{\sigma}^2 = \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{n-p} = \frac{\sigma^2}{n-p}\frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2} \sim \frac{\sigma^2}{n-p}\chi^2_{n-p} \]</span></p>
</div>
</div>
<div id="independence-of-hatmathbfe-and-hatmathbfy" class="section level3 hasAnchor" number="9.3.3">
<h3><span class="header-section-number">9.3.3</span> Independence of <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span><a href="normality-assumption.html#independence-of-hatmathbfe-and-hatmathbfy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have seen that <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span> are uncorrelated, that is</p>
<p><span class="math display">\[ \mathbb{C}[\hat{\mathbf{e}}, \hat{\mathbf{y}}] = \mathbf{0}\]</span>
however, this doesn’t necessarily mean they are independent. Independence is a
much more stronger property. For example, if two random variables are independent
then any function of this random variables will be independent. However, if two
random variables are uncorrelated then not necessarily any function of this
random variables will be uncorrelated.</p>
<p>Now, after our assumption of normality, then <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span> are
independent, because uncorrelated random variables implies independence for
normally distributed random variables, as is the case of <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span>.</p>
<p>Then, any variable that is a function of <span class="math inline">\(\hat{\mathbf{e}}\)</span> will be independent of any
random variable that is a function of <span class="math inline">\(\hat{\mathbf{y}}\)</span>, even if these new random
variables are not normal themselves.</p>
</div>
</div>
<div id="interval-estimation" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Interval Estimation<a href="normality-assumption.html#interval-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have obtained point estimates of different quantities of interest,
<span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(\mathbf{e}\)</span> and <span class="math inline">\(\hat{\sigma}^2\)</span>, however the fact
we have the distribution of estimates of this quantities will allow us to obtain
interval estimators.</p>
<div id="confidence-intervals-for-coefficients" class="section level3 hasAnchor" number="9.4.1">
<h3><span class="header-section-number">9.4.1</span> Confidence Intervals for Coefficients<a href="normality-assumption.html#confidence-intervals-for-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We know that the OLS estimate for the coefficients, has the following distribution:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\beta}} \sim N(\mathbf{0}, \sigma^2 (\mathbf{X}&#39;\mathbf{X})^{-1}) \]</span></p>
<p>then, this means that each of the entries of <span class="math inline">\(\hat{\boldsymbol{\beta}} = (\hat{\beta}_0, \hat{\beta}_1,\ldots,\hat{\beta}_{p-1})\)</span>
is normally distributed, as follows:</p>
<p><span class="math display">\[ \hat{\beta}_i \sim N(\beta_i, \sigma^2 [(\mathbf{X}&#39;\mathbf{X})^{-1}]_{ii}) \]</span>
we will call</p>
<p><span class="math display">\[ \sigma^2_{\beta_i} =  \sigma^2 [(\mathbf{X}&#39;\mathbf{X})^{-1}]_{ii} \]</span>
then, we can re-write the distribution as follows:</p>
<p><span class="math display">\[ \hat{\beta}_i \sim N \left(\beta_i, \sigma^2_{\beta_i} \right) \]</span>
unfortunately this distribution depends on <span class="math inline">\(\sigma^2\)</span> which is unknown, so we
can’t use it to build a confidence interval. So we transform the statistic first
by removing the mean <span class="math inline">\(\beta_i\)</span> and by diving by the standard deviation:</p>
<p><span class="math display">\[ t^0_{\beta_i}=\frac{\hat{\beta}_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}} \]</span>
Then, this new quantity is normally distributed, since it a linear transformation
of a random variable that is normally distributed, and has mean as variance as
follows:</p>
<p><span class="math display">\[ \mathbb{E}[t^0_{\beta_i}] = \mathbb{E}\left[\frac{\hat{\beta}_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}}\right]= \frac{\hat{\mathbb{E}[\beta}_i] - \beta_i}{\sqrt{\sigma^2_{\beta_i}}} = \frac{\beta_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}} = 0  \]</span>
<span class="math display">\[ \mathbb{V}[t^0_{\beta_i}] = \mathbb{V}\left[\frac{\hat{\beta}_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}}\right]= \left(\frac{1}{\sqrt{\sigma^2_{\beta_i}}}\right)^2 \mathbb{V}[\hat{\beta}_i - \beta_i] = \frac{1}{\sigma^2_{\beta_i}} \mathbb{V}[\hat{\beta}_i] = \frac{1}{\sigma^2_{\beta_i}} \sigma^2_{\beta_i} = 1  \]</span>
then:</p>
<p><span class="math display">\[ t^0_{\beta_i} \sim N(0, 1) \]</span>
that is <span class="math inline">\(t^0_{\beta_i}\)</span> is distributed like a standard normal. Now the distribution
doesn’t depend on any unknown parameter (on top of <span class="math inline">\(\beta_i\)</span>, that is the parameter
of interest), but the quantity itself depends on <span class="math inline">\(\sigma^2\)</span> through <span class="math inline">\(\sigma^2_{\beta_i}\)</span>,
so we can’t use it to build a confidence interval.</p>
<p>We consider a new quantity</p>
<p><span class="math display">\[ t_{\beta_i} = \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}} \]</span>
where <span class="math inline">\(\hat{\sigma}^2_{\beta_i} = \hat{\sigma}^2 [(\mathbf{X}&#39;\mathbf{X})^{-1}]_{ii}\)</span>, so <span class="math inline">\(t_{\beta_i}\)</span>
doesn’t depend on <span class="math inline">\(\sigma^2\)</span>. Let’s compute the distribution of this quantity,
first lets re-write the statistic as follows:</p>
<p><span class="math display">\[ t_{\beta_i} = \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}}
  = \frac{\sqrt{\frac{1}{\sigma^2}}}{\sqrt{\frac{1}{\sigma^2}}}\frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2[(\mathbf{X}&#39;\mathbf{X})^{-1}]_{ii}}}   
  = \frac{\frac{\left(\hat{\beta}_i - \beta_i\right)}{\sqrt{\sigma^2[(\mathbf{X}&#39;\mathbf{X})^{-1}]_{ii}}}}{ \sqrt{\frac{\hat{\sigma}^2}{\sigma^2}}}   
  = \frac{\frac{\left(\hat{\beta}_i - \beta_i\right)}{\sqrt{\sigma^2_{\beta_i}}}}{            \sqrt{\frac{(n-p)\frac{\hat{\sigma}^2}{\sigma^2}}{n-p}}}
  = \frac{t^0_{\beta_i}}{ \sqrt{\frac{\frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2}}{n-p}}}\]</span></p>
<p>Now, we know <span class="math inline">\(t^0_{\beta_i}\)</span> is standard normal distributed, and from
<a href="normality-assumption.html#distribution-of-hatsigma2-step-3">the distribution of <span class="math inline">\(\hat{\sigma}^2\)</span></a> we
have that:</p>
<p><span class="math display">\[ \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2} \sim \chi^2_{n-p} \]</span>
and from
<a href="normality-assumption.html#independence-of-hatmathbfe-and-hatmathbfy">the independence of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> and <span class="math inline">\(\hat{\mathbf{e}}\)</span></a>
we have that any function of both variables is independent, in particular</p>
<p><span class="math display">\[ t^0_{\beta_i} \quad \text{and} \quad \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2} \]</span>
are independent. Therefore</p>
<p><span class="math display">\[ t_{\beta_i} \sim t_{n-p} \]</span>
Now, let <span class="math inline">\(t \sim t_m\)</span> a random variable with a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(m\)</span> degrees
of freedom. Then call:</p>
<p><span class="math display">\[ t_m\left(a\right) \quad \text{such that} \quad \mathbb{P}\left(t\leq t_m\left(a\right) \right) = a\]</span>
for any <span class="math inline">\(a\in[0,1]\)</span></p>
<p>Then, we have that:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{P}
  &amp;\left( -t_{n-p}\left(\frac{\alpha}{2}\right) \leq t_{\beta_i} \leq t_{n-p}\left(\frac{\alpha}{2}\right) \right) = \alpha  
    &amp;&amp; \text{since the $t$ distribution is symmetric} \\
  &amp;\implies \mathbb{P}\left( -t_{n-p}\left(\frac{\alpha}{2}\right) \leq \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}} \leq t_{n-p}\left(\frac{\alpha}{2}\right) \right) = \alpha
    &amp;&amp; \text{since the $t_{\beta_i} = \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}}$} \\
  &amp;\implies \mathbb{P}\left( -t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \leq \hat{\beta}_i - \beta_i \leq t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) = \alpha \\
  &amp;\implies \mathbb{P}\left( -t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \leq \beta_i - \hat{\beta}_i \leq t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) = \alpha \\
  &amp;\implies \mathbb{P}\left( \hat{\beta}_i - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \leq \beta_i \leq \hat{\beta}_i + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) = \alpha \\
\end{align*}\]</span></p>
<p>So</p>
<p><span class="math display">\[ \left(\hat{\beta}_i - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}}, \hat{\beta}_i + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) \]</span>
is a random interval, that is an interval that is a function of a random variables,
in this case the random variables <span class="math inline">\(\hat{\beta}_i\)</span> and <span class="math inline">\(\hat{\sigma}^2_{\beta_i}\)</span>.
This random interval will capture the true parameter <span class="math inline">\(\beta_i\)</span> with probability
<span class="math inline">\(\alpha\)</span>. However, when data is observed and the interval is fixed (at the observed
values), the interval either captures the true parameter or not (something we
don’t know in general).</p>
</div>
<div id="confidence-intervals-for-the-expected-mean-of-a-new-observation-mathbfx_new" class="section level3 hasAnchor" number="9.4.2">
<h3><span class="header-section-number">9.4.2</span> Confidence intervals for the expected mean of a new observation <span class="math inline">\(\mathbf{x}_{new}\)</span><a href="normality-assumption.html#confidence-intervals-for-the-expected-mean-of-a-new-observation-mathbfx_new" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Note that the expected mean of a new observation <span class="math inline">\(\mathbf{x}_{new}\)</span> is given by:</p>
<p><span class="math display">\[ \mathbb{E}[y_{new}] = \mathbf{x}_{new}&#39; \boldsymbol{\beta}\]</span>
then we can consider, an estimate of this parameter, as:</p>
<p><span class="math display">\[ \mathbf{x}_{new}&#39; \hat{\boldsymbol{\beta}} \]</span>
this estimate is a linear combination of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, therefore it has a normal
distribution with mean and variance as follows:</p>
<p><span class="math display">\[\mathbb{E}[\mathbf{x}_{new} \hat{\boldsymbol{\beta}}] = \mathbf{x}_{new} \mathbb{E}[\hat{\boldsymbol{\beta}}] = \mathbf{x}_{new}&#39; \boldsymbol{\beta}\]</span>
<span class="math display">\[\mathbb{V}[\mathbf{x}_{new}&#39; \hat{\boldsymbol{\beta}}] = \mathbf{x}_{new}&#39; \mathbb{V}[\hat{\boldsymbol{\beta}}] \mathbf{x}_{new} = \mathbf{x}_{new}&#39; \sigma^2 (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{x}_{new} =  \sigma^2 \mathbf{x}_{new}&#39; (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{x}_{new} \]</span>
that is:
<span class="math display">\[ \mathbf{x}_{new}&#39; \hat{\boldsymbol{\beta}} \sim N \left(\mathbf{x}_{new}&#39; \boldsymbol{\beta}, \sigma^2 \mathbf{x}_{new}&#39; (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{x}_{new} \right)  \]</span></p>
<p>so, similarly, we can consider</p>
<p><span class="math display">\[ t_{\mathbf{x}_{new}&#39;\boldsymbol{\beta}} = \frac{\mathbf{x}_{new}&#39;\hat{\boldsymbol{\beta}} - \mathbf{x}_{new}&#39;\boldsymbol{\beta}}{\sqrt{\hat{\sigma}^2_{\mathbf{x}_{new}&#39;\boldsymbol{\beta}}}}=\frac{\frac{\mathbf{x}_{new}&#39;\hat{\boldsymbol{\beta}} - \mathbf{x}_{new}&#39;\boldsymbol{\beta}}{\sqrt{\sigma^2_{\mathbf{x}_{new}&#39;\boldsymbol{\beta}}}}}{\sqrt{\frac{\frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2}}{n-p}}} \sim \chi^2_{n-p} \]</span>
where <span class="math inline">\(\hat{\sigma}^2_{\mathbf{x}_{new}&#39;\boldsymbol{\beta}} = \hat{\sigma}^2 \mathbf{x}_{new}&#39; (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{x}_{new}\)</span>.
This quantity is distributed as a <span class="math inline">\(t\)</span> with <span class="math inline">\(n-p\)</span> degrees of freedom since:</p>
<p><span class="math display">\[\frac{\mathbf{x}_{new}&#39;\hat{\boldsymbol{\beta}} - \mathbf{x}_{new}&#39;\boldsymbol{\beta}}{\sqrt{\sigma^2_{\mathbf{x}_{new}&#39;\boldsymbol{\beta}}}} \sim N(0, 1)\]</span>
<span class="math display">\[ \frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2} \sim \chi^2_{n-p}\]</span>
and this random variables are independent since one is a function of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>
and the other a function of <span class="math inline">\(\hat{\mathbf{e}}\)</span>.</p>
<p>Then we can conclude that:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{P}
  &amp;\left( -t_{n-p}\left(\frac{\alpha}{2}\right) \leq t_{\mathbf{x}_{new}&#39;\boldsymbol{\beta}} \leq t_{n-p}\left(\frac{\alpha}{2}\right) \right) = \alpha  \\
  &amp;\implies \mathbb{P}\left( \mathbf{x}_{new}&#39;\hat{\boldsymbol{\beta}} - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\mathbf{x}_{new}&#39;\boldsymbol{\beta}}} \leq \mathbf{x}_{new}&#39;\boldsymbol{\beta}\leq \mathbf{x}_{new}&#39;\hat{\boldsymbol{\beta}} + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\mathbf{x}_{new}&#39;\boldsymbol{\beta}}} \right) = \alpha \\
\end{align*}\]</span></p>
<p>so, the random interval is given by:</p>
<p><span class="math display">\[ \left( \mathbf{x}_{new}&#39;\hat{\boldsymbol{\beta}} - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\mathbf{x}_{new}&#39;\boldsymbol{\beta}}} , \mathbf{x}_{new}&#39;\hat{\boldsymbol{\beta}} + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\mathbf{x}_{new}&#39;\boldsymbol{\beta}}} \right) \]</span>
is a random interval that captures <span class="math inline">\(\mathbf{x}_{new}&#39;\boldsymbol{\beta}\)</span> with probability <span class="math inline">\(\alpha\)</span>.</p>
</div>
<div id="confidence-intervals-for-linear-combinations-of-boldsymbolbeta" class="section level3 hasAnchor" number="9.4.3">
<h3><span class="header-section-number">9.4.3</span> Confidence intervals for linear combinations of <span class="math inline">\(\boldsymbol{\beta}\)</span><a href="normality-assumption.html#confidence-intervals-for-linear-combinations-of-boldsymbolbeta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Note that if we consider the parameter</p>
<p><span class="math display">\[ \mathbf{a}&#39; \hat{\boldsymbol{\beta}}\]</span>
then we note that <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\mathbf{x}_{new} \boldsymbol{\beta}\)</span> are particular cases, where the
value of <span class="math inline">\(\mathbf{a}\)</span> is a s follows:</p>
<p><span class="math display">\[ \mathbf{a}= (0,\ldots,0,1,0,\ldots,0) \quad \text{for} \quad \mathbf{a}\boldsymbol{\beta}= \beta_i\]</span>
<span class="math display">\[ \mathbf{a}= \mathbf{x}_{new} \quad \text{for} \quad \mathbf{a}\boldsymbol{\beta}= \mathbf{x}_{new}&#39; \boldsymbol{\beta}\]</span>
then, performing similar operations as before, we can create random intervals to
estimate <span class="math inline">\(\mathbf{a}&#39; \boldsymbol{\beta}\)</span> as follows:</p>
<p><span class="math display">\[ \left( \mathbf{a}&#39;\hat{\boldsymbol{\beta}} - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\mathbf{a}&#39;\boldsymbol{\beta}}} , \mathbf{a}&#39;\hat{\boldsymbol{\beta}} + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\mathbf{a}&#39;\boldsymbol{\beta}}} \right) \]</span>
that captures <span class="math inline">\(\mathbf{a}&#39; \boldsymbol{\beta}\)</span> with probability <span class="math inline">\(\alpha\)</span>. Where
<span class="math inline">\(\hat{\sigma}^2_{\mathbf{a}&#39;\boldsymbol{\beta}} = \hat{\sigma}^2 \mathbf{a}&#39; (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{a}\)</span>.</p>
</div>
</div>
<div id="hypothesis-testing" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Hypothesis Testing<a href="normality-assumption.html#hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will approach hypothesis testing using an implausibility framework.
This involves formulating a null hypothesis, <span class="math inline">\(H_0\)</span>, and assuming it to be true.
Next, we calculate a test statistic that follows a specific distribution under
the null hypothesis. By comparing the observed value of the statistic to this
distribution, we assess how plausible it is to observe such a value if <span class="math inline">\(H_0\)</span> is true.</p>
<div id="testing-for-the-overall-regression" class="section level3 hasAnchor" number="9.5.1">
<h3><span class="header-section-number">9.5.1</span> Testing for the Overall Regression<a href="normality-assumption.html#testing-for-the-overall-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For this hypothesis, we will use the notation of:</p>
<p><span class="math display">\[ \mathbf{X}^* = [\mathbf{1}\mathbf{X}] \quad \text{and} \quad \boldsymbol{\beta}^* = [\beta_0, \boldsymbol{\beta}]&#39; \in \mathbb{R}^{p}\]</span>
that is, the <span class="math inline">\(*\)</span> indicates all the independent variables. With <span class="math inline">\(\mathbf{X}\)</span> of full rank.</p>
<p>Our first test is to see if the Linear Regression framework is useful at all.
That is, we want to test <span class="math inline">\(\mathcal{H}_0: \boldsymbol{\beta}= \mathbf{0}\)</span>. Before designing our test
statistic we will show the following auxiliary results:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(SS_{reg} = \mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}\)</span>.</li>
<li><span class="math inline">\(\mathbf{H}\mathbf{H}_0 = \mathbf{H}_0 \mathbf{H}= \mathbf{H}_0\)</span>.</li>
<li><span class="math inline">\((\mathbf{H}- \mathbf{H}_0)\)</span> is idempotent.</li>
<li><span class="math inline">\(\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}\)</span> are independent.</li>
<li>Under the null hypothesis <span class="math inline">\(\mathcal{H}_0: \boldsymbol{\beta}= \mathbf{0}\)</span>, <span class="math inline">\(\frac{\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}}{\sigma^2}\)</span>
is distributed like a <span class="math inline">\(\chi^2_{p-1}\)</span>.</li>
</ol>
<p>For auxiliary result 1, we have that:</p>
<p><span class="math display">\[\begin{align*}
SS_{tot}
  &amp;= SS_{reg} + SS_{res} \\
  &amp;\implies \mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}= SS_{reg} + \mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}&amp;&amp; \text{since $SS_{tot} = \mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}$ and $SS_{res} = \mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}$.} \\
  &amp;\implies SS_{reg} = \mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}- \mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}&amp;&amp;                                                                                   \\
  &amp;\implies SS_{reg} = \mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0 - \mathbf{I}+ \mathbf{H})\mathbf{y}&amp;&amp;                                                                                   \\
  &amp;\implies SS_{reg} = \mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}&amp;&amp;                                                                                   \\
\end{align*}\]</span></p>
<p>For auxiliary result 2, we have that:</p>
<p>Both <span class="math inline">\(\mathbf{H}\)</span> and <span class="math inline">\(\mathbf{H}0\)</span> are symmetric, then <span class="math inline">\(\mathbf{H}\mathbf{H}_0 = \mathbf{H}_0 \mathbf{H}\)</span>, and:</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{H}\mathbf{H}_0 = \mathbf{H}\mathbf{1}(\mathbf{1}&#39; \mathbf{1})^{-1} \mathbf{1}&#39; &amp;&amp;                                     \\
  \mathbf{H}\mathbf{H}_0 = \mathbf{1}(\mathbf{1}&#39; \mathbf{1})^{-1} \mathbf{1}&#39;     &amp;&amp; \text{since $\mathbf{H}\mathbf{1}= \mathbf{1}$.} \\
  \mathbf{H}\mathbf{H}_0 = \mathbf{H}_0                                    &amp;&amp;                                     \\
\end{align*}\]</span></p>
<p>For auxiliary result 3, we have that:</p>
<p><span class="math display">\[\begin{align*}
  (\mathbf{H}- \mathbf{H}_0)(\mathbf{H}- \mathbf{H}_0)
    &amp;= \mathbf{H}\mathbf{H}- \mathbf{H}\mathbf{H}_0 - \mathbf{H}_0 \mathbf{H}+ \mathbf{H}_0 \mathbf{H}_0 &amp;&amp;                                                \\
    &amp;= \mathbf{H}- \mathbf{H}\mathbf{H}_0 - \mathbf{H}_0 \mathbf{H}+ \mathbf{H}_0           &amp;&amp; \text{since $\mathbf{H}_0$ and $\mathbf{H}$ are idempotent.} \\
    &amp;= \mathbf{H}- \mathbf{H}_0 - \mathbf{H}_0 + \mathbf{H}_0                   &amp;&amp; \text{since $\mathbf{H}\mathbf{H}_0 = \mathbf{H}_0 \mathbf{H}= \mathbf{H}_0$.}  \\
    &amp;= \mathbf{H}- \mathbf{H}_0                                   &amp;&amp;                                                \\
\end{align*}\]</span></p>
<p>so, <span class="math inline">\((\mathbf{H}- \mathbf{H}_0)\)</span> is idempotent.</p>
<p>For auxiliary result 4, first we have that:</p>
<p><span class="math display">\[\begin{align*}
  \mathbb{C}[(\mathbf{H}- \mathbf{H}_0) \mathbf{y}, (\mathbf{I}- \mathbf{H}) \mathbf{y}]
    &amp;= (\mathbf{H}- \mathbf{H}_0) \mathbb{C}[\mathbf{y},\mathbf{y}] (\mathbf{I}- \mathbf{H})        \\
    &amp;= (\mathbf{H}- \mathbf{H}_0) \mathbb{V}[\mathbf{y}] (\mathbf{I}- \mathbf{H})            \\
    &amp;= \sigma^2 (\mathbf{H}- \mathbf{H}_0) (\mathbf{I}- \mathbf{H})            \\
    &amp;= \sigma^2 (\mathbf{H}- \mathbf{H}_0  - \mathbf{H}\mathbf{H}+ \mathbf{H}_0 \mathbf{H}) \\
    &amp;= \sigma^2 (\mathbf{H}- \mathbf{H}_0  - \mathbf{H}+ \mathbf{H}_0 \mathbf{H})     &amp;&amp; \text{since $\mathbf{H}$ is idempotent.} \\
    &amp;= \sigma^2 (\mathbf{H}- \mathbf{H}_0  - \mathbf{H}+ \mathbf{H}_0)         &amp;&amp; \text{since $\mathbf{H}_0 \mathbf{H}= \mathbf{H}_0$.} \\
    &amp;= \sigma^2 \mathbf{0}&amp;&amp;                                   \\
    &amp;= \mathbf{0}&amp;&amp;                                   \\
\end{align*}\]</span></p>
<p>This tells us that <span class="math inline">\((\mathbf{H}- \mathbf{H}_0)\mathbf{y}\)</span> and <span class="math inline">\((\mathbf{I}- \mathbf{H})\mathbf{y}\)</span> are uncorrelated. Now,
since <span class="math inline">\((\mathbf{H}- \mathbf{H}_0)\mathbf{y}\)</span> and <span class="math inline">\((\mathbf{I}- \mathbf{H})\mathbf{y}\)</span> are normally distributed, then
zero correlation implies independence. Then, any function of this 2 quantities
are independent. Note that:</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}
    &amp;= \mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)(\mathbf{H}- \mathbf{H}_0)(\mathbf{H}- \mathbf{H}_0)\mathbf{y}&amp;&amp; \text{since $(\mathbf{H}- \mathbf{H}_0)$ is idempotent.}
\end{align*}\]</span></p>
<p>Then, <span class="math inline">\(\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}\)</span> is a quadratic function of <span class="math inline">\((\mathbf{H}- \mathbf{H}_0)\mathbf{y}\)</span>. Similarly,
<span class="math inline">\(\mathbf{y}&#39;(\mathbf{H}- \mathbf{H})\mathbf{y}\)</span> is a quadratic function of <span class="math inline">\((\mathbf{H}- \mathbf{H})\mathbf{y}\)</span>. Therefore,
<span class="math inline">\(\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{y}&#39;(\mathbf{H}- \mathbf{H})\mathbf{y}\)</span> are independent.</p>
<p>For result 5, we have that:</p>
<p><span class="math display">\[\begin{align*}
  (\mathbf{H}- \mathbf{H}_0)\mathbf{y}
    &amp;= (\mathbf{H}- \mathbf{H}_0)(\mathbf{X}^* \boldsymbol{\beta}^* + \mathbf{e}) &amp;&amp; \text{since $\mathbf{y}= \mathbf{X}\boldsymbol{\beta}+ \mathbf{e}$.} \\
    &amp;= (\mathbf{H}- \mathbf{H}_0)([\mathbf{1}\mathbf{X}] [\beta_0, \boldsymbol{\beta}]&#39; + \mathbf{e})                &amp;&amp; \text{since $\mathbf{X}^* = [\mathbf{1}\mathbf{X}] \quad \text{and} \quad \boldsymbol{\beta}^* = [\beta_0, \boldsymbol{\beta}]&#39;$.} \\
    &amp;= (\mathbf{H}- \mathbf{H}_0)(\mathbf{1}\beta_0 + \mathbf{X}\boldsymbol{\beta}+ \mathbf{e}) &amp;&amp;  \\
    &amp;= (\mathbf{H}- \mathbf{H}_0)(\mathbf{1}\beta_0) + (\mathbf{H}- \mathbf{H}_0)(\mathbf{X}\boldsymbol{\beta}+ \mathbf{e}) &amp;&amp;  \\
    &amp;= (\mathbf{H}\mathbf{1}- \mathbf{H}_0 \mathbf{1})\beta_0 + (\mathbf{H}- \mathbf{H}_0)(\mathbf{X}\boldsymbol{\beta}+ \mathbf{e}) &amp;&amp;  \\
    &amp;= (\mathbf{1}- \mathbf{1})\beta_0 + (\mathbf{H}- \mathbf{H}_0)(\mathbf{X}\boldsymbol{\beta}+ \mathbf{e}) &amp;&amp;  \\
    &amp;= (\mathbf{H}- \mathbf{H}_0)(\mathbf{X}\boldsymbol{\beta}+ \mathbf{e}) &amp;&amp;  \\
    &amp;= (\mathbf{H}- \mathbf{H}_0)\mathbf{e}&amp;&amp; \text{iff $\mathcal{H}_0: \boldsymbol{\beta}= \mathbf{0}$ for any full rank $\mathbf{X}$.}
\end{align*}\]</span></p>
<p>That is, for any full rank <span class="math inline">\(\mathbf{X}\)</span>, we have that:</p>
<p><span class="math display">\[ (\mathbf{H}- \mathbf{H}_0)\mathbf{y}= (\mathbf{H}- \mathbf{H}_0)\mathbf{e}\iff \mathcal{H}_0: \boldsymbol{\beta}= \mathbf{0}\]</span></p>
<p>Then:</p>
<p><span class="math display">\[\begin{align*}
  \mathbf{e}\sim N(0, \sigma^2 \mathbf{I})
    &amp;\implies \mathbf{e}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{e}\sim \sigma^2 \chi^2_{p-1}                                   &amp;&amp; \text{since $(\mathbf{H}- \mathbf{H}_0)$ is idempotent of rank $p-1$}. \\
    &amp;\implies \frac{\mathbf{e}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{e}}{\sigma^2} \sim \chi^2_{p-1}                           &amp;&amp;                                                           \\
    &amp;\implies \frac{\mathbf{e}&#39;(\mathbf{H}- \mathbf{H}_0)(\mathbf{H}- \mathbf{H}_0)(\mathbf{H}- \mathbf{H}_0)\mathbf{e}}{\sigma^2} \sim \chi^2_{p-1} &amp;&amp; \text{since $(\mathbf{H}- \mathbf{H}_0)$ is idempotent}.               \\
    &amp;\implies \frac{\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)(\mathbf{H}- \mathbf{H}_0)(\mathbf{H}- \mathbf{H}_0)\mathbf{y}}{\sigma^2} \sim \chi^2_{p-1} &amp;&amp; \text{iff the null hypothesis holds}.                     \\
    &amp;\implies \frac{\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}}{\sigma^2} \sim \chi^2_{p-1}                           &amp;&amp; \text{since $(\mathbf{H}- \mathbf{H}_0)$ is idempotent}.               \\
\end{align*}\]</span></p>
<p>That is:</p>
<p><span class="math display">\[ \frac{\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}}{\sigma^2} \sim \chi^2_{p-1} \iff \mathcal{H}_0: \boldsymbol{\beta}= \mathbf{0}\]</span></p>
<p>With this results, we propose the following statistic:</p>
<p><span class="math display">\[ F_{\boldsymbol{\beta}= 0} = \frac{\frac{SS_{reg}}{p-1}}{\frac{SS_{res}}{n-p}} \]</span></p>
<p>and we will show that, this statistic is distributed like an <span class="math inline">\(F_{p-1,n-p}\)</span> only
under the null hypothesis.</p>
<p><span class="math display">\[\begin{align*}
  F_{\boldsymbol{\beta}= 0}
    &amp;= \frac{\frac{SS_{reg}}{p-1}}{\frac{SS_{res}}{n-p}}                                                           &amp;&amp;                                                                                                   \\
    &amp;= \frac{\frac{\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}}{p-1}}{\frac{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}}{n-p}}                                     &amp;&amp; \text{since $SS_{reg} = \mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}$ and $SS_{res}=\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}$}                  \\
    &amp;= \frac{\frac{\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}}{\sigma^2}\frac{1}{p-1}}{\frac{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}}{\sigma^2}\frac{1}{n-p}} &amp;&amp;                                                                                                   \\
    &amp;\sim \frac{\frac{\chi^2_{p-1}}{p-1}}{\frac{\chi^2_{n-p}}{n-p}}                                                &amp;&amp; \text{since $\frac{\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}}{\sigma^2} \sim \chi^2_{p-1}$ under the null hypothesis.} \\
    &amp;\sim F_{p-1,n-p}                                                                                              &amp;&amp; \text{since $\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_0)\mathbf{y}$ and $\mathbf{y}&#39;(\mathbf{H}- \mathbf{H})\mathbf{y}$ are independent.}
\end{align*}\]</span></p>
<p>So, once we observe the value of this statistic, we can contrast it with respect
respect to this distribution. Call <span class="math inline">\(F^*_{\boldsymbol{\beta}= 0}\)</span> the observed value, and consider
a random variable <span class="math inline">\(F \sim F_{p-1,n-p}\)</span>, then we can see what would be the
probability of observing the value of the statistic (or a more extreme value).</p>
<p><span class="math display">\[ \mathbb{P}(F \geq F^*_{\boldsymbol{\beta}= 0}) \]</span>
depending on how small or big is this probability, we can reject or not reject the
null hypothesis. This value is a called a p-value.</p>
</div>
<div id="testing-if-one-variable-is-not-relevant" class="section level3 hasAnchor" number="9.5.2">
<h3><span class="header-section-number">9.5.2</span> Testing if one variable is not relevant<a href="normality-assumption.html#testing-if-one-variable-is-not-relevant" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can test if a particular variable is not relevant for the regression. That is,
<span class="math inline">\(\mathcal{H}_0: \beta_i = 0\)</span>. We will use the same strategy, that is, we will build a
test statistic that has a certain distribution only under the null hypothesis.</p>
<p>For this hypothesis we propose the following test statistic:</p>
<p><span class="math display">\[ t_{\beta_i = 0} = \frac{\hat{\beta}_i}{\sqrt{\hat{\sigma}^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii}}} \]</span>
First note that:</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}} \sim N \left(\boldsymbol{\beta}, \sigma^2(\mathbf{X}\mathbf{X})^{-1}\right)
  &amp;\implies \hat{\beta}_i \sim H(\beta_i, \sigma^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii})                                                                              \\
  &amp;\implies \frac{\hat{\beta}_i}{\sqrt{\sigma^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii}}} \sim N \left(\frac{\beta_i}{\sqrt{\sigma^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii}}}, 1 \right) \\
  &amp;\implies \frac{\hat{\beta}_i}{\sqrt{\sigma^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii}}} \sim N \left( 0, 1 \right) &amp;&amp;  \iff \mathcal{H}_0: \beta_i = 0                         \\
\end{align*}\]</span></p>
<p>Then we have:</p>
<p>$$$$</p>
<p><span class="math display">\[\begin{align*}
t_{\beta_i = 0}
  &amp;= \frac{\hat{\beta}_i}{\sqrt{\hat{\sigma}^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii}}} \\
  &amp;= \frac{\frac{\hat{\beta}_i }{\sqrt{\sigma^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii}}}}{\frac{\sqrt{\hat{\sigma}^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii}}}{\sqrt{\sigma^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii}}}} \\
  &amp;= \frac{\frac{\hat{\beta}_i }{\sqrt{\sigma^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii}}}}{\sqrt{\frac{\hat{\sigma}^2}{\sigma^2}}}                                                    \\
  &amp;= \frac{\frac{\hat{\beta}_i }{\sqrt{\sigma^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii}}}}{\sqrt{\frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2}\frac{1}{n-p}}}                                  &amp;&amp; \text{since $\hat{\sigma}^2 = \frac{\hat{\mathbf{e}}\hat{\mathbf{e}}}{n-p}$} \\
  &amp;\sim \frac{N \left(\frac{\beta_i}{\sqrt{\sigma^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii}}}, 1 \right)}{\sqrt{\frac{\chi^2_{n-p}}{n-p}}}                                            &amp;&amp; \text{since $\frac{\hat{\beta}_i}{\sqrt{\sigma^2 [(\mathbf{X}\mathbf{X})^{-1}]_{ii}}} \sim N \left( 0, 1 \right)$ and $\frac{\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}}{\sigma^2} \sim \chi^2_{n-p}$} \\
  &amp;\sim \frac{N \left(0, 1 \right)}{\sqrt{\frac{\chi^2_{n-p}}{n-p}}}                                                                                                &amp;&amp; \iff \mathcal{H}_0: \beta_i = 0  \\
  &amp;\sim t_{n-p}                                                                                                                                                     &amp;&amp; \text{since $\hat{\beta}_i$ and $\hat{\sigma}^2$ are independent}.  \\
\end{align*}\]</span></p>
<p>Then, under the null hypothesis we have that:</p>
<p><span class="math display">\[ t_{\beta_i = 0} \sim  t_{n-p}\]</span>
So, if we call <span class="math inline">\(t_{\beta_i = 0}^*\)</span> the observed value of <span class="math inline">\(t_{\beta_i = 0}\)</span>, and
if we let <span class="math inline">\(t\)</span> be distributed as <span class="math inline">\(t_{n-p}\)</span>, we can compute:</p>
<p><span class="math display">\[ \mathbb{P}(t \geq t_{\beta_i = 0}^*) \]</span>
and depending on the value, we can reject or accept the null hypothesis.</p>
</div>
<div id="testing-if-a-subgroup-of-the-variables-is-relevant" class="section level3 hasAnchor" number="9.5.3">
<h3><span class="header-section-number">9.5.3</span> Testing if a Subgroup of the Variables is Relevant<a href="normality-assumption.html#testing-if-a-subgroup-of-the-variables-is-relevant" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For this test, we can assume without loss of generality, that the variables we
want to see if it is relevant are the first <span class="math inline">\(k\)</span>. So we can divide the design
matrix as:</p>
<p><span class="math display">\[ \mathbf{X}= [\mathbf{X}_1 \mathbf{X}_2] \]</span>
where the variables to test are in <span class="math inline">\(\mathbf{X}_1\)</span> and the rest of the variables are in
<span class="math inline">\(\mathbf{X}_2\)</span> (including possibly the intercept). And similarly we have <span class="math inline">\(\boldsymbol{\beta}= [\boldsymbol{\beta}_1 \boldsymbol{\beta}_2]&#39;\)</span>.</p>
<p>This test is similar to the first test once we express it accordingly. We will
consider two linear regressions. One including all variables and one excluding
the variables to be tested indexed by <span class="math inline">\(2\)</span>. With this we can build the following
test statistics:</p>
<p><span class="math display">\[ F_{\boldsymbol{\beta}_1=\mathbf{0}} = \frac{\frac{SS_{res,2} - SS_{res}}{k}}{\frac{SS_{res}}{n-p}} \]</span>
Then note the following:</p>
<p><span class="math display">\[\begin{align*}
  SS_{res,2} - SS_{res}
    &amp;= \mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_2)\mathbf{y}- \mathbf{y}&#39;(\mathbf{I}- \mathbf{H}) \mathbf{y}\\
    &amp;= \mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_2 - \mathbf{I}+ \mathbf{H}) \mathbf{y}\\
    &amp;= \mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_2) \mathbf{y}\\
\end{align*}\]</span></p>
<p>Again, we will see that <span class="math inline">\((\mathbf{H}- \mathbf{H}_2)\)</span> is idempotent and <span class="math inline">\((\mathbf{H}- \mathbf{H}_2)\mathbf{y}= (\mathbf{H}- \mathbf{H}_2)\mathbf{e}\)</span>
only under the null hypothesis.</p>
<p>First, let us see that <span class="math inline">\((\mathbf{H}- \mathbf{H}_2)\)</span> is idempotent. First note that:</p>
<p><span class="math display">\[ \mathbf{H}\mathbf{H}_2 = \mathbf{H}_2 \mathbf{H}= \mathbf{H}_2\]</span>
since <span class="math inline">\(\mathbf{H}_2\)</span> is the projection matrix of the columns of <span class="math inline">\(\mathbf{X}_2\)</span> a subspace of the
columns of <span class="math inline">\(\mathbf{X}\)</span>. Then:</p>
<p><span class="math display">\[\begin{align*}
  (\mathbf{H}- \mathbf{H}_2)(\mathbf{H}- \mathbf{H}_2)
    &amp;= \mathbf{H}\mathbf{H}- \mathbf{H}_2 \mathbf{H}- \mathbf{H}\mathbf{H}_2 + \mathbf{H}_2 \mathbf{H}_2 \\
    &amp;= \mathbf{H}- \mathbf{H}_2 \mathbf{H}- \mathbf{H}\mathbf{H}_2 + \mathbf{H}_2           &amp;&amp; \text{since $\mathbf{H}_2$ and $\mathbf{H}$ are idempotent}. \\
    &amp;= \mathbf{H}- \mathbf{H}_2 - \mathbf{H}_2 + \mathbf{H}_2                   &amp;&amp; \text{since $\mathbf{H}\mathbf{H}_2 = \mathbf{H}_2 \mathbf{H}= \mathbf{H}_2$}.  \\
    &amp;= \mathbf{H}- \mathbf{H}_2                                   &amp;&amp;                                                \\
\end{align*}\]</span></p>
<p>then <span class="math inline">\((\mathbf{H}- \mathbf{H}_2)\)</span> is idempotent.</p>
<p>Now let us see that <span class="math inline">\((\mathbf{H}- \mathbf{H}_R)\mathbf{y}= (\mathbf{H}- \mathbf{H}_R)\mathbf{e}\)</span> under the null hypothesis.
First, let us note that:</p>
<p><span class="math display">\[ \mathbf{H}\mathbf{X}_2 = \mathbf{X}_2 \]</span>
since space generated by <span class="math inline">\(\mathbf{X}_2\)</span> is a subspace of the space generated by <span class="math inline">\(\mathbf{X}\)</span>,
since <span class="math inline">\(\mathbf{X}\)</span> contains the columns of <span class="math inline">\(\mathbf{X}_2\)</span>. And we also note that:</p>
<p><span class="math display">\[ \mathbf{H}_2 \mathbf{X}_2 = \mathbf{X}_2 \]</span>
since <span class="math inline">\(\mathbf{H}_2\)</span> is the projection matrix of the space generated by the columns of
<span class="math inline">\(\mathbf{X}_2\)</span>. We note that this results can be proven algebraically.</p>
<p>Then:</p>
<p><span class="math display">\[\begin{align*}
  (\mathbf{H}- \mathbf{H}_2)\mathbf{y}
    &amp;= (\mathbf{H}- \mathbf{H}_2)(\mathbf{X}\boldsymbol{\beta}+ \mathbf{e})                                      \\
    &amp;= (\mathbf{H}- \mathbf{H}_2)([\mathbf{X}_1 \mathbf{X}_2] [\boldsymbol{\beta}_1&#39; \boldsymbol{\beta}_2&#39;]&#39; + \mathbf{e})              \\
    &amp;= (\mathbf{H}- \mathbf{H}_2)(\mathbf{X}_1 \boldsymbol{\beta}_1 \mathbf{X}_2 \boldsymbol{\beta}_2 + \mathbf{e})                     \\
    &amp;= (\mathbf{H}- \mathbf{H}_2)(\mathbf{X}_2 \boldsymbol{\beta}_2) + (\mathbf{H}- \mathbf{H}_1)(\mathbf{X}_1 \boldsymbol{\beta}_1 + \mathbf{e})    \\
    &amp;= (\mathbf{H}\mathbf{X}_2 - \mathbf{H}_2\mathbf{X}_2)\boldsymbol{\beta}_2 + (\mathbf{H}- \mathbf{H}_1)(\mathbf{X}_1 \boldsymbol{\beta}_1 + \mathbf{e}) \\
    &amp;= (\mathbf{X}_2 - \mathbf{X}_2)\boldsymbol{\beta}_2 + (\mathbf{H}- \mathbf{H}_1)(\mathbf{X}_1 \boldsymbol{\beta}_1 + \mathbf{e})          &amp;&amp; \text{since $\mathbf{H}\mathbf{X}_1 = \mathbf{X}_1$ and $\mathbf{H}_1 \mathbf{X}_1 = \mathbf{X}_1$} \\
    &amp;= (\mathbf{H}- \mathbf{H}_R)(\mathbf{X}_1 \boldsymbol{\beta}_1 + \mathbf{e})                                  &amp;&amp;                                                            \\
    &amp;= (\mathbf{H}- \mathbf{H}_R)\mathbf{e}&amp;&amp; \iff \mathcal{H}_0: \boldsymbol{\beta}_1 = \mathbf{0}\\
\end{align*}\]</span></p>
<p>So, if <span class="math inline">\(\mathbf{X}_1\)</span> is full rank, then we have that:</p>
<p><span class="math display">\[ (\mathbf{H}- \mathbf{H}_2)\mathbf{y}= (\mathbf{H}- \mathbf{H}_R)\mathbf{e}\iff \mathcal{H}_0: \boldsymbol{\beta}_1 = \mathbf{0}\]</span></p>
<p>Then we can proceed to see what is the distribution of our test statistic under
the null hypothesis.</p>
<p><span class="math display">\[\begin{align*}
  F_{\boldsymbol{\beta}_1=\mathbf{0}}
    &amp;= \frac{\frac{SS_{res,2} - SS_{res}}{k}}{\frac{SS_{res}}{n-p}}                                                                          &amp;&amp;                                                                                                   \\
    &amp;= \frac{\frac{\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_2)\mathbf{y}}{k}}{\frac{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}}{n-p}}                                                                 &amp;&amp; \text{since $SS_{res,2} - SS_{res} = \mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_2)\mathbf{y}$ and $SS_{res}=\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}$}     \\
    &amp;= \frac{\frac{\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_2)\mathbf{y}}{\sigma^2}\frac{1}{k}}{\frac{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}}{\sigma^2}\frac{1}{n-p}}                             &amp;&amp;                                                                                                   \\
    &amp;= \frac{\frac{\mathbf{y}&#39;(\mathbf{H}- \mathbf{H}_2)(\mathbf{H}- \mathbf{H}_2)(\mathbf{H}- \mathbf{H}_2)\mathbf{y}}{\sigma^2}\frac{1}{k}}{\frac{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}}{\sigma^2}\frac{1}{n-p}}   &amp;&amp; \text{snce $(\mathbf{H}- \mathbf{H}_2)$ is idempotent}.                                                        \\
    &amp;= \frac{\frac{\mathbf{e}&#39;(\mathbf{H}- \mathbf{H}_2)(\mathbf{H}- \mathbf{H}_2)(\mathbf{H}- \mathbf{H}_2)\mathbf{e}}{\sigma^2}\frac{1}{k}}{\frac{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}}{\sigma^2}\frac{1}{n-p}}   &amp;&amp; \iff  \mathcal{H}_0: \boldsymbol{\beta}_1 = \mathbf{0}\\
    &amp;= \frac{\frac{\mathbf{e}&#39;(\mathbf{H}- \mathbf{H}_2)\mathbf{e}}{\sigma^2}\frac{1}{k}}{\frac{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}}{\sigma^2}\frac{1}{n-p}}                             &amp;&amp; \text{since $(\mathbf{H}- \mathbf{H}_2)$ is idempotent}.                                                       \\
    &amp;\sim \frac{\frac{\chi^2_{k}}{k}}{\frac{\chi^2_{n-p}}{n-p}}                                                                              &amp;&amp; \text{since $(\mathbf{H}- \mathbf{H}_2)$ is idempotent and $\frac{\mathbf{e}}{\sqrt{\sigma^2}} \sim N(0, \mathbf{I})$}.      \\
    &amp;\sim F_{k,n-p}                                                                                                                          &amp;&amp;
\end{align*}\]</span></p>
<p>So, before we observe the data, <span class="math inline">\(F_{\boldsymbol{\beta}_1=\mathbf{0}}\)</span> has a <span class="math inline">\(F_{k,n-p}\)</span> distribution.
Then, once we observe the data, call <span class="math inline">\(F_{\boldsymbol{\beta}_1=\mathbf{0}}^*\)</span> the observed value of
the statistic, and let <span class="math inline">\(F\)</span> be distributed as an <span class="math inline">\(F_{k,n-p}\)</span>, we can compute:</p>
<p><span class="math display">\[ \mathbb{P}(F \geq F_{\boldsymbol{\beta}_1=\mathbf{0}}^*) \]</span>
and reject the null hypothesis if this probability is small and not reject if
this probability is small.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mean-and-varaince-assumptions.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
