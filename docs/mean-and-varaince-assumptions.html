<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Mean and Varaince Assumptions | _main.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="8 Mean and Varaince Assumptions | _main.knit" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Mean and Varaince Assumptions | _main.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bootstrapping.html"/>
<link rel="next" href="normality-assumption.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Stat 5385/6385 Fall 2025</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#calendar"><i class="fa fa-check"></i><b>1.1</b> Calendar</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#important-dates"><i class="fa fa-check"></i><b>1.1.1</b> Important Dates</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#class-schedule"><i class="fa fa-check"></i><b>1.1.2</b> Class Schedule</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#course-overview"><i class="fa fa-check"></i><b>1.2</b> Course Overview</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#chapter-2-mathematical-prerequisites"><i class="fa fa-check"></i><b>1.2.1</b> Chapter 2 — Mathematical Prerequisites</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#chapter-3-the-linear-regression-problem"><i class="fa fa-check"></i><b>1.2.2</b> Chapter 3 — The Linear Regression Problem</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#chapters-4-to-6-linear-regression-as-an-optimization-problem"><i class="fa fa-check"></i><b>1.2.3</b> Chapters 4 to 6 — Linear Regression as an Optimization Problem</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#chapter-7-introducing-uncertainty"><i class="fa fa-check"></i><b>1.2.4</b> Chapter 7 — Introducing Uncertainty</a></li>
<li class="chapter" data-level="1.2.5" data-path="index.html"><a href="index.html#chapters-8-and-9-probabilistic-modeling-and-statistical-inference"><i class="fa fa-check"></i><b>1.2.5</b> Chapters 8 and 9 — Probabilistic Modeling and Statistical Inference</a></li>
<li class="chapter" data-level="1.2.6" data-path="index.html"><a href="index.html#summary"><i class="fa fa-check"></i><b>1.2.6</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#general-math"><i class="fa fa-check"></i><b>2.1</b> General Math</a></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#zero-matrix"><i class="fa fa-check"></i><b>2.2.1</b> Zero Matrix</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.2.2</b> Linear Independence</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.2.3</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.2.4</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.2.5" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.2.5</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.2.6" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.2.6</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.2.7" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.2.7</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.2.8" data-path="prerequisites.html"><a href="prerequisites.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.2.8</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="2.2.9" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.2.9</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.2.10" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.2.10</b> Idempotent Matrix</a></li>
<li class="chapter" data-level="2.2.11" data-path="prerequisites.html"><a href="prerequisites.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>2.2.11</b> Determinant of a Matrix</a></li>
<li class="chapter" data-level="2.2.12" data-path="prerequisites.html"><a href="prerequisites.html#trace-of-a-matrix"><i class="fa fa-check"></i><b>2.2.12</b> Trace of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.3</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.3.1</b> Gradient</a></li>
<li class="chapter" data-level="2.3.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.3.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.3.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.3.3</b> Applications:</a></li>
<li class="chapter" data-level="2.3.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.3.4</b> Matrix Calculus</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.4</b> Probability</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="prerequisites.html"><a href="prerequisites.html#expected-value"><i class="fa fa-check"></i><b>2.4.1</b> Expected Value</a></li>
<li class="chapter" data-level="2.4.2" data-path="prerequisites.html"><a href="prerequisites.html#variance"><i class="fa fa-check"></i><b>2.4.2</b> Variance</a></li>
<li class="chapter" data-level="2.4.3" data-path="prerequisites.html"><a href="prerequisites.html#cross-covariance-matrix"><i class="fa fa-check"></i><b>2.4.3</b> Cross-Covariance Matrix</a></li>
<li class="chapter" data-level="2.4.4" data-path="prerequisites.html"><a href="prerequisites.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.4.4</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="2.4.5" data-path="prerequisites.html"><a href="prerequisites.html#chi2-distribution"><i class="fa fa-check"></i><b>2.4.5</b> <span class="math inline">\(\chi^2\)</span> Distribution</a></li>
<li class="chapter" data-level="2.4.6" data-path="prerequisites.html"><a href="prerequisites.html#t-distribution"><i class="fa fa-check"></i><b>2.4.6</b> <span class="math inline">\(t\)</span> Distribution</a></li>
<li class="chapter" data-level="2.4.7" data-path="prerequisites.html"><a href="prerequisites.html#f-distribution"><i class="fa fa-check"></i><b>2.4.7</b> <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.5</b> Statistics</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="prerequisites.html"><a href="prerequisites.html#bias-of-an-estimator"><i class="fa fa-check"></i><b>2.5.1</b> Bias of an Estimator</a></li>
<li class="chapter" data-level="2.5.2" data-path="prerequisites.html"><a href="prerequisites.html#unbiased-estimator"><i class="fa fa-check"></i><b>2.5.2</b> Unbiased Estimator</a></li>
<li class="chapter" data-level="2.5.3" data-path="prerequisites.html"><a href="prerequisites.html#mean-square-error-of-an-estimator"><i class="fa fa-check"></i><b>2.5.3</b> Mean Square Error of an Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#objectives-of-linear-regression"><i class="fa fa-check"></i><b>3.1</b> Objectives of Linear Regression</a></li>
<li class="chapter" data-level="3.2" data-path="introduction.html"><a href="introduction.html#examples-1"><i class="fa fa-check"></i><b>3.2</b> Examples</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="introduction.html"><a href="introduction.html#ad-spending"><i class="fa fa-check"></i><b>3.2.1</b> Ad Spending</a></li>
<li class="chapter" data-level="3.2.2" data-path="introduction.html"><a href="introduction.html#winw-example"><i class="fa fa-check"></i><b>3.2.2</b> Wine and Life Expectancy</a></li>
<li class="chapter" data-level="3.2.3" data-path="introduction.html"><a href="introduction.html#burger-demand"><i class="fa fa-check"></i><b>3.2.3</b> Burger Demand</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#intro-to-slr"><i class="fa fa-check"></i><b>4.1</b> Intro to SLR</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#a-path-not-taken"><i class="fa fa-check"></i><b>4.1.1</b> A path not Taken</a></li>
<li class="chapter" data-level="4.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#slr-model"><i class="fa fa-check"></i><b>4.1.2</b> SLR Model</a></li>
<li class="chapter" data-level="4.1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#possible-optimization-problems"><i class="fa fa-check"></i><b>4.1.3</b> Possible Optimization Problems</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>4.2</b> Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#other-estimated-quantites"><i class="fa fa-check"></i><b>4.2.1</b> Other estimated quantites</a></li>
<li class="chapter" data-level="4.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#properties-of-the-slr-problem"><i class="fa fa-check"></i><b>4.2.2</b> Properties of the SLR problem</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#properties-of-the-estimates"><i class="fa fa-check"></i><b>4.3</b> Properties of the Estimates</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hatbeta_0-and-hatbeta_1-are-linear-combinations-of"><i class="fa fa-check"></i><b>4.3.1</b> <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are linear combinations of</a></li>
<li class="chapter" data-level="4.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-sum-of-the-residuals-is-0"><i class="fa fa-check"></i><b>4.3.2</b> The sum of the residuals is <span class="math inline">\(0\)</span></a></li>
<li class="chapter" data-level="4.3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hatmathbfe-and-mathbfx-are-orthogonal"><i class="fa fa-check"></i><b>4.3.3</b> <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> are orthogonal</a></li>
<li class="chapter" data-level="4.3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#hatmathbfy-and-hatmathbfe-are-orthogonal"><i class="fa fa-check"></i><b>4.3.4</b> <span class="math inline">\(\hat{\mathbf{y}}\)</span> and <span class="math inline">\(\hat{\mathbf{e}}\)</span> are orthogonal</a></li>
<li class="chapter" data-level="4.3.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-average-of-hatmathbfy-and-mathbfy-are-the-same"><i class="fa fa-check"></i><b>4.3.5</b> The average of <span class="math inline">\(\hat{\mathbf{y}}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are the same</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-and-standarizing-the-data"><i class="fa fa-check"></i><b>4.4</b> Centering and Standarizing the Data</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#remarks-on-centering-and-standarization"><i class="fa fa-check"></i><b>4.4.1</b> Remarks on Centering and Standarization</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary-centering-and-standarizing"><i class="fa fa-check"></i><b>4.4.2</b> Summary Centering and Standarizing</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-and-standarizing-in-slr"><i class="fa fa-check"></i><b>4.5</b> Centering and Standarizing in SLR</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centered-independent-variable"><i class="fa fa-check"></i><b>4.5.1</b> Centered Independent Variable</a></li>
<li class="chapter" data-level="4.5.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#both-variables-centered"><i class="fa fa-check"></i><b>4.5.2</b> Both Variables Centered</a></li>
<li class="chapter" data-level="4.5.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#both-variables-standarized"><i class="fa fa-check"></i><b>4.5.3</b> Both variables Standarized</a></li>
<li class="chapter" data-level="4.5.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary-of-centering-and-standarizing-in-slr"><i class="fa fa-check"></i><b>4.5.4</b> Summary of Centering and Standarizing in SLR</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>4.6</b> Coefficient of Determination</a></li>
<li class="chapter" data-level="4.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residual-analysis"><i class="fa fa-check"></i><b>4.7</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#non-linear-regression-function"><i class="fa fa-check"></i><b>4.7.1</b> Non-linear regression function</a></li>
<li class="chapter" data-level="4.7.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#heteroscedasticity"><i class="fa fa-check"></i><b>4.7.2</b> Heteroscedasticity</a></li>
<li class="chapter" data-level="4.7.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outliers"><i class="fa fa-check"></i><b>4.7.3</b> Outliers</a></li>
<li class="chapter" data-level="4.7.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#variables-ommited"><i class="fa fa-check"></i><b>4.7.4</b> Variables Ommited</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#cross-validation"><i class="fa fa-check"></i><b>4.8</b> Cross-Validation</a></li>
<li class="chapter" data-level="4.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>4.9</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="4.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-in-matrix-form"><i class="fa fa-check"></i><b>4.10</b> Model in Matrix Form</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares-in-matrix-form"><i class="fa fa-check"></i><b>4.10.1</b> Weighted Least Squares in Matrix Form</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="polynomial-regression.html"><a href="polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Polynomial Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="polynomial-regression.html"><a href="polynomial-regression.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="polynomial-regression.html"><a href="polynomial-regression.html#taylor-polynomials-and-polynomial-regression"><i class="fa fa-check"></i><b>5.1.1</b> Taylor polynomials and polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="polynomial-regression.html"><a href="polynomial-regression.html#why-use-the-sample-mean-as-the-expansion-center-point"><i class="fa fa-check"></i><b>5.2</b> Why use the sample mean as the expansion (center) point?</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="polynomial-regression.html"><a href="polynomial-regression.html#examples-2"><i class="fa fa-check"></i><b>5.2.1</b> Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#example-modeling-gdp"><i class="fa fa-check"></i><b>6.2</b> Example: Modeling GDP</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#simple-regressions"><i class="fa fa-check"></i><b>6.2.1</b> Simple Regressions</a></li>
<li class="chapter" data-level="6.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-regression"><i class="fa fa-check"></i><b>6.2.2</b> Multiple Regression</a></li>
<li class="chapter" data-level="6.2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#summary-2"><i class="fa fa-check"></i><b>6.2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#least-squares-estimation-1"><i class="fa fa-check"></i><b>6.3</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#properties-of-the-estimates-1"><i class="fa fa-check"></i><b>6.4</b> Properties of the Estimates</a></li>
<li class="chapter" data-level="6.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-r2"><i class="fa fa-check"></i><b>6.5</b> Multiple <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="6.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#geometric-interpretation-of-multiple-linear-regression"><i class="fa fa-check"></i><b>6.6</b> Geometric Interpretation of Multiple Linear Regression</a></li>
<li class="chapter" data-level="6.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#centered-and-standarized-variables"><i class="fa fa-check"></i><b>6.7</b> Centered and Standarized Variables</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#centered-variables"><i class="fa fa-check"></i><b>6.7.1</b> Centered Variables</a></li>
<li class="chapter" data-level="6.7.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sample-covariance"><i class="fa fa-check"></i><b>6.7.2</b> Sample Covariance</a></li>
<li class="chapter" data-level="6.7.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#satandard-variables"><i class="fa fa-check"></i><b>6.7.3</b> Satandard Variables</a></li>
<li class="chapter" data-level="6.7.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sample-correlation-matrix"><i class="fa fa-check"></i><b>6.7.4</b> Sample Correlation Matrix</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#variable-cross-effects"><i class="fa fa-check"></i><b>6.8</b> Variable Cross-Effects</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#single-variable-cross-effects"><i class="fa fa-check"></i><b>6.8.1</b> Single Variable Cross-Effects</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#outliers-and-leverage"><i class="fa fa-check"></i><b>6.9</b> Outliers and Leverage</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#leverage"><i class="fa fa-check"></i><b>6.9.1</b> Leverage</a></li>
<li class="chapter" data-level="6.9.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#influential-observations"><i class="fa fa-check"></i><b>6.9.2</b> Influential Observations</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#stability-of-the-solution"><i class="fa fa-check"></i><b>6.10</b> Stability of the Solution</a></li>
<li class="chapter" data-level="6.11" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#other-approaches-to-regression"><i class="fa fa-check"></i><b>6.11</b> Other Approaches to Regression</a>
<ul>
<li class="chapter" data-level="6.11.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#least-absolute-error-regression"><i class="fa fa-check"></i><b>6.11.1</b> Least Absolute Error Regression</a></li>
<li class="chapter" data-level="6.11.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#ridge-regression"><i class="fa fa-check"></i><b>6.11.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="6.11.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#lasso-regression"><i class="fa fa-check"></i><b>6.11.3</b> Lasso Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bootstrapping.html"><a href="bootstrapping.html"><i class="fa fa-check"></i><b>7</b> Bootstrapping</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bootstrapping.html"><a href="bootstrapping.html#introduction-2"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="bootstrapping.html"><a href="bootstrapping.html#key-points-6"><i class="fa fa-check"></i><b>7.1.1</b> Key Points</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bootstrapping.html"><a href="bootstrapping.html#bootstrapping-example"><i class="fa fa-check"></i><b>7.2</b> Bootstrapping Example</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="bootstrapping.html"><a href="bootstrapping.html#bootstrapping-predictions"><i class="fa fa-check"></i><b>7.2.1</b> Bootstrapping Predictions</a></li>
<li class="chapter" data-level="7.2.2" data-path="bootstrapping.html"><a href="bootstrapping.html#adding-prediction-intervals"><i class="fa fa-check"></i><b>7.2.2</b> Adding Prediction Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html"><i class="fa fa-check"></i><b>8</b> Mean and Varaince Assumptions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#mean-assumptions"><i class="fa fa-check"></i><b>8.1</b> Mean Assumptions</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-mathbfy"><i class="fa fa-check"></i><b>8.1.1</b> Expectation of <span class="math inline">\(\mathbf{y}\)</span>:</a></li>
<li class="chapter" data-level="8.1.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-hatboldsymbolbeta"><i class="fa fa-check"></i><b>8.1.2</b> Expectation of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
<li class="chapter" data-level="8.1.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-hatmathbfy"><i class="fa fa-check"></i><b>8.1.3</b> Expectation of <span class="math inline">\(\hat{\mathbf{y}}\)</span></a></li>
<li class="chapter" data-level="8.1.4" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-hatmathbfe"><i class="fa fa-check"></i><b>8.1.4</b> Expectation of <span class="math inline">\(\hat{\mathbf{e}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-assumptions"><i class="fa fa-check"></i><b>8.2</b> Variance Assumptions</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-mathbfy"><i class="fa fa-check"></i><b>8.2.1</b> Variance of <span class="math inline">\(\mathbf{y}\)</span></a></li>
<li class="chapter" data-level="8.2.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-hatboldsymbolbeta"><i class="fa fa-check"></i><b>8.2.2</b> Variance of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
<li class="chapter" data-level="8.2.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-hatmathbfy"><i class="fa fa-check"></i><b>8.2.3</b> Variance of <span class="math inline">\(\hat{\mathbf{y}}\)</span></a></li>
<li class="chapter" data-level="8.2.4" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-hatmathbfe"><i class="fa fa-check"></i><b>8.2.4</b> Variance of <span class="math inline">\(\hat{\mathbf{e}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#cross-covariances"><i class="fa fa-check"></i><b>8.3</b> Cross-Covariances</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#cross-covaraince-of-haty-and-hate"><i class="fa fa-check"></i><b>8.3.1</b> Cross-covaraince of <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{e}\)</span></a></li>
<li class="chapter" data-level="8.3.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#cross-covaraince-of-haty-and-hatboldsymbolbeta"><i class="fa fa-check"></i><b>8.3.2</b> Cross-covaraince of <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#gauss-markov-theorem"><i class="fa fa-check"></i><b>8.4</b> Gauss-Markov Theorem</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#assumptions"><i class="fa fa-check"></i><b>8.4.1</b> Assumptions</a></li>
<li class="chapter" data-level="8.4.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#statement"><i class="fa fa-check"></i><b>8.4.2</b> Statement</a></li>
<li class="chapter" data-level="8.4.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#proof"><i class="fa fa-check"></i><b>8.4.3</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#estimate-of-sigma2"><i class="fa fa-check"></i><b>8.5</b> Estimate of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="normality-assumption.html"><a href="normality-assumption.html"><i class="fa fa-check"></i><b>9</b> Normality Assumption</a>
<ul>
<li class="chapter" data-level="9.1" data-path="normality-assumption.html"><a href="normality-assumption.html#introduction-3"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="normality-assumption.html"><a href="normality-assumption.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.2</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="normality-assumption.html"><a href="normality-assumption.html#distribution-of-estimates"><i class="fa fa-check"></i><b>9.3</b> Distribution of Estimates</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="normality-assumption.html"><a href="normality-assumption.html#distribution-of-hatboldsymbolbeta-hatmathbfy-and-hatmathbfe"><i class="fa fa-check"></i><b>9.3.1</b> Distribution of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, <span class="math inline">\(\hat{\mathbf{y}}\)</span> and <span class="math inline">\(\hat{\mathbf{e}}\)</span></a></li>
<li class="chapter" data-level="9.3.2" data-path="normality-assumption.html"><a href="normality-assumption.html#distribution-of-hatsigma2"><i class="fa fa-check"></i><b>9.3.2</b> Distribution of <span class="math inline">\(\hat{\sigma}^2\)</span></a></li>
<li class="chapter" data-level="9.3.3" data-path="normality-assumption.html"><a href="normality-assumption.html#independence-of-hatmathbfe-and-hatmathbfy"><i class="fa fa-check"></i><b>9.3.3</b> Independence of <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="normality-assumption.html"><a href="normality-assumption.html#interval-estimation"><i class="fa fa-check"></i><b>9.4</b> Interval Estimation</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="normality-assumption.html"><a href="normality-assumption.html#confidence-intervals-for-coefficients"><i class="fa fa-check"></i><b>9.4.1</b> Confidence Intervals for Coefficients</a></li>
<li class="chapter" data-level="9.4.2" data-path="normality-assumption.html"><a href="normality-assumption.html#confidence-intervals-for-the-expected-mean-of-a-new-observation-mathbfx_new"><i class="fa fa-check"></i><b>9.4.2</b> Confidence intervals for the expected mean of a new observation <span class="math inline">\(\mathbf{x}_{new}\)</span></a></li>
<li class="chapter" data-level="9.4.3" data-path="normality-assumption.html"><a href="normality-assumption.html#confidence-intervals-for-linear-combinations-of-boldsymbolbeta"><i class="fa fa-check"></i><b>9.4.3</b> Confidence intervals for linear combinations of <span class="math inline">\(\boldsymbol{\beta}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="normality-assumption.html"><a href="normality-assumption.html#hypothesis-testing"><i class="fa fa-check"></i><b>9.5</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="normality-assumption.html"><a href="normality-assumption.html#testing-for-the-overall-regression"><i class="fa fa-check"></i><b>9.5.1</b> Testing for the Overall Regression</a></li>
<li class="chapter" data-level="9.5.2" data-path="normality-assumption.html"><a href="normality-assumption.html#testing-if-one-variable-is-not-relevant"><i class="fa fa-check"></i><b>9.5.2</b> Testing if one variable is not relevant</a></li>
<li class="chapter" data-level="9.5.3" data-path="normality-assumption.html"><a href="normality-assumption.html#testing-if-a-subgroup-of-the-variables-is-relevant"><i class="fa fa-check"></i><b>9.5.3</b> Testing if a Subgroup of the Variables is Relevant</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mean-and-varaince-assumptions" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">8</span> Mean and Varaince Assumptions<a href="mean-and-varaince-assumptions.html#mean-and-varaince-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>So far we have not made any probabilistic assumptions about the different
elements in our model. So the model errors are unknown but are not random.</p>
<p>Now we will assume that the random errors are random variables. However, we will
not specify the complete distribution of the errors and will limit ourselves to
make assumptions about the mean and variance of the errors.</p>
<div id="mean-assumptions" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Mean Assumptions<a href="mean-and-varaince-assumptions.html#mean-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will begin by making the assumptions about the mean of the errors.
Specifically, we will assume that:</p>
<p><span class="math display">\[ \mathbb{E}[e_i] = 0 \in \mathbb{R}\quad \forall i\in\{1,\ldots,n\}\]</span></p>
<p>This can be expressed in vector form as follows:</p>
<p><span class="math display">\[ \mathbb{E}[\mathbf{e}] = \mathbf{0}\in \mathbb{R}^n\]</span></p>
<p>Note that <span class="math inline">\(\mathbf{X}\)</span> is a known constant and <span class="math inline">\(\boldsymbol{\beta}\)</span> is an unknown constant (an unknown
parameter). This implies that + <span class="math inline">\(\mathbf{y}\)</span> is a random vector. And therefor, any function
of <span class="math inline">\(\mathbf{y}\)</span> will be a random varaible. In particular, our estimates:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\beta}} = (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{y}\]</span>
<span class="math display">\[ \hat{\mathbf{y}} = \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{y}= \mathbf{H}\mathbf{y}\]</span>
<span class="math display">\[ \hat{\mathbf{e}} = \mathbf{y}- \hat{\mathbf{y}} = (\mathbf{I}- \mathbf{H})\mathbf{y}\]</span></p>
<p>are random vectors. Then we can try to compute the mean of these values. This
should be possible since all 3 estimates are linear combinations of <span class="math inline">\(\mathbf{y}\)</span> and
we can compute the mean of <span class="math inline">\(\mathbf{y}\)</span>.</p>
<div id="expectation-of-mathbfy" class="section level3 hasAnchor" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Expectation of <span class="math inline">\(\mathbf{y}\)</span>:<a href="mean-and-varaince-assumptions.html#expectation-of-mathbfy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[\mathbf{y}]
  &amp;=  \mathbb{E}[\mathbf{X}\boldsymbol{\beta}+ \mathbf{e}] \\
  &amp;= \mathbf{X}\boldsymbol{\beta}+ \mathbb{E}[\mathbf{e}] \\
  &amp;= \mathbf{X}\boldsymbol{\beta}
\end{align*}\]</span></p>
</div>
<div id="expectation-of-hatboldsymbolbeta" class="section level3 hasAnchor" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> Expectation of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span><a href="mean-and-varaince-assumptions.html#expectation-of-hatboldsymbolbeta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[\hat{\boldsymbol{\beta}}]
  &amp;= \mathbb{E}[(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{y}] \\
  &amp;= (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbb{E}[\mathbf{y}] \\
  &amp;= (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{X}\boldsymbol{\beta}\\
  &amp;= \boldsymbol{\beta}\\
\end{align*}\]</span></p>
<p>So, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is an unbiased estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
</div>
<div id="expectation-of-hatmathbfy" class="section level3 hasAnchor" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> Expectation of <span class="math inline">\(\hat{\mathbf{y}}\)</span><a href="mean-and-varaince-assumptions.html#expectation-of-hatmathbfy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[\hat{\mathbf{y}}]
  &amp;= \mathbb{E}[\mathbf{X}\hat{\boldsymbol{\beta}}] \\
  &amp;= \mathbf{X}\mathbb{E}[\hat{\boldsymbol{\beta}}] \\
  &amp;= \mathbf{X}\boldsymbol{\beta}\\
  &amp;= \mathbb{E}[\mathbf{y}] \\
\end{align*}\]</span></p>
<p>So <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span> have the same mean.</p>
</div>
<div id="expectation-of-hatmathbfe" class="section level3 hasAnchor" number="8.1.4">
<h3><span class="header-section-number">8.1.4</span> Expectation of <span class="math inline">\(\hat{\mathbf{e}}\)</span><a href="mean-and-varaince-assumptions.html#expectation-of-hatmathbfe" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[\hat{\mathbf{e}}]
  &amp;= \mathbb{E}[\mathbf{y}- \hat{\mathbf{y}}] \\
  &amp;= \mathbb{E}[\mathbf{y}] - \mathbb{E}[\hat{\mathbf{y}}] \\
  &amp;= \mathbf{0}
\end{align*}\]</span></p>
<p>So <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span> have the same mean.</p>
<p>Without any further assumptions, we can get more additional results. Next, we
move to assumptions on the variance of the errors.</p>
</div>
</div>
<div id="variance-assumptions" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Variance Assumptions<a href="mean-and-varaince-assumptions.html#variance-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>While the assumptions on the mean are assumptions about the first moment of the
errors, now we will make assumptions about the second moment of the errors. In
particular, we will assume that all the errors have the same (finite) variance
and are uncorrelated. That is:</p>
<p><span class="math display">\[ \mathbb{V}[e_i] = \sigma^2 &lt; \infty \quad \forall i \in \{1,\ldots,n\}, \quad \text{and} \quad \mathbb{C}[e_i, e_j] = 0 \quad \forall i \neq j\]</span></p>
<p>We can express this assumption in vector form as:</p>
<p><span class="math display">\[ \mathbb{V}[\mathbf{e}] = \sigma^2 \mathbf{I}\quad \sigma^2 &lt; \infty \]</span></p>
<p>As we did before, we can try to compute the variance of all the random quantities
we have.</p>
<div id="variance-of-mathbfy" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Variance of <span class="math inline">\(\mathbf{y}\)</span><a href="mean-and-varaince-assumptions.html#variance-of-mathbfy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{align*}
\mathbb{V}[\mathbf{y}]
  &amp;= \mathbb{V}[\mathbf{X}+ \mathbf{e}] \\
  &amp;= \mathbb{V}[\mathbf{e}]       \\
  &amp;= \sigma^2 \mathbf{I}
\end{align*}\]</span></p>
<p>So, the observations <span class="math inline">\(\mathbf{y}\)</span> and the errors <span class="math inline">\(\mathbf{e}\)</span> have the same variance. This makes
sense since they only differ by a non-random element <span class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>.</p>
</div>
<div id="variance-of-hatboldsymbolbeta" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Variance of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span><a href="mean-and-varaince-assumptions.html#variance-of-hatboldsymbolbeta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{align*}
\mathbb{V}[\hat{\boldsymbol{\beta}}]
  &amp;= \mathbb{V}[(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{y}]                         \\
  &amp;= (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbb{V}[\mathbf{y}] \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}       \\
  &amp;= (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; (\sigma^2 \mathbf{I}) \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1} \\
  &amp;= \sigma^2 (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}        \\
  &amp;= \sigma^2 (\mathbf{X}&#39;\mathbf{X})^{-1}                             \\
\end{align*}\]</span></p>
</div>
<div id="variance-of-hatmathbfy" class="section level3 hasAnchor" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Variance of <span class="math inline">\(\hat{\mathbf{y}}\)</span><a href="mean-and-varaince-assumptions.html#variance-of-hatmathbfy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{align*}
\mathbb{V}[\hat{\mathbf{y}}]
  &amp;= \mathbb{V}[\mathbf{H}\mathbf{y}]                                              \\
  &amp;= \mathbf{H}\mathbb{V}[\mathbf{y}] \mathbf{H}&#39;                                         \\
  &amp;= \mathbf{H}(\sigma^2 \mathbf{I}) \mathbf{H}&amp; \text{since $\mathbf{H}$ is symmetric}  \\
  &amp;= \sigma^2 \mathbf{H}\mathbf{H}\\
  &amp;= \sigma^2 \mathbf{H}&amp; \text{since $\mathbf{H}$ is idempotent} \\
\end{align*}\]</span></p>
<p>So, while <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span> have the same mean, they do not have the same
variance. We will see that the variance of <span class="math inline">\(\hat{\mathbf{y}}\)</span> is “smaller” than the
variance of <span class="math inline">\(\mathbf{y}\)</span>. I say “smaller” since for matrices it is not clear what does
it mean to be smaller or bigger.</p>
<p>Not for now, that the diagonal elements of these matrices satisfy the following:</p>
<p><span class="math display">\[[\mathbb{V}[\mathbf{y}]]_{ii} = \sigma^2 \leq \sigma^2 h_{ii} = [\mathbb{V}[\hat{\mathbf{y}}]]_{ii}\]</span>
since the leverages <span class="math inline">\(h_{ii}\)</span> satisfy <span class="math inline">\(0 \leq h_{ii} \leq 1\)</span>.</p>
</div>
<div id="variance-of-hatmathbfe" class="section level3 hasAnchor" number="8.2.4">
<h3><span class="header-section-number">8.2.4</span> Variance of <span class="math inline">\(\hat{\mathbf{e}}\)</span><a href="mean-and-varaince-assumptions.html#variance-of-hatmathbfe" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{align*}
\mathbb{V}[\hat{\mathbf{e}}]
  &amp;= \mathbb{V}[(\mathbf{I}- \mathbf{H}) \mathbf{y}]                                                             \\
  &amp;= (\mathbf{I}- \mathbf{H}) \mathbb{V}[\mathbf{y}] (\mathbf{I}- \mathbf{H})&#39;                                                \\
  &amp;= (\mathbf{I}- \mathbf{H}) (\sigma^2 \mathbf{I}) (\mathbf{I}- \mathbf{H}) &amp; \text{since $(\mathbf{I}- \mathbf{H})$ is symmetric} \\
  &amp;= \sigma^2 (\mathbf{I}- \mathbf{H})(\mathbf{I}- \mathbf{H})                                                  \\
  &amp;= \sigma^2 (\mathbf{I}- \mathbf{H})           &amp; \text{since $(\mathbf{I}- \mathbf{H})$ is idempotent}        \\
\end{align*}\]</span></p>
<p>Then we can see that <span class="math inline">\(\mathbf{y}\)</span>, <span class="math inline">\(\hat{\mathbf{y}}\)</span> and <span class="math inline">\(\hat{\mathbf{e}}\)</span> have variances that are
idempotent matrices multiplied by the scalar <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
</div>
<div id="cross-covariances" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Cross-Covariances<a href="mean-and-varaince-assumptions.html#cross-covariances" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we introduce the assumption of the variance, we can check the
cross-covariances of our estimators. This will help us later in the course.</p>
<p>We can check several cross-covariances but for now I will only check 2.</p>
<div id="cross-covaraince-of-haty-and-hate" class="section level3 hasAnchor" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> Cross-covaraince of <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{e}\)</span><a href="mean-and-varaince-assumptions.html#cross-covaraince-of-haty-and-hate" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{align*}
\mathbb{C}[\hat{\mathbf{y}}, \hat{\mathbf{e}}]
  &amp;=\mathbb{C}[(\mathbf{I}- \mathbf{H}) \mathbf{y}, \mathbf{H}\mathbf{y}]                                                           \\
  &amp;=(\mathbf{I}- \mathbf{H})\mathbb{C}[\mathbf{y},\mathbf{y}] \mathbf{H}&#39;                                                            \\
  &amp;=(\mathbf{I}- \mathbf{H})\mathbb{V}[\mathbf{y}] \mathbf{H}&amp; \text{since $\mathbf{H}$ is symmetric and $\mathbb{C}[\mathbf{y},\mathbf{y}] = \mathbb{V}[\mathbf{y}]$} \\
  &amp;=(\mathbf{I}- \mathbf{H})(\sigma^2 \mathbf{I}) \mathbf{H}&amp; \text{since $\mathbb{V}[\mathbf{y}] = \sigma^2 \mathbf{I}$}                  \\
  &amp;=\sigma^2 (\mathbf{I}- \mathbf{H}) \mathbf{H}\\
  &amp;=\sigma^2 (\mathbf{H}- \mathbf{H}\mathbf{H})                                                                \\
  &amp;=\sigma^2 (\mathbf{H}- \mathbf{H}) &amp; \text{since $\mathbf{H}$ is idempotent}                                 \\
  &amp;=\mathbf{0}\in \mathbb{R}^{n \times n}                                                             \\
\end{align*}\]</span></p>
<p>Then, the residuals <span class="math inline">\(\hat{\mathbf{e}}\)</span> and the estimates of the observations
<span class="math inline">\(\hat{\mathbf{y}}\)</span> are uncorrelated. We had a similar result before, that however is not
the same (we couldn’t have even talked before expectation and covariance since we
didn’t have random variables). That result was:</p>
<p><span class="math display">\[ \hat{\mathbf{e}}&#39;\hat{\mathbf{y}} = 0 \in \mathbb{R}\]</span></p>
<p>Notice that, the dimension of the zero’s.</p>
</div>
<div id="cross-covaraince-of-haty-and-hatboldsymbolbeta" class="section level3 hasAnchor" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Cross-covaraince of <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span><a href="mean-and-varaince-assumptions.html#cross-covaraince-of-haty-and-hatboldsymbolbeta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{align*}
\mathbb{C}[\hat{\mathbf{e}}, \hat{\boldsymbol{\beta}}]
  &amp;= \mathbb{C}[(\mathbf{I}- \mathbf{H}) \mathbf{y}, (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39; \mathbf{y}]                \\
  &amp;= (\mathbf{I}- \mathbf{H}) \mathbb{C}[\mathbf{y}, \mathbf{y}] \mathbf{X}(\mathbf{X}&#39; \mathbf{X})^{-1}                 \\
  &amp;= (\mathbf{I}- \mathbf{H}) \mathbb{V}[\mathbf{y}] \mathbf{X}(\mathbf{X}&#39; \mathbf{X})^{-1}                      \\
  &amp;= (\mathbf{I}- \mathbf{H}) (\sigma^2 \mathbf{I}) \mathbf{X}(\mathbf{X}&#39; \mathbf{X})^{-1}                \\
  &amp;= \sigma^2 (\mathbf{I}- \mathbf{H}) \mathbf{X}(\mathbf{X}&#39; \mathbf{X})^{-1}                      \\
  &amp;= \sigma^2 (\mathbf{I}- \mathbf{X}(\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39;) \mathbf{X}(\mathbf{X}&#39; \mathbf{X})^{-1} \\
  &amp;= \sigma^2 (\mathbf{X}- \mathbf{X}(\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39;\mathbf{X})(\mathbf{X}&#39; \mathbf{X})^{-1}   \\
  &amp;= \sigma^2 (\mathbf{X}- \mathbf{X})(\mathbf{X}&#39; \mathbf{X})^{-1}                           \\
  &amp;= \mathbf{0}\in \mathbb{R}^{n \times p}                                   \\
\end{align*}\]</span></p>
<p>So the residuals and the estimate of <span class="math inline">\(\boldsymbol{\beta}\)</span> are uncorrelated. We will use this
result in the next chapter.</p>
</div>
</div>
<div id="gauss-markov-theorem" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Gauss-Markov Theorem<a href="mean-and-varaince-assumptions.html#gauss-markov-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This theorem justifies the use of the Ordinary Least Squares (OLS) estimator,
since it is the “best” estimator in a way.</p>
<div id="assumptions" class="section level3 hasAnchor" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> Assumptions<a href="mean-and-varaince-assumptions.html#assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The theorem assumes that:</p>
<ul>
<li>The errors have the same finite variance.</li>
<li>The errors are uncorrelated.</li>
</ul>
<p>This assumption is equivalent to our assumption of:</p>
<p><span class="math display">\[ \mathbb{V}[\mathbf{e}] = \sigma^2 \mathbf{I}\quad \sigma^2 &lt; \infty \]</span></p>
</div>
<div id="statement" class="section level3 hasAnchor" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span> Statement<a href="mean-and-varaince-assumptions.html#statement" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the linear regression model, the OLS estimator <span class="math inline">\(\hat{\boldsymbol{\beta}} = (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39; \mathbf{y}\)</span>
has the smallest variance among all unbiased linear estimators.</p>
<p>That is, if <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span> is a linear estimator, then:</p>
<p><span class="math display">\[ \mathbb{V}[\tilde{\boldsymbol{\beta}}] - \mathbb{V}[\hat{\boldsymbol{\beta}}] \]</span></p>
<p>is semi-positive definite.</p>
<p>Because of this, the OLS estimator is called the Best Linear Unbiased Estimator
(BLUE), where best means smaller variance.</p>
</div>
<div id="proof" class="section level3 hasAnchor" number="8.4.3">
<h3><span class="header-section-number">8.4.3</span> Proof<a href="mean-and-varaince-assumptions.html#proof" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span> be an unbiased linear estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>. Then, we can write
<span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span> as follows:</p>
<p><span class="math display">\[\tilde{\boldsymbol{\beta}} = \mathbf{A}\mathbf{y}\]</span></p>
<p>where <span class="math inline">\(\mathbf{A}\)</span> is a constant matrix of the appropriate dimensions. Then we can write
<span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span> as follows:</p>
<p><span class="math display">\[\begin{align*}
\tilde{\boldsymbol{\beta}}
  &amp;=(\mathbf{A}- (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39; + (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39;) \mathbf{y}\\
  &amp;=(\mathbf{A}- (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39;)\mathbf{y}+ (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39; \mathbf{y}\\
  &amp;=(\mathbf{D}) + \hat{\boldsymbol{\beta}} &amp; \text{with $\mathbf{D}= \mathbf{A}- (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39;$} \\
\end{align*}\]</span></p>
<p>Now, since <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span> is unbiased, we have that:</p>
<p><span class="math display">\[\mathbb{E}[\tilde{\boldsymbol{\beta}}] = \boldsymbol{\beta}\quad \forall \boldsymbol{\beta}\in \mathbb{R}^p \]</span></p>
<p>And</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[\tilde{\boldsymbol{\beta}}]
  &amp;= \mathbb{E}[\mathbf{D}\mathbf{y}+ \hat{\boldsymbol{\beta}}] \quad \forall \boldsymbol{\beta}\in \mathbb{R}^p      \\
  &amp;= \mathbf{D}\mathbb{E}[\mathbf{y}] + \mathbb{E}[\hat{\boldsymbol{\beta}}] \quad \forall \boldsymbol{\beta}\in \mathbb{R}^p \\
  &amp;= \mathbf{D}\mathbf{X}\boldsymbol{\beta}+ \boldsymbol{\beta}\quad \forall \boldsymbol{\beta}\in \mathbb{R}^p &amp; \text{since $\mathbb{E}[\mathbf{y}] = \mathbf{X}\boldsymbol{\beta}$ and $\mathbb{E}[\hat{\boldsymbol{\beta}}] = \boldsymbol{\beta}$} \\
\end{align*}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{align*}
  \mathbb{E}[\tilde{\boldsymbol{\beta}}] = \boldsymbol{\beta}\quad \forall \boldsymbol{\beta}\in \mathbb{R}^p           \\
  &amp;\implies \mathbf{D}\mathbf{X}\boldsymbol{\beta}+ \boldsymbol{\beta}= \boldsymbol{\beta}\quad \forall \boldsymbol{\beta}\in \mathbb{R}^p  \\
  &amp;\implies \mathbf{D}\mathbf{X}\boldsymbol{\beta}= \mathbf{0}\quad \forall \boldsymbol{\beta}\in \mathbb{R}^p       \\
  &amp;\implies \mathbf{D}\mathbf{X}= \mathbf{0}\\
\end{align*}\]</span></p>
<p>Now, we can analyze the variance of <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span>.</p>
<p><span class="math display">\[\begin{align*}
  \mathbb{V}[\tilde{\boldsymbol{\beta}}]
    &amp;= \mathbb{V}[\mathbf{D}\mathbf{y}+ \hat{\boldsymbol{\beta}}]                                                                                            \\
    &amp;= \mathbb{V}[\mathbf{D}\mathbf{y}+ (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39; \mathbf{y}] &amp;&amp; \text{since $\hat{\boldsymbol{\beta}} = (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39; \mathbf{y}$}                      \\
    &amp;= \mathbb{V}[(\mathbf{D}+ (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39;) \mathbf{y}]                                                                                \\
    &amp;= (\mathbf{D}+ (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39;) \mathbb{V}[\mathbf{y}] (\mathbf{D}+ (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39;)&#39;                                                  \\
    &amp;= (\mathbf{D}+ (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39;) \mathbb{V}[\mathbf{y}] (\mathbf{D}&#39; + \mathbf{X}(\mathbf{X}&#39; \mathbf{X})^{-1})                                                   \\
    &amp;= (\mathbf{D}+ (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39;) (\sigma^2 \mathbf{I}) (\mathbf{D}&#39; + \mathbf{X}(\mathbf{X}&#39; \mathbf{X})^{-1}) &amp;&amp; \text{since $\mathbb{V}[\mathbf{y}] = \sigma^2 \mathbf{I}$}   \\
    &amp;= \sigma^2 (\mathbf{D}+ (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39;) (\mathbf{D}&#39; + \mathbf{X}(\mathbf{X}&#39; \mathbf{X})^{-1})                                                   \\
    &amp;= \sigma^2 (\mathbf{D}\mathbf{D}&#39; + (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39; \mathbf{D}&#39;                                                                       \\
    &amp;\quad + \mathbf{D}\mathbf{X}(\mathbf{X}&#39; \mathbf{X})^{-1} + (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39; \mathbf{X}(\mathbf{X}&#39; \mathbf{X})^{-1})                                            \\
    &amp;= \sigma^2 (\mathbf{D}\mathbf{D}&#39; + (\mathbf{X}&#39; \mathbf{X})^{-1}) &amp;&amp; \text{since $\mathbf{D}\mathbf{X}= \mathbf{0}$}                                             \\
    &amp;= \sigma^2 (\mathbf{X}&#39; \mathbf{X})^{-1} + \sigma^2 (\mathbf{D}\mathbf{D}&#39;)                                                                       \\
    &amp;= \mathbb{V}[\hat{\boldsymbol{\beta}}] + \sigma^2 (\mathbf{D}\mathbf{D}&#39;) &amp;&amp; \text{since $\mathbb{V}[\hat{\boldsymbol{\beta}}] = \sigma^2 (\mathbf{X}&#39; \mathbf{X})^{-1}$}                   \\
\end{align*}\]</span></p>
<p>Since <span class="math inline">\(\mathbf{D}\mathbf{D}&#39;\)</span> is semi-positive definite, we have that:</p>
<p><span class="math display">\[ \mathbb{V}[\tilde{\boldsymbol{\beta}}] - \mathbb{V}[\hat{\boldsymbol{\beta}}] = \sigma^2 (\mathbf{D}\mathbf{D}&#39;) \]</span></p>
<p>is semi-positive definite. This concludes the end of the proof.</p>
<p>So, as it turns out, the OLS estimator that came out of minimizing the least
squares, and made no assumptions about the expectation and variances is a very
good estimator, since it is:</p>
<ul>
<li>Unbiased</li>
<li>Among the unbiased and linear estimators, it has the smallest variance.</li>
</ul>
<p>Another common measure of performance is the Mean Square Error (MSE) of an estimator
with respect to a parameter. Using the Gauss-Markov Theorem we can conclude that,
among the unbiased and linear estimators, the OLS estimator has the smallest MSE.</p>
<p>Since, both estimators are unbiased, we have that:</p>
<p><span class="math display">\[ \mathbb{M}(\hat{\mathbf{\boldsymbol{\beta}}}) = \text{tr}(\text{Var}(\hat{\mathbf{\boldsymbol{\beta}}})) + \|\text{Bias}(\hat{\mathbf{\boldsymbol{\beta}}})\|^2 = \text{tr}(\text{Var}(\hat{\mathbf{\boldsymbol{\beta}}})) \]</span>
<span class="math display">\[ \mathbb{M}(\tilde{\mathbf{\boldsymbol{\beta}}}) = \text{tr}(\text{Var}(\tilde{\mathbf{\boldsymbol{\beta}}})) + \|\text{Bias}(\tilde{\mathbf{\boldsymbol{\beta}}})\|^2 = \text{tr}(\text{Var}(\tilde{\mathbf{\boldsymbol{\beta}}})) \]</span></p>
<p>Then, by the Gauss-Markov theorem:</p>
<p><span class="math display">\[\begin{align*}
  \mathbb{M}[\tilde{\boldsymbol{\beta}}] - \mathbb{M}[\hat{\boldsymbol{\beta}}]
    &amp;= \text{tr}(\text{Var}(\tilde{\mathbf{\boldsymbol{\beta}}})) - \text{tr}(\text{Var}(\hat{\mathbf{\boldsymbol{\beta}}}))                                     \\
    &amp;= \text{tr}(\text{Var}(\tilde{\mathbf{\boldsymbol{\beta}}})) - \text{Var}(\hat{\mathbf{\boldsymbol{\beta}}})) &amp;&amp; \text{since the trace operator is linear.} \\
    &amp;\geq 0          \\
\end{align*}\]</span></p>
<p>Where we use the fact that <span class="math inline">\(\text{Var}(\tilde{\mathbf{\boldsymbol{\beta}}})) - \text{Var}(\hat{\mathbf{\boldsymbol{\beta}}})\)</span>
is positive semi-definite by the Gauss-Markov theorem and the trace of a positive
semi-definite matrix is non-negative.</p>
</div>
</div>
<div id="estimate-of-sigma2" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Estimate of <span class="math inline">\(\sigma^2\)</span><a href="mean-and-varaince-assumptions.html#estimate-of-sigma2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Since we have introduced a new parameter <span class="math inline">\(\sigma^2\)</span>, we might be interested in
estimate it. We will propose an estimate of <span class="math inline">\(\sigma^2\)</span> of the following shape:</p>
<p><span class="math display">\[\hat{\sigma}^2 = a \hat{\mathbf{e}}&#39;\hat{\mathbf{e}}\]</span></p>
<p>where we will choose the scalar <span class="math inline">\(a\)</span> later.</p>
<p>In particular, we want to take the expectation of this estimate. To do so, we will
take a roundabout way. First we will show that:</p>
<p><span class="math display">\[(\mathbf{y}- \mathbb{E}[\mathbf{y}])&#39;(\mathbf{y}- \mathbb{E}[\mathbf{y}]) = \hat{\mathbf{e}}&#39;\hat{\mathbf{e}} + (\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])&#39;(\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])\]</span></p>
<p>and we will use this fact to compute the expectation of <span class="math inline">\(\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}\)</span>.</p>
<p><span class="math display">\[\begin{align*}
  (\mathbf{y}- \mathbb{E}[\mathbf{y}])&#39;&amp; (\mathbf{y}- \mathbb{E}[\mathbf{y}])                                                                                           \\
    &amp;= (\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) &amp;&amp; \text{since $\mathbb{E}[\mathbf{y}] = \mathbf{X}\boldsymbol{\beta}$}                                                  \\
    &amp;= (\mathbf{y}- \mathbf{X}\hat{\boldsymbol{\beta}} + \mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\hat{\boldsymbol{\beta}} + \mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\beta})                    \\
    &amp;= (\mathbf{y}- \mathbf{X}\hat{\boldsymbol{\beta}})&#39;(\mathbf{y}- \mathbf{X}\hat{\boldsymbol{\beta}}) + 2(\mathbf{y}- \mathbf{X}\hat{\boldsymbol{\beta}})&#39;(\mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\beta})                      \\
    &amp;\quad + (\mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\beta})                                                            \\
    &amp;= (\mathbf{y}- \hat{\mathbf{y}})&#39;(\mathbf{y}- \hat{\mathbf{y}}) + 2(\mathbf{y}- \hat{\mathbf{y}})&#39;(\mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\beta})                                     \\
    &amp;\quad + (\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])&#39;(\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])                                                          \\
    &amp;= \hat{\mathbf{e}}&#39;\hat{\mathbf{e}} + 2\hat{\mathbf{e}}&#39;(\mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\beta}) + (\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])&#39;(\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}]) \\
    &amp;= \hat{\mathbf{e}}&#39;\hat{\mathbf{e}} + 2\hat{\mathbf{e}}&#39;\mathbf{X}(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) + (\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])&#39;(\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])     \\
    &amp;= \hat{\mathbf{e}}&#39;\hat{\mathbf{e}} + (\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])&#39;(\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}]) &amp;&amp; \text{since $\hat{\mathbf{e}}&#39;\mathbf{X}=\mathbf{0}$}   \\
\end{align*}\]</span></p>
<p>Since</p>
<p><span class="math display">\[\mathbb{E}[(\mathbf{y}- \mathbb{E}[\mathbf{y}])&#39;(\mathbf{y}- \mathbb{E}[\mathbf{y}])] = \text{tr}(\mathbb{V}[\mathbf{y}]) = \text{tr}(\sigma^2 \mathbf{I}) = \sigma^2 \text{tr}(\mathbf{I}) = \sigma^2 n\]</span>
<span class="math display">\[\mathbb{E}[(\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])&#39;(\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])] = \text{tr}(\mathbb{V}[\hat{\mathbf{y}}]) = \text{tr}(\sigma^2 \mathbf{H}) = \sigma^2 \text{tr}(\mathbf{H}) = \sigma^2 p\]</span>
where we use the fact that the trace of an idempotent matrix is equal to the rank of the matrix. Then we get that:</p>
<p><span class="math display">\[\begin{align*}
  (\mathbf{y}- \mathbb{E}[\mathbf{y}])&#39;&amp; (\mathbf{y}- \mathbb{E}[\mathbf{y}]) = \hat{\mathbf{e}}&#39;\hat{\mathbf{e}} + (\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])&#39;(\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])                          \\
    &amp;\implies \mathbb{E}[(\mathbf{y}- \mathbb{E}[\mathbf{y}])&#39;(\mathbf{y}- \mathbb{E}[\mathbf{y}])] = \mathbb{E}[\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}] + \mathbb{E}[(\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])&#39;(\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])] \\
    &amp;\implies \text{tr}(\mathbb{V}[\mathbf{y}]) = \mathbb{E}[\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}] + \text{tr}(\mathbb{V}[\hat{\mathbf{y}}])                                                         \\
    &amp;\implies \sigma^2 n = \mathbb{E}[\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}] + \sigma^2 p                                                                                 \\
    &amp;\implies \mathbb{E}[\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}] = \sigma^2 n - \sigma^2 p                                                                                 \\
    &amp;\implies \mathbb{E}[\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}] = \sigma^2 (n - p)                                                                                        \\
\end{align*}\]</span></p>
<p>Then, if we set <span class="math inline">\(a=(\frac{1}{n-p})\)</span> we have that our proposed estimator <span class="math inline">\(\hat{\sigma}^2\)</span>
is unbiased. Since:</p>
<p><span class="math display">\[\begin{align*}
  \mathbb{E}[a \hat{\sigma}^2]
    &amp;= \mathbb{E}[a \hat{\mathbf{e}}&#39;\hat{\mathbf{e}}]   \\
    &amp;= a \mathbb{E}[\hat{\mathbf{e}}&#39;\hat{\mathbf{e}}]   \\
    &amp;= a \sigma^2 (n-p)             \\
    &amp;= \frac{1}{n-p} \sigma^2 (n-p) \\
    &amp;= \sigma^2                     \\
\end{align*}\]</span></p>
<p>We can choose or obtain other values for <span class="math inline">\(a\)</span>, however the estimator will not be
unbiased.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bootstrapping.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="normality-assumption.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
