<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Multiple Linear Regression | _main.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Multiple Linear Regression | _main.knit" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Multiple Linear Regression | _main.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="polynomial-regression.html"/>
<link rel="next" href="bootstrapping.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Stat 5385/6385 Fall 2024</a></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#general-math"><i class="fa fa-check"></i><b>2.1</b> General Math</a></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.2.3</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.2.4</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.2.5" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.2.5</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.2.6" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.2.6</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.2.7" data-path="prerequisites.html"><a href="prerequisites.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.2.7</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="2.2.8" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.2.8</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.2.9" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.2.9</b> Idempotent Matrix</a></li>
<li class="chapter" data-level="2.2.10" data-path="prerequisites.html"><a href="prerequisites.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>2.2.10</b> Determinant of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.3</b> Probability</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="prerequisites.html"><a href="prerequisites.html#expected-value"><i class="fa fa-check"></i><b>2.3.1</b> Expected Value</a></li>
<li class="chapter" data-level="2.3.2" data-path="prerequisites.html"><a href="prerequisites.html#variance"><i class="fa fa-check"></i><b>2.3.2</b> Variance</a></li>
<li class="chapter" data-level="2.3.3" data-path="prerequisites.html"><a href="prerequisites.html#cross-covariance-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Cross-Covariance Matrix</a></li>
<li class="chapter" data-level="2.3.4" data-path="prerequisites.html"><a href="prerequisites.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.3.4</b> Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.4</b> Statistics</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="prerequisites.html"><a href="prerequisites.html#bias-of-an-estimator"><i class="fa fa-check"></i><b>2.4.1</b> Bias of an Estimator</a></li>
<li class="chapter" data-level="2.4.2" data-path="prerequisites.html"><a href="prerequisites.html#unbiased-estimator"><i class="fa fa-check"></i><b>2.4.2</b> Unbiased Estimator</a></li>
<li class="chapter" data-level="2.4.3" data-path="prerequisites.html"><a href="prerequisites.html#mean-square-error-of-an-estimator"><i class="fa fa-check"></i><b>2.4.3</b> Mean Square Error of an Estimator</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.5</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.5.1</b> Gradient</a></li>
<li class="chapter" data-level="2.5.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.5.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.5.3</b> Applications:</a></li>
<li class="chapter" data-level="2.5.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.5.4</b> Matrix Calculus</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#examples-1"><i class="fa fa-check"></i><b>3.1</b> Examples</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction.html"><a href="introduction.html#ad-spending"><i class="fa fa-check"></i><b>3.1.1</b> Ad Spending</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction.html"><a href="introduction.html#winw-example"><i class="fa fa-check"></i><b>3.1.2</b> Wine and Life Expectancy</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction.html"><a href="introduction.html#burger-demand"><i class="fa fa-check"></i><b>3.1.3</b> Burger Demand</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model"><i class="fa fa-check"></i><b>4.1</b> Model</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>4.2</b> Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#other-estimated-quantites"><i class="fa fa-check"></i><b>4.2.1</b> Other estimated quantites</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#properties-of-the-estimates"><i class="fa fa-check"></i><b>4.3</b> Properties of the Estimates</a></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-and-standarizing-the-data"><i class="fa fa-check"></i><b>4.4</b> Centering and Standarizing the Data</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independent-variable-centered"><i class="fa fa-check"></i><b>4.4.1</b> Independent variable centered</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#both-variables-centered"><i class="fa fa-check"></i><b>4.4.2</b> Both Variables centered</a></li>
<li class="chapter" data-level="4.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independent-and-dependent-variable-standardized"><i class="fa fa-check"></i><b>4.4.3</b> Independent and dependent variable standardized</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>4.5</b> Coefficient of Determination</a></li>
<li class="chapter" data-level="4.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residual-analysis"><i class="fa fa-check"></i><b>4.6</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#non-linear-regression-function"><i class="fa fa-check"></i><b>4.6.1</b> Non-linear regression function</a></li>
<li class="chapter" data-level="4.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#heteroscedasticity"><i class="fa fa-check"></i><b>4.6.2</b> Heteroscedasticity</a></li>
<li class="chapter" data-level="4.6.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outliers"><i class="fa fa-check"></i><b>4.6.3</b> Outliers</a></li>
<li class="chapter" data-level="4.6.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#variables-ommited"><i class="fa fa-check"></i><b>4.6.4</b> Variables Ommited</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#cross-validation"><i class="fa fa-check"></i><b>4.7</b> Cross-Validation</a></li>
<li class="chapter" data-level="4.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>4.8</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="4.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-in-matrix-form"><i class="fa fa-check"></i><b>4.9</b> Model in Matrix Form</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares-in-matrix-form"><i class="fa fa-check"></i><b>4.9.1</b> Weighted Least Squares in Matrix Form</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="polynomial-regression.html"><a href="polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Polynomial Regression</a></li>
<li class="chapter" data-level="6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#example-9"><i class="fa fa-check"></i><b>6.2</b> Example</a></li>
<li class="chapter" data-level="6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#least-squares-estimation-1"><i class="fa fa-check"></i><b>6.3</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#properties-of-the-estimates-1"><i class="fa fa-check"></i><b>6.4</b> Properties of the Estimates</a></li>
<li class="chapter" data-level="6.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-r2"><i class="fa fa-check"></i><b>6.5</b> Multiple <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="6.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#geometric-interpretation-of-multiple-linear-regression"><i class="fa fa-check"></i><b>6.6</b> Geometric Interpretation of Multiple Linear Regression</a></li>
<li class="chapter" data-level="6.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#centered-and-standarized-variables"><i class="fa fa-check"></i><b>6.7</b> Centered and Standarized Variables</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#centered-variables"><i class="fa fa-check"></i><b>6.7.1</b> Centered Variables</a></li>
<li class="chapter" data-level="6.7.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sample-covariance"><i class="fa fa-check"></i><b>6.7.2</b> Sample Covariance</a></li>
<li class="chapter" data-level="6.7.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#satandard-variables"><i class="fa fa-check"></i><b>6.7.3</b> Satandard Variables</a></li>
<li class="chapter" data-level="6.7.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sample-correlation-matrix"><i class="fa fa-check"></i><b>6.7.4</b> Sample Correlation Matrix</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#variable-cross-effects"><i class="fa fa-check"></i><b>6.8</b> Variable Cross-Effects</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#single-variable-cross-effects"><i class="fa fa-check"></i><b>6.8.1</b> Single Variable Cross-Effects</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#outliers-and-leverage"><i class="fa fa-check"></i><b>6.9</b> Outliers and Leverage</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#leverage"><i class="fa fa-check"></i><b>6.9.1</b> Leverage</a></li>
<li class="chapter" data-level="6.9.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#outliers-identification"><i class="fa fa-check"></i><b>6.9.2</b> Outliers identification</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#stability-of-the-solution"><i class="fa fa-check"></i><b>6.10</b> Stability of the Solution</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bootstrapping.html"><a href="bootstrapping.html"><i class="fa fa-check"></i><b>7</b> Bootstrapping</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bootstrapping.html"><a href="bootstrapping.html#introduction-2"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="bootstrapping.html"><a href="bootstrapping.html#bootstrapping-example"><i class="fa fa-check"></i><b>7.2</b> Bootstrapping Example</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html"><i class="fa fa-check"></i><b>8</b> Mean and Varaince Assumptions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#mean-assumptions"><i class="fa fa-check"></i><b>8.1</b> Mean Assumptions</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-mathbfy"><i class="fa fa-check"></i><b>8.1.1</b> Expectation of <span class="math inline">\(\mathbf{y}\)</span>:</a></li>
<li class="chapter" data-level="8.1.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-hatboldsymbolbeta"><i class="fa fa-check"></i><b>8.1.2</b> Expectation of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
<li class="chapter" data-level="8.1.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-hatmathbfy"><i class="fa fa-check"></i><b>8.1.3</b> Expectation of <span class="math inline">\(\hat{\mathbf{y}}\)</span></a></li>
<li class="chapter" data-level="8.1.4" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#expectation-of-hatmathbfe"><i class="fa fa-check"></i><b>8.1.4</b> Expectation of <span class="math inline">\(\hat{\mathbf{e}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-assumptions"><i class="fa fa-check"></i><b>8.2</b> Variance Assumptions</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-mathbfy"><i class="fa fa-check"></i><b>8.2.1</b> Variance of <span class="math inline">\(\mathbf{y}\)</span></a></li>
<li class="chapter" data-level="8.2.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-hatboldsymbolbeta"><i class="fa fa-check"></i><b>8.2.2</b> Variance of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
<li class="chapter" data-level="8.2.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-hatmathbfy"><i class="fa fa-check"></i><b>8.2.3</b> Variance of <span class="math inline">\(\hat{\mathbf{y}}\)</span></a></li>
<li class="chapter" data-level="8.2.4" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#variance-of-hatmathbfe"><i class="fa fa-check"></i><b>8.2.4</b> Variance of <span class="math inline">\(\hat{\mathbf{e}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#cross-covariances"><i class="fa fa-check"></i><b>8.3</b> Cross-Covariances</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#cross-covaraince-of-haty-and-hate"><i class="fa fa-check"></i><b>8.3.1</b> Cross-covaraince of <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{e}\)</span></a></li>
<li class="chapter" data-level="8.3.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#cross-covaraince-of-haty-and-hatboldsymbolbeta"><i class="fa fa-check"></i><b>8.3.2</b> Cross-covaraince of <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#gauss-markov-theorem"><i class="fa fa-check"></i><b>8.4</b> Gauss-Markov Theorem</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#assumptions"><i class="fa fa-check"></i><b>8.4.1</b> Assumptions</a></li>
<li class="chapter" data-level="8.4.2" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#statement"><i class="fa fa-check"></i><b>8.4.2</b> Statement</a></li>
<li class="chapter" data-level="8.4.3" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#proof"><i class="fa fa-check"></i><b>8.4.3</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mean-and-varaince-assumptions.html"><a href="mean-and-varaince-assumptions.html#estimate-of-sigma2"><i class="fa fa-check"></i><b>8.5</b> Estimate of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="normality-assumption.html"><a href="normality-assumption.html"><i class="fa fa-check"></i><b>9</b> Normality Assumption</a>
<ul>
<li class="chapter" data-level="9.1" data-path="normality-assumption.html"><a href="normality-assumption.html#introduction-3"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="normality-assumption.html"><a href="normality-assumption.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.2</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="normality-assumption.html"><a href="normality-assumption.html#distribution-of-estimates"><i class="fa fa-check"></i><b>9.3</b> Distribution of Estimates</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="normality-assumption.html"><a href="normality-assumption.html#distribution-of-hatboldsymbolbeta-hatmathbfy-and-hatmathbfe"><i class="fa fa-check"></i><b>9.3.1</b> Distribution of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, <span class="math inline">\(\hat{\mathbf{y}}\)</span> and <span class="math inline">\(\hat{\mathbf{e}}\)</span></a></li>
<li class="chapter" data-level="9.3.2" data-path="normality-assumption.html"><a href="normality-assumption.html#distribution-of-hatsigma2"><i class="fa fa-check"></i><b>9.3.2</b> Distribution of <span class="math inline">\(\hat{\sigma}^2\)</span></a></li>
<li class="chapter" data-level="9.3.3" data-path="normality-assumption.html"><a href="normality-assumption.html#independence-of-hatmathbfe-and-hatmathbfy"><i class="fa fa-check"></i><b>9.3.3</b> Independence of <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="normality-assumption.html"><a href="normality-assumption.html#interval-estimation"><i class="fa fa-check"></i><b>9.4</b> Interval Estimation</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="normality-assumption.html"><a href="normality-assumption.html#confidence-intervals-for-coefficients"><i class="fa fa-check"></i><b>9.4.1</b> Confidence Intervals for Coefficients</a></li>
<li class="chapter" data-level="9.4.2" data-path="normality-assumption.html"><a href="normality-assumption.html#confidence-intervals-for-the-expected-mean-of-a-new-observation-mathbfx_new"><i class="fa fa-check"></i><b>9.4.2</b> Confidence intervals for the expected mean of a new observation <span class="math inline">\(\mathbf{x}_{new}\)</span></a></li>
<li class="chapter" data-level="9.4.3" data-path="normality-assumption.html"><a href="normality-assumption.html#confidence-intervals-for-linear-combinations-of-boldsymbolbeta"><i class="fa fa-check"></i><b>9.4.3</b> Confidence intervals for linear combinations of <span class="math inline">\(\boldsymbol{\beta}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="normality-assumption.html"><a href="normality-assumption.html#hypothesis-testing"><i class="fa fa-check"></i><b>9.5</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="normality-assumption.html"><a href="normality-assumption.html#testing-for-the-overall-regression"><i class="fa fa-check"></i><b>9.5.1</b> Testing for the Overall Regression</a></li>
<li class="chapter" data-level="9.5.2" data-path="normality-assumption.html"><a href="normality-assumption.html#testing-if-one-variable-is-not-relevant"><i class="fa fa-check"></i><b>9.5.2</b> Testing if one variable is not relevant</a></li>
<li class="chapter" data-level="9.5.3" data-path="normality-assumption.html"><a href="normality-assumption.html#testing-if-a-subgroup-of-the-variables-is-relevant"><i class="fa fa-check"></i><b>9.5.3</b> Testing if a Subgroup of the Variables is Relevant</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-linear-regression" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Multiple Linear Regression<a href="multiple-linear-regression.html#multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-1" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Introduction<a href="multiple-linear-regression.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Multiple regression is a statistical technique used to model the relationship between a dependent variable and two or more independent variables. It extends simple linear regression by allowing for a more complex analysis of how various factors impact an outcome. The general form of the multiple regression equation is:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \ldots + \beta_p x_{p,i} + e_i \quad i=\{1,\ldots,n\}
\]</span></p>
<p>Where:
- <span class="math inline">\(y\)</span> is the dependent variable.
- <span class="math inline">\(\beta_0\)</span> is the intercept.
- <span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_p\)</span> are the coefficients of the independent variables <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>.
- $e represents the error term.</p>
<p>This technique is widely used across various fields, including economics, social sciences, and healthcare, to control for multiple factors and enhance prediction accuracy. However, it requires careful attention to assumptions. Beyond linearity, now we have to consider issues like independence, multicollinearity, and variable selection which can affect the results.</p>
<p>We already have seen an example of Multiple linear regression when we worked with
Polynomial regression. However, multiple linear regression is more general.</p>
</div>
<div id="example-9" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Example<a href="multiple-linear-regression.html#example-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider, for example, the task of explaining a country’s GDP using other economic variables such as inflation, unemployment, reference interest rate, government spending (as a percentage of GDP), and exports (as a percentage of GDP).</p>
<p>In this case, visualization is not as straightforward, and visually inspecting these relationships is much less practical. Nevertheless, with this number of variables, we can visually explore the relationships between them as follows:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="multiple-linear-regression.html#cb61-1" tabindex="-1"></a><span class="co"># Reads Data</span></span>
<span id="cb61-2"><a href="multiple-linear-regression.html#cb61-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">&quot;Gdp Data.csv&quot;</span>)</span>
<span id="cb61-3"><a href="multiple-linear-regression.html#cb61-3" tabindex="-1"></a></span>
<span id="cb61-4"><a href="multiple-linear-regression.html#cb61-4" tabindex="-1"></a><span class="co"># Plot the scatterplots for each pair of variables</span></span>
<span id="cb61-5"><a href="multiple-linear-regression.html#cb61-5" tabindex="-1"></a><span class="fu">pairs</span>(dat)</span></code></pre></div>
<p><img src="_main_files/figure-html/paris-plot-gdp-1.png" width="672" /></p>
<p>Here we can see, that some independent variables are more related to <code>GDP</code> and
some independent variables are more related between themselves. This is valuable information that will help us
to develop the right linear model with this variables.</p>
<p>We can also observe the correlation between these variables as follows:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="multiple-linear-regression.html#cb62-1" tabindex="-1"></a><span class="co"># Computes the correlation between variables</span></span>
<span id="cb62-2"><a href="multiple-linear-regression.html#cb62-2" tabindex="-1"></a><span class="fu">cor</span>(dat)</span></code></pre></div>
<pre><code>##            gdp          inf         une        int         gov         exp
## gdp  1.0000000  0.875131082 -0.74874795  0.6964256  0.22172279 0.173602651
## inf  0.8751311  1.000000000 -0.78173033  0.8292061  0.31103644 0.005685918
## une -0.7487479 -0.781730327  1.00000000 -0.3642453 -0.16674407 0.010553855
## int  0.6964256  0.829206121 -0.36424525  1.0000000  0.21389456 0.015699798
## gov  0.2217228  0.311036436 -0.16674407  0.2138946  1.00000000 0.018475446
## exp  0.1736027  0.005685918  0.01055386  0.0156998  0.01847545 1.000000000</code></pre>
<p>We can also fit simple linear regression with each one of the independent
variables.</p>
<p><strong>Inflation Rate</strong></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="multiple-linear-regression.html#cb64-1" tabindex="-1"></a><span class="co"># Fits with Inflation</span></span>
<span id="cb64-2"><a href="multiple-linear-regression.html#cb64-2" tabindex="-1"></a>outRegInf <span class="ot">&lt;-</span> <span class="fu">lm</span>(gdp <span class="sc">~</span> inf, <span class="at">data =</span> dat)</span>
<span id="cb64-3"><a href="multiple-linear-regression.html#cb64-3" tabindex="-1"></a>varVal    <span class="ot">&lt;-</span> dat<span class="sc">$</span>inf</span>
<span id="cb64-4"><a href="multiple-linear-regression.html#cb64-4" tabindex="-1"></a>out       <span class="ot">&lt;-</span> outRegInf</span>
<span id="cb64-5"><a href="multiple-linear-regression.html#cb64-5" tabindex="-1"></a>varNam    <span class="ot">&lt;-</span> <span class="st">&quot;Inflation Rate&quot;</span></span>
<span id="cb64-6"><a href="multiple-linear-regression.html#cb64-6" tabindex="-1"></a><span class="co"># Plots Regression Line and Scatterplot and residuals plot</span></span>
<span id="cb64-7"><a href="multiple-linear-regression.html#cb64-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb64-8"><a href="multiple-linear-regression.html#cb64-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb64-9"><a href="multiple-linear-regression.html#cb64-9" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>gd,</span>
<span id="cb64-10"><a href="multiple-linear-regression.html#cb64-10" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb64-11"><a href="multiple-linear-regression.html#cb64-11" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;GDP&quot;</span>)</span>
<span id="cb64-12"><a href="multiple-linear-regression.html#cb64-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> out<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb64-13"><a href="multiple-linear-regression.html#cb64-13" tabindex="-1"></a>       <span class="at">b   =</span> out<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb64-14"><a href="multiple-linear-regression.html#cb64-14" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb64-15"><a href="multiple-linear-regression.html#cb64-15" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb64-16"><a href="multiple-linear-regression.html#cb64-16" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb64-17"><a href="multiple-linear-regression.html#cb64-17" tabindex="-1"></a>     <span class="at">y    =</span> out<span class="sc">$</span>residuals,</span>
<span id="cb64-18"><a href="multiple-linear-regression.html#cb64-18" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb64-19"><a href="multiple-linear-regression.html#cb64-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb64-20"><a href="multiple-linear-regression.html#cb64-20" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb64-21"><a href="multiple-linear-regression.html#cb64-21" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/gdp-inf-fit-1.png" width="672" />
<strong>Unemployment Rate</strong></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="multiple-linear-regression.html#cb65-1" tabindex="-1"></a><span class="co"># Fits with Inflation</span></span>
<span id="cb65-2"><a href="multiple-linear-regression.html#cb65-2" tabindex="-1"></a>outRegUne <span class="ot">&lt;-</span> <span class="fu">lm</span>(gdp <span class="sc">~</span> une, <span class="at">data =</span> dat)</span>
<span id="cb65-3"><a href="multiple-linear-regression.html#cb65-3" tabindex="-1"></a>varVal    <span class="ot">&lt;-</span> dat<span class="sc">$</span>une</span>
<span id="cb65-4"><a href="multiple-linear-regression.html#cb65-4" tabindex="-1"></a>out       <span class="ot">&lt;-</span> outRegUne</span>
<span id="cb65-5"><a href="multiple-linear-regression.html#cb65-5" tabindex="-1"></a>varNam    <span class="ot">&lt;-</span> <span class="st">&quot;Unemplyment Rate&quot;</span></span>
<span id="cb65-6"><a href="multiple-linear-regression.html#cb65-6" tabindex="-1"></a><span class="co"># Plots Regression Line and Scatterplot and residuals plot</span></span>
<span id="cb65-7"><a href="multiple-linear-regression.html#cb65-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb65-8"><a href="multiple-linear-regression.html#cb65-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb65-9"><a href="multiple-linear-regression.html#cb65-9" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>gd,</span>
<span id="cb65-10"><a href="multiple-linear-regression.html#cb65-10" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb65-11"><a href="multiple-linear-regression.html#cb65-11" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;GDP&quot;</span>)</span>
<span id="cb65-12"><a href="multiple-linear-regression.html#cb65-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> out<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb65-13"><a href="multiple-linear-regression.html#cb65-13" tabindex="-1"></a>       <span class="at">b   =</span> out<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb65-14"><a href="multiple-linear-regression.html#cb65-14" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb65-15"><a href="multiple-linear-regression.html#cb65-15" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb65-16"><a href="multiple-linear-regression.html#cb65-16" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb65-17"><a href="multiple-linear-regression.html#cb65-17" tabindex="-1"></a>     <span class="at">y    =</span> out<span class="sc">$</span>residuals,</span>
<span id="cb65-18"><a href="multiple-linear-regression.html#cb65-18" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb65-19"><a href="multiple-linear-regression.html#cb65-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb65-20"><a href="multiple-linear-regression.html#cb65-20" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb65-21"><a href="multiple-linear-regression.html#cb65-21" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/gdp-unp-fit-1.png" width="672" />
<strong>Interest Rate</strong></p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="multiple-linear-regression.html#cb66-1" tabindex="-1"></a><span class="co"># Fits with Inflation</span></span>
<span id="cb66-2"><a href="multiple-linear-regression.html#cb66-2" tabindex="-1"></a>outRegInt <span class="ot">&lt;-</span> <span class="fu">lm</span>(gdp <span class="sc">~</span> int, <span class="at">data =</span> dat)</span>
<span id="cb66-3"><a href="multiple-linear-regression.html#cb66-3" tabindex="-1"></a>varVal    <span class="ot">&lt;-</span> dat<span class="sc">$</span>int</span>
<span id="cb66-4"><a href="multiple-linear-regression.html#cb66-4" tabindex="-1"></a>out       <span class="ot">&lt;-</span> outRegInt</span>
<span id="cb66-5"><a href="multiple-linear-regression.html#cb66-5" tabindex="-1"></a>varNam    <span class="ot">&lt;-</span> <span class="st">&quot;Interest Rate&quot;</span></span>
<span id="cb66-6"><a href="multiple-linear-regression.html#cb66-6" tabindex="-1"></a><span class="co"># Plots Regression Line and Scatterplot and residuals plot</span></span>
<span id="cb66-7"><a href="multiple-linear-regression.html#cb66-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb66-8"><a href="multiple-linear-regression.html#cb66-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb66-9"><a href="multiple-linear-regression.html#cb66-9" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>gd,</span>
<span id="cb66-10"><a href="multiple-linear-regression.html#cb66-10" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb66-11"><a href="multiple-linear-regression.html#cb66-11" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;GDP&quot;</span>)</span>
<span id="cb66-12"><a href="multiple-linear-regression.html#cb66-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> out<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb66-13"><a href="multiple-linear-regression.html#cb66-13" tabindex="-1"></a>       <span class="at">b   =</span> out<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb66-14"><a href="multiple-linear-regression.html#cb66-14" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb66-15"><a href="multiple-linear-regression.html#cb66-15" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb66-16"><a href="multiple-linear-regression.html#cb66-16" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb66-17"><a href="multiple-linear-regression.html#cb66-17" tabindex="-1"></a>     <span class="at">y    =</span> out<span class="sc">$</span>residuals,</span>
<span id="cb66-18"><a href="multiple-linear-regression.html#cb66-18" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb66-19"><a href="multiple-linear-regression.html#cb66-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb66-20"><a href="multiple-linear-regression.html#cb66-20" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb66-21"><a href="multiple-linear-regression.html#cb66-21" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/gdp-int-fit-1.png" width="672" /></p>
<p><strong>Goverment Spending</strong></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="multiple-linear-regression.html#cb67-1" tabindex="-1"></a><span class="co"># Fits with Inflation</span></span>
<span id="cb67-2"><a href="multiple-linear-regression.html#cb67-2" tabindex="-1"></a>outRegGov <span class="ot">&lt;-</span> <span class="fu">lm</span>(gdp <span class="sc">~</span> gov, <span class="at">data =</span> dat)</span>
<span id="cb67-3"><a href="multiple-linear-regression.html#cb67-3" tabindex="-1"></a>varVal    <span class="ot">&lt;-</span> dat<span class="sc">$</span>gov</span>
<span id="cb67-4"><a href="multiple-linear-regression.html#cb67-4" tabindex="-1"></a>out       <span class="ot">&lt;-</span> outRegGov</span>
<span id="cb67-5"><a href="multiple-linear-regression.html#cb67-5" tabindex="-1"></a>varNam    <span class="ot">&lt;-</span> <span class="st">&quot;Goverment Spending&quot;</span></span>
<span id="cb67-6"><a href="multiple-linear-regression.html#cb67-6" tabindex="-1"></a><span class="co"># Plots Regression Line and Scatterplot and residuals plot</span></span>
<span id="cb67-7"><a href="multiple-linear-regression.html#cb67-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb67-8"><a href="multiple-linear-regression.html#cb67-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb67-9"><a href="multiple-linear-regression.html#cb67-9" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>gd,</span>
<span id="cb67-10"><a href="multiple-linear-regression.html#cb67-10" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb67-11"><a href="multiple-linear-regression.html#cb67-11" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;GDP&quot;</span>)</span>
<span id="cb67-12"><a href="multiple-linear-regression.html#cb67-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> out<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb67-13"><a href="multiple-linear-regression.html#cb67-13" tabindex="-1"></a>       <span class="at">b   =</span> out<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb67-14"><a href="multiple-linear-regression.html#cb67-14" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb67-15"><a href="multiple-linear-regression.html#cb67-15" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb67-16"><a href="multiple-linear-regression.html#cb67-16" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb67-17"><a href="multiple-linear-regression.html#cb67-17" tabindex="-1"></a>     <span class="at">y    =</span> out<span class="sc">$</span>residuals,</span>
<span id="cb67-18"><a href="multiple-linear-regression.html#cb67-18" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb67-19"><a href="multiple-linear-regression.html#cb67-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb67-20"><a href="multiple-linear-regression.html#cb67-20" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb67-21"><a href="multiple-linear-regression.html#cb67-21" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/gdp-gov-fit-1.png" width="672" /></p>
<p><em>Exports</em></p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="multiple-linear-regression.html#cb68-1" tabindex="-1"></a><span class="co"># Fits with Inflation</span></span>
<span id="cb68-2"><a href="multiple-linear-regression.html#cb68-2" tabindex="-1"></a>outRegExp <span class="ot">&lt;-</span> <span class="fu">lm</span>(gdp <span class="sc">~</span> exp, <span class="at">data =</span> dat)</span>
<span id="cb68-3"><a href="multiple-linear-regression.html#cb68-3" tabindex="-1"></a>varVal    <span class="ot">&lt;-</span> dat<span class="sc">$</span>exp</span>
<span id="cb68-4"><a href="multiple-linear-regression.html#cb68-4" tabindex="-1"></a>out       <span class="ot">&lt;-</span> outRegExp</span>
<span id="cb68-5"><a href="multiple-linear-regression.html#cb68-5" tabindex="-1"></a>varNam    <span class="ot">&lt;-</span> <span class="st">&quot;Exports&quot;</span></span>
<span id="cb68-6"><a href="multiple-linear-regression.html#cb68-6" tabindex="-1"></a><span class="co"># Plots Regression Line and Scatterplot and residuals plot</span></span>
<span id="cb68-7"><a href="multiple-linear-regression.html#cb68-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb68-8"><a href="multiple-linear-regression.html#cb68-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb68-9"><a href="multiple-linear-regression.html#cb68-9" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>gd,</span>
<span id="cb68-10"><a href="multiple-linear-regression.html#cb68-10" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb68-11"><a href="multiple-linear-regression.html#cb68-11" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;GDP&quot;</span>)</span>
<span id="cb68-12"><a href="multiple-linear-regression.html#cb68-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> out<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb68-13"><a href="multiple-linear-regression.html#cb68-13" tabindex="-1"></a>       <span class="at">b   =</span> out<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb68-14"><a href="multiple-linear-regression.html#cb68-14" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb68-15"><a href="multiple-linear-regression.html#cb68-15" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb68-16"><a href="multiple-linear-regression.html#cb68-16" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb68-17"><a href="multiple-linear-regression.html#cb68-17" tabindex="-1"></a>     <span class="at">y    =</span> out<span class="sc">$</span>residuals,</span>
<span id="cb68-18"><a href="multiple-linear-regression.html#cb68-18" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb68-19"><a href="multiple-linear-regression.html#cb68-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb68-20"><a href="multiple-linear-regression.html#cb68-20" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb68-21"><a href="multiple-linear-regression.html#cb68-21" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/gdp-exp-fit-1.png" width="672" /></p>
<p>All of them seem like good candidates for a linear relationship with the GDP,
however when we use them all together, a more careful analysis should be made.</p>
<p>We can see the summary reports for the individual regressions and the regression
with all independent variables as follows:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="multiple-linear-regression.html#cb69-1" tabindex="-1"></a>outRegAll <span class="ot">&lt;-</span> <span class="fu">lm</span>(gdp <span class="sc">~</span> inf <span class="sc">+</span> une <span class="sc">+</span> int <span class="sc">+</span> gov <span class="sc">+</span> exp, <span class="at">data =</span> dat)</span>
<span id="cb69-2"><a href="multiple-linear-regression.html#cb69-2" tabindex="-1"></a></span>
<span id="cb69-3"><a href="multiple-linear-regression.html#cb69-3" tabindex="-1"></a><span class="co"># Summary All</span></span>
<span id="cb69-4"><a href="multiple-linear-regression.html#cb69-4" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;All Independent Variables&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;All Independent Variables&quot;</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="multiple-linear-regression.html#cb71-1" tabindex="-1"></a><span class="fu">summary</span>(outRegAll)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp ~ inf + une + int + gov + exp, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.56610 -0.38300 -0.00634  0.36630  1.22542 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.555301   0.380440   4.088 6.41e-05 ***
## inf          0.312218   0.090012   3.469 0.000647 ***
## une         -0.377334   0.129275  -2.919 0.003938 ** 
## int          0.177827   0.128312   1.386 0.167403    
## gov         -0.008483   0.010361  -0.819 0.413950    
## exp          0.064657   0.011930   5.420 1.80e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5024 on 190 degrees of freedom
## Multiple R-squared:  0.8096, Adjusted R-squared:  0.8046 
## F-statistic: 161.6 on 5 and 190 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="multiple-linear-regression.html#cb73-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Only Inflation Rate&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Only Inflation Rate&quot;</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="multiple-linear-regression.html#cb75-1" tabindex="-1"></a><span class="fu">summary</span>(outRegInf)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp ~ inf, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.40960 -0.38896  0.03562  0.37998  1.33364 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.02551    0.08705   11.78   &lt;2e-16 ***
## inf          0.49489    0.01965   25.19   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5514 on 194 degrees of freedom
## Multiple R-squared:  0.7659, Adjusted R-squared:  0.7646 
## F-statistic: 634.5 on 1 and 194 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="multiple-linear-regression.html#cb77-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Only Unemployment Rate&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Only Unemployment Rate&quot;</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="multiple-linear-regression.html#cb79-1" tabindex="-1"></a><span class="fu">summary</span>(outRegUne)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp ~ une, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.42276 -0.49693  0.02667  0.49525  2.79562 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   6.0868     0.2046   29.74   &lt;2e-16 ***
## une          -1.0400     0.0661  -15.73   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7554 on 194 degrees of freedom
## Multiple R-squared:  0.5606, Adjusted R-squared:  0.5584 
## F-statistic: 247.5 on 1 and 194 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="multiple-linear-regression.html#cb81-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Only Interest Rate&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Only Interest Rate&quot;</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="multiple-linear-regression.html#cb83-1" tabindex="-1"></a><span class="fu">summary</span>(outRegInt)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp ~ int, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.27807 -0.50801 -0.00257  0.50336  2.69719 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.31600    0.32323  -4.071  6.8e-05 ***
## int          0.86483    0.06398  13.517  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8178 on 194 degrees of freedom
## Multiple R-squared:  0.485,  Adjusted R-squared:  0.4824 
## F-statistic: 182.7 on 1 and 194 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="multiple-linear-regression.html#cb85-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Only Government Spending&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Only Government Spending&quot;</code></pre>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="multiple-linear-regression.html#cb87-1" tabindex="-1"></a><span class="fu">summary</span>(outRegGov)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp ~ gov, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4370 -0.6442 -0.1258  0.7429  3.1839 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  1.37117    0.51450   2.665  0.00835 **
## gov          0.06464    0.02041   3.167  0.00179 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.111 on 194 degrees of freedom
## Multiple R-squared:  0.04916,    Adjusted R-squared:  0.04426 
## F-statistic: 10.03 on 1 and 194 DF,  p-value: 0.001789</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="multiple-linear-regression.html#cb89-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Only Exports&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Only Exports&quot;</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="multiple-linear-regression.html#cb91-1" tabindex="-1"></a><span class="fu">summary</span>(outRegExp)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp ~ exp, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0084 -0.6679 -0.1133  0.6581  3.0810 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.32709    0.27818   8.365 1.16e-14 ***
## exp          0.06540    0.02664   2.455    0.015 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.122 on 194 degrees of freedom
## Multiple R-squared:  0.03014,    Adjusted R-squared:  0.02514 
## F-statistic: 6.028 on 1 and 194 DF,  p-value: 0.01496</code></pre>
<p>As we can see, the values for the coefficients can change when doing simple linear
regression and multiple linear regression. If the changes are very dramatic (like change
in the sign of the coefficient) further inspection is necessary for that variable.</p>
</div>
<div id="least-squares-estimation-1" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Least Squares Estimation<a href="multiple-linear-regression.html#least-squares-estimation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For least squares estimation, we need to solve the problem:</p>
<p><span class="math display">\[
\min_\boldsymbol{\beta}Q(\boldsymbol{\beta}) = \sum_{i=1}^n (y_i - \hat{y}(\boldsymbol{\beta}))^2 = (\mathbf{y}- \hat{\mathbf{y}})&#39;(\mathbf{y}- \hat{\mathbf{y}}) = (\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})
\]</span>
The representation in matrix notation of the problem, allows us to use the same
expression to solve this problem as with simple linear regression. The solution
is obtained in the exact same way, and is given by:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = (\mathbf{X}&#39; \mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}
\]</span>
however in this case:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = \left(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2,\ldots,\hat{\beta}_p\right)&#39;
\]</span>
this is the reason, working in matrix form is very useful.</p>
</div>
<div id="properties-of-the-estimates-1" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Properties of the Estimates<a href="multiple-linear-regression.html#properties-of-the-estimates-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As with simple linear regression, we can consider several estimates:</p>
<ul>
<li><span class="math inline">\(\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{\beta}\)</span> the estimates of the observations,</li>
<li><span class="math inline">\(\hat{\mathbf{e}} = \mathbf{y}- \hat{\mathbf{y}} = \mathbf{y}- \mathbf{X}\hat{\boldsymbol{\beta}}\)</span> the estimates of the errors.</li>
</ul>
<p>We also note that:</p>
<p><span class="math display">\[
\hat{\mathbf{y}} = \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{y}= \mathbf{H}y
\]</span>
where <span class="math inline">\(\mathbf{H}\)</span> is called the hat matrix, because it transforms <span class="math inline">\(\mathbf{y}\)</span> into <span class="math inline">\(\hat{\mathbf{y}}\)</span>,
or the projection matrix.</p>
<p>We will see that:</p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is a linear combination of <span class="math inline">\(y\)</span>.</li>
<li>The sum of the estimated errors is equal to zero, <span class="math inline">\(\sum_{i=1}^n \hat{e_i} = 0\)</span>.</li>
<li><span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{x}_j}\)</span> are orthogonal for <span class="math inline">\(j=\{1,\ldots,p\}\)</span>.</li>
<li><span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span> are orthogonal.</li>
<li><span class="math inline">\(\bar{y} = \hat{\bar{y}}\)</span>.</li>
</ul>
<p>To see that <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is a linear combination of <span class="math inline">\(y\)</span>, we need to express <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>
as follows:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = \mathbf{A}\mathbf{y}
\]</span></p>
<p>for some matrix <span class="math inline">\(\mathbf{A}\)</span>. This is very easy to do, we just let <span class="math inline">\(\mathbf{A}= (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\)</span>, so:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}= \mathbf{A}\mathbf{y}
\]</span>
Now to see that the sum of the estimated errors is equal to zero, <span class="math inline">\(\sum_{i=1}^n \hat{e_i} = 0\)</span>, we
notice that we need to show that:</p>
<p><span class="math display">\[
\hat{\mathbf{e}}&#39; \mathbf{1}= 0
\]</span></p>
<p>To do so we notice that:</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}} = (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}
  &amp;\implies (\mathbf{X}&#39;\mathbf{X})\hat{\boldsymbol{\beta}} = \mathbf{X}&#39;\mathbf{y}\\
  &amp;\implies \mathbf{X}&#39;\mathbf{y}- \mathbf{X}&#39;\mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{0}\\
  &amp;\implies \mathbf{X}&#39;\left(\mathbf{y}- \hat{\mathbf{y}}\right) = \mathbf{0}\\
  &amp;\implies \mathbf{X}&#39;\hat{\mathbf{e}} = \mathbf{0}
\end{align*}\]</span></p>
<p>Now focusing on the product <span class="math inline">\(\mathbf{X}&#39;\hat{\mathbf{e}}\)</span> we have that:</p>
<p><span class="math display">\[
\mathbf{X}&#39;\hat{\mathbf{e}} =
  \left[\begin{matrix}
    \mathbf{1}&#39; \\
    \mathbf{x}_1   \\
    \mathbf{x}_2   \\
    \vdots  \\
    \mathbf{x}_p
  \end{matrix}\right] \hat{\mathbf{e}} =
  \left[\begin{matrix}
    \mathbf{1}&#39; \hat{\mathbf{e}} \\
    \mathbf{x}_1   \hat{\mathbf{e}} \\
    \mathbf{x}_2   \hat{\mathbf{e}} \\
    \vdots            \\
    \mathbf{x}_p   \hat{\mathbf{e}}
  \end{matrix}\right]
\]</span>
So we have that:</p>
<p><span class="math display">\[
\left[\begin{matrix}
    \mathbf{1}&#39; \hat{\mathbf{e}} \\
    \mathbf{x}_1   \hat{\mathbf{e}} \\
    \mathbf{x}_2   \hat{\mathbf{e}} \\
    \vdots            \\
    \mathbf{x}_p   \hat{\mathbf{e}}
  \end{matrix}\right] =
  \left[\begin{matrix}
    0 \\
    0 \\
    0 \\
    \vdots            \\
    0
  \end{matrix}\right]
\]</span>
So from the first line of this result, we have that:</p>
<p><span class="math display">\[
\mathbf{1}&#39; \hat{\mathbf{e}} = 0
\]</span>
which is the result we wanted to proof.</p>
<p>Now, to show that <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{x}_j}\)</span> are orthogonal for <span class="math inline">\(j=\{1,\ldots,p\}\)</span>,
we use again on:</p>
<p><span class="math display">\[
\left[\begin{matrix}
    \mathbf{1}&#39; \hat{\mathbf{e}} \\
    \mathbf{x}_1   \hat{\mathbf{e}} \\
    \mathbf{x}_2   \hat{\mathbf{e}} \\
    \vdots            \\
    \mathbf{x}_p   \hat{\mathbf{e}}
  \end{matrix}\right] =
  \left[\begin{matrix}
    0 \\
    0 \\
    0 \\
    \vdots            \\
    0
  \end{matrix}\right]
\]</span></p>
<p>And notice that lines 2 to <span class="math inline">\(p+1\)</span> proof this results, that is</p>
<p><span class="math display">\[
\mathbf{x}_i &#39; \hat{\mathbf{e}} = 0 \quad i=\{1,\ldots,p\}
\]</span>
Now to show that <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span> are orthogonal, we show that:</p>
<p><span class="math display">\[
\hat{\mathbf{e}}&#39;\hat{\mathbf{y}} = 0
\]</span>
Now</p>
<p><span class="math display">\[\begin{align*}
\hat{\mathbf{e}}&#39;\hat{\mathbf{y}}
  &amp;= (\mathbf{y}- \hat{\mathbf{y}})&#39;\hat{\mathbf{y}} \\
  &amp;= \left(\mathbf{y}- \mathbf{X}\hat{\boldsymbol{\beta}}\right)&#39;\mathbf{X}\hat{\boldsymbol{\beta}} \\
  &amp;= \left(\mathbf{y}- \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}\right)&#39;\mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}\\
  &amp;= \mathbf{y}&#39; \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}- \mathbf{y}&#39; \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{y}\\
  &amp;= \mathbf{y}&#39; \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}- \mathbf{y}&#39; \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{y}\\
  &amp;= 0
\end{align*}\]</span></p>
<p>Finally, to show that <span class="math inline">\(\bar{y} = \hat{\bar{y}}\)</span>, we use:</p>
<p><span class="math display">\[
\hat{\mathbf{e}}&#39;\mathbf{1}= (\mathbf{y}- \hat{\mathbf{y}})&#39;\mathbf{1}= \mathbf{y}&#39;\mathbf{1}- \hat{\mathbf{y}}&#39;\mathbf{1}= \sum_{i=1}^ny_i - \sum_{i=1}^n\hat{y}_i = n\bar{y} - n\hat{\bar{y}}
\]</span>
since <span class="math inline">\(\hat{\mathbf{e}}&#39;\mathbf{1}= 0\)</span>, then we have that</p>
<p><span class="math display">\[
n\bar{y} - n\hat{\bar{y}} = 0 \implies n\bar{y} = n\hat{\bar{y}} \implies \bar{y} = \hat{\bar{y}}  
\]</span></p>
</div>
<div id="multiple-r2" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Multiple <span class="math inline">\(R^2\)</span><a href="multiple-linear-regression.html#multiple-r2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As with simple linear regression we can explain the total variability, by decomposing the
variability in two parts, the regression variability and the error variability.</p>
<p>First, we define this concepts:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Total Sum of Squares <span class="math inline">\(SS_{tot}\)</span></strong>:<br />
The total sum of squares measures the total variability in <span class="math inline">\(\mathbf{y}\)</span>:</p>
<p><span class="math display">\[
SS_{tot} = (\mathbf{y} - \bar{y} \mathbf{1})&#39; (\mathbf{y} - \bar{y} \mathbf{1})
\]</span></p></li>
<li><p><strong>Residual Sum of Squares <span class="math inline">\(SS_{res}\)</span></strong>:<br />
The residual sum of squares measures the unexplained variability in the regression model:</p>
<p><span class="math display">\[
SS_{res} = (\mathbf{y} - \hat{\mathbf{y}})&#39; (\mathbf{y} - \hat{\mathbf{y}})
\]</span></p></li>
<li><p><strong>Explained Sum of Squares <span class="math inline">\(SS_{reg}\)</span></strong></p></li>
</ol>
<p>The explained sum of squares measures how much of the total variability is explained by the regression model. It is the difference between the predicted values and the mean of <span class="math inline">\(\mathbf{y}\)</span>:</p>
<p><span class="math display">\[
SS_{reg} = (\hat{\mathbf{y}} - \bar{y} \mathbf{1})&#39; (\hat{\mathbf{y}} - \bar{y} \mathbf{1})
\]</span>
As with simple linear regression, it can be shown that:</p>
<p><span class="math display">\[
SS_{tot} = SS_{reg} + SS_{res}
\]</span>
To see this, we start form <span class="math inline">\(SS_{tot}\)</span>, and do the adding and subtracting trick:</p>
<p><span class="math display">\[\begin{align*}
SS_{tot}
  &amp;= (\mathbf{y} - \bar{y} \mathbf{1})&#39; (\mathbf{y} - \bar{y} \mathbf{1}) \\
  &amp;= (\mathbf{y} - \hat{\mathbf{y}} + \hat{\mathbf{y}} - \bar{y} \mathbf{1})&#39; (\mathbf{y} - \hat{\mathbf{y}} + \hat{\mathbf{y}} - \bar{y} \mathbf{1}) \\
  &amp;= (\mathbf{y} - \hat{\mathbf{y}})&#39; (\mathbf{y} - \hat{\mathbf{y}}) + (\mathbf{y} - \hat{\mathbf{y}})&#39; (\hat{\mathbf{y}} - \bar{y} \mathbf{1}) + (\hat{\mathbf{y}} - \bar{y} \mathbf{1})&#39; (\mathbf{y} - \hat{\mathbf{y}}\mathbf{1}) + (\hat{\mathbf{y}} - \bar{y} \mathbf{1})&#39; (\hat{\mathbf{y}} - \bar{y} \mathbf{1})
\end{align*}\]</span></p>
<p>Now, notice that:</p>
<p><span class="math display">\[
(\mathbf{y} - \hat{\mathbf{y}})&#39; (\hat{\mathbf{y}} - \bar{y} \mathbf{1}) = \hat{\mathbf{e}}&#39; (\hat{\mathbf{y}} - \bar{y} \mathbf{1}) = \hat{\mathbf{e}}&#39;\hat{\mathbf{y}} - \bar{y}\hat{\mathbf{e}}&#39; \mathbf{1} = 0 - \bar{y}0 = 0
\]</span>
And similarly for <span class="math inline">\((\hat{\mathbf{y}} - \bar{y} \mathbf{1})&#39; (\mathbf{y} - \hat{\mathbf{y}}\mathbf{1}) = 0\)</span>, then:</p>
<p><span class="math display">\[
SS_{tot} = (\mathbf{y} - \hat{\mathbf{y}})&#39; (\mathbf{y} - \hat{\mathbf{y}}) + (\hat{\mathbf{y}} - \bar{y} \mathbf{1})&#39; (\hat{\mathbf{y}} - \bar{y} \mathbf{1}) = SS_{reg} + SS_{res}
\]</span>
The multiple <span class="math inline">\(R^2\)</span> is the variability explained by the regression with respect to the total variability
and can be expressed as:</p>
<p><span class="math display">\[
R^2 = \frac{SS_{reg}}{SS_{tot}}
\]</span>
or using the previous expression</p>
<p><span class="math display">\[
1 = \frac{SS_{tot}}{SS_{tot}} = \frac{SS_{reg}}{SS_{tot}} + \frac{SS_{res}}{SS_{tot}} = R^2 + \frac{SS_{res}}{SS_{tot}} \implies R^2 = 1 - \frac{SS_{res}}{SS_{tot}}  
\]</span></p>
<p>Finally, we work on the expressions of <span class="math inline">\(SS_{res}\)</span> and <span class="math inline">\(SS_{tot}\)</span>, to express them
in terms of projection matrices.</p>
<p>First note that:</p>
<p><span class="math display">\[
\mathbf{y}- \hat{\mathbf{y}} = \mathbf{y}- \mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{y}- \mathbf{H}\mathbf{y}= (\mathbf{I}- \mathbf{H})\mathbf{y}
\]</span>
and also notice that <span class="math inline">\((\mathbf{I}- \mathbf{H})\)</span> is symmetric and:</p>
<p><span class="math display">\[
(\mathbf{I}- \mathbf{H})(\mathbf{I}- \mathbf{H}) = \mathbf{I}-\mathbf{H}- \mathbf{H}+ \mathbf{H}\mathbf{H}
\]</span>
and</p>
<p><span class="math display">\[
\mathbf{H}\mathbf{H}= \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; = \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; = \mathbf{H}
\]</span>
this means <span class="math inline">\(\mathbf{H}\)</span> is idempotent. In fact, all projection matrices are idempotent.</p>
<p>Then, we have that:</p>
<p><span class="math display">\[
(\mathbf{I}- \mathbf{H})(\mathbf{I}- \mathbf{H}) = \mathbf{I}-\mathbf{H}- \mathbf{H}+ \mathbf{H}= \mathbf{I}-\mathbf{H}- \mathbf{H}
\]</span>
which makes <span class="math inline">\(\mathbf{I}- \mathbf{H}\)</span> also idempotent. Therefore:</p>
<p><span class="math display">\[
SS_{res} = (\mathbf{y}- \hat{\mathbf{y}})&#39;(\mathbf{y}- \hat{\mathbf{y}}) = ((\mathbf{I}- \mathbf{H})\mathbf{y})&#39;((\mathbf{I}- \mathbf{H})\mathbf{y}) = \mathbf{y}&#39;(\mathbf{I}- \mathbf{H})&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}= \mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}
\]</span>
And we can do a similar trick for the <span class="math inline">\(SS_{tot}\)</span> by writing <span class="math inline">\(\bar{y} \mathbf{1}\)</span> as a result
of projecting <span class="math inline">\(\mathbf{y}\)</span> with a design matrix <span class="math inline">\(\mathbf{1}\)</span>:</p>
<p><span class="math display">\[
\bar{y} \mathbf{1}= \mathbf{1}\bar{y} = \mathbf{1}\frac{1}{n} \sum_{i=1}^n y_i = \mathbf{1}\frac{1}{n}\mathbf{1}&#39; \mathbf{y}= \mathbf{1}(\mathbf{1}&#39;\mathbf{1})^{-1}\mathbf{1}&#39; \mathbf{y}
\]</span>
where we use the fact that <span class="math inline">\(\mathbf{1}&#39;\mathbf{1}= n\)</span>.</p>
<p>We call <span class="math inline">\(\mathbf{H}_0 = \mathbf{1}(\mathbf{1}&#39;\mathbf{1})^{-1}\mathbf{1}&#39;\)</span>, since <span class="math inline">\(\mathbf{1}(\mathbf{1}&#39;\mathbf{1})^{-1}\mathbf{1}&#39;\)</span>
is a projection matrix. And since it is a projection matrix it is idempotent (it is also not difficult to check this manually)
and <span class="math inline">\(\mathbf{I}- \mathbf{H}_0\)</span> is also idempotent.</p>
<p>So we can do:</p>
<p><span class="math display">\[
\mathbf{y}- \hat{y}\mathbf{1}= \mathbf{y}- \mathbf{H}_0 \mathbf{y}= (\mathbf{I}-\mathbf{H}_0)\mathbf{y}
\]</span></p>
<p><span class="math display">\[
SS_{tot} = (\mathbf{y}- \bar{y}\mathbf{1})&#39;(\mathbf{y}- \bar{y}\mathbf{1}) = ((\mathbf{I}- \mathbf{H}_0)\mathbf{y})&#39;((\mathbf{I}- \mathbf{H}_0)\mathbf{y}) = \mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}= \mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}
\]</span></p>
<p>so the <span class="math inline">\(R^2\)</span> can be expressed as follows:</p>
<p><span class="math display">\[
R^2 = 1 - \frac{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}}{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}}
\]</span>
When written like this, it is easy to see that:</p>
<p><span class="math display">\[
\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}= \min_\boldsymbol{\beta}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})
\]</span>
the solution to this minimization problem, since we are using the optimal value
<span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>. And</p>
<p><span class="math display">\[
\mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}= \min_{\beta_0} (\mathbf{y}- \mathbf{X}_0 \beta_0)&#39;(\mathbf{y}- \mathbf{X}_0 \beta_0)
\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}_0\)</span> is just a matrix with one column <span class="math inline">\(\mathbf{1}\)</span>.</p>
<p>Now, we also have that:</p>
<p><span class="math display">\[
\min_\boldsymbol{\beta}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) \leq \min_{\beta_0} (\mathbf{y}- \mathbf{X}_0 \beta_0)&#39;(\mathbf{y}- \mathbf{X}_0 \beta_0)
\]</span></p>
<p>therefore</p>
<p><span class="math display">\[
\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}\leq \mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}
\]</span>
and since both of them are quadratic forms, we have that:</p>
<p><span class="math inline">\(\mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}, \mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}\geq 0\)</span></p>
<p>then:</p>
<p><span class="math display">\[0 \leq \frac{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}}{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}} \leq 0\]</span>
then:</p>
<p><span class="math display">\[0 \leq R^2 \leq 0\]</span>.</p>
<p>Where we use the fact that all symmetric idempotent matrices are symmetric positive semi-definite.</p>
<p>Another interpretation of <span class="math inline">\(R^2\)</span> is the percentage of the variability explained
by multiple regression of a “<em>poor man’s regression</em>” in which you don’t have
independent variables (that is you are independent variable poor). In this way,
we can define</p>
<p><span class="math display">\[
\bar{y} \mathbf{1}= \hat{\mathbf{y}}_0
\]</span>
the “<em>poor man’s prediction</em>”, of which <span class="math inline">\(\mathbf{H}_0\)</span> is it’s projection matrix (or hat matrix).</p>
</div>
<div id="geometric-interpretation-of-multiple-linear-regression" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Geometric Interpretation of Multiple Linear Regression<a href="multiple-linear-regression.html#geometric-interpretation-of-multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Multiple linear regression can be thought as projecting <span class="math inline">\(\mathbf{y}\)</span> in the column
space of the design matrix <span class="math inline">\(\mathbf{X}\)</span>. The following diagram pictures multiple linear regression.</p>
<p><img src="_main_files/figure-html/geo-int-1.png" width="672" /></p>
<p>Here we can see several components:</p>
<ul>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of observations. Is a vector in <span class="math inline">\(\mathbb{R}^n\)</span>.</li>
<li>The grey hyper-plane is the column space generated by <span class="math inline">\(\mathbf{X}\)</span>, a sub-space of <span class="math inline">\(\mathbb{R}^n\)</span>.</li>
<li>The multiple regression prediction <span class="math inline">\(\hat{\mathbf{y}}\)</span> of <span class="math inline">\(\mathbf{y}\)</span> is the projection of <span class="math inline">\(\mathbf{y}\)</span>
on the space generated by the column of <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>The poor man’s prediction <span class="math inline">\(\hat{\mathbf{y}}_0\)</span>, in the column space of <span class="math inline">\(\mathbf{X}\)</span> (since,
one of the columns is <span class="math inline">\(\mathbf{1}\)</span>), but in most cases it is different to <span class="math inline">\(\hat{\mathbf{y}}\)</span> (the closest vector in
the column space of <span class="math inline">\(\mathbf{X}\)</span> to <span class="math inline">\(\mathbf{y}\)</span>).</li>
<li>We notice that the differences:
<ul>
<li><span class="math inline">\(\mathbf{y}- \hat{\mathbf{y}}\)</span>.</li>
<li><span class="math inline">\(\mathbf{y}- \hat{\mathbf{y}}_0\)</span></li>
<li><span class="math inline">\(\hat{\mathbf{y}} - \hat{\mathbf{y}}_0\)</span></li>
</ul>
form a right triangle, then it must be that:
<span class="math display">\[
||\mathbf{y}- \hat{\mathbf{y}}_0||^2 = ||\mathbf{y}- \hat{\mathbf{y}}||^2 + ||\hat{\mathbf{y}} - \hat{\mathbf{y}}_0||^2
\]</span>
which is the same as
<span class="math display">\[
(\mathbf{y}- \hat{\mathbf{y}})&#39;(\mathbf{y}- \hat{\mathbf{y}}_0) = (\mathbf{y}- \hat{\mathbf{y}})&#39;(\mathbf{y}- \hat{\mathbf{y}}) + (\hat{\mathbf{y}} - \hat{\mathbf{y}}_0)&#39;(\hat{\mathbf{y}} - \hat{\mathbf{y}}_0)
\]</span>
that can be expressed as:
<span class="math display">\[
SS_{tot} = SS_{res} + SS_{reg}
\]</span></li>
</ul>
</div>
<div id="centered-and-standarized-variables" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Centered and Standarized Variables<a href="multiple-linear-regression.html#centered-and-standarized-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="centered-variables" class="section level3 hasAnchor" number="6.7.1">
<h3><span class="header-section-number">6.7.1</span> Centered Variables<a href="multiple-linear-regression.html#centered-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Like with simple linear regression we can center and standardize our variables.</p>
<p>For this section, let us use the notation:</p>
<p><span class="math display">\[\mathbf{X}= \left[\mathbf{x}_1, \mathbf{x}_2, \ldots \mathbf{x}_p \right]\]</span>
a matrix with <span class="math inline">\(p\)</span> variables in which each column is a variable <span class="math inline">\(\mathbf{x}_i\)</span>. In the
context of linear regression you can this is similar to the design matrix except
that it doesn’t have the column of ones. In this way, we will rename the design
matrix as</p>
<p><span class="math display">\[\mathbf{X}_{*} = \left[\mathbf{1}\mathbf{X}\right] \]</span>
we introduce this notation, since we don’t want to center or standardize the
column of ones.</p>
<p>To center matrix <span class="math inline">\(\mathbf{X}\)</span> we need to remove the mean of every column. Notice that the
vector of means is given by:</p>
<p><span class="math display">\[\bar{\mathbf{x}} =
  \left[\begin{matrix}
    \bar{\mathbf{x}}_1 \\
    \bar{\mathbf{x}}_2 \\
    \vdots      \\
    \bar{\mathbf{x}}_p
  \end{matrix}\right] =
  \left[\begin{matrix}
    \frac{1}{n} \mathbf{x}_1&#39;\mathbf{1}\\
    \frac{1}{n} \mathbf{x}_2&#39;\mathbf{1}\\
    \vdots                   \\
    \frac{1}{n} \mathbf{x}_p&#39;\mathbf{1}
  \end{matrix}\right] = \frac{1}{n}
  \left[\begin{matrix}
    \mathbf{x}_1&#39;\mathbf{1}\\
    \mathbf{x}_2&#39;\mathbf{1}\\
    \vdots                   \\
    \mathbf{x}_p&#39;\mathbf{1}
  \end{matrix}\right] = \frac{1}{n} \mathbf{X}&#39; \mathbf{1}\]</span></p>
<p>then the centered data <span class="math inline">\(\mathbf{X}_c\)</span> is given by:</p>
<p><span class="math display">\[\mathbf{X}_c = \mathbf{X}- \left[\begin{matrix}
    \bar{\mathbf{x}}_1 &amp; \bar{\mathbf{x}}_2 &amp; \dots  &amp; \bar{\mathbf{x}}_p\\
    \bar{\mathbf{x}}_1 &amp; \bar{\mathbf{x}}_2 &amp; \dots  &amp; \bar{\mathbf{x}}_p\\
    \vdots      &amp; \vdots      &amp; \ddots &amp; \vdots     \\
    \bar{\mathbf{x}}_1 &amp; \bar{\mathbf{x}}_2 &amp; \dots  &amp; \bar{\mathbf{x}}_p
  \end{matrix}\right] = \mathbf{X}- \mathbf{1}\bar{\mathbf{x}}&#39; = \mathbf{X}- \mathbf{1}\left(\frac{1}{n} \mathbf{X}&#39; \mathbf{1}\right)&#39; = \mathbf{X}- \frac{1}{n}\mathbf{1}\mathbf{1}&#39; \mathbf{X}= \left(\mathbf{I}- \frac{1}{n}\mathbf{1}\mathbf{1}&#39; \right)\mathbf{X}\]</span></p>
<p>We call <span class="math display">\[\mathbf{C}=  \left(\mathbf{I}- \frac{1}{n}\mathbf{1}\mathbf{1}&#39; \right) = \mathbf{I}- \mathbf{H}_0 \]</span> the centering
matrix, since it centers the variables of matrix <span class="math inline">\(\mathbf{X}\)</span>. Note also, that <span class="math inline">\(\mathbf{C}\)</span> also
centers any matrix with <span class="math inline">\(n\)</span> rows, in particular a vector of size <span class="math inline">\(n\)</span> is also centered
by <span class="math inline">\(\mathbf{C}\)</span>. So we can center <span class="math inline">\(y\)</span> the dependent variable, the same way:</p>
<p><span class="math display">\[\mathbf{y}_c = \mathbf{C}\mathbf{y}\]</span></p>
</div>
<div id="sample-covariance" class="section level3 hasAnchor" number="6.7.2">
<h3><span class="header-section-number">6.7.2</span> Sample Covariance<a href="multiple-linear-regression.html#sample-covariance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having defined, the centered matrix <span class="math inline">\(\mathbf{X}_c\)</span> we can define the sample covariance
of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{S}_{XX} \in \mathbb{R}^{p \times p}\)</span> as follows:</p>
<p><span class="math display">\[ \mathbf{S}_{XX} = \frac{1}{n-1} \left(\mathbf{X}- \mathbf{1}\bar{\mathbf{x}}&#39;\right)&#39;\left(\mathbf{X}- \mathbf{1}\bar{\mathbf{x}}&#39;\right) = \frac{1}{n-1} \mathbf{X}_c&#39;\mathbf{X}_c = \frac{1}{n-1} \mathbf{X}&#39;\mathbf{C}&#39; \mathbf{C}\mathbf{X}\]</span>
Now, since <span class="math inline">\(\mathbf{C}= \mathbf{I}- \mathbf{H}_0\)</span> is idempotent and symmetric we have that:</p>
<p><span class="math display">\[ \mathbf{S}_{XX} = \frac{1}{n-1} \mathbf{X}&#39;\mathbf{C}&#39; \mathbf{C}\mathbf{X}= \frac{1}{n-1} \mathbf{X}&#39; \mathbf{C}\mathbf{X}= \frac{1}{n-1} \mathbf{X}_c&#39;\mathbf{X}= \frac{1}{n-1} \mathbf{X}&#39;\mathbf{X}_c \]</span>
So the sample covariance, is the same for the original variables and the centered variables.</p>
<p>We can also define the covariance vector between variables <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \ldots \mathbf{x}_p\)</span> and variable <span class="math inline">\(\mathbf{y}\)</span>,
<span class="math inline">\(\mathbf{S}_{Xy} \in \mathbb{R}^{p \times 1}\)</span>, as follows:</p>
<p><span class="math display">\[ \mathbf{S}_{Xy} = \frac{1}{n-1} \left(\mathbf{X}- \mathbf{1}\bar{\mathbf{x}}&#39;\right)&#39;\left(\mathbf{y}- \mathbf{1}\bar{y}\right) = \frac{1}{n-1} \mathbf{X}_c&#39;\mathbf{y}_c = \frac{1}{n-1} \mathbf{X}&#39;\mathbf{C}&#39; \mathbf{C}\mathbf{y}\]</span></p>
<p>and, in the same way than before, we have that:</p>
<p><span class="math display">\[\mathbf{S}_{Xy} = \frac{1}{n-1} \mathbf{X}&#39; \mathbf{C}&#39; \mathbf{C}\mathbf{y}= \frac{1}{n-1} \mathbf{X}&#39; \mathbf{C}\mathbf{y}= \frac{1}{n-1} \mathbf{X}_c&#39;\mathbf{y}= \frac{1}{n-1} \mathbf{X}&#39;\mathbf{y}_c\]</span></p>
<p>so, you don’t need to center both variables. As long as you center one of them
the result will be the same.</p>
<p>With this measures, we can focus on splitting the vector of estimated coefficients <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, into the estimate for
the intercept and the estimates for the independent variables, as follows:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
  \left[\begin{matrix}
    \hat{\beta}_0 \\
    \hat{\boldsymbol{\beta}}_{-0}
  \end{matrix}\right]\]</span></p>
<p>with <span class="math inline">\(\hat{\beta}_0\)</span> the estimate of the intercept and <span class="math inline">\(\hat{\boldsymbol{\beta}}_{-0}\)</span> the coefficients
for all independent variables. That is:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}}_{-0} =
  \left[\begin{matrix}
    \hat{\beta}_1 \\
    \hat{\beta}_2 \\
    \vdots        \\
    \hat{\beta}_p  
  \end{matrix}\right] \in \mathbb{R}^{p \times 1}\]</span></p>
<p>Under the new notation, <span class="math inline">\(\mathbf{X}_{*}\)</span> for the design matrix, we have that:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\beta}} = \left(\mathbf{X}_{*}&#39;\mathbf{X}_{*}\right)^{-1}\mathbf{X}_{*}&#39;\mathbf{y}\]</span>
so:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\beta}} = \left[\begin{matrix}
    \hat{\beta}_0 \\
    \hat{\boldsymbol{\beta}}_{-0}
  \end{matrix}\right] =
  \left( \left[\mathbf{1}\mathbf{X}\right]&#39; \left[\mathbf{1}\mathbf{X}\right] \right)^{-1} \left[\mathbf{1}\mathbf{X}\right]&#39; \mathbf{y}\]</span></p>
<p>so we need to compute <span class="math inline">\(\left( \left[\mathbf{1}\mathbf{X}\right]&#39; \left[\mathbf{1}\mathbf{X}\right] \right)^{-1}\)</span>.</p>
<p>We start by computing:</p>
<p><span class="math display">\[\left[\mathbf{1}\mathbf{X}\right]&#39; \left[\mathbf{1}\mathbf{X}\right] =
  \left[\begin{matrix}
    \mathbf{1}&#39; \\
    \mathbf{X}&#39;  
  \end{matrix}\right] \left[\mathbf{1}\mathbf{X}\right] =
  \left[\begin{matrix}
    \mathbf{1}&#39; \mathbf{1}&amp; \mathbf{1}&#39; \mathbf{X}\\
    \mathbf{X}&#39;\mathbf{1}&amp; \mathbf{X}&#39; \mathbf{X}&#39;  
  \end{matrix}\right] =
  \left[\begin{matrix}
    n          &amp; n\bar{\mathbf{x}}&#39; \\
    n\bar{\mathbf{x}} &amp; \mathbf{X}&#39; \mathbf{X}&#39;  
  \end{matrix}\right]
\]</span>
Now, we need to invert a 2 by 2 block matrix (luckily there is a formula for this).
The formula is in the prerequisites section, however note that in this case
one of the blocks is of height 1, since the first value we are looking for <span class="math inline">\(\hat{\beta}_0\)</span>
is a scalar. After applying the formula, we have:</p>
<p><span class="math display">\[\begin{align*}
\left( \left[\mathbf{1}\mathbf{X}\right]&#39; \left[\mathbf{1}\mathbf{X}\right] \right)^{-1}
  &amp;= \left[\begin{matrix}
    n^{-1} + n^{-1} n\bar{\mathbf{x}}&#39; \left(\mathbf{X}&#39; \mathbf{X}&#39; -  n\bar{\mathbf{x}} n^{-1} n\bar{\mathbf{x}}&#39; \right)^{-1}n\bar{\mathbf{x}} n^{-1} &amp; -n^{-1} n\bar{\mathbf{x}}&#39; \left(\mathbf{X}&#39; \mathbf{X}&#39; -  n\bar{\mathbf{x}} n^{-1} n\bar{\mathbf{x}}&#39;     \right)^{-1} \\
    -\left(\mathbf{X}&#39; \mathbf{X}&#39; -  n\bar{\mathbf{x}} n^{-1} n\bar{\mathbf{x}}&#39; \right)^{-1}n\bar{\mathbf{x}} n^{-1} &amp; \left(\mathbf{X}&#39; \mathbf{X}&#39; -  n\bar{\mathbf{x}} n^{-1} n\bar{\mathbf{x}}&#39; \right)^{-1}  
  \end{matrix}\right] \\
  &amp;= \left[\begin{matrix}
    n^{-1} + \bar{\mathbf{x}}&#39; \left(\mathbf{X}&#39; \mathbf{X}-  n\bar{\mathbf{x}}\bar{\mathbf{x}}&#39; \right)^{-1}\bar{\mathbf{x}} &amp; \bar{\mathbf{x}}&#39; \left(\mathbf{X}&#39; \mathbf{X}-  n\bar{\mathbf{x}} \bar{\mathbf{x}}&#39; \right)^{-1} \\
    -\left(\mathbf{X}&#39; \mathbf{X}-  n\bar{\mathbf{x}} \bar{\mathbf{x}}&#39; \right)^{-1}\bar{\mathbf{x}}                   &amp; \left(\mathbf{X}&#39; \mathbf{X}-  n\bar{\mathbf{x}} \bar{\mathbf{x}}&#39; \right)^{-1}  
  \end{matrix}\right]  
\end{align*}\]</span></p>
<p>Now, notice that:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{X}&#39; \mathbf{X}-  n\bar{\mathbf{x}} \bar{\mathbf{x}}&#39;
  &amp;= \mathbf{X}&#39; \mathbf{X}-  n\left(\frac{\mathbf{X}&#39;\mathbf{1}}{n}\right)\left(\frac{\mathbf{X}&#39;\mathbf{1}}{n}\right)&#39; \\
  &amp;= \mathbf{X}&#39; \mathbf{X}-  \frac{1}{n}\mathbf{X}&#39;\mathbf{1}\mathbf{1}&#39; \mathbf{X}\\
  &amp;= \mathbf{X}&#39;\left(\mathbf{I}- \frac{1}{n}\mathbf{1}\mathbf{1}&#39;\right) \mathbf{X}\\
  &amp;= \mathbf{X}&#39;\mathbf{C}\mathbf{X}\\
  &amp;= (n-1)\mathbf{S}_{XX}
\end{align*}\]</span></p>
<p>then:</p>
<p><span class="math display">\[\begin{align*}
\left( \left[\mathbf{1}\mathbf{X}\right]&#39; \left[\mathbf{1}\mathbf{X}\right] \right)^{-1}  
  &amp;=
  \left[\begin{matrix}
    n^{-1} + \bar{\mathbf{x}}&#39; \left((n-1)\mathbf{S}_{XX} \right)^{-1}\bar{\mathbf{x}} &amp; -\bar{\mathbf{x}}&#39; \left((n-1)\mathbf{S}_{XX} \right)^{-1} \\
    -\left((n-1)\mathbf{S}_{XX} \right)^{-1}\bar{\mathbf{x}}                    &amp; \left((n-1)\mathbf{S}_{XX} \right)^{-1}   
  \end{matrix}\right] \\
  &amp;=
  \left[\begin{matrix}
    n^{-1} + \frac{1}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1} \bar{\mathbf{x}} &amp; -\frac{1}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1} \\
    -\frac{1}{n-1}\mathbf{S}_{XX}^{-1} \bar{\mathbf{x}}                    &amp; \frac{1}{n-1}\mathbf{S}_{XX}^{-1}   
  \end{matrix}\right]
\end{align*}\]</span></p>
<p>In a similar way we can easily compute:</p>
<p><span class="math display">\[\left[\mathbf{1}\mathbf{X}\right]&#39; \mathbf{y}=
  \left[\begin{matrix}
    \mathbf{1}&#39; \mathbf{y}\\
    \mathbf{X}&#39;\mathbf{y}
  \end{matrix}\right] =
  \left[\begin{matrix}
    n\bar{y} \\
    \mathbf{X}&#39;\mathbf{y}
  \end{matrix}\right]\]</span></p>
<p>Then, we have that:</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}
  &amp;= \left[\begin{matrix} \\
    \hat{\beta}_0 \\
    \hat{\boldsymbol{\beta}}_{-0}
  \end{matrix}\right] \\
  &amp;=
  \left[\begin{matrix}
    n^{-1} + \frac{1}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1} \bar{\mathbf{x}} &amp; -\frac{1}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1} \\
    -\frac{1}{n-1}\mathbf{S}_{XX}^{-1} \bar{\mathbf{x}}                    &amp; \frac{1}{n-1}\mathbf{S}_{XX}^{-1}   
  \end{matrix}\right]
  \left[\begin{matrix}
    n\bar{y} \\
    \mathbf{X}&#39;\mathbf{y}
  \end{matrix}\right] \\
  &amp;=
  \left[\begin{matrix}
    n^{-1}n\bar{y} + \frac{n\bar{y}}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1} \bar{\mathbf{x}} - \frac{1}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1}\mathbf{X}&#39;\mathbf{y}\\
    -\frac{n\bar{y}}{n-1}\mathbf{S}_{XX}^{-1} \bar{\mathbf{x}} + \frac{1}{n-1}\mathbf{S}_{XX}^{-1}\mathbf{X}&#39;\mathbf{y}
  \end{matrix}\right]
\end{align*}\]</span></p>
<p>Then, working first with the second row-block:</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}_{-0}
  &amp;=  -\frac{n\bar{y}}{n-1}\mathbf{S}_{XX}^{-1} \bar{\mathbf{x}} + \frac{1}{n-1}\mathbf{S}_{XX}^{-1}\mathbf{X}&#39;y \\
  &amp;= \frac{1}{n-1}\mathbf{S}_{XX}^{-1}\mathbf{X}&#39;y -\frac{1}{n-1}\mathbf{S}_{XX}^{-1} \bar{\mathbf{x}}n\bar{y}   \\
  &amp;= \frac{1}{n-1}\mathbf{S}_{XX}^{-1}\left(\mathbf{X}&#39;y - n\bar{\mathbf{x}}\bar{y}\right)
\end{align*}\]</span></p>
<p>Now, we note that:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{X}&#39;\mathbf{y}- n\bar{\mathbf{x}}\bar{y}
  &amp;= \mathbf{X}&#39;\mathbf{y}- n\bar{\mathbf{x}}\bar{y}                                                   \\
  &amp;= \mathbf{X}&#39;\mathbf{y}- n\left(\frac{\mathbf{X}&#39;\mathbf{1}}{n}\right)\left(\frac{\mathbf{1}&#39;\mathbf{y}}{n}\right) \\
  &amp;= \mathbf{X}&#39;\mathbf{y}- \frac{1}{n}\mathbf{X}&#39;\mathbf{1}\mathbf{1}&#39;\mathbf{y}\\
  &amp;= \mathbf{X}&#39; \left(\mathbf{I}- \frac{1}{n} \mathbf{1}\mathbf{1}&#39; \right) \mathbf{y}\\
  &amp;= \mathbf{X}&#39; \mathbf{C}\mathbf{y}\\
  &amp;= (n-1)\mathbf{S}_{Xy}
\end{align*}\]</span></p>
<p>Then, we have that:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}}_{-0} = \frac{1}{n-1}\mathbf{S}_{XX}^{-1}(n-1)\mathbf{S}_{Xy} = \mathbf{S}_{XX}^{-1}\mathbf{S}_{Xy}\]</span>
An equivalent result to that of simple linear regression, both using the covariance
matrix of the independent variables and the covariance vector of the independent
variables and the dependent variable.</p>
<p>This way of writing the coefficients shows the influence of each component:</p>
<ul>
<li><span class="math inline">\(\mathbf{S}_{XX}^{-1}\)</span>: The relationship between the independent variables.</li>
<li><span class="math inline">\(\mathbf{S}_{Xy}\)</span>: The relationship between the independent variables and the dependent variables.</li>
</ul>
<p>Also notice that, since centralizing doesn’t change the values of <span class="math inline">\(\mathbf{S}_{X_cX_c}^{-1}\)</span> and <span class="math inline">\(\mathbf{S}_{X_cy_c}\)</span>, centralizing
the independent variables or the dependent variable (or both), doesn’t change the
value of the coefficients of the independent variables.</p>
<p>Now, we can work more easily with the intercept estimate:</p>
<p><span class="math display">\[\begin{align*}
\hat{\beta}_0
  &amp;= n^{-1}n\bar{y} + \frac{n\bar{y}}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1} \bar{\mathbf{x}} - \frac{1}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1}\mathbf{X}&#39;\mathbf{y}\\
  &amp;= \bar{y} + \frac{1}{n-1}\bar{\mathbf{x}}&#39;\mathbf{S}_{XX}^{-1} n\bar{\mathbf{x}}\bar{y} - \frac{1}{n-1}\bar{\mathbf{x}}&#39;\mathbf{S}_{XX}^{-1}\mathbf{X}&#39;\mathbf{y}\\
  &amp;= \bar{y} + \frac{1}{n-1}\bar{\mathbf{x}}&#39;\mathbf{S}_{XX}^{-1} \left(n\bar{\mathbf{x}}\bar{y}  - \mathbf{X}&#39;\mathbf{y}\right)                              \\
  &amp;= \bar{y} - \frac{1}{n-1}\bar{\mathbf{x}}&#39;\mathbf{S}_{XX}^{-1} \left(\mathbf{X}&#39;\mathbf{y}- n\bar{\mathbf{x}}\bar{y} \right)                               \\
  &amp;= \bar{y} - \frac{1}{n-1}\bar{\mathbf{x}}&#39;\mathbf{S}_{XX}^{-1} (n-1)\mathbf{S}_{Xy}                                                           \\
  &amp;= \bar{y} - \bar{\mathbf{x}}&#39;\mathbf{S}_{XX}^{-1} \mathbf{S}_{Xy}                                                                             \\
  &amp;= \bar{y} - \bar{\mathbf{x}}&#39; \hat{\boldsymbol{\beta}}_{-0}
\end{align*}\]</span></p>
<p>Again, an equivalent result to that of simple linear regression. Like in simple
linear regression, centering the independent variables does affect the intercept
estimate, since <span class="math inline">\(\bar{\mathbf{x}}=\mathbf{0}\)</span>, we have that the coefficient after centering
the independent variables is <span class="math inline">\(\bar{y}\)</span> the mean of the dependent variable. And
if we also center the dependent variable, then <span class="math inline">\(\bar{y}=0\)</span> so the estimate of the
intercept is <span class="math inline">\(0\)</span> also. Therefore, if you are centering all variables, it is not
necessary to add the column of ones in the design matrix, since the estimate of the
intercept is <span class="math inline">\(0\)</span>.</p>
</div>
<div id="satandard-variables" class="section level3 hasAnchor" number="6.7.3">
<h3><span class="header-section-number">6.7.3</span> Satandard Variables<a href="multiple-linear-regression.html#satandard-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the same way we worked with centered variables, we can work with standard variables
to define the sample correlations.</p>
<p>The standardization of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{X}_s\)</span>, is given by:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{X}_s
  &amp;=
  \left[\begin{matrix}
    \frac{x_{11} - \bar{\mathbf{x}}_1}{S_{x_1x_1}^{1/2}} &amp; \frac{x_{12} - \bar{\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \dots  &amp; \frac{x_{1p} - \bar{\mathbf{x}}_p}{S_{x_px_p}^{1/2}} \\
    \frac{x_{21} - \bar{\mathbf{x}}_1}{S_{x_1x_1}^{1/2}} &amp; \frac{x_{22} - \bar{\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \dots  &amp; \frac{x_{2p} - \bar{\mathbf{x}}_p}{S_{x_px_p}^{1/2}} \\
    \vdots                                        &amp; \vdots                                        &amp; \ddots &amp; \vdots                                        \\
    \frac{x_{n1} - \bar{\mathbf{x}}_1}{S_{x_1x_1}^{1/2}} &amp; \frac{x_{n2} - \bar{\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \dots  &amp; \frac{x_{np} - \bar{\mathbf{x}}_p}{S_{x_px_p}^{1/2}}
  \end{matrix}\right] \\
  &amp;=
  \left[\begin{matrix}
    x_{11} - \bar{\mathbf{x}}_1 &amp; x_{12} - \bar{\mathbf{x}}_2 &amp; \dots  &amp; x_{1p} - \bar{\mathbf{x}}_p \\
    x_{21} - \bar{\mathbf{x}}_1 &amp; x_{22} - \bar{\mathbf{x}}_2 &amp; \dots  &amp; x_{2p} - \bar{\mathbf{x}}_p \\
    \vdots               &amp; \vdots               &amp; \ddots &amp; \vdots               \\
    x_{n1} - \bar{\mathbf{x}}_1 &amp; x_{n2} - \bar{\mathbf{x}}_2 &amp; \dots  &amp; x_{np} - \bar{\mathbf{x}}_p
  \end{matrix}\right]
  \left[\begin{matrix}
    \frac{1}{S_{x_1x_1}^{1/2}} &amp; 0                                             &amp; \dots  &amp; 0 \\
    0                          &amp; \frac{x_{22} - \bar{\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \dots  &amp; 0 \\
    \vdots                     &amp; \vdots                                        &amp; \ddots &amp; \vdots                                        \\
    0                          &amp; 0                                             &amp; \dots  &amp; \frac{x_{np} - \bar{\mathbf{x}}_p}{S_{x_px_p}^{1/2}}
  \end{matrix}\right]
  &amp;= \mathbf{X}_c \mathbf{D}_X
  &amp;= \mathbf{C}\mathbf{X}\mathbf{D}_X
\end{align*}\]</span></p>
<p>where</p>
<p><span class="math display">\[ \mathbf{D}_X =  \left[\begin{matrix}
    \frac{1}{S_{x_1x_1}^{1/2}} &amp; 0                                             &amp; \dots  &amp; 0 \\
    0                          &amp; \frac{x_{22} - \bar{\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \dots  &amp; 0 \\
    \vdots                     &amp; \vdots                                        &amp; \ddots &amp; \vdots                                        \\
    0                          &amp; 0                                             &amp; \dots  &amp; \frac{x_{np} - \bar{\mathbf{x}}_p}{S_{x_px_p}^{1/2}}
  \end{matrix}\right]\]</span></p>
<p>is the matrix that standardizes <span class="math inline">\(\mathbf{X}\)</span>. Notice that unlike <span class="math inline">\(\mathbf{C}\)</span>, that centers
any matrix with the appropriate number of rows, <span class="math inline">\(\mathbf{D}_X\)</span> only standardizes <span class="math inline">\(\mathbf{X}\)</span>.</p>
</div>
<div id="sample-correlation-matrix" class="section level3 hasAnchor" number="6.7.4">
<h3><span class="header-section-number">6.7.4</span> Sample Correlation Matrix<a href="multiple-linear-regression.html#sample-correlation-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We define the sample correlation matrix as:</p>
<p><span class="math display">\[ r_{XX} = \frac{\mathbf{X}_s&#39;\mathbf{X}_s}{n-1} \]</span>
where we break a little our notation convention of using bold capital letters for
matrices. Entry at row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> of <span class="math inline">\(r_{XX}\)</span> is given by:</p>
<p><span class="math display">\[ \left[r_{XX}\right]_{ij} = \frac{S_{x_i x_j}}{S_{x_ix_i}^{1/2}S_{x_jx_j}^{1/2}} \]</span></p>
<p>We can also define the sample correlation between <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> as follows:</p>
<p><span class="math display">\[r_{Xy} = \frac{\mathbf{X}_s&#39; \mathbf{y}_s}{n-1} \]</span></p>
<p>where <span class="math inline">\(\mathbf{y}_s\)</span> is the standardization of <span class="math inline">\(\mathbf{y}\)</span>.</p>
<p>With this definitions in hand, we can see how the coefficients look like with
standardized variables. Since, standardized variables are also centered, it is not
necessary to include the column of ones in the design matrix, as the intercept
estimate is always 0. Then the estimate of the coefficients of the independent
variables is given by:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}_s} = \left(\mathbf{X}_s&#39; \mathbf{X}_s \right)^{-1} \mathbf{X}_s&#39; \mathbf{y}_s = \frac{1}{n-1}r_{XX}^{-1}(n-1)r_{Xy}=r_{XX}^{-1}r_{Xy}\]</span>
Notice that the coefficients estimates do change for standardized variables, since
in general <span class="math inline">\(r_{XX}\neq\mathbf{S}_{XX}\)</span> and <span class="math inline">\(r_{Xy}\neq\mathbf{S}_{Xy}\)</span>. In the case of standardized
variables the coefficient estimates depend in part from the correlation between
independent variables <span class="math inline">\(r_{XX}\)</span> and the correlations between independent and
dependent variables <span class="math inline">\(r_{Xy}\)</span>.</p>
<p>Working with standardized variables is useful, since standardized variables are unit-less,
so the estimated coefficients magnitudes are comparable.</p>
<p>We can see these results in practice with our GDP data:</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="multiple-linear-regression.html#cb93-1" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb93-2"><a href="multiple-linear-regression.html#cb93-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Gdp data.csv&quot;</span>)</span>
<span id="cb93-3"><a href="multiple-linear-regression.html#cb93-3" tabindex="-1"></a></span>
<span id="cb93-4"><a href="multiple-linear-regression.html#cb93-4" tabindex="-1"></a><span class="co"># Design Matrix Independent Variables</span></span>
<span id="cb93-5"><a href="multiple-linear-regression.html#cb93-5" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(dat[, <span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb93-6"><a href="multiple-linear-regression.html#cb93-6" tabindex="-1"></a><span class="co"># Dependent Variable</span></span>
<span id="cb93-7"><a href="multiple-linear-regression.html#cb93-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>gdp</span>
<span id="cb93-8"><a href="multiple-linear-regression.html#cb93-8" tabindex="-1"></a><span class="co"># Number of Observations</span></span>
<span id="cb93-9"><a href="multiple-linear-regression.html#cb93-9" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb93-10"><a href="multiple-linear-regression.html#cb93-10" tabindex="-1"></a><span class="co"># Design Matrix with the column of Ones</span></span>
<span id="cb93-11"><a href="multiple-linear-regression.html#cb93-11" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, n), X)</span>
<span id="cb93-12"><a href="multiple-linear-regression.html#cb93-12" tabindex="-1"></a></span>
<span id="cb93-13"><a href="multiple-linear-regression.html#cb93-13" tabindex="-1"></a><span class="co"># Centering</span></span>
<span id="cb93-14"><a href="multiple-linear-regression.html#cb93-14" tabindex="-1"></a><span class="co"># Vector of Ones</span></span>
<span id="cb93-15"><a href="multiple-linear-regression.html#cb93-15" tabindex="-1"></a>v1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, n)</span>
<span id="cb93-16"><a href="multiple-linear-regression.html#cb93-16" tabindex="-1"></a><span class="co"># Centering Matrix</span></span>
<span id="cb93-17"><a href="multiple-linear-regression.html#cb93-17" tabindex="-1"></a>C  <span class="ot">&lt;-</span> <span class="fu">diag</span>(n) <span class="sc">-</span> (<span class="dv">1</span><span class="sc">/</span>n) <span class="sc">*</span> v1 <span class="sc">%*%</span> <span class="fu">t</span>(v1)</span>
<span id="cb93-18"><a href="multiple-linear-regression.html#cb93-18" tabindex="-1"></a><span class="co"># Independent Variables Centered</span></span>
<span id="cb93-19"><a href="multiple-linear-regression.html#cb93-19" tabindex="-1"></a>Xc <span class="ot">&lt;-</span> C <span class="sc">%*%</span> X</span>
<span id="cb93-20"><a href="multiple-linear-regression.html#cb93-20" tabindex="-1"></a><span class="co"># Dependent Variable Centered</span></span>
<span id="cb93-21"><a href="multiple-linear-regression.html#cb93-21" tabindex="-1"></a>yc <span class="ot">&lt;-</span> C <span class="sc">%*%</span> y</span>
<span id="cb93-22"><a href="multiple-linear-regression.html#cb93-22" tabindex="-1"></a><span class="co"># Design Matrix with Independent Variables Centered</span></span>
<span id="cb93-23"><a href="multiple-linear-regression.html#cb93-23" tabindex="-1"></a>Zc <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, n), Xc)</span>
<span id="cb93-24"><a href="multiple-linear-regression.html#cb93-24" tabindex="-1"></a></span>
<span id="cb93-25"><a href="multiple-linear-regression.html#cb93-25" tabindex="-1"></a><span class="co"># Checks that the Variables are actually centered</span></span>
<span id="cb93-26"><a href="multiple-linear-regression.html#cb93-26" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">colMeans</span>(Xc), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>## inf une int gov exp 
##   0   0   0   0   0</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="multiple-linear-regression.html#cb95-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">mean</span>(yc), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="multiple-linear-regression.html#cb97-1" tabindex="-1"></a><span class="co"># Shows the Mean of y</span></span>
<span id="cb97-2"><a href="multiple-linear-regression.html#cb97-2" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">mean</span>(y), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>## [1] 2.981131</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="multiple-linear-regression.html#cb99-1" tabindex="-1"></a><span class="co"># Compute the Estimates of the Coefficients with Original Variables</span></span>
<span id="cb99-2"><a href="multiple-linear-regression.html#cb99-2" tabindex="-1"></a>b   <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> Z, <span class="fu">t</span>(Z) <span class="sc">%*%</span> y)</span>
<span id="cb99-3"><a href="multiple-linear-regression.html#cb99-3" tabindex="-1"></a><span class="co"># Compute the Estimates of the Coefficients with Centered Independent Variables Only</span></span>
<span id="cb99-4"><a href="multiple-linear-regression.html#cb99-4" tabindex="-1"></a>bs1 <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Zc) <span class="sc">%*%</span> Zc, <span class="fu">t</span>(Zc) <span class="sc">%*%</span> y)</span>
<span id="cb99-5"><a href="multiple-linear-regression.html#cb99-5" tabindex="-1"></a><span class="co"># Compute the Estimates of the Coefficients with All Variables Centered</span></span>
<span id="cb99-6"><a href="multiple-linear-regression.html#cb99-6" tabindex="-1"></a>bs2 <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Zc) <span class="sc">%*%</span> Zc, <span class="fu">t</span>(Zc) <span class="sc">%*%</span> yc)</span>
<span id="cb99-7"><a href="multiple-linear-regression.html#cb99-7" tabindex="-1"></a><span class="co"># Compute the Estimates of the Coefficients with All variables Centered and no column of ones</span></span>
<span id="cb99-8"><a href="multiple-linear-regression.html#cb99-8" tabindex="-1"></a>bs3 <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Xc) <span class="sc">%*%</span> Xc, <span class="fu">t</span>(Xc) <span class="sc">%*%</span> yc)</span>
<span id="cb99-9"><a href="multiple-linear-regression.html#cb99-9" tabindex="-1"></a></span>
<span id="cb99-10"><a href="multiple-linear-regression.html#cb99-10" tabindex="-1"></a><span class="co"># Shows the Estimated Coefficients Side-by-Side</span></span>
<span id="cb99-11"><a href="multiple-linear-regression.html#cb99-11" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">cbind</span>(b, bs1, bs2, <span class="fu">c</span>(<span class="dv">0</span>, bs3)), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>##            [,1]        [,2]        [,3]        [,4]
##      1.55530096  2.98113131  0.00000000  0.00000000
## inf  0.31221813  0.31221813  0.31221813  0.31221813
## une -0.37733403 -0.37733403 -0.37733403 -0.37733403
## int  0.17782709  0.17782709  0.17782709  0.17782709
## gov -0.00848329 -0.00848329 -0.00848329 -0.00848329
## exp  0.06465692  0.06465692  0.06465692  0.06465692</code></pre>
<p>Here we can appreciate that the estimated coefficients for the independent variables
do not change, however the estimate for the intercept changes depending on if the
independent variables are centered or the dependent variable is centered of both. Also, notice that
this are the that we had using the <code>lm</code> function of <code>R</code>.</p>
<p>We can also check that computing the sample covariance matrix using our formula
results in the same quantities that using the <code>cov</code> function in <code>R</code>.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="multiple-linear-regression.html#cb101-1" tabindex="-1"></a><span class="co"># Sample Covariance of X</span></span>
<span id="cb101-2"><a href="multiple-linear-regression.html#cb101-2" tabindex="-1"></a>SXX <span class="ot">&lt;-</span> <span class="fu">t</span>(Xc) <span class="sc">%*%</span> Xc <span class="sc">/</span> (n<span class="dv">-1</span>)</span>
<span id="cb101-3"><a href="multiple-linear-regression.html#cb101-3" tabindex="-1"></a><span class="co"># Sample Covariance between X and y</span></span>
<span id="cb101-4"><a href="multiple-linear-regression.html#cb101-4" tabindex="-1"></a>SXy <span class="ot">&lt;-</span> <span class="fu">t</span>(Xc) <span class="sc">%*%</span> yc <span class="sc">/</span> (n<span class="dv">-1</span>)</span>
<span id="cb101-5"><a href="multiple-linear-regression.html#cb101-5" tabindex="-1"></a></span>
<span id="cb101-6"><a href="multiple-linear-regression.html#cb101-6" tabindex="-1"></a><span class="co"># Shows the comparison in covariance matrices</span></span>
<span id="cb101-7"><a href="multiple-linear-regression.html#cb101-7" tabindex="-1"></a><span class="fu">print</span>(SXX)</span></code></pre></div>
<pre><code>##             inf         une         int        gov        exp
## inf  4.03991957 -1.28576595  1.52551017  2.4374124 0.03447976
## une -1.28576595  0.66963355 -0.27282160 -0.5319862 0.02605596
## int  1.52551017 -0.27282160  0.83778484  0.7633037 0.04335484
## gov  2.43741235 -0.53198621  0.76330368 15.2006711 0.21732195
## exp  0.03447976  0.02605596  0.04335484  0.2173220 9.10237220</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="multiple-linear-regression.html#cb103-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">cov</span>(X))</span></code></pre></div>
<pre><code>##             inf         une         int        gov        exp
## inf  4.03991957 -1.28576595  1.52551017  2.4374124 0.03447976
## une -1.28576595  0.66963355 -0.27282160 -0.5319862 0.02605596
## int  1.52551017 -0.27282160  0.83778484  0.7633037 0.04335484
## gov  2.43741235 -0.53198621  0.76330368 15.2006711 0.21732195
## exp  0.03447976  0.02605596  0.04335484  0.2173220 9.10237220</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="multiple-linear-regression.html#cb105-1" tabindex="-1"></a><span class="co"># Shows the comparison in covariance vectors</span></span>
<span id="cb105-2"><a href="multiple-linear-regression.html#cb105-2" tabindex="-1"></a><span class="fu">print</span>(SXy)</span></code></pre></div>
<pre><code>##           [,1]
## inf  1.9993285
## une -0.6964324
## int  0.7245455
## gov  0.9825766
## exp  0.5953308</code></pre>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="multiple-linear-regression.html#cb107-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">cov</span>(X, y))</span></code></pre></div>
<pre><code>##           [,1]
## inf  1.9993285
## une -0.6964324
## int  0.7245455
## gov  0.9825766
## exp  0.5953308</code></pre>
<p>Finally, we can test our new formulas for the estimates:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="multiple-linear-regression.html#cb109-1" tabindex="-1"></a><span class="co"># Computes the estimates of the coefficients using the covariance matrices</span></span>
<span id="cb109-2"><a href="multiple-linear-regression.html#cb109-2" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> <span class="fu">solve</span>(SXX, SXy)</span>
<span id="cb109-3"><a href="multiple-linear-regression.html#cb109-3" tabindex="-1"></a>b0 <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) <span class="sc">-</span> <span class="fu">t</span>(<span class="fu">colMeans</span>(X)) <span class="sc">%*%</span> b1</span>
<span id="cb109-4"><a href="multiple-linear-regression.html#cb109-4" tabindex="-1"></a></span>
<span id="cb109-5"><a href="multiple-linear-regression.html#cb109-5" tabindex="-1"></a><span class="co"># Shows the Estimates Side by Side</span></span>
<span id="cb109-6"><a href="multiple-linear-regression.html#cb109-6" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">cbind</span>(b, <span class="fu">c</span>(b0, b1)), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>##            [,1]        [,2]
##      1.55530096  1.55530096
## inf  0.31221813  0.31221813
## une -0.37733403 -0.37733403
## int  0.17782709  0.17782709
## gov -0.00848329 -0.00848329
## exp  0.06465692  0.06465692</code></pre>
<p>In the same way, we can work with the sample correlations</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="multiple-linear-regression.html#cb111-1" tabindex="-1"></a><span class="co"># Standardizing matrix of X</span></span>
<span id="cb111-2"><a href="multiple-linear-regression.html#cb111-2" tabindex="-1"></a>DX <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">diag</span>(SXX)))</span>
<span id="cb111-3"><a href="multiple-linear-regression.html#cb111-3" tabindex="-1"></a><span class="co"># Standardize X</span></span>
<span id="cb111-4"><a href="multiple-linear-regression.html#cb111-4" tabindex="-1"></a>Xs <span class="ot">&lt;-</span> Xc <span class="sc">%*%</span> DX</span>
<span id="cb111-5"><a href="multiple-linear-regression.html#cb111-5" tabindex="-1"></a><span class="co"># Shows that Xs is indeed standardize</span></span>
<span id="cb111-6"><a href="multiple-linear-regression.html#cb111-6" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">colMeans</span>(Xs), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>## [1] 0 0 0 0 0</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="multiple-linear-regression.html#cb113-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">apply</span>(<span class="at">X =</span> Xs, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> sd))</span></code></pre></div>
<pre><code>## [1] 1 1 1 1 1</code></pre>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="multiple-linear-regression.html#cb115-1" tabindex="-1"></a><span class="co"># Standardizes y</span></span>
<span id="cb115-2"><a href="multiple-linear-regression.html#cb115-2" tabindex="-1"></a>ys <span class="ot">&lt;-</span> yc <span class="sc">/</span> <span class="fu">sd</span>(y)</span>
<span id="cb115-3"><a href="multiple-linear-regression.html#cb115-3" tabindex="-1"></a><span class="co"># Shows that ys is standardized</span></span>
<span id="cb115-4"><a href="multiple-linear-regression.html#cb115-4" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">mean</span>(ys), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="multiple-linear-regression.html#cb117-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">sd</span>(ys), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="multiple-linear-regression.html#cb119-1" tabindex="-1"></a><span class="co"># Sample Correlation of X</span></span>
<span id="cb119-2"><a href="multiple-linear-regression.html#cb119-2" tabindex="-1"></a>rXX <span class="ot">&lt;-</span> <span class="fu">t</span>(Xs) <span class="sc">%*%</span> Xs <span class="sc">/</span> (n<span class="dv">-1</span>)</span>
<span id="cb119-3"><a href="multiple-linear-regression.html#cb119-3" tabindex="-1"></a><span class="co"># Sample Correlation between X and y</span></span>
<span id="cb119-4"><a href="multiple-linear-regression.html#cb119-4" tabindex="-1"></a>rXy <span class="ot">&lt;-</span> <span class="fu">t</span>(Xs) <span class="sc">%*%</span> ys <span class="sc">/</span> (n<span class="dv">-1</span>)</span>
<span id="cb119-5"><a href="multiple-linear-regression.html#cb119-5" tabindex="-1"></a></span>
<span id="cb119-6"><a href="multiple-linear-regression.html#cb119-6" tabindex="-1"></a><span class="co"># Shows the comparison in correlation matrices</span></span>
<span id="cb119-7"><a href="multiple-linear-regression.html#cb119-7" tabindex="-1"></a><span class="fu">print</span>(rXX)</span></code></pre></div>
<pre><code>##              [,1]        [,2]       [,3]        [,4]        [,5]
## [1,]  1.000000000 -0.78173033  0.8292061  0.31103644 0.005685918
## [2,] -0.781730327  1.00000000 -0.3642453 -0.16674407 0.010553855
## [3,]  0.829206121 -0.36424525  1.0000000  0.21389456 0.015699798
## [4,]  0.311036436 -0.16674407  0.2138946  1.00000000 0.018475446
## [5,]  0.005685918  0.01055386  0.0156998  0.01847545 1.000000000</code></pre>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="multiple-linear-regression.html#cb121-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">cor</span>(X))</span></code></pre></div>
<pre><code>##              inf         une        int         gov         exp
## inf  1.000000000 -0.78173033  0.8292061  0.31103644 0.005685918
## une -0.781730327  1.00000000 -0.3642453 -0.16674407 0.010553855
## int  0.829206121 -0.36424525  1.0000000  0.21389456 0.015699798
## gov  0.311036436 -0.16674407  0.2138946  1.00000000 0.018475446
## exp  0.005685918  0.01055386  0.0156998  0.01847545 1.000000000</code></pre>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="multiple-linear-regression.html#cb123-1" tabindex="-1"></a><span class="co"># Shows the comparison in correlation vectors</span></span>
<span id="cb123-2"><a href="multiple-linear-regression.html#cb123-2" tabindex="-1"></a><span class="fu">print</span>(rXy)</span></code></pre></div>
<pre><code>##            [,1]
## [1,]  0.8751311
## [2,] -0.7487479
## [3,]  0.6964256
## [4,]  0.2217228
## [5,]  0.1736027</code></pre>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="multiple-linear-regression.html#cb125-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">cor</span>(X, y))</span></code></pre></div>
<pre><code>##           [,1]
## inf  0.8751311
## une -0.7487479
## int  0.6964256
## gov  0.2217228
## exp  0.1736027</code></pre>
<p>and can test that:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}_s} = r_{XX}^{-1}r_{Xy}\]</span></p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="multiple-linear-regression.html#cb127-1" tabindex="-1"></a><span class="co"># Computes the estimates of the coefficients using the covariance matrices</span></span>
<span id="cb127-2"><a href="multiple-linear-regression.html#cb127-2" tabindex="-1"></a>bs1 <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Xs) <span class="sc">%*%</span> Xs, <span class="fu">t</span>(Xs) <span class="sc">%*%</span> ys)</span>
<span id="cb127-3"><a href="multiple-linear-regression.html#cb127-3" tabindex="-1"></a>bs2 <span class="ot">&lt;-</span> <span class="fu">solve</span>(rXX, rXy)</span>
<span id="cb127-4"><a href="multiple-linear-regression.html#cb127-4" tabindex="-1"></a></span>
<span id="cb127-5"><a href="multiple-linear-regression.html#cb127-5" tabindex="-1"></a><span class="co"># Shows the Estimates Side by Side</span></span>
<span id="cb127-6"><a href="multiple-linear-regression.html#cb127-6" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">cbind</span>(bs1, bs2), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>##             [,1]        [,2]
## [1,]  0.55210259  0.55210259
## [2,] -0.27165637 -0.27165637
## [3,]  0.14319884  0.14319884
## [4,] -0.02909852 -0.02909852
## [5,]  0.17161988  0.17161988</code></pre>
<p>We can also contrast the effects of the standarization on the coefficients</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="multiple-linear-regression.html#cb129-1" tabindex="-1"></a><span class="co"># Computes the estimates of the coefficients using the covariance matrices</span></span>
<span id="cb129-2"><a href="multiple-linear-regression.html#cb129-2" tabindex="-1"></a>bs1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X)<span class="sc">$</span>coefficients</span>
<span id="cb129-3"><a href="multiple-linear-regression.html#cb129-3" tabindex="-1"></a>bs2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(ys <span class="sc">~</span> Xs)<span class="sc">$</span>coefficients</span>
<span id="cb129-4"><a href="multiple-linear-regression.html#cb129-4" tabindex="-1"></a></span>
<span id="cb129-5"><a href="multiple-linear-regression.html#cb129-5" tabindex="-1"></a><span class="co"># Shows the Estimates Side by Side</span></span>
<span id="cb129-6"><a href="multiple-linear-regression.html#cb129-6" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">cbind</span>(bs1, bs2), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>##                     bs1         bs2
## (Intercept)  1.55530096  0.00000000
## Xinf         0.31221813  0.55210259
## Xune        -0.37733403 -0.27165637
## Xint         0.17782709  0.14319884
## Xgov        -0.00848329 -0.02909852
## Xexp         0.06465692  0.17161988</code></pre>
<p>First, we can observe the 0 intercept estimate on the standardize values, so it
is not necessary to estimate it, we could have done so by using <code>lm(ys ~ Xs - 1)</code>
instead of <code>lm(ys ~ Xs)</code>. Next we observe, the change in magnitudes for the
estimated coefficients of the independent variables.</p>
</div>
</div>
<div id="variable-cross-effects" class="section level2 hasAnchor" number="6.8">
<h2><span class="header-section-number">6.8</span> Variable Cross-Effects<a href="multiple-linear-regression.html#variable-cross-effects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this sub-section we will work with standardized values so there is no need to
estimate the intercept. Since all variables are standardized, we will not use
the <span class="math inline">\(\mathbf{X}_s\)</span> notation, but instead only <span class="math inline">\(\mathbf{X}\)</span>, to make notation less confusing.</p>
<p>The same goes with <span class="math inline">\(\hat{\boldsymbol{\beta}}_s\)</span>, it will be only <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</p>
<p>The idea is to analyze the estimated coefficients when you divide the independent
variables in to groups <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>. So we can divide the design matrix in two:</p>
<p><span class="math display">\[\mathbf{X}= [\mathbf{X}_1 \mathbf{X}_2]\]</span>
Then, we can compute the coefficient estimates:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
  \left[\begin{matrix}
    \hat{\boldsymbol{\beta}}_1 \\
    \hat{\boldsymbol{\beta}}_2  
  \end{matrix}\right] =
  \left([\mathbf{X}_1 \mathbf{X}_2]&#39;[\mathbf{X}_1 \mathbf{X}_2]\right)^{-1}[\mathbf{X}_1 \mathbf{X}_2]&#39; \mathbf{y}\]</span></p>
<p>So, we can work with these computations in the same way we did before:</p>
<p><span class="math display">\[\begin{align*}
[\mathbf{X}_1 \mathbf{X}_2]&#39;[\mathbf{X}_1 \mathbf{X}_2]
  &amp;=
  \left[\begin{matrix}
    \mathbf{X}_1&#39; \\
    \mathbf{X}_2&#39;  
  \end{matrix}\right] [\mathbf{X}_1 \mathbf{X}_2] \\
  &amp;=
  \left[\begin{matrix}
    \mathbf{X}_1&#39;\mathbf{X}_1 &amp; \mathbf{X}_1&#39;\mathbf{X}_2 \\
    \mathbf{X}_2&#39;\mathbf{X}_1 &amp; \mathbf{X}_2&#39;\mathbf{X}_2  
  \end{matrix}\right] \\
  &amp;= (n-1)
  \left[\begin{matrix}
    \frac{\mathbf{X}_1&#39;\mathbf{X}_1}{n-1} &amp; \frac{\mathbf{X}_1&#39;\mathbf{X}_2}{n-1} \\
    \frac{\mathbf{X}_2&#39;\mathbf{X}_1}{n-1} &amp; \frac{\mathbf{X}_2&#39;\mathbf{X}_2}{n-1}  
  \end{matrix}\right] \\
  &amp;= (n-1)
  \left[\begin{matrix}
    r_{X_1X_1} &amp; r_{X_1X_2} \\
    r_{X_2X_1} &amp; r_{X_2X_2}  
  \end{matrix}\right]
\end{align*}\]</span></p>
<p>In the same way, we have that:</p>
<p><span class="math display">\[\begin{align*}
[\mathbf{X}_1 \mathbf{X}_2]&#39;\mathbf{y}
  &amp;=
  \left[\begin{matrix}
    \mathbf{X}_1&#39; \\
    \mathbf{X}_2&#39;  
  \end{matrix}\right] \mathbf{y}\\
  &amp;=
  \left[\begin{matrix}
    \mathbf{X}_1&#39;\mathbf{y}\\
    \mathbf{X}_2&#39;\mathbf{y}
  \end{matrix}\right] \\
  &amp;= (n-1)
  \left[\begin{matrix}
    \frac{\mathbf{X}_1&#39;\mathbf{y}}{n-1} \\
    \frac{\mathbf{X}_2&#39;\mathbf{y}}{n-1}   
  \end{matrix}\right] \\
  &amp;= (n-1)
  \left[\begin{matrix}
    r_{X_1y} \\
    r_{X_2y}  
  \end{matrix}\right]
\end{align*}\]</span></p>
<p>Then we have that:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} =
  \left[\begin{matrix}
    \hat{\boldsymbol{\beta}}_1 \\
    \hat{\boldsymbol{\beta}}_2  
  \end{matrix}\right] =
  \left((n-1)
  \left[\begin{matrix}
    r_{X_1X_1} &amp; r_{X_1X_2} \\
    r_{X_2X_1} &amp; r_{X_2X_2}  
  \end{matrix}\right]\right)^{-1} (n-1)
  \left[\begin{matrix}
    r_{Xy,1} \\
    r_{Xy,2}  
  \end{matrix}\right] =
  \left[\begin{matrix}
    r_{X_1X_1} &amp; r_{X_1X_2} \\
    r_{X_2X_1} &amp; r_{X_2X_2}  
  \end{matrix}\right]^{-1}  
  \left[\begin{matrix}
    r_{X_1y} \\
    r_{X_2y}  
  \end{matrix}\right]
\]</span></p>
<p>Now we can compute the inverse of the 2 by 2 block matrix as before with, but
first we define:</p>
<p><span class="math display">\[r_{X_1|X_2} = r_{X_1X_1} - r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2X_1}\]</span>
this is also, the Schur component. This can also be seen as the sample correlation
matrix of <span class="math inline">\(X_1\)</span> after accounting by the relationships with <span class="math inline">\(X_2\)</span>.</p>
<p><span class="math display">\[
\left[\begin{matrix}
    r_{XX,11} &amp; r_{XX,12} \\
    r_{XX,12} &amp; r_{XX,22}  
  \end{matrix}\right]^{-1} =
\left[\begin{matrix}
                              r_{X_1|X_2}^{-1} &amp;                         -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1} \\
    -r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1} &amp; r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}  \end{matrix}\right]
\]</span>
Then, we have that:</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}
  &amp;=
  \left[\begin{matrix}
    \hat{\boldsymbol{\beta}}_1 \\
    \hat{\boldsymbol{\beta}}_2  
  \end{matrix}\right] \\
  &amp;=
  \left[\begin{matrix}
                              r_{X_1|X_2}^{-1} &amp;                         -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1} \\
    -r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1} &amp; r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}
  \end{matrix}\right]
  \left[\begin{matrix}
    r_{X_1y} \\
    r_{X_2y}  
  \end{matrix}\right] \\
  &amp;=
  \left[\begin{matrix}
    r_{X_1|X_2}^{-1}r_{X_1y} -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y} \\
    -r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1y} + r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y}
  \end{matrix}\right]
\end{align*}\]</span></p>
<p>Since the variables in the groupings 1 and 2 can be switched and have no special
characteristics, we only need to analyze the structure of <span class="math inline">\(\hat{\boldsymbol{\beta}}_1\)</span> in relation
to the variables of group 2, the results would be analogous for <span class="math inline">\(\hat{\boldsymbol{\beta}}_1\)</span>.</p>
<p>Then we have that:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}_1
  = r_{X_1|X_2}^{-1}r_{X_1y} -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y}
  = r_{X_1|X_2}^{-1}\left(r_{X_1y} -r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y}\right)
\]</span>
Now, suppose that we want to fit the following linear models:</p>
<p><span class="math display">\[\mathbf{y}= \mathbf{X}_1 \boldsymbol{\beta}_1 + \mathbf{e}\quad \text{and} \quad \mathbf{y}= \mathbf{X}_2 \boldsymbol{\beta}_2 + \mathbf{e}\]</span>
that is a linear model of <span class="math inline">\(\mathbf{y}\)</span> using only the variables in group 1 for one model
and only variables from the group 2 in the second model. Then,
the coefficient estimates will be:</p>
<p><span class="math display">\[\tilde{\boldsymbol{\beta}}_1 = r_{X_1X_1}^{-1}r_{X_1y} \quad \text{and} \quad \tilde{\boldsymbol{\beta}}_2 = r_{X_2X_2}^{-1}r_{X_2y}\]</span>
Note that, in general, this estimates will be different to the estimates using
all the variables, that is:</p>
<p><span class="math display">\[\tilde{\boldsymbol{\beta}}_1 \neq \hat{\boldsymbol{\beta}}_1 \quad \text{and} \quad \tilde{\boldsymbol{\beta}}_2 \neq \hat{\boldsymbol{\beta}}_2\]</span>
Then, we can re-write, our coefficient estimate for <span class="math inline">\(\boldsymbol{\beta}_1\)</span> as follows:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}_1
  = r_{X_1|X_2}^{-1}\left(r_{X_1y} -r_{X_1X_2}\tilde{\boldsymbol{\beta}}_2\right)
\]</span>
Now, if <span class="math inline">\(r_{X_1X_2} = \mathbf{0}\)</span>, that is the variables in group 1 and group 2 are
uncorrelated, then</p>
<p><span class="math display">\[r_{X_1|X_2} = r_{X_1X_1} - r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2X_1} = r_{X_1X_1}\]</span>
so:
<span class="math display">\[\hat{\boldsymbol{\beta}}_1
  = r_{X_1|X_2}^{-1}\left(r_{X_1y} -r_{X_1X_2}\tilde{\boldsymbol{\beta}}_2\right)
  = r_{X_1X_1}^{-1}r_{X_1y}
  = \tilde{\boldsymbol{\beta}}_1\]</span>
So, when the variables in 1 group are uncorrelated with the other the coefficient
estimates are the same for the full and partial model.</p>
<p>We can also deduce that even when the sample correlation between
<span class="math inline">\(X_1\)</span> and <span class="math inline">\(y\)</span> is <span class="math inline">\(\mathbf{0}\)</span>, that is <span class="math inline">\(r_{X_1y} = \mathbf{0}\)</span>, the estimated coefficients
will not be <span class="math inline">\(\mathbf{0}\)</span> in general. In fact, we have that:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}_1
  = -r_{X_1|X_2}^{-1}r_{X_1X_2}\tilde{\boldsymbol{\beta}}_2
\]</span></p>
<div id="single-variable-cross-effects" class="section level3 hasAnchor" number="6.8.1">
<h3><span class="header-section-number">6.8.1</span> Single Variable Cross-Effects<a href="multiple-linear-regression.html#single-variable-cross-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the special case where group 1 consists of only 1 variable, we have that:</p>
<p><span class="math inline">\(\hat{\beta}_1, \quad r_{X_1X_1} = r_{x_1x_1}=1, \quad r_{X_1|X_2}=r_{x_1|X_2}, \quad r_{X_1y}=r_{x_1y}=\tilde{\beta}_1\)</span></p>
<p>are scalars, while</p>
<p><span class="math display">\[r_{X_1X_2} = r_{x_1X_2}=r_{X_2X_1}&#39; = r_{X_2x_1}&#39;, \quad r_{X_2y}, \quad \tilde{\boldsymbol{\beta}}_2\]</span>
are vectors of size <span class="math inline">\(p-1\)</span>.</p>
<p>Now, consider the following linear model:</p>
<p><span class="math display">\[\mathbf{x}_1 = \mathbf{X}_2 \boldsymbol{\alpha}_2 + \mathbf{e}\]</span></p>
<p>that is, fitting the single variable in group 1, as the dependent variable, and
the variables in group 2, as the independent variables. Then we have that the
estimated coefficients are:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\alpha}}_2 = r_{X_2X_2}^{-1}r_{X_2x_1}\]</span>
then, we notice the following</p>
<p><span class="math display">\[r_{x_1|X_2}
  = r_{X_1X_1} - r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2X_1}
  = r_{x_1X_1} - r_{x_1X_2}r_{X_2X_2}^{-1}r_{X_2x_1}
  = 1 - r_{x_1X_2}\hat{\boldsymbol{\alpha}}_2
  = 1 - r_{x_1X_2}r_{X_2X_2}^{-1}r_{X_2X_2}\hat{\boldsymbol{\alpha}}_2
  = 1 - \hat{\boldsymbol{\alpha}}_2&#39;r_{X_2X_2}\hat{\boldsymbol{\alpha}}_2
  = 1 - R^2_1
\]</span></p>
<p>where, <span class="math inline">\(R^2_1\)</span> is the multiple coefficient of determination for model:</p>
<p><span class="math display">\[\mathbf{x}_1 = \mathbf{X}_2 \boldsymbol{\alpha}_2 + \mathbf{e}\]</span></p>
<p>that is, how much of <span class="math inline">\(\mathbf{x}_1\)</span> is explained by <span class="math inline">\(\mathbf{X}_2\)</span>.</p>
<p>Then, we have that:</p>
<p><span class="math display">\[\hat{\beta}_1 = r_{1_1|X_2}^{-1}\left(r_{x_1y} -r_{x_1X_2}\tilde{\boldsymbol{\beta}}_2\right) = \frac{1}{1-R^2_1} \left(\tilde{\beta}_1 -r_{x_1X_2}\tilde{\boldsymbol{\beta}}_2\right)\]</span>
We name <span class="math inline">\(\frac{1}{1 - R^2_1}\)</span> as:</p>
<p><span class="math display">\[VIF_1 = \frac{1}{1 - R^2_1} \]</span>
the bigger, the more distorted is the value of <span class="math inline">\(\hat{\beta}_1\)</span>. This number increases
as <span class="math inline">\(R^2_1\)</span> in creases.</p>
<p>If <span class="math inline">\(R^2_1 = 0\)</span>, that is <span class="math inline">\(\mathbf{X}_2\)</span> explains nothing of <span class="math inline">\(\mathbf{x}_1\)</span>, then</p>
<p><span class="math display">\[ VIF_1 = 1 \]</span>
that represents no distortion. If <span class="math inline">\(R^2_1 \to 1\)</span>, then:</p>
<p><span class="math display">\[ VIF_1 \to 1 \]</span>
which represents infinite distortion.</p>
<p>The name <span class="math inline">\(VIF\)</span> stands for variance inflation factor. We will see that later, how
it relates to the variance of the estimate <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p>Of course, variable 1, didn’t have any particular property, so we can generalize
this to any variable <span class="math inline">\(k\)</span>.</p>
<p>On the other hand, we also have an effect <span class="math inline">\(r_{x_1X_2}\tilde{\boldsymbol{\beta}}_2\)</span> on the
coefficient</p>
<div id="single-variable-cross-effects-example" class="section level4 hasAnchor" number="6.8.1.1">
<h4><span class="header-section-number">6.8.1.1</span> Single Variable Cross-effects example<a href="multiple-linear-regression.html#single-variable-cross-effects-example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can see the effects the other variables have in changing the coefficients in
our GDP data set, by computing <span class="math inline">\(R^2_k\)</span> for each variable.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="multiple-linear-regression.html#cb131-1" tabindex="-1"></a><span class="co"># Re-names Standardized Variables</span></span>
<span id="cb131-2"><a href="multiple-linear-regression.html#cb131-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> ys</span>
<span id="cb131-3"><a href="multiple-linear-regression.html#cb131-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> Xs</span>
<span id="cb131-4"><a href="multiple-linear-regression.html#cb131-4" tabindex="-1"></a></span>
<span id="cb131-5"><a href="multiple-linear-regression.html#cb131-5" tabindex="-1"></a><span class="co"># Coefficients for the Full Model</span></span>
<span id="cb131-6"><a href="multiple-linear-regression.html#cb131-6" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X <span class="sc">-</span><span class="dv">1</span>)<span class="sc">$</span>coefficients</span>
<span id="cb131-7"><a href="multiple-linear-regression.html#cb131-7" tabindex="-1"></a></span>
<span id="cb131-8"><a href="multiple-linear-regression.html#cb131-8" tabindex="-1"></a><span class="co"># Individual Coefficients</span></span>
<span id="cb131-9"><a href="multiple-linear-regression.html#cb131-9" tabindex="-1"></a><span class="co"># Number of Variables</span></span>
<span id="cb131-10"><a href="multiple-linear-regression.html#cb131-10" tabindex="-1"></a>P <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb131-11"><a href="multiple-linear-regression.html#cb131-11" tabindex="-1"></a><span class="co"># Vector to Save Individual Coefficients</span></span>
<span id="cb131-12"><a href="multiple-linear-regression.html#cb131-12" tabindex="-1"></a>bi <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="at">length =</span> P)</span>
<span id="cb131-13"><a href="multiple-linear-regression.html#cb131-13" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>P){</span>
<span id="cb131-14"><a href="multiple-linear-regression.html#cb131-14" tabindex="-1"></a>  bi[k] <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X[, k] <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">$</span>coefficients</span>
<span id="cb131-15"><a href="multiple-linear-regression.html#cb131-15" tabindex="-1"></a>}</span>
<span id="cb131-16"><a href="multiple-linear-regression.html#cb131-16" tabindex="-1"></a></span>
<span id="cb131-17"><a href="multiple-linear-regression.html#cb131-17" tabindex="-1"></a><span class="co"># Coefficients without variable k</span></span>
<span id="cb131-18"><a href="multiple-linear-regression.html#cb131-18" tabindex="-1"></a>bt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="cn">NA</span>, <span class="at">nrow =</span> P<span class="dv">-1</span>, <span class="at">ncol =</span> P)</span>
<span id="cb131-19"><a href="multiple-linear-regression.html#cb131-19" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>P){</span>
<span id="cb131-20"><a href="multiple-linear-regression.html#cb131-20" tabindex="-1"></a>  bt[, k] <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X[, <span class="sc">-</span>k] <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">$</span>coefficients</span>
<span id="cb131-21"><a href="multiple-linear-regression.html#cb131-21" tabindex="-1"></a>}</span>
<span id="cb131-22"><a href="multiple-linear-regression.html#cb131-22" tabindex="-1"></a></span>
<span id="cb131-23"><a href="multiple-linear-regression.html#cb131-23" tabindex="-1"></a><span class="co"># Coefficient of Determination of X_k ~ X_{-k} models and coefficients</span></span>
<span id="cb131-24"><a href="multiple-linear-regression.html#cb131-24" tabindex="-1"></a>R2k <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="at">length =</span> P)</span>
<span id="cb131-25"><a href="multiple-linear-regression.html#cb131-25" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>P){</span>
<span id="cb131-26"><a href="multiple-linear-regression.html#cb131-26" tabindex="-1"></a>  outReg <span class="ot">&lt;-</span> <span class="fu">lm</span>(X[, k] <span class="sc">~</span> X[, <span class="sc">-</span>k] <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb131-27"><a href="multiple-linear-regression.html#cb131-27" tabindex="-1"></a>  R2k[k] <span class="ot">&lt;-</span> <span class="fu">summary</span>(outReg)<span class="sc">$</span>r.squared</span>
<span id="cb131-28"><a href="multiple-linear-regression.html#cb131-28" tabindex="-1"></a>}</span>
<span id="cb131-29"><a href="multiple-linear-regression.html#cb131-29" tabindex="-1"></a></span>
<span id="cb131-30"><a href="multiple-linear-regression.html#cb131-30" tabindex="-1"></a><span class="co"># Cross product of correlation and coefficients</span></span>
<span id="cb131-31"><a href="multiple-linear-regression.html#cb131-31" tabindex="-1"></a>rbt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="at">length =</span> P)</span>
<span id="cb131-32"><a href="multiple-linear-regression.html#cb131-32" tabindex="-1"></a>rXX <span class="ot">&lt;-</span> <span class="fu">cor</span>(X)</span>
<span id="cb131-33"><a href="multiple-linear-regression.html#cb131-33" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>P){</span>
<span id="cb131-34"><a href="multiple-linear-regression.html#cb131-34" tabindex="-1"></a>  rbt[k] <span class="ot">&lt;-</span> rXX[k, <span class="sc">-</span>k] <span class="sc">%*%</span> bt[, k]</span>
<span id="cb131-35"><a href="multiple-linear-regression.html#cb131-35" tabindex="-1"></a>}</span>
<span id="cb131-36"><a href="multiple-linear-regression.html#cb131-36" tabindex="-1"></a></span>
<span id="cb131-37"><a href="multiple-linear-regression.html#cb131-37" tabindex="-1"></a><span class="co"># Formats and Show Info Table</span></span>
<span id="cb131-38"><a href="multiple-linear-regression.html#cb131-38" tabindex="-1"></a>tab           <span class="ot">&lt;-</span> <span class="fu">cbind</span>(b, bi, R2k, <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>R2k), rbt)</span>
<span id="cb131-39"><a href="multiple-linear-regression.html#cb131-39" tabindex="-1"></a><span class="fu">colnames</span>(tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;b_full&quot;</span>, <span class="st">&quot;b_ind&quot;</span>, <span class="st">&quot;R2k&quot;</span>, <span class="st">&quot;VIF&quot;</span>, <span class="st">&quot;rb_ind&quot;</span>)</span>
<span id="cb131-40"><a href="multiple-linear-regression.html#cb131-40" tabindex="-1"></a><span class="fu">rownames</span>(tab) <span class="ot">&lt;-</span> <span class="fu">colnames</span>(dat)[<span class="dv">2</span><span class="sc">:</span><span class="dv">6</span>]</span>
<span id="cb131-41"><a href="multiple-linear-regression.html#cb131-41" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(tab)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">b_full</th>
<th align="right">b_ind</th>
<th align="right">R2k</th>
<th align="right">VIF</th>
<th align="right">rb_ind</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">inf</td>
<td align="right">0.5521026</td>
<td align="right">0.8751311</td>
<td align="right">0.9604527</td>
<td align="right">25.286192</td>
<td align="right">0.8532969</td>
</tr>
<tr class="even">
<td align="left">une</td>
<td align="right">-0.2716564</td>
<td align="right">-0.7487479</td>
<td align="right">0.8843302</td>
<td align="right">8.645299</td>
<td align="right">-0.7173255</td>
</tr>
<tr class="odd">
<td align="left">int</td>
<td align="right">0.1431988</td>
<td align="right">0.6964256</td>
<td align="right">0.9061532</td>
<td align="right">10.655663</td>
<td align="right">0.6829869</td>
</tr>
<tr class="even">
<td align="left">gov</td>
<td align="right">-0.0290985</td>
<td align="right">0.2217228</td>
<td align="right">0.2067576</td>
<td align="right">1.260649</td>
<td align="right">0.2448050</td>
</tr>
<tr class="odd">
<td align="left">exp</td>
<td align="right">0.1716199</td>
<td align="right">0.1736027</td>
<td align="right">0.0008505</td>
<td align="right">1.000851</td>
<td align="right">0.0021287</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="outliers-and-leverage" class="section level2 hasAnchor" number="6.9">
<h2><span class="header-section-number">6.9</span> Outliers and Leverage<a href="multiple-linear-regression.html#outliers-and-leverage" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It is harder to identify outliers visually in multiple linear regression, specially
if the number of variables is big. The effect of an observation <span class="math inline">\(i\)</span> on the
linear regression fit will depend on 2 things:</p>
<ul>
<li>The size of the residual for observation <span class="math inline">\(i\)</span>, that is <span class="math inline">\(\hat{e}_i^2\)</span>.</li>
<li>How “close” is observation <span class="math inline">\(i\)</span> to other observations.</li>
</ul>
<p><strong>Note</strong>: For this section, we are going to use <span class="math inline">\(X_i\in\mathbb{R}^{1 \times p}\)</span> as the <span class="math inline">\(i-th\)</span> row of the
design matrix. That is the <span class="math inline">\(i\)</span>-th observation of the independent varaibles.</p>
<div id="leverage" class="section level3 hasAnchor" number="6.9.1">
<h3><span class="header-section-number">6.9.1</span> Leverage<a href="multiple-linear-regression.html#leverage" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In multiple linear regression, <strong>leverage</strong> refers to the influence that a
particular data point has on the estimation of the regression model.
Specifically, it measures how far an observation’s values for the independent
variables deviate from the mean of those variables. Leverage identifies data
points that could potentially have a significant effect on the regression line,
particularly those that are outliers in terms of the input variables.</p>
<p>Leverage values are calculated from the <strong>hat matrix</strong> <span class="math inline">\(\mathbf{H}\)</span>, which is derived
from the design matrix <span class="math inline">\(\mathbf{X}\)</span> used in the regression. The diagonal elements of
the hat matrix, <span class="math inline">\(h_{ii}\)</span>, represent the leverage of each data point.
These values range from 0 to 1, where:</p>
<ul>
<li>A leverage value close to 0 indicates that the point is not influential in
determining the regression line.</li>
<li>A leverage value closer to 1 suggests that the point has a higher influence on
the fitted model.</li>
</ul>
<p>High-leverage points are those whose independent variable values are far from
the average, and they may disproportionately affect the slope and intercept of
the regression line. These points can distort the regression model, leading to
biased estimates if not handled properly.</p>
<p>A high-leverage point may not necessarily be an outlier in the response variable
(dependent variable), but it could have a significant impact on the overall
model fit, making leverage a critical diagnostic in regression analysis.</p>
<div id="properties-of-leverages" class="section level4 hasAnchor" number="6.9.1.1">
<h4><span class="header-section-number">6.9.1.1</span> Properties of Leverages<a href="multiple-linear-regression.html#properties-of-leverages" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Lets analyze the structure of the hat matrix.</p>
<p>First, let us see that <span class="math inline">\(\mathbf{H}\)</span> is symmetric semi-positive definite. Pick <span class="math inline">\(z \in \mathbb{R}^n\)</span>.
Recall, that <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span> is positive definite, then <span class="math display">\[(\mathbf{X}&#39;\mathbf{X})^{-1}\]</span> is positive
definite. Then</p>
<p><span class="math display">\[\begin{align*}
z&#39;Hz
   &amp;= \mathbf{z}&#39;\mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;z \\
   &amp;= (\mathbf{X}&#39;\mathbf{z})&#39;(\mathbf{X}&#39;\mathbf{X})^{-1}(\mathbf{X}&#39;\mathbf{z}) \\
   &amp;= \mathbf{w}&#39; (\mathbf{X}&#39;\mathbf{X})^{-1} \mathbf{w}&amp; \mathbf{w}= \mathbf{X}&#39;\mathbf{z}\\
   &amp;\geq 0
\end{align*}\]</span></p>
<p>Therefore, <span class="math inline">\(\mathbf{H}\)</span> is semi-positive definite. This implies that all the eigenvalues
of <span class="math inline">\(\mathbf{H}\)</span> are non-negative, that is positive or <span class="math inline">\(0\)</span>.</p>
<p>Next, let us see that the diagonal elements are between 0 and 1.</p>
<p>To see this, consider leverage <span class="math inline">\(i\)</span> (<span class="math inline">\(h_{ii}\)</span>), and the basis vectors <span class="math inline">\(v_j \in \mathbb{R}^n\)</span> such
that are full of zeros except for entry <span class="math inline">\(j\)</span>. Then:</p>
<p><span class="math display">\[h_{ij} = v_i &#39;\mathbf{H}v_j\]</span>
in particular, leverage <span class="math inline">\(i\)</span> is given by:</p>
<p><span class="math display">\[h_{ii} = v_i &#39;\mathbf{H}v_i\]</span>
Now, consider <span class="math inline">\(\mathbf{H}v_i\)</span> and notice that:</p>
<p><span class="math display">\[\begin{align*}
||v_i - \mathbf{H}v_i||_2^2
  &amp;= (v_i - \mathbf{H}v_i)&#39;(v_i - \mathbf{H}v_i) \\
  &amp;= v_i&#39;v_i - v_i&#39;\mathbf{H}v_i - v_i&#39;\mathbf{H}v_i + v_i&#39; \mathbf{H}\mathbf{H}v_i \\
  &amp;= v_i&#39;v_i - v_i&#39;\mathbf{H}\mathbf{H}v_i - v_i&#39;\mathbf{H}\mathbf{H}v_i + v_i&#39; \mathbf{H}\mathbf{H}v_i \\
  &amp;= v_i&#39;v_i - v_i&#39;\mathbf{H}\mathbf{H}v_i \\
  &amp;= ||v_i||_2^2 - ||\mathbf{H}v_i||_2^2 \\
  &amp;= 1 - ||\mathbf{H}v_i||_2^2
\end{align*}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{align*}
||v_i - \mathbf{H}v_i||_2^2 = 1 - ||\mathbf{H}v_i||_2^2
  &amp; \implies ||v_i - \mathbf{H}v_i||_2^2 + ||\mathbf{H}v_i||_2^2 = 1 \\
  &amp; \implies ||\mathbf{H}v_i||_2^2 \leq 1 \\
  &amp; \implies v_i&#39; \mathbf{H}\mathbf{H}v_i  \leq 1 \\
  &amp; \implies v_i&#39; \mathbf{H}v_i  \leq 1 \\
  &amp; \implies h_{ii}  \leq 1 \\
\end{align*}\]</span></p>
<p>Since <span class="math inline">\(h_{ii}=||\mathbf{H}v_i||_2^2\)</span> is a norm then <span class="math inline">\(h_{ii} \geq 0\)</span>, then:</p>
<p><span class="math display">\[ 0 \leq h_{ii} \leq 1 \]</span>
So, leverages are bounded.</p>
<p>Furthermore, we have that:</p>
<p><span class="math display">\[ \sum_{i=1}^p h_{ii} = p \]</span></p>
<p>To see this, we will need the following properties:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\text{rank}(\mathbf{H}) = p.\)</span></li>
<li>All the eigenvalues of <span class="math inline">\(\mathbf{H}\)</span> are either 0 or 1.</li>
</ol>
<p>As usual, we assume that <span class="math inline">\(\text{rank}(\mathbf{X}) = p\)</span> and <span class="math inline">\(n&gt;p\)</span>, that is there are more observations
than independent variables. Then</p>
<p><span class="math display">\[\begin{align*}
\text{rank}(\mathbf{X}) = p
  &amp;\implies \text{rank}(\mathbf{X}&#39;\mathbf{X}) = p \\
  &amp;\implies \text{rank}\left((\mathbf{X}&#39;\mathbf{X})^{-1}\right) = p \\
  &amp;\implies \text{rank}\left(\mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\right) = p \\
  &amp;\implies \text{rank}\left(\mathbf{H}\right) = p \\
\end{align*}\]</span></p>
<p>Now, let <span class="math inline">\(\lambda\)</span> be an eigenvalue of <span class="math inline">\(\mathbf{H}\)</span>, with respective <span class="math inline">\(\mathbf{v}\)</span> as eigenvector.
Then:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{H}\mathbf{v}= \lambda \mathbf{v}\\
  &amp;\implies \mathbf{H}\mathbf{H}\mathbf{v}= \mathbf{H}\lambda \mathbf{v}= \lambda \mathbf{H}\mathbf{v}\\
  &amp;\implies \mathbf{H}\mathbf{v}= \lambda \lambda \mathbf{v}= \lambda^2 \mathbf{v}\\
  &amp;\implies \lambda = 1 \land \lambda = 0
\end{align*}\]</span></p>
<p>Now since, <span class="math inline">\(\mathbf{H}\)</span> is of rank <span class="math inline">\(p\)</span>, then it must have <span class="math inline">\(p\)</span> non-zero eigenvalues. And,
since all non-zero eigenvalues are 1, then we have that:</p>
<p><span class="math inline">\(\sum_{i=1}^n \lambda_i = p\)</span></p>
<p>that is the sum of the eigenvalues is <span class="math inline">\(p\)</span>. We also know that for any square
matrix, the sum of the eigenvalues is equal to the trace of the matrix. Then:</p>
<p><span class="math display">\[ \text{trace}(\mathbf{H}) = p \implies \sum_{i=1}^n h_{ii} = p\]</span>
This is telling us important information about the leverages. You can compute a
proportional leverage as:</p>
<p><span class="math display">\[ \frac{h_{ii}}{p}\]</span>
the proportion of the leverage <span class="math inline">\(i\)</span> out of “all” the leverage. Or in terms of
“observations”, you can compute:</p>
<p><span class="math display">\[\frac{n h_{ii}}{p}\]</span>
if this number is several times bigger than <span class="math inline">\(1\)</span>, it will be an observation with
considerable leverage.</p>
<p>Another way to characterize the leverages can be found as follows:</p>
<p>Recall that the hat matrix is given by:</p>
<p><span class="math display">\[\mathbf{H}= \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \]</span>
and we can express the design matrix as follows:</p>
<p><span class="math display">\[ \mathbf{X}= \left[\begin{matrix}
    X_1    \\
    X_2    \\
    \vdots \\
    X_n
  \end{matrix}\right] \]</span></p>
<p>and</p>
<p><span class="math display">\[ \mathbf{X}&#39; = [X_1&#39; X_2&#39; \ldots X_n&#39;] \]</span></p>
<p>Then, the hat matrix is given by:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{H}
  &amp;= \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;                                                                 \\
  &amp;= \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1} [X_1&#39; X_2&#39; \ldots X_n&#39;]                                             \\
  &amp;= \mathbf{X}[(\mathbf{X}&#39;\mathbf{X})^{-1}X_1&#39; (\mathbf{X}&#39;\mathbf{X})^{-1}X_2&#39; \ldots (\mathbf{X}&#39;\mathbf{X})^{-1}X_n&#39;]                 \\
  &amp;= \left[\begin{matrix}
      X_1    \\
      X_2    \\
      \vdots \\
      X_n
     \end{matrix}\right] [(\mathbf{X}&#39;\mathbf{X})^{-1}X_1&#39; (\mathbf{X}&#39;\mathbf{X})^{-1}X_2&#39; \ldots (\mathbf{X}&#39;\mathbf{X})^{-1}X_n&#39;] \\
  &amp;= \left[\begin{matrix}
      X_1 (\mathbf{X}&#39;\mathbf{X})^{-1}X_1&#39; &amp; X_1 (\mathbf{X}&#39;\mathbf{X})^{-1}X_2&#39; &amp; \ldots &amp; X_1 (\mathbf{X}&#39;\mathbf{X})^{-1}X_n&#39;    \\
      X_2 (\mathbf{X}&#39;\mathbf{X})^{-1}X_1&#39; &amp; X_2 (\mathbf{X}&#39;\mathbf{X})^{-1}X_2&#39; &amp; \ldots &amp; X_2 (\mathbf{X}&#39;\mathbf{X})^{-1}X_n&#39;    \\
      \vdots                 &amp; \vdots                 &amp; \vdots                             \\
      X_n (\mathbf{X}&#39;\mathbf{X})^{-1}X_1&#39; &amp; X_n (\mathbf{X}&#39;\mathbf{X})^{-1}X_2&#39; &amp; \ldots &amp; X_n (\mathbf{X}&#39;\mathbf{X})^{-1}X_n&#39;    \\
     \end{matrix}\right]
\end{align*}\]</span></p>
<p>we are going to use this expression latter, but we can also notice that:</p>
<p><span class="math display">\[h_{ij} = X_i (\mathbf{X}&#39;\mathbf{X})^{-1}X_j&#39;\]</span>
So we can characterize the leverages as:</p>
<p><span class="math display">\[h_{ij} = X_i (\mathbf{X}&#39;\mathbf{X})^{-1}X_j&#39;\]</span></p>
</div>
</div>
<div id="outliers-identification" class="section level3 hasAnchor" number="6.9.2">
<h3><span class="header-section-number">6.9.2</span> Outliers identification<a href="multiple-linear-regression.html#outliers-identification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Identifying outliers in multiple regression is hard. One way to measure the influence
of an observation is to see how different are the predicted values if an observation
is removed. We will call:</p>
<ul>
<li><span class="math inline">\(\mathbf{X}_{(i)}\)</span> the design matrix, when the <span class="math inline">\(i\)</span>-th observation is removed.</li>
<li><span class="math inline">\(\mathbf{y}_{(i)}\)</span> the dependent variables without the observation <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(\hat{\mathbf{y}}_{(i)}\)</span> the predicted values when the <span class="math inline">\(i\)</span>-th observation is removed.</li>
<li><span class="math inline">\(\mathbf{H}_{(i)}\)</span> the hat matrix when the <span class="math inline">\(i\)</span>-th observation is removed.</li>
</ul>
<p>Then a measure of influence can be:</p>
<p><span class="math display">\[||\mathbf{y}- \hat{\mathbf{y}}_{(i)}||^2_2 = (\mathbf{y}- \hat{\mathbf{y}}_{(i)})&#39;(\mathbf{y}- \hat{\mathbf{y}}_{(i)}) = \sum_{j=1}^n (\mathbf{y}_j - \hat{\mathbf{y}}_{(i),j})^2\]</span>
Notice that:</p>
<p><span class="math display">\[\hat{\mathbf{y}} = \mathbf{H}\mathbf{y}\]</span>
and, similarly</p>
<p><span class="math display">\[\hat{\mathbf{y}}_{(i)} = \mathbf{H}_{(i)} \mathbf{y}_{(i)}\]</span></p>
<p>Now, let us compute:</p>
<p><span class="math display">\[\begin{align*}
\hat{\mathbf{y}}
  &amp;= \mathbf{H}\mathbf{y}\\
  &amp;= \left[\begin{matrix}
      X_1 (\mathbf{X}&#39;\mathbf{X})^{-1}X_1&#39; &amp; X_1 (\mathbf{X}&#39;\mathbf{X})^{-1}X_2&#39; &amp; \ldots &amp; X_1 (\mathbf{X}&#39;\mathbf{X})^{-1}X_n&#39;    \\
      X_2 (\mathbf{X}&#39;\mathbf{X})^{-1}X_1&#39; &amp; X_2 (\mathbf{X}&#39;\mathbf{X})^{-1}X_2&#39; &amp; \ldots &amp; X_2 (\mathbf{X}&#39;\mathbf{X})^{-1}X_n&#39;    \\
      \vdots                 &amp; \vdots                 &amp; \vdots                             \\
      X_n (\mathbf{X}&#39;\mathbf{X})^{-1}X_1&#39; &amp; X_n (\mathbf{X}&#39;\mathbf{X})^{-1}X_2&#39; &amp; \ldots &amp; X_n (\mathbf{X}&#39;\mathbf{X})^{-1}X_n&#39;    \\
     \end{matrix}\right] \left[\begin{matrix}
      y_1    \\
      y_2    \\
      \vdots \\
      y_n
      \end{matrix}\right] \\
  &amp;=
      \left[\begin{matrix}
        \sum_{k=1}^n X_1 (\mathbf{X}&#39;\mathbf{X})^{-1}X_k&#39;y_k  \\
        \sum_{k=1}^n X_2 (\mathbf{X}&#39;\mathbf{X})^{-1}X_k&#39;y_k    \\
        \vdots \\
        \sum_{k=1}^n X_n (\mathbf{X}&#39;\mathbf{X})^{-1}X_k&#39;y_k
      \end{matrix}\right]
\end{align*}\]</span></p>
<p><span class="math display">\[
\]</span></p>
<p>Then</p>
<p><span class="math display">\[\hat{y}_j = \sum_{k=1}^n X_j (\mathbf{X}&#39;\mathbf{X})^{-1}X_k&#39;y_k = X_j (\mathbf{X}&#39;\mathbf{X})^{-1} \sum_{k=1}^n X_k&#39;y_k\]</span>
in a similar way, observation :</p>
<p><span class="math display">\[\hat{y}_{(i)j} = \sum_{k=1}^n X_j (\mathbf{X}_{(i)}&#39;\mathbf{X}_{(i)})^{-1}X_k&#39;y_k = X_j (\mathbf{X}_{(i)}&#39;\mathbf{X}_{(i)})^{-1} \sum_{k=1}^n X_k&#39;y_k\]</span>
Notice that it is not necessary to use the subscript <span class="math inline">\((i)\)</span> when we are dealing
with single observations.</p>
<p>Now, we will relate <span class="math inline">\((\mathbf{X}&#39;\mathbf{X})^{-1}\)</span> to <span class="math inline">\((\mathbf{X}_{(i)}&#39;\mathbf{X}_{(i)})^{-1}\)</span> using the
<a href="prerequisites.html#sherman-morrison-formula">Sherman–Morrison formula</a>. First notice that:</p>
<p><span class="math display">\[\mathbf{X}&#39; \mathbf{X}= \mathbf{X}_{(i)}&#39;\mathbf{X}_{(i)} + X_i&#39;X_i\]</span></p>
<p>then</p>
<p><span class="math display">\[\mathbf{X}_{(i)}&#39;\mathbf{X}_{(i)} = \mathbf{X}&#39; \mathbf{X}- X_i&#39;X_i\]</span></p>
<p>Now we apply the <a href="prerequisites.html#sherman-morrison-formula">Sherman–Morrison formula</a>.</p>
<p><span class="math display">\[\begin{align*}
\left(\mathbf{X}_{(i)}&#39;\mathbf{X}_{(i)}\right)^{-1} = \left(\mathbf{X}&#39; \mathbf{X}- X_i&#39;X_i\right)^{-1}
  &amp;= \left(\mathbf{X}&#39; \mathbf{X}+ (-X_i)&#39;X_i\right)^{-1} \\
  &amp;= \left(\mathbf{X}&#39; \mathbf{X}\right)^{-1} - \frac{\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}(-X_i)&#39;X_i\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}}{1 + (-X_i)&#39;\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}X_i} \\
  &amp;= \left(\mathbf{X}&#39; \mathbf{X}\right)^{-1} + \frac{\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}X_i&#39;X_i\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}}{1 - X_i&#39;\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}X_i} \\
  &amp;= \left(\mathbf{X}&#39; \mathbf{X}\right)^{-1} + \frac{\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}X_i&#39;X_i\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}}{1 - h_{ii}}
\end{align*}\]</span></p>
<p>Then,</p>
<p><span class="math display">\[\hat{y}_{(i)j} = X_j \left(\mathbf{X}_{(i)}&#39;\mathbf{X}_{(i)}\right)^{-1} \sum_{k \neq i}X_k&#39;y_k = X_j\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}\sum_{k \neq i}X_k&#39;y_k + \frac{X_j\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}X_i&#39;X_i\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}\sum_{k \neq i}X_k&#39;y_k}{1 - h_{ii}}\]</span>
Now</p>
<p><span class="math display">\[\begin{align*}
X_j\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}\sum_{k \neq i}X_k&#39;y_k
  &amp;= X_j\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}\left(\sum_{k \neq i}X_k&#39;y_k + X_i&#39;y_i - X_i&#39;y_i\right) \\
  &amp;= X_j\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}\left(\sum_{k = 1}^nX_k&#39;y_k - X_i&#39;y_i\right) \\
  &amp;= X_j\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}\sum_{k = 1}^nX_k&#39;y_k - X_j\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}X_i&#39;y_i \\
  &amp;= \hat{y}_j - h_{ij}y_i
\end{align*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align*}
X_j\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}X_i&#39;X_i\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}\sum_{k \neq i}X_k&#39;y_k
  &amp;= h_{ij}X_i\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}\sum_{k \neq i}X_k&#39;y_k \\
  &amp;= h_{ij}\sum_{k \neq i}X_i\left(\mathbf{X}&#39; \mathbf{X}\right)^{-1}X_k&#39;y_k \\
  &amp;= h_{ij}\sum_{k \neq i}h_{ik}y_k
\end{align*}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{align*}
\hat{y}_{(i)j} = \hat{y}_j - h_{ij}y_i + \frac{h_{ij}\sum_{k \neq i}h_{ik}y_k}{1 - h_{ii}}
  &amp;= \hat{y}_j - h_{ij}\left(y_i  - \frac{\sum_{k \neq i}h_{ik}y_k}{1 - h_{ii}} \right) \\
  &amp;= \hat{y}_j - h_{ij}\left(\frac{y_i - y_ih_{ii}}{1 - h_{ii}} - \frac{\sum_{k \neq i}h_{ik}y_k}{1 - h_{ii}} \right) \\
  &amp;= \hat{y}_j - h_{ij}\left(\frac{y_i - y_ih_{ii} - \sum_{k \neq i}h_{ik}y_k}{1 - h_{ii}} \right) \\
  &amp;= \hat{y}_j - h_{ij}\left(\frac{y_i - \sum_{k=1}^n h_{ik}y_k}{1 - h_{ii}} \right) \\
  &amp;= \hat{y}_j - h_{ij}\left(\frac{y_i - \hat{y}_i}{1 - h_{ii}} \right) \\
  &amp;= \hat{y}_j - h_{ij}\left(\frac{\hat{e}_i}{1 - h_{ii}} \right)
\end{align*}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{align*}
\hat{y}_{(i)j} - \hat{y}_j = - h_{ij}\left(\frac{\hat{e}_i}{1 - h_{ii}} \right)
  &amp;\implies \left(\hat{y}_{(i)j} - \hat{y}_j \right)^2= \left(h_{ij}\left(\frac{\hat{e}_i}{1 - h_{ii}} \right) \right)^2 = \frac{\hat{e}_i^2}{(1 - h_{ii})^2}h_{ij}^2 \\
  &amp;\implies \left(\hat{\mathbf{y}}_{(i)} - \hat{\mathbf{y}} \right)&#39;\left(\hat{\mathbf{y}}_{(i)} - \hat{\mathbf{y}} \right) = \sum_{j=1}^n \frac{\hat{e}_i^2}{(1 - h_{ii})^2}h_{ij}^2 \\
  &amp;\implies ||\hat{\mathbf{y}}_{(i)} - \hat{\mathbf{y}}||_2^2 = \frac{\hat{e}_i^2}{(1 - h_{ii})^2} \sum_{j=1}^n h_{ij}^2
\end{align*}\]</span></p>
<p>Now for any symmetric and idempotent matrix <span class="math inline">\(\mathbf{M}\in \mathbb{R}^{n \times n}\)</span> we have that:</p>
<p><span class="math display">\[m_{ii} = \sum_{j = 1}^n m_{ij}^2\]</span>
Then</p>
<p><span class="math display">\[||\hat{\mathbf{y}}_{(i)} - \hat{\mathbf{y}}||_2^2 = \frac{\hat{e}_i^2}{(1 - h_{ii})^2} h_{ii}^2 = \frac{h_{ii}}{(1 - h_{ii})^2}\hat{e}_i^2 \]</span>
That is the change in the predictions is the result of a function of the leverages
and a function of the error of observation <span class="math inline">\(i\)</span>.</p>
<p>We can see this with our GDP dataset. We generate new observations, in which</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="multiple-linear-regression.html#cb132-1" tabindex="-1"></a><span class="co"># Reads Data</span></span>
<span id="cb132-2"><a href="multiple-linear-regression.html#cb132-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">&quot;Gdp data.csv&quot;</span>)</span>
<span id="cb132-3"><a href="multiple-linear-regression.html#cb132-3" tabindex="-1"></a></span>
<span id="cb132-4"><a href="multiple-linear-regression.html#cb132-4" tabindex="-1"></a><span class="co"># Extracts the Data</span></span>
<span id="cb132-5"><a href="multiple-linear-regression.html#cb132-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(dat[,  <span class="dv">1</span>])</span>
<span id="cb132-6"><a href="multiple-linear-regression.html#cb132-6" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(dat[, <span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb132-7"><a href="multiple-linear-regression.html#cb132-7" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb132-8"><a href="multiple-linear-regression.html#cb132-8" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, n), X)</span>
<span id="cb132-9"><a href="multiple-linear-regression.html#cb132-9" tabindex="-1"></a></span>
<span id="cb132-10"><a href="multiple-linear-regression.html#cb132-10" tabindex="-1"></a><span class="co"># Obtains the Estimates</span></span>
<span id="cb132-11"><a href="multiple-linear-regression.html#cb132-11" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X, <span class="fu">t</span>(X) <span class="sc">%*%</span> y)</span>
<span id="cb132-12"><a href="multiple-linear-regression.html#cb132-12" tabindex="-1"></a></span>
<span id="cb132-13"><a href="multiple-linear-regression.html#cb132-13" tabindex="-1"></a><span class="co"># Predicted Values</span></span>
<span id="cb132-14"><a href="multiple-linear-regression.html#cb132-14" tabindex="-1"></a>yh <span class="ot">&lt;-</span> X <span class="sc">%*%</span> b</span>
<span id="cb132-15"><a href="multiple-linear-regression.html#cb132-15" tabindex="-1"></a></span>
<span id="cb132-16"><a href="multiple-linear-regression.html#cb132-16" tabindex="-1"></a><span class="co"># Errors</span></span>
<span id="cb132-17"><a href="multiple-linear-regression.html#cb132-17" tabindex="-1"></a>eh <span class="ot">&lt;-</span> <span class="fu">abs</span>(y <span class="sc">-</span> yh)</span>
<span id="cb132-18"><a href="multiple-linear-regression.html#cb132-18" tabindex="-1"></a></span>
<span id="cb132-19"><a href="multiple-linear-regression.html#cb132-19" tabindex="-1"></a><span class="co"># Obtains the Minimum, Median and Maximum Errors</span></span>
<span id="cb132-20"><a href="multiple-linear-regression.html#cb132-20" tabindex="-1"></a>err <span class="ot">&lt;-</span> <span class="fu">quantile</span>(eh, <span class="at">probs =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">1</span>))</span>
<span id="cb132-21"><a href="multiple-linear-regression.html#cb132-21" tabindex="-1"></a></span>
<span id="cb132-22"><a href="multiple-linear-regression.html#cb132-22" tabindex="-1"></a><span class="co"># Generates New Observations</span></span>
<span id="cb132-23"><a href="multiple-linear-regression.html#cb132-23" tabindex="-1"></a><span class="co"># An observation at one extreme of the Observed Independent Variables</span></span>
<span id="cb132-24"><a href="multiple-linear-regression.html#cb132-24" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> X, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> max)</span>
<span id="cb132-25"><a href="multiple-linear-regression.html#cb132-25" tabindex="-1"></a><span class="co"># An observation at the center of the Observed Independent Variables</span></span>
<span id="cb132-26"><a href="multiple-linear-regression.html#cb132-26" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> X, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> mean)</span>
<span id="cb132-27"><a href="multiple-linear-regression.html#cb132-27" tabindex="-1"></a><span class="co"># Dependent Variables with different Error levels</span></span>
<span id="cb132-28"><a href="multiple-linear-regression.html#cb132-28" tabindex="-1"></a>y1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">t</span>(X1) <span class="sc">%*%</span> b) <span class="sc">+</span> <span class="fu">c</span>(<span class="dv">0</span>, err, err[<span class="dv">3</span>] <span class="sc">*</span> <span class="dv">2</span>)</span>
<span id="cb132-29"><a href="multiple-linear-regression.html#cb132-29" tabindex="-1"></a>y2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">t</span>(X2) <span class="sc">%*%</span> b) <span class="sc">+</span> <span class="fu">c</span>(<span class="dv">0</span>, err, err[<span class="dv">3</span>] <span class="sc">*</span> <span class="dv">2</span>)</span>
<span id="cb132-30"><a href="multiple-linear-regression.html#cb132-30" tabindex="-1"></a></span>
<span id="cb132-31"><a href="multiple-linear-regression.html#cb132-31" tabindex="-1"></a><span class="co"># Outlier Level</span></span>
<span id="cb132-32"><a href="multiple-linear-regression.html#cb132-32" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">length</span>(y1)</span>
<span id="cb132-33"><a href="multiple-linear-regression.html#cb132-33" tabindex="-1"></a></span>
<span id="cb132-34"><a href="multiple-linear-regression.html#cb132-34" tabindex="-1"></a><span class="co"># Predictions without the Observations</span></span>
<span id="cb132-35"><a href="multiple-linear-regression.html#cb132-35" tabindex="-1"></a>yhi <span class="ot">&lt;-</span> yh</span>
<span id="cb132-36"><a href="multiple-linear-regression.html#cb132-36" tabindex="-1"></a>bi  <span class="ot">&lt;-</span> b</span>
<span id="cb132-37"><a href="multiple-linear-regression.html#cb132-37" tabindex="-1"></a>yi  <span class="ot">&lt;-</span> y</span>
<span id="cb132-38"><a href="multiple-linear-regression.html#cb132-38" tabindex="-1"></a>Xi  <span class="ot">&lt;-</span> X</span>
<span id="cb132-39"><a href="multiple-linear-regression.html#cb132-39" tabindex="-1"></a></span>
<span id="cb132-40"><a href="multiple-linear-regression.html#cb132-40" tabindex="-1"></a><span class="co"># Table with Changes</span></span>
<span id="cb132-41"><a href="multiple-linear-regression.html#cb132-41" tabindex="-1"></a>tab <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="cn">NA</span>, <span class="at">nrow =</span> <span class="dv">0</span>, <span class="at">ncol =</span> <span class="dv">4</span>)</span>
<span id="cb132-42"><a href="multiple-linear-regression.html#cb132-42" tabindex="-1"></a></span>
<span id="cb132-43"><a href="multiple-linear-regression.html#cb132-43" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>m){</span>
<span id="cb132-44"><a href="multiple-linear-regression.html#cb132-44" tabindex="-1"></a>  <span class="co"># Complete Data Set</span></span>
<span id="cb132-45"><a href="multiple-linear-regression.html#cb132-45" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rbind</span>(Xi, X2)</span>
<span id="cb132-46"><a href="multiple-linear-regression.html#cb132-46" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">c</span>(yi, y2[i])</span>
<span id="cb132-47"><a href="multiple-linear-regression.html#cb132-47" tabindex="-1"></a>  </span>
<span id="cb132-48"><a href="multiple-linear-regression.html#cb132-48" tabindex="-1"></a>  <span class="co"># Coefficients</span></span>
<span id="cb132-49"><a href="multiple-linear-regression.html#cb132-49" tabindex="-1"></a>  b <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X, <span class="fu">t</span>(X) <span class="sc">%*%</span> y)</span>
<span id="cb132-50"><a href="multiple-linear-regression.html#cb132-50" tabindex="-1"></a>  </span>
<span id="cb132-51"><a href="multiple-linear-regression.html#cb132-51" tabindex="-1"></a>  <span class="co"># Predictions</span></span>
<span id="cb132-52"><a href="multiple-linear-regression.html#cb132-52" tabindex="-1"></a>  yh  <span class="ot">&lt;-</span> <span class="fu">c</span>(X <span class="sc">%*%</span> b)</span>
<span id="cb132-53"><a href="multiple-linear-regression.html#cb132-53" tabindex="-1"></a>  yhi <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rbind</span>(Xi, X2) <span class="sc">%*%</span> bi)</span>
<span id="cb132-54"><a href="multiple-linear-regression.html#cb132-54" tabindex="-1"></a>  </span>
<span id="cb132-55"><a href="multiple-linear-regression.html#cb132-55" tabindex="-1"></a>  <span class="co"># Hat matrix</span></span>
<span id="cb132-56"><a href="multiple-linear-regression.html#cb132-56" tabindex="-1"></a>  H <span class="ot">&lt;-</span> X <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X, <span class="fu">t</span>(X))</span>
<span id="cb132-57"><a href="multiple-linear-regression.html#cb132-57" tabindex="-1"></a>  </span>
<span id="cb132-58"><a href="multiple-linear-regression.html#cb132-58" tabindex="-1"></a>  <span class="co"># Leverage New Observation</span></span>
<span id="cb132-59"><a href="multiple-linear-regression.html#cb132-59" tabindex="-1"></a>  h <span class="ot">&lt;-</span> H[n<span class="sc">+</span><span class="dv">1</span>, n<span class="sc">+</span><span class="dv">1</span>]</span>
<span id="cb132-60"><a href="multiple-linear-regression.html#cb132-60" tabindex="-1"></a>  </span>
<span id="cb132-61"><a href="multiple-linear-regression.html#cb132-61" tabindex="-1"></a>  <span class="co"># Change in Predictions</span></span>
<span id="cb132-62"><a href="multiple-linear-regression.html#cb132-62" tabindex="-1"></a>  preCha <span class="ot">&lt;-</span> <span class="fu">sum</span>((yh <span class="sc">-</span> yhi)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb132-63"><a href="multiple-linear-regression.html#cb132-63" tabindex="-1"></a>  </span>
<span id="cb132-64"><a href="multiple-linear-regression.html#cb132-64" tabindex="-1"></a>  <span class="co"># Leverage Component</span></span>
<span id="cb132-65"><a href="multiple-linear-regression.html#cb132-65" tabindex="-1"></a>  levCom <span class="ot">&lt;-</span> h <span class="sc">/</span> ((<span class="dv">1</span> <span class="sc">-</span> h)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb132-66"><a href="multiple-linear-regression.html#cb132-66" tabindex="-1"></a>  </span>
<span id="cb132-67"><a href="multiple-linear-regression.html#cb132-67" tabindex="-1"></a>  <span class="co"># Error Component</span></span>
<span id="cb132-68"><a href="multiple-linear-regression.html#cb132-68" tabindex="-1"></a>  errCom <span class="ot">&lt;-</span> (y[n<span class="sc">+</span><span class="dv">1</span>] <span class="sc">-</span> yh[n<span class="sc">+</span><span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb132-69"><a href="multiple-linear-regression.html#cb132-69" tabindex="-1"></a>  </span>
<span id="cb132-70"><a href="multiple-linear-regression.html#cb132-70" tabindex="-1"></a>  <span class="co"># Saves Values</span></span>
<span id="cb132-71"><a href="multiple-linear-regression.html#cb132-71" tabindex="-1"></a>  tab <span class="ot">&lt;-</span> <span class="fu">rbind</span>(tab, <span class="fu">c</span>(preCha, levCom, errCom, levCom <span class="sc">*</span> errCom))</span>
<span id="cb132-72"><a href="multiple-linear-regression.html#cb132-72" tabindex="-1"></a>}</span>
<span id="cb132-73"><a href="multiple-linear-regression.html#cb132-73" tabindex="-1"></a></span>
<span id="cb132-74"><a href="multiple-linear-regression.html#cb132-74" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>m){</span>
<span id="cb132-75"><a href="multiple-linear-regression.html#cb132-75" tabindex="-1"></a>  <span class="co"># Complete Data Set</span></span>
<span id="cb132-76"><a href="multiple-linear-regression.html#cb132-76" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rbind</span>(Xi, X1)</span>
<span id="cb132-77"><a href="multiple-linear-regression.html#cb132-77" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">c</span>(yi, y1[i])</span>
<span id="cb132-78"><a href="multiple-linear-regression.html#cb132-78" tabindex="-1"></a>  </span>
<span id="cb132-79"><a href="multiple-linear-regression.html#cb132-79" tabindex="-1"></a>  <span class="co"># Coefficients</span></span>
<span id="cb132-80"><a href="multiple-linear-regression.html#cb132-80" tabindex="-1"></a>  b <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X, <span class="fu">t</span>(X) <span class="sc">%*%</span> y)</span>
<span id="cb132-81"><a href="multiple-linear-regression.html#cb132-81" tabindex="-1"></a>  </span>
<span id="cb132-82"><a href="multiple-linear-regression.html#cb132-82" tabindex="-1"></a>  <span class="co"># Predictions</span></span>
<span id="cb132-83"><a href="multiple-linear-regression.html#cb132-83" tabindex="-1"></a>  yh  <span class="ot">&lt;-</span> <span class="fu">c</span>(X <span class="sc">%*%</span> b)</span>
<span id="cb132-84"><a href="multiple-linear-regression.html#cb132-84" tabindex="-1"></a>  yhi <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rbind</span>(Xi, X1) <span class="sc">%*%</span> bi)</span>
<span id="cb132-85"><a href="multiple-linear-regression.html#cb132-85" tabindex="-1"></a>  </span>
<span id="cb132-86"><a href="multiple-linear-regression.html#cb132-86" tabindex="-1"></a>  <span class="co"># Hat matrix</span></span>
<span id="cb132-87"><a href="multiple-linear-regression.html#cb132-87" tabindex="-1"></a>  H <span class="ot">&lt;-</span> X <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X, <span class="fu">t</span>(X))</span>
<span id="cb132-88"><a href="multiple-linear-regression.html#cb132-88" tabindex="-1"></a>  </span>
<span id="cb132-89"><a href="multiple-linear-regression.html#cb132-89" tabindex="-1"></a>  <span class="co"># Leverage New Observation</span></span>
<span id="cb132-90"><a href="multiple-linear-regression.html#cb132-90" tabindex="-1"></a>  h <span class="ot">&lt;-</span> H[n<span class="sc">+</span><span class="dv">1</span>, n<span class="sc">+</span><span class="dv">1</span>]</span>
<span id="cb132-91"><a href="multiple-linear-regression.html#cb132-91" tabindex="-1"></a>  </span>
<span id="cb132-92"><a href="multiple-linear-regression.html#cb132-92" tabindex="-1"></a>  <span class="co"># Change in Predictions</span></span>
<span id="cb132-93"><a href="multiple-linear-regression.html#cb132-93" tabindex="-1"></a>  preCha <span class="ot">&lt;-</span> <span class="fu">sum</span>((yh <span class="sc">-</span> yhi)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb132-94"><a href="multiple-linear-regression.html#cb132-94" tabindex="-1"></a>  </span>
<span id="cb132-95"><a href="multiple-linear-regression.html#cb132-95" tabindex="-1"></a>  <span class="co"># Leverage Component</span></span>
<span id="cb132-96"><a href="multiple-linear-regression.html#cb132-96" tabindex="-1"></a>  levCom <span class="ot">&lt;-</span> h <span class="sc">/</span> ((<span class="dv">1</span> <span class="sc">-</span> h)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb132-97"><a href="multiple-linear-regression.html#cb132-97" tabindex="-1"></a>  </span>
<span id="cb132-98"><a href="multiple-linear-regression.html#cb132-98" tabindex="-1"></a>  <span class="co"># Error Component</span></span>
<span id="cb132-99"><a href="multiple-linear-regression.html#cb132-99" tabindex="-1"></a>  errCom <span class="ot">&lt;-</span> (y[n<span class="sc">+</span><span class="dv">1</span>] <span class="sc">-</span> yh[n<span class="sc">+</span><span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb132-100"><a href="multiple-linear-regression.html#cb132-100" tabindex="-1"></a>  </span>
<span id="cb132-101"><a href="multiple-linear-regression.html#cb132-101" tabindex="-1"></a>  <span class="co"># Saves Values</span></span>
<span id="cb132-102"><a href="multiple-linear-regression.html#cb132-102" tabindex="-1"></a>  tab <span class="ot">&lt;-</span> <span class="fu">rbind</span>(tab, <span class="fu">c</span>(preCha, levCom, errCom, levCom <span class="sc">*</span> errCom))</span>
<span id="cb132-103"><a href="multiple-linear-regression.html#cb132-103" tabindex="-1"></a>}</span>
<span id="cb132-104"><a href="multiple-linear-regression.html#cb132-104" tabindex="-1"></a></span>
<span id="cb132-105"><a href="multiple-linear-regression.html#cb132-105" tabindex="-1"></a>  tab <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(tab)</span>
<span id="cb132-106"><a href="multiple-linear-regression.html#cb132-106" tabindex="-1"></a>  tab <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">&quot;Low&quot;</span>, <span class="dv">5</span>), <span class="fu">rep</span>(<span class="st">&quot;High&quot;</span>, <span class="dv">5</span>)), tab)</span>
<span id="cb132-107"><a href="multiple-linear-regression.html#cb132-107" tabindex="-1"></a>  tab <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;Minimum&quot;</span>, <span class="st">&quot;Low&quot;</span>, <span class="st">&quot;Medium&quot;</span>, <span class="st">&quot;High&quot;</span>, <span class="st">&quot;Very High&quot;</span>), <span class="dv">2</span>), tab)</span>
<span id="cb132-108"><a href="multiple-linear-regression.html#cb132-108" tabindex="-1"></a>  <span class="fu">colnames</span>(tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Error Level&quot;</span>, <span class="st">&quot;Leverage Level&quot;</span>, <span class="st">&quot;Sum of Sq. Diff.&quot;</span>, <span class="st">&quot;Lev. Comp.&quot;</span>, <span class="st">&quot;Err. Comp.&quot;</span>, <span class="st">&quot;Using Formula&quot;</span>)</span>
<span id="cb132-109"><a href="multiple-linear-regression.html#cb132-109" tabindex="-1"></a>  knitr<span class="sc">::</span><span class="fu">kable</span>(tab, <span class="at">digits =</span> <span class="dv">5</span>)</span></code></pre></div>
<table>
<colgroup>
<col width="15%" />
<col width="18%" />
<col width="21%" />
<col width="13%" />
<col width="13%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Error Level</th>
<th align="left">Leverage Level</th>
<th align="right">Sum of Sq. Diff.</th>
<th align="right">Lev. Comp.</th>
<th align="right">Err. Comp.</th>
<th align="right">Using Formula</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Minimum</td>
<td align="left">Low</td>
<td align="right">0.00000</td>
<td align="right">0.00513</td>
<td align="right">0.00000</td>
<td align="right">0.00000</td>
</tr>
<tr class="even">
<td align="left">Low</td>
<td align="left">Low</td>
<td align="right">0.00000</td>
<td align="right">0.00513</td>
<td align="right">0.00002</td>
<td align="right">0.00000</td>
</tr>
<tr class="odd">
<td align="left">Medium</td>
<td align="left">Low</td>
<td align="right">0.00070</td>
<td align="right">0.00513</td>
<td align="right">0.13617</td>
<td align="right">0.00070</td>
</tr>
<tr class="even">
<td align="left">High</td>
<td align="left">Low</td>
<td align="right">0.01245</td>
<td align="right">0.00513</td>
<td align="right">2.42785</td>
<td align="right">0.01245</td>
</tr>
<tr class="odd">
<td align="left">Very High</td>
<td align="left">Low</td>
<td align="right">0.04980</td>
<td align="right">0.00513</td>
<td align="right">9.71139</td>
<td align="right">0.04980</td>
</tr>
<tr class="even">
<td align="left">Minimum</td>
<td align="left">High</td>
<td align="right">0.00000</td>
<td align="right">0.86603</td>
<td align="right">0.00000</td>
<td align="right">0.00000</td>
</tr>
<tr class="odd">
<td align="left">Low</td>
<td align="left">High</td>
<td align="right">0.00001</td>
<td align="right">0.86603</td>
<td align="right">0.00001</td>
<td align="right">0.00001</td>
</tr>
<tr class="even">
<td align="left">Medium</td>
<td align="left">High</td>
<td align="right">0.04918</td>
<td align="right">0.86603</td>
<td align="right">0.05679</td>
<td align="right">0.04918</td>
</tr>
<tr class="odd">
<td align="left">High</td>
<td align="left">High</td>
<td align="right">0.87684</td>
<td align="right">0.86603</td>
<td align="right">1.01248</td>
<td align="right">0.87684</td>
</tr>
<tr class="even">
<td align="left">Very High</td>
<td align="left">High</td>
<td align="right">3.50735</td>
<td align="right">0.86603</td>
<td align="right">4.04991</td>
<td align="right">3.50735</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="stability-of-the-solution" class="section level2 hasAnchor" number="6.10">
<h2><span class="header-section-number">6.10</span> Stability of the Solution<a href="multiple-linear-regression.html#stability-of-the-solution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The results of the regression can be subject to numerical problems or the impact
of changes in the data. In general, the results would depend on the stability
of <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span>. One way to measure the worst case scenario is the condition number
of the matrix <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span>, denoted <span class="math inline">\(\kappa(\mathbf{X}&#39; \mathbf{X})\)</span>, and is equal to:</p>
<p><span class="math display">\[ \kappa(\mathbf{X}&#39; \mathbf{X}) = \frac{\lambda_{max}}{\lambda_{min}} \]</span>
where, <span class="math inline">\(\lambda_{max}\)</span> and <span class="math inline">\(\lambda_{min}\)</span> are the maximum and minimum eigenvalues
of <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span>.</p>
<p>There are some considerations, about the condition number:</p>
<ul>
<li>The condition number is a measure of the worst case scenario, a bigger condition
number of a matrix, does not necessarily will be more unstable than a matrix with
a lower condition number.</li>
<li><span class="math inline">\(\lambda_{max} = \max_i \sigma_i^2\)</span> and <span class="math inline">\(\lambda_{max} = \min_i \sigma_i^2\)</span>, where
<span class="math inline">\(\sigma_i\)</span> are the singular values of <span class="math inline">\(\mathbf{X}\)</span>.</li>
</ul>
<p>The condition number will be affected by:</p>
<ol style="list-style-type: decimal">
<li>Disparities in the magnitude of the variables.</li>
<li>The level of multicoliniarity.</li>
<li>Influential observations.</li>
</ol>
<p>Since, the condition number is affected by the magnitude of the variables, it is common
to work with centered and standardized variables when inspecting this issues.</p>
<p>Another approach is to check the determinant of a matrix. A matrix would be in general
more unstable the close is it’s determinant closer to 0. Indeed if the determinant is
0, then <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span> will not be invertible and the solution
<span class="math display">\[ \hat{\boldsymbol{\beta}} = \left(\mathbf{X}&#39;\mathbf{X}\right)^{-1}\mathbf{X}&#39; \mathbf{y}\]</span>
is not applicable. However, just by looking at the determinant is hard to know
what is close to 0. Consider for example the determinant of a multiple of the identity,
then:</p>
<p><span class="math inline">\(\det(c\mathbf{I}_n) = c^n \mathbf{I}\)</span></p>
<p>if <span class="math inline">\(c&lt;1\)</span> this will make the determinant closer to zero. However the matrix <span class="math inline">\(c \mathbf{I}\)</span>
has condition number of <span class="math inline">\(1\)</span>.</p>
<p>It is better to evaluate the change in the determinant after some normalization
to avoid this scenarios.</p>
<p>Consider, for a moment, the standardized case, with estimated coefficients:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\beta}}=r_{XX}^{-1}r_{Xy}\]</span>
then we can see how the determinant the correlation matrix, changes when we remove
a variable. Without loss of generality, we can assume we remove variable 1. Then,
the correlation matrix will be given by:</p>
<p><span class="math display">\[r_{XX} = \left[ \begin{matrix}
r_{X_2X_2} &amp; r_{X_2x_{1}} \\
r_{x_{1}X} &amp; r_{x_{1}x_{1}} \\
\end{matrix} \right]\]</span></p>
<p>Then, using the <a href="prerequisites.html#properties-of-the-determinant">properties of the determinant</a>
of a block matrix, and the results of the
<a href="multiple-linear-regression.html#single-variable-cross-effects">Single Variable Cross-Effects</a> subsection,
we have that:</p>
<p><span class="math display">\[\begin{align*}
\det(r_{XX})
  &amp;= \det \left(
    \left[ \begin{matrix}
    r_{X_2X_2} &amp; r_{X_2x_{1}} \\
    r_{x_{1}X} &amp; r_{x_{1}x_{1}} \\
    \end{matrix} \right]\right) \\
  &amp;= \det(r_{X_2X_2})\det(r_{x_{1}x_{1}} - r_{X_2x_{1}}r_{X_2X_2}^{-1}r_{x_{1}X_2}) \\
  &amp;= \det(r_{X_2X_2})\det(r_{x_{1}|X_2}) \\
  &amp;= \det(r_{X_2X_2})\det(1-R^2_{1}) \\
  &amp;= \det(r_{X_2X_2})(1-R^2_{1})
\end{align*}\]</span></p>
<p>We can see that the determinant is closer to 0 the more correlated is the first
variable to the rest of the variables, that is:</p>
<p><span class="math display">\[R^2_1 \to 1 \implies \det(r_{XX}) \to 0\]</span></p>
<p>In a similar way, assume that we are working with centered values, we can analyze
what would happen if we remove an observation <span class="math inline">\(X_{i}\)</span>. Here we analyze <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span>.
Again, using <a href="prerequisites.html#properties-of-the-determinant">properties of the determinant</a>, and results of the
<a href="multiple-linear-regression.html#outliers-identification">Outliers Identification</a> we have that:</p>
<p><span class="math display">\[\det(\mathbf{X}_{(i)}&#39;\mathbf{X}_{(i)}) = \det(\mathbf{X}_{(i)}&#39;\mathbf{X}_{(i)}) \\
  =\det(\mathbf{X}&#39;\mathbf{X}&#39; - X_iX_i&#39;) \\
  =\det(\mathbf{X}&#39;\mathbf{X}&#39;)\det(1 - X_i(\mathbf{X}&#39;\mathbf{X})^{-1}X_i&#39;) \\
  =\det(\mathbf{X}&#39;\mathbf{X}&#39;)\det(1 - h_{ii}) \\
  =\det(\mathbf{X}&#39;\mathbf{X}&#39;)(1 - h_{ii}) \\\]</span></p>
<p>So, the determinant deteriorates the more leverage observation <span class="math inline">\(i\)</span> has.</p>
<p>We can see examples of the previous operations with our GDP data.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="multiple-linear-regression.html#cb133-1" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb133-2"><a href="multiple-linear-regression.html#cb133-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Gdp data.csv&quot;</span>)</span>
<span id="cb133-3"><a href="multiple-linear-regression.html#cb133-3" tabindex="-1"></a></span>
<span id="cb133-4"><a href="multiple-linear-regression.html#cb133-4" tabindex="-1"></a><span class="co"># Design Matrix Independent Variables</span></span>
<span id="cb133-5"><a href="multiple-linear-regression.html#cb133-5" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(dat[, <span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb133-6"><a href="multiple-linear-regression.html#cb133-6" tabindex="-1"></a></span>
<span id="cb133-7"><a href="multiple-linear-regression.html#cb133-7" tabindex="-1"></a><span class="co"># Centering</span></span>
<span id="cb133-8"><a href="multiple-linear-regression.html#cb133-8" tabindex="-1"></a><span class="co"># Vector of Ones</span></span>
<span id="cb133-9"><a href="multiple-linear-regression.html#cb133-9" tabindex="-1"></a>v1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, n)</span>
<span id="cb133-10"><a href="multiple-linear-regression.html#cb133-10" tabindex="-1"></a><span class="co"># Centering Matrix</span></span>
<span id="cb133-11"><a href="multiple-linear-regression.html#cb133-11" tabindex="-1"></a>C  <span class="ot">&lt;-</span> <span class="fu">diag</span>(n) <span class="sc">-</span> (<span class="dv">1</span><span class="sc">/</span>n) <span class="sc">*</span> v1 <span class="sc">%*%</span> <span class="fu">t</span>(v1)</span>
<span id="cb133-12"><a href="multiple-linear-regression.html#cb133-12" tabindex="-1"></a><span class="co"># Independent Variables Centered</span></span>
<span id="cb133-13"><a href="multiple-linear-regression.html#cb133-13" tabindex="-1"></a>Xc <span class="ot">&lt;-</span> C <span class="sc">%*%</span> X</span>
<span id="cb133-14"><a href="multiple-linear-regression.html#cb133-14" tabindex="-1"></a></span>
<span id="cb133-15"><a href="multiple-linear-regression.html#cb133-15" tabindex="-1"></a><span class="co"># Standardizing matrix of X</span></span>
<span id="cb133-16"><a href="multiple-linear-regression.html#cb133-16" tabindex="-1"></a>DX <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">diag</span>(SXX)))</span>
<span id="cb133-17"><a href="multiple-linear-regression.html#cb133-17" tabindex="-1"></a><span class="co"># Standardize X</span></span>
<span id="cb133-18"><a href="multiple-linear-regression.html#cb133-18" tabindex="-1"></a>Xs <span class="ot">&lt;-</span> Xc <span class="sc">%*%</span> DX</span>
<span id="cb133-19"><a href="multiple-linear-regression.html#cb133-19" tabindex="-1"></a></span>
<span id="cb133-20"><a href="multiple-linear-regression.html#cb133-20" tabindex="-1"></a><span class="co"># Condition numbers</span></span>
<span id="cb133-21"><a href="multiple-linear-regression.html#cb133-21" tabindex="-1"></a></span>
<span id="cb133-22"><a href="multiple-linear-regression.html#cb133-22" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Condition number of X&#39;X = &quot;</span>,   <span class="fu">round</span>(<span class="fu">kappa</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X), <span class="dv">2</span>)))</span></code></pre></div>
<pre><code>## [1] &quot;Condition number of X&#39;X = 28072.42&quot;</code></pre>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="multiple-linear-regression.html#cb135-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Condition number of Xc&#39;Xc = &quot;</span>, <span class="fu">round</span>(<span class="fu">kappa</span>(<span class="fu">t</span>(Xc) <span class="sc">%*%</span> Xc), <span class="dv">2</span>)))</span></code></pre></div>
<pre><code>## [1] &quot;Condition number of Xc&#39;Xc = 1186.14&quot;</code></pre>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="multiple-linear-regression.html#cb137-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Condition number of Xs&#39;Xs = &quot;</span>, <span class="fu">round</span>(<span class="fu">kappa</span>(<span class="fu">t</span>(Xs) <span class="sc">%*%</span> Xs), <span class="dv">2</span>)))</span></code></pre></div>
<pre><code>## [1] &quot;Condition number of Xs&#39;Xs = 105.68&quot;</code></pre>
<p>as we can see the condition number improves as with the centering and the standardization.
However, this is not necessarily so.</p>
<p>Now we evaluate the effect of each variable on the determinant.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="multiple-linear-regression.html#cb139-1" tabindex="-1"></a>P      <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb139-2"><a href="multiple-linear-regression.html#cb139-2" tabindex="-1"></a>vecDet <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="at">length =</span> P)</span>
<span id="cb139-3"><a href="multiple-linear-regression.html#cb139-3" tabindex="-1"></a>vecR2  <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="at">length =</span> P)</span>
<span id="cb139-4"><a href="multiple-linear-regression.html#cb139-4" tabindex="-1"></a></span>
<span id="cb139-5"><a href="multiple-linear-regression.html#cb139-5" tabindex="-1"></a><span class="co"># Computes</span></span>
<span id="cb139-6"><a href="multiple-linear-regression.html#cb139-6" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>P){</span>
<span id="cb139-7"><a href="multiple-linear-regression.html#cb139-7" tabindex="-1"></a>  vecDet[i] <span class="ot">&lt;-</span> <span class="fu">det</span>(<span class="fu">cor</span>(X[, <span class="sc">-</span>i]))</span>
<span id="cb139-8"><a href="multiple-linear-regression.html#cb139-8" tabindex="-1"></a>  vecR2[i]  <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(Xs[, i] <span class="sc">~</span> Xs[, <span class="sc">-</span>i] <span class="sc">-</span> <span class="dv">1</span>))<span class="sc">$</span>r.squared</span>
<span id="cb139-9"><a href="multiple-linear-regression.html#cb139-9" tabindex="-1"></a>}</span>
<span id="cb139-10"><a href="multiple-linear-regression.html#cb139-10" tabindex="-1"></a></span>
<span id="cb139-11"><a href="multiple-linear-regression.html#cb139-11" tabindex="-1"></a><span class="co"># Formats and Show Info Table</span></span>
<span id="cb139-12"><a href="multiple-linear-regression.html#cb139-12" tabindex="-1"></a>tab           <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="fu">det</span>(<span class="fu">cor</span>(X)), P), vecDet, vecR2)</span>
<span id="cb139-13"><a href="multiple-linear-regression.html#cb139-13" tabindex="-1"></a><span class="fu">colnames</span>(tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;r_{X&#39;X}&quot;</span>, <span class="st">&quot;r_{X_(i)&#39;X_(i)}&quot;</span>, <span class="st">&quot;R2&quot;</span>)</span>
<span id="cb139-14"><a href="multiple-linear-regression.html#cb139-14" tabindex="-1"></a><span class="fu">rownames</span>(tab) <span class="ot">&lt;-</span> <span class="fu">colnames</span>(dat)[<span class="dv">2</span><span class="sc">:</span><span class="dv">6</span>]</span>
<span id="cb139-15"><a href="multiple-linear-regression.html#cb139-15" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(tab)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">r_{X’X}</th>
<th align="right">r_{X_(i)’X_(i)}</th>
<th align="right">R2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">inf</td>
<td align="right">0.0323914</td>
<td align="right">0.8190564</td>
<td align="right">0.9604527</td>
</tr>
<tr class="even">
<td align="left">une</td>
<td align="right">0.0323914</td>
<td align="right">0.2800338</td>
<td align="right">0.8843302</td>
</tr>
<tr class="odd">
<td align="left">int</td>
<td align="right">0.0323914</td>
<td align="right">0.3451523</td>
<td align="right">0.9061532</td>
</tr>
<tr class="even">
<td align="left">gov</td>
<td align="right">0.0323914</td>
<td align="right">0.0408342</td>
<td align="right">0.2067576</td>
</tr>
<tr class="odd">
<td align="left">exp</td>
<td align="right">0.0323914</td>
<td align="right">0.0324190</td>
<td align="right">0.0008505</td>
</tr>
</tbody>
</table>
<p>Finally we can evaluate the effects of a new observations depending on the leverage:</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="multiple-linear-regression.html#cb140-1" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb140-2"><a href="multiple-linear-regression.html#cb140-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Gdp data.csv&quot;</span>)</span>
<span id="cb140-3"><a href="multiple-linear-regression.html#cb140-3" tabindex="-1"></a></span>
<span id="cb140-4"><a href="multiple-linear-regression.html#cb140-4" tabindex="-1"></a><span class="co"># Design Matrix Independent Variables</span></span>
<span id="cb140-5"><a href="multiple-linear-regression.html#cb140-5" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(dat[, <span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb140-6"><a href="multiple-linear-regression.html#cb140-6" tabindex="-1"></a>P <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb140-7"><a href="multiple-linear-regression.html#cb140-7" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb140-8"><a href="multiple-linear-regression.html#cb140-8" tabindex="-1"></a></span>
<span id="cb140-9"><a href="multiple-linear-regression.html#cb140-9" tabindex="-1"></a><span class="co"># Centering</span></span>
<span id="cb140-10"><a href="multiple-linear-regression.html#cb140-10" tabindex="-1"></a><span class="co"># Vector of Ones</span></span>
<span id="cb140-11"><a href="multiple-linear-regression.html#cb140-11" tabindex="-1"></a>v1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, n)</span>
<span id="cb140-12"><a href="multiple-linear-regression.html#cb140-12" tabindex="-1"></a><span class="co"># Centering Matrix</span></span>
<span id="cb140-13"><a href="multiple-linear-regression.html#cb140-13" tabindex="-1"></a>C  <span class="ot">&lt;-</span> <span class="fu">diag</span>(n) <span class="sc">-</span> (<span class="dv">1</span><span class="sc">/</span>n) <span class="sc">*</span> v1 <span class="sc">%*%</span> <span class="fu">t</span>(v1)</span>
<span id="cb140-14"><a href="multiple-linear-regression.html#cb140-14" tabindex="-1"></a><span class="co"># Independent Variables Centered</span></span>
<span id="cb140-15"><a href="multiple-linear-regression.html#cb140-15" tabindex="-1"></a>Xc <span class="ot">&lt;-</span> C <span class="sc">%*%</span> X</span>
<span id="cb140-16"><a href="multiple-linear-regression.html#cb140-16" tabindex="-1"></a></span>
<span id="cb140-17"><a href="multiple-linear-regression.html#cb140-17" tabindex="-1"></a><span class="co"># Standardizing matrix of X</span></span>
<span id="cb140-18"><a href="multiple-linear-regression.html#cb140-18" tabindex="-1"></a>DX <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">diag</span>(SXX)))</span>
<span id="cb140-19"><a href="multiple-linear-regression.html#cb140-19" tabindex="-1"></a><span class="co"># Standardize X</span></span>
<span id="cb140-20"><a href="multiple-linear-regression.html#cb140-20" tabindex="-1"></a>Xs <span class="ot">&lt;-</span> Xc <span class="sc">%*%</span> DX</span>
<span id="cb140-21"><a href="multiple-linear-regression.html#cb140-21" tabindex="-1"></a></span>
<span id="cb140-22"><a href="multiple-linear-regression.html#cb140-22" tabindex="-1"></a><span class="co"># New Observations</span></span>
<span id="cb140-23"><a href="multiple-linear-regression.html#cb140-23" tabindex="-1"></a><span class="co"># Very Leveraged Observation</span></span>
<span id="cb140-24"><a href="multiple-linear-regression.html#cb140-24" tabindex="-1"></a>Xn <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="dv">3</span>, P))</span>
<span id="cb140-25"><a href="multiple-linear-regression.html#cb140-25" tabindex="-1"></a><span class="co"># Mildly Leveraged Observation</span></span>
<span id="cb140-26"><a href="multiple-linear-regression.html#cb140-26" tabindex="-1"></a>Xn <span class="ot">&lt;-</span> <span class="fu">rbind</span>(Xn, <span class="fu">t</span>(<span class="fu">rep</span>(<span class="dv">1</span>, P)))</span>
<span id="cb140-27"><a href="multiple-linear-regression.html#cb140-27" tabindex="-1"></a><span class="co"># No Leverage</span></span>
<span id="cb140-28"><a href="multiple-linear-regression.html#cb140-28" tabindex="-1"></a>Xn <span class="ot">&lt;-</span> <span class="fu">rbind</span>(Xn, <span class="fu">t</span>(<span class="fu">rep</span>(<span class="dv">0</span>, P)))</span>
<span id="cb140-29"><a href="multiple-linear-regression.html#cb140-29" tabindex="-1"></a></span>
<span id="cb140-30"><a href="multiple-linear-regression.html#cb140-30" tabindex="-1"></a><span class="co"># Number of New Observations</span></span>
<span id="cb140-31"><a href="multiple-linear-regression.html#cb140-31" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(Xn)</span>
<span id="cb140-32"><a href="multiple-linear-regression.html#cb140-32" tabindex="-1"></a></span>
<span id="cb140-33"><a href="multiple-linear-regression.html#cb140-33" tabindex="-1"></a><span class="co"># Computes the Effects of the New Observation</span></span>
<span id="cb140-34"><a href="multiple-linear-regression.html#cb140-34" tabindex="-1"></a>vecDet <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="at">length =</span> m)</span>
<span id="cb140-35"><a href="multiple-linear-regression.html#cb140-35" tabindex="-1"></a>vecLev <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="at">length =</span> m)</span>
<span id="cb140-36"><a href="multiple-linear-regression.html#cb140-36" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>m){</span>
<span id="cb140-37"><a href="multiple-linear-regression.html#cb140-37" tabindex="-1"></a>  Z         <span class="ot">&lt;-</span> <span class="fu">rbind</span>(Xs, Xn[i, ])</span>
<span id="cb140-38"><a href="multiple-linear-regression.html#cb140-38" tabindex="-1"></a>  vecDet[i] <span class="ot">&lt;-</span> <span class="fu">det</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> Z)</span>
<span id="cb140-39"><a href="multiple-linear-regression.html#cb140-39" tabindex="-1"></a>  vecLev[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(Xn[i, ]) <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> Z, Xn[i, ])</span>
<span id="cb140-40"><a href="multiple-linear-regression.html#cb140-40" tabindex="-1"></a>}</span>
<span id="cb140-41"><a href="multiple-linear-regression.html#cb140-41" tabindex="-1"></a></span>
<span id="cb140-42"><a href="multiple-linear-regression.html#cb140-42" tabindex="-1"></a><span class="co"># Formats and Show Info Table</span></span>
<span id="cb140-43"><a href="multiple-linear-regression.html#cb140-43" tabindex="-1"></a>tab           <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="fu">det</span>(<span class="fu">t</span>(Xs) <span class="sc">%*%</span> Xs), <span class="dv">3</span>), vecDet, vecLev, vecLev <span class="sc">*</span> n <span class="sc">/</span> P)</span>
<span id="cb140-44"><a href="multiple-linear-regression.html#cb140-44" tabindex="-1"></a><span class="fu">colnames</span>(tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;det(Xs_(i)&#39;Xs_(i))&quot;</span>,<span class="st">&quot;det(Xs&#39;Xs)&quot;</span>, <span class="st">&quot;Leverage&quot;</span>, <span class="st">&quot;Obs. Equivalent&quot;</span>)</span>
<span id="cb140-45"><a href="multiple-linear-regression.html#cb140-45" tabindex="-1"></a><span class="fu">rownames</span>(tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Big Leverage&quot;</span>, <span class="st">&quot;Medium Leverage&quot;</span>, <span class="st">&quot;No Leverage&quot;</span>)</span>
<span id="cb140-46"><a href="multiple-linear-regression.html#cb140-46" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(tab)</span></code></pre></div>
<table style="width:100%;">
<colgroup>
<col width="21%" />
<col width="26%" />
<col width="16%" />
<col width="13%" />
<col width="21%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">det(Xs_(i)’Xs_(i))</th>
<th align="right">det(Xs’Xs)</th>
<th align="right">Leverage</th>
<th align="right">Obs. Equivalent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Big Leverage</td>
<td align="right">9132788636</td>
<td align="right">18778457801</td>
<td align="right">0.5136561</td>
<td align="right">20.135319</td>
</tr>
<tr class="even">
<td align="left">Medium Leverage</td>
<td align="right">9132788636</td>
<td align="right">10204529654</td>
<td align="right">0.1050260</td>
<td align="right">4.117019</td>
</tr>
<tr class="odd">
<td align="left">No Leverage</td>
<td align="right">9132788636</td>
<td align="right">9132788636</td>
<td align="right">0.0000000</td>
<td align="right">0.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="polynomial-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bootstrapping.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
