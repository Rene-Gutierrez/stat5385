<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Multiple Linear Regression | _main.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Multiple Linear Regression | _main.knit" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Multiple Linear Regression | _main.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="polynomial-regression.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Stat 5385/6385</a></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#general-math"><i class="fa fa-check"></i><b>2.1</b> General Math</a></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.2.3</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.2.4</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.2.5" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.2.5</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.2.6" data-path="prerequisites.html"><a href="prerequisites.html#conditions-for-the-inverse-to-exist"><i class="fa fa-check"></i><b>2.2.6</b> Conditions for the Inverse to Exist:</a></li>
<li class="chapter" data-level="2.2.7" data-path="prerequisites.html"><a href="prerequisites.html#explanation-of-the-terms"><i class="fa fa-check"></i><b>2.2.7</b> Explanation of the Terms:</a></li>
<li class="chapter" data-level="2.2.8" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.2.8</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.2.9" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.2.9</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.2.10" data-path="prerequisites.html"><a href="prerequisites.html#geometric-interpretation-2"><i class="fa fa-check"></i><b>2.2.10</b> Geometric Interpretation:</a></li>
<li class="chapter" data-level="2.2.11" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.2.11</b> Idempotent Matrix</a></li>
<li class="chapter" data-level="2.2.12" data-path="prerequisites.html"><a href="prerequisites.html#key-properties-of-idempotent-matrices"><i class="fa fa-check"></i><b>2.2.12</b> Key Properties of Idempotent Matrices:</a></li>
<li class="chapter" data-level="2.2.13" data-path="prerequisites.html"><a href="prerequisites.html#examples"><i class="fa fa-check"></i><b>2.2.13</b> Examples:</a></li>
<li class="chapter" data-level="2.2.14" data-path="prerequisites.html"><a href="prerequisites.html#use-in-statistics"><i class="fa fa-check"></i><b>2.2.14</b> Use in Statistics:</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.3</b> Probability</a></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.4</b> Statistics</a></li>
<li class="chapter" data-level="2.5" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.5</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.5.1</b> Gradient</a></li>
<li class="chapter" data-level="2.5.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.5.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.5.3</b> Applications:</a></li>
<li class="chapter" data-level="2.5.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.5.4</b> Matrix Calculus</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#examples-1"><i class="fa fa-check"></i><b>3.1</b> Examples</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction.html"><a href="introduction.html#ad-spending"><i class="fa fa-check"></i><b>3.1.1</b> Ad Spending</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction.html"><a href="introduction.html#winw-example"><i class="fa fa-check"></i><b>3.1.2</b> Wine and Life Expectancy</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction.html"><a href="introduction.html#burger-demand"><i class="fa fa-check"></i><b>3.1.3</b> Burger Demand</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model"><i class="fa fa-check"></i><b>4.1</b> Model</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>4.2</b> Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#other-estimated-quantites"><i class="fa fa-check"></i><b>4.2.1</b> Other estimated quantites</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#properties-of-the-estimates"><i class="fa fa-check"></i><b>4.3</b> Properties of the Estimates</a></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-and-standarizing-the-data"><i class="fa fa-check"></i><b>4.4</b> Centering and Standarizing the Data</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independent-variable-centered"><i class="fa fa-check"></i><b>4.4.1</b> Independent variable centered</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#both-variables-centered"><i class="fa fa-check"></i><b>4.4.2</b> Both Variables centered</a></li>
<li class="chapter" data-level="4.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#independent-and-dependent-variable-standardized"><i class="fa fa-check"></i><b>4.4.3</b> Independent and dependent variable standardized</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>4.5</b> Coefficient of Determination</a></li>
<li class="chapter" data-level="4.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residual-analysis"><i class="fa fa-check"></i><b>4.6</b> Residual Analysis</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#non-linear-regression-function"><i class="fa fa-check"></i><b>4.6.1</b> Non-linear regression function</a></li>
<li class="chapter" data-level="4.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#heteroscedasticity"><i class="fa fa-check"></i><b>4.6.2</b> Heteroscedasticity</a></li>
<li class="chapter" data-level="4.6.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outliers"><i class="fa fa-check"></i><b>4.6.3</b> Outliers</a></li>
<li class="chapter" data-level="4.6.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#variables-ommited"><i class="fa fa-check"></i><b>4.6.4</b> Variables Ommited</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#cross-validation"><i class="fa fa-check"></i><b>4.7</b> Cross-Validation</a></li>
<li class="chapter" data-level="4.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>4.8</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="4.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-in-matrix-form"><i class="fa fa-check"></i><b>4.9</b> Model in Matrix Form</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weighted-least-squares-in-matrix-form"><i class="fa fa-check"></i><b>4.9.1</b> Weighted Least Squares in Matrix Form</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="polynomial-regression.html"><a href="polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Polynomial Regression</a></li>
<li class="chapter" data-level="6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#example-8"><i class="fa fa-check"></i><b>6.2</b> Example</a></li>
<li class="chapter" data-level="6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#least-squares-estimation-1"><i class="fa fa-check"></i><b>6.3</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#properties-of-the-estimates-1"><i class="fa fa-check"></i><b>6.4</b> Properties of the Estimates</a></li>
<li class="chapter" data-level="6.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-r2"><i class="fa fa-check"></i><b>6.5</b> Multiple <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="6.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#geometric-interpretation-of-multiple-linear-regression"><i class="fa fa-check"></i><b>6.6</b> Geometric Interpretation of Multiple Linear Regression</a></li>
<li class="chapter" data-level="6.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#centered-and-standarized-variables"><i class="fa fa-check"></i><b>6.7</b> Centered and Standarized Variables</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#centered-variables"><i class="fa fa-check"></i><b>6.7.1</b> Centered Variables</a></li>
<li class="chapter" data-level="6.7.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sample-covariance"><i class="fa fa-check"></i><b>6.7.2</b> Sample Covariance</a></li>
<li class="chapter" data-level="6.7.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#satandard-variables"><i class="fa fa-check"></i><b>6.7.3</b> Satandard Variables</a></li>
<li class="chapter" data-level="6.7.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sample-correlation-matrix"><i class="fa fa-check"></i><b>6.7.4</b> Sample Correlation Matrix</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#variable-cross-effects"><i class="fa fa-check"></i><b>6.8</b> Variable Cross-Effects</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#single-variable-cross-effects"><i class="fa fa-check"></i><b>6.8.1</b> Single Variable Cross-Effects</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-linear-regression" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Multiple Linear Regression<a href="multiple-linear-regression.html#multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-1" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Introduction<a href="multiple-linear-regression.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Multiple regression is a statistical technique used to model the relationship between a dependent variable and two or more independent variables. It extends simple linear regression by allowing for a more complex analysis of how various factors impact an outcome. The general form of the multiple regression equation is:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \ldots + \beta_p x_{p,i} + e_i \quad i=\{1,\ldots,n\}
\]</span></p>
<p>Where:
- <span class="math inline">\(y\)</span> is the dependent variable.
- <span class="math inline">\(\beta_0\)</span> is the intercept.
- <span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_p\)</span> are the coefficients of the independent variables <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>.
- $e represents the error term.</p>
<p>This technique is widely used across various fields, including economics, social sciences, and healthcare, to control for multiple factors and enhance prediction accuracy. However, it requires careful attention to assumptions. Beyond linearity, now we have to consider issues like independence, multicollinearity, and variable selection which can affect the results.</p>
<p>We already have seen an example of Multiple linear regression when we worked with
Polynomial regression. However, multiple linear regression is more general.</p>
</div>
<div id="example-8" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Example<a href="multiple-linear-regression.html#example-8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider, for example, the task of explaining a countryâ€™s GDP using other economic variables such as inflation, unemployment, reference interest rate, government spending (as a percentage of GDP), and exports (as a percentage of GDP).</p>
<p>In this case, visualization is not as straightforward, and visually inspecting these relationships is much less practical. Nevertheless, with this number of variables, we can visually explore the relationships between them as follows:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="multiple-linear-regression.html#cb61-1" tabindex="-1"></a><span class="co"># Reads Data</span></span>
<span id="cb61-2"><a href="multiple-linear-regression.html#cb61-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">&quot;Gdp Data.csv&quot;</span>)</span>
<span id="cb61-3"><a href="multiple-linear-regression.html#cb61-3" tabindex="-1"></a></span>
<span id="cb61-4"><a href="multiple-linear-regression.html#cb61-4" tabindex="-1"></a><span class="co"># Plot the scatterplots for each pair of variables</span></span>
<span id="cb61-5"><a href="multiple-linear-regression.html#cb61-5" tabindex="-1"></a><span class="fu">pairs</span>(dat)</span></code></pre></div>
<p><img src="_main_files/figure-html/paris-plot-gdp-1.png" width="672" /></p>
<p>Here we can see, that some independent variables are more related to <code>GDP</code> and
some independent variables are more related between themselves. This is valuable information that will help us
to develop the right linear model with this variables.</p>
<p>We can also observe the correlation between these variables as follows:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="multiple-linear-regression.html#cb62-1" tabindex="-1"></a><span class="co"># Computes the correlation between variables</span></span>
<span id="cb62-2"><a href="multiple-linear-regression.html#cb62-2" tabindex="-1"></a><span class="fu">cor</span>(dat)</span></code></pre></div>
<pre><code>##            gdp          inf         une        int         gov         exp
## gdp  1.0000000  0.875131082 -0.74874795  0.6964256  0.22172279 0.173602651
## inf  0.8751311  1.000000000 -0.78173033  0.8292061  0.31103644 0.005685918
## une -0.7487479 -0.781730327  1.00000000 -0.3642453 -0.16674407 0.010553855
## int  0.6964256  0.829206121 -0.36424525  1.0000000  0.21389456 0.015699798
## gov  0.2217228  0.311036436 -0.16674407  0.2138946  1.00000000 0.018475446
## exp  0.1736027  0.005685918  0.01055386  0.0156998  0.01847545 1.000000000</code></pre>
<p>We can also fit simple linear regression with each one of the independent
variables.</p>
<p><strong>Inflation Rate</strong></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="multiple-linear-regression.html#cb64-1" tabindex="-1"></a><span class="co"># Fits with Inflation</span></span>
<span id="cb64-2"><a href="multiple-linear-regression.html#cb64-2" tabindex="-1"></a>outRegInf <span class="ot">&lt;-</span> <span class="fu">lm</span>(gdp <span class="sc">~</span> inf, <span class="at">data =</span> dat)</span>
<span id="cb64-3"><a href="multiple-linear-regression.html#cb64-3" tabindex="-1"></a>varVal    <span class="ot">&lt;-</span> dat<span class="sc">$</span>inf</span>
<span id="cb64-4"><a href="multiple-linear-regression.html#cb64-4" tabindex="-1"></a>out       <span class="ot">&lt;-</span> outRegInf</span>
<span id="cb64-5"><a href="multiple-linear-regression.html#cb64-5" tabindex="-1"></a>varNam    <span class="ot">&lt;-</span> <span class="st">&quot;Inflation Rate&quot;</span></span>
<span id="cb64-6"><a href="multiple-linear-regression.html#cb64-6" tabindex="-1"></a><span class="co"># Plots Regression Line and Scatterplot and residuals plot</span></span>
<span id="cb64-7"><a href="multiple-linear-regression.html#cb64-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb64-8"><a href="multiple-linear-regression.html#cb64-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb64-9"><a href="multiple-linear-regression.html#cb64-9" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>gd,</span>
<span id="cb64-10"><a href="multiple-linear-regression.html#cb64-10" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb64-11"><a href="multiple-linear-regression.html#cb64-11" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;GDP&quot;</span>)</span>
<span id="cb64-12"><a href="multiple-linear-regression.html#cb64-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> out<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb64-13"><a href="multiple-linear-regression.html#cb64-13" tabindex="-1"></a>       <span class="at">b   =</span> out<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb64-14"><a href="multiple-linear-regression.html#cb64-14" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb64-15"><a href="multiple-linear-regression.html#cb64-15" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb64-16"><a href="multiple-linear-regression.html#cb64-16" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb64-17"><a href="multiple-linear-regression.html#cb64-17" tabindex="-1"></a>     <span class="at">y    =</span> out<span class="sc">$</span>residuals,</span>
<span id="cb64-18"><a href="multiple-linear-regression.html#cb64-18" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb64-19"><a href="multiple-linear-regression.html#cb64-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb64-20"><a href="multiple-linear-regression.html#cb64-20" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb64-21"><a href="multiple-linear-regression.html#cb64-21" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/gdp-inf-fit-1.png" width="672" />
<strong>Unemployment Rate</strong></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="multiple-linear-regression.html#cb65-1" tabindex="-1"></a><span class="co"># Fits with Inflation</span></span>
<span id="cb65-2"><a href="multiple-linear-regression.html#cb65-2" tabindex="-1"></a>outRegUne <span class="ot">&lt;-</span> <span class="fu">lm</span>(gdp <span class="sc">~</span> une, <span class="at">data =</span> dat)</span>
<span id="cb65-3"><a href="multiple-linear-regression.html#cb65-3" tabindex="-1"></a>varVal    <span class="ot">&lt;-</span> dat<span class="sc">$</span>une</span>
<span id="cb65-4"><a href="multiple-linear-regression.html#cb65-4" tabindex="-1"></a>out       <span class="ot">&lt;-</span> outRegUne</span>
<span id="cb65-5"><a href="multiple-linear-regression.html#cb65-5" tabindex="-1"></a>varNam    <span class="ot">&lt;-</span> <span class="st">&quot;Unemplyment Rate&quot;</span></span>
<span id="cb65-6"><a href="multiple-linear-regression.html#cb65-6" tabindex="-1"></a><span class="co"># Plots Regression Line and Scatterplot and residuals plot</span></span>
<span id="cb65-7"><a href="multiple-linear-regression.html#cb65-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb65-8"><a href="multiple-linear-regression.html#cb65-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb65-9"><a href="multiple-linear-regression.html#cb65-9" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>gd,</span>
<span id="cb65-10"><a href="multiple-linear-regression.html#cb65-10" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb65-11"><a href="multiple-linear-regression.html#cb65-11" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;GDP&quot;</span>)</span>
<span id="cb65-12"><a href="multiple-linear-regression.html#cb65-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> out<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb65-13"><a href="multiple-linear-regression.html#cb65-13" tabindex="-1"></a>       <span class="at">b   =</span> out<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb65-14"><a href="multiple-linear-regression.html#cb65-14" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb65-15"><a href="multiple-linear-regression.html#cb65-15" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb65-16"><a href="multiple-linear-regression.html#cb65-16" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb65-17"><a href="multiple-linear-regression.html#cb65-17" tabindex="-1"></a>     <span class="at">y    =</span> out<span class="sc">$</span>residuals,</span>
<span id="cb65-18"><a href="multiple-linear-regression.html#cb65-18" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb65-19"><a href="multiple-linear-regression.html#cb65-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb65-20"><a href="multiple-linear-regression.html#cb65-20" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb65-21"><a href="multiple-linear-regression.html#cb65-21" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/gdp-unp-fit-1.png" width="672" />
<strong>Interest Rate</strong></p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="multiple-linear-regression.html#cb66-1" tabindex="-1"></a><span class="co"># Fits with Inflation</span></span>
<span id="cb66-2"><a href="multiple-linear-regression.html#cb66-2" tabindex="-1"></a>outRegInt <span class="ot">&lt;-</span> <span class="fu">lm</span>(gdp <span class="sc">~</span> int, <span class="at">data =</span> dat)</span>
<span id="cb66-3"><a href="multiple-linear-regression.html#cb66-3" tabindex="-1"></a>varVal    <span class="ot">&lt;-</span> dat<span class="sc">$</span>int</span>
<span id="cb66-4"><a href="multiple-linear-regression.html#cb66-4" tabindex="-1"></a>out       <span class="ot">&lt;-</span> outRegInt</span>
<span id="cb66-5"><a href="multiple-linear-regression.html#cb66-5" tabindex="-1"></a>varNam    <span class="ot">&lt;-</span> <span class="st">&quot;Interest Rate&quot;</span></span>
<span id="cb66-6"><a href="multiple-linear-regression.html#cb66-6" tabindex="-1"></a><span class="co"># Plots Regression Line and Scatterplot and residuals plot</span></span>
<span id="cb66-7"><a href="multiple-linear-regression.html#cb66-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb66-8"><a href="multiple-linear-regression.html#cb66-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb66-9"><a href="multiple-linear-regression.html#cb66-9" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>gd,</span>
<span id="cb66-10"><a href="multiple-linear-regression.html#cb66-10" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb66-11"><a href="multiple-linear-regression.html#cb66-11" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;GDP&quot;</span>)</span>
<span id="cb66-12"><a href="multiple-linear-regression.html#cb66-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> out<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb66-13"><a href="multiple-linear-regression.html#cb66-13" tabindex="-1"></a>       <span class="at">b   =</span> out<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb66-14"><a href="multiple-linear-regression.html#cb66-14" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb66-15"><a href="multiple-linear-regression.html#cb66-15" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb66-16"><a href="multiple-linear-regression.html#cb66-16" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb66-17"><a href="multiple-linear-regression.html#cb66-17" tabindex="-1"></a>     <span class="at">y    =</span> out<span class="sc">$</span>residuals,</span>
<span id="cb66-18"><a href="multiple-linear-regression.html#cb66-18" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb66-19"><a href="multiple-linear-regression.html#cb66-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb66-20"><a href="multiple-linear-regression.html#cb66-20" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb66-21"><a href="multiple-linear-regression.html#cb66-21" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/gdp-int-fit-1.png" width="672" /></p>
<p><strong>Goverment Spending</strong></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="multiple-linear-regression.html#cb67-1" tabindex="-1"></a><span class="co"># Fits with Inflation</span></span>
<span id="cb67-2"><a href="multiple-linear-regression.html#cb67-2" tabindex="-1"></a>outRegGov <span class="ot">&lt;-</span> <span class="fu">lm</span>(gdp <span class="sc">~</span> gov, <span class="at">data =</span> dat)</span>
<span id="cb67-3"><a href="multiple-linear-regression.html#cb67-3" tabindex="-1"></a>varVal    <span class="ot">&lt;-</span> dat<span class="sc">$</span>gov</span>
<span id="cb67-4"><a href="multiple-linear-regression.html#cb67-4" tabindex="-1"></a>out       <span class="ot">&lt;-</span> outRegGov</span>
<span id="cb67-5"><a href="multiple-linear-regression.html#cb67-5" tabindex="-1"></a>varNam    <span class="ot">&lt;-</span> <span class="st">&quot;Goverment Spending&quot;</span></span>
<span id="cb67-6"><a href="multiple-linear-regression.html#cb67-6" tabindex="-1"></a><span class="co"># Plots Regression Line and Scatterplot and residuals plot</span></span>
<span id="cb67-7"><a href="multiple-linear-regression.html#cb67-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb67-8"><a href="multiple-linear-regression.html#cb67-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb67-9"><a href="multiple-linear-regression.html#cb67-9" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>gd,</span>
<span id="cb67-10"><a href="multiple-linear-regression.html#cb67-10" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb67-11"><a href="multiple-linear-regression.html#cb67-11" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;GDP&quot;</span>)</span>
<span id="cb67-12"><a href="multiple-linear-regression.html#cb67-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> out<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb67-13"><a href="multiple-linear-regression.html#cb67-13" tabindex="-1"></a>       <span class="at">b   =</span> out<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb67-14"><a href="multiple-linear-regression.html#cb67-14" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb67-15"><a href="multiple-linear-regression.html#cb67-15" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb67-16"><a href="multiple-linear-regression.html#cb67-16" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb67-17"><a href="multiple-linear-regression.html#cb67-17" tabindex="-1"></a>     <span class="at">y    =</span> out<span class="sc">$</span>residuals,</span>
<span id="cb67-18"><a href="multiple-linear-regression.html#cb67-18" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb67-19"><a href="multiple-linear-regression.html#cb67-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb67-20"><a href="multiple-linear-regression.html#cb67-20" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb67-21"><a href="multiple-linear-regression.html#cb67-21" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/gdp-gov-fit-1.png" width="672" /></p>
<p><em>Exports</em></p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="multiple-linear-regression.html#cb68-1" tabindex="-1"></a><span class="co"># Fits with Inflation</span></span>
<span id="cb68-2"><a href="multiple-linear-regression.html#cb68-2" tabindex="-1"></a>outRegExp <span class="ot">&lt;-</span> <span class="fu">lm</span>(gdp <span class="sc">~</span> exp, <span class="at">data =</span> dat)</span>
<span id="cb68-3"><a href="multiple-linear-regression.html#cb68-3" tabindex="-1"></a>varVal    <span class="ot">&lt;-</span> dat<span class="sc">$</span>exp</span>
<span id="cb68-4"><a href="multiple-linear-regression.html#cb68-4" tabindex="-1"></a>out       <span class="ot">&lt;-</span> outRegExp</span>
<span id="cb68-5"><a href="multiple-linear-regression.html#cb68-5" tabindex="-1"></a>varNam    <span class="ot">&lt;-</span> <span class="st">&quot;Exports&quot;</span></span>
<span id="cb68-6"><a href="multiple-linear-regression.html#cb68-6" tabindex="-1"></a><span class="co"># Plots Regression Line and Scatterplot and residuals plot</span></span>
<span id="cb68-7"><a href="multiple-linear-regression.html#cb68-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb68-8"><a href="multiple-linear-regression.html#cb68-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb68-9"><a href="multiple-linear-regression.html#cb68-9" tabindex="-1"></a>     <span class="at">y    =</span> dat<span class="sc">$</span>gd,</span>
<span id="cb68-10"><a href="multiple-linear-regression.html#cb68-10" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb68-11"><a href="multiple-linear-regression.html#cb68-11" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;GDP&quot;</span>)</span>
<span id="cb68-12"><a href="multiple-linear-regression.html#cb68-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a   =</span> out<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb68-13"><a href="multiple-linear-regression.html#cb68-13" tabindex="-1"></a>       <span class="at">b   =</span> out<span class="sc">$</span>coefficients[<span class="dv">2</span>],</span>
<span id="cb68-14"><a href="multiple-linear-regression.html#cb68-14" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&#39;red&#39;</span>,</span>
<span id="cb68-15"><a href="multiple-linear-regression.html#cb68-15" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb68-16"><a href="multiple-linear-regression.html#cb68-16" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x    =</span> varVal,</span>
<span id="cb68-17"><a href="multiple-linear-regression.html#cb68-17" tabindex="-1"></a>     <span class="at">y    =</span> out<span class="sc">$</span>residuals,</span>
<span id="cb68-18"><a href="multiple-linear-regression.html#cb68-18" tabindex="-1"></a>     <span class="at">xlab =</span> varNam,</span>
<span id="cb68-19"><a href="multiple-linear-regression.html#cb68-19" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb68-20"><a href="multiple-linear-regression.html#cb68-20" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h   =</span> <span class="dv">0</span>,</span>
<span id="cb68-21"><a href="multiple-linear-regression.html#cb68-21" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/gdp-exp-fit-1.png" width="672" /></p>
<p>All of them seem like good candidates for a linear relationship with the GDP,
however when we use them all together, a more careful analysis should be made.</p>
<p>We can see the summary reports for the individual regressions and the regression
with all independent variables as follows:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="multiple-linear-regression.html#cb69-1" tabindex="-1"></a>outRegAll <span class="ot">&lt;-</span> <span class="fu">lm</span>(gdp <span class="sc">~</span> inf <span class="sc">+</span> une <span class="sc">+</span> int <span class="sc">+</span> gov <span class="sc">+</span> exp, <span class="at">data =</span> dat)</span>
<span id="cb69-2"><a href="multiple-linear-regression.html#cb69-2" tabindex="-1"></a></span>
<span id="cb69-3"><a href="multiple-linear-regression.html#cb69-3" tabindex="-1"></a><span class="co"># Summary All</span></span>
<span id="cb69-4"><a href="multiple-linear-regression.html#cb69-4" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;All Independent Variables&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;All Independent Variables&quot;</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="multiple-linear-regression.html#cb71-1" tabindex="-1"></a><span class="fu">summary</span>(outRegAll)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp ~ inf + une + int + gov + exp, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.56610 -0.38300 -0.00634  0.36630  1.22542 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.555301   0.380440   4.088 6.41e-05 ***
## inf          0.312218   0.090012   3.469 0.000647 ***
## une         -0.377334   0.129275  -2.919 0.003938 ** 
## int          0.177827   0.128312   1.386 0.167403    
## gov         -0.008483   0.010361  -0.819 0.413950    
## exp          0.064657   0.011930   5.420 1.80e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5024 on 190 degrees of freedom
## Multiple R-squared:  0.8096, Adjusted R-squared:  0.8046 
## F-statistic: 161.6 on 5 and 190 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="multiple-linear-regression.html#cb73-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Only Inflation Rate&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Only Inflation Rate&quot;</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="multiple-linear-regression.html#cb75-1" tabindex="-1"></a><span class="fu">summary</span>(outRegInf)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp ~ inf, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.40960 -0.38896  0.03562  0.37998  1.33364 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.02551    0.08705   11.78   &lt;2e-16 ***
## inf          0.49489    0.01965   25.19   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5514 on 194 degrees of freedom
## Multiple R-squared:  0.7659, Adjusted R-squared:  0.7646 
## F-statistic: 634.5 on 1 and 194 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="multiple-linear-regression.html#cb77-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Only Unemployment Rate&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Only Unemployment Rate&quot;</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="multiple-linear-regression.html#cb79-1" tabindex="-1"></a><span class="fu">summary</span>(outRegUne)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp ~ une, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.42276 -0.49693  0.02667  0.49525  2.79562 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   6.0868     0.2046   29.74   &lt;2e-16 ***
## une          -1.0400     0.0661  -15.73   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7554 on 194 degrees of freedom
## Multiple R-squared:  0.5606, Adjusted R-squared:  0.5584 
## F-statistic: 247.5 on 1 and 194 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="multiple-linear-regression.html#cb81-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Only Interest Rate&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Only Interest Rate&quot;</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="multiple-linear-regression.html#cb83-1" tabindex="-1"></a><span class="fu">summary</span>(outRegInt)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp ~ int, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.27807 -0.50801 -0.00257  0.50336  2.69719 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.31600    0.32323  -4.071  6.8e-05 ***
## int          0.86483    0.06398  13.517  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8178 on 194 degrees of freedom
## Multiple R-squared:  0.485,  Adjusted R-squared:  0.4824 
## F-statistic: 182.7 on 1 and 194 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="multiple-linear-regression.html#cb85-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Only Government Spending&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Only Government Spending&quot;</code></pre>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="multiple-linear-regression.html#cb87-1" tabindex="-1"></a><span class="fu">summary</span>(outRegGov)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp ~ gov, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4370 -0.6442 -0.1258  0.7429  3.1839 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  1.37117    0.51450   2.665  0.00835 **
## gov          0.06464    0.02041   3.167  0.00179 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.111 on 194 degrees of freedom
## Multiple R-squared:  0.04916,    Adjusted R-squared:  0.04426 
## F-statistic: 10.03 on 1 and 194 DF,  p-value: 0.001789</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="multiple-linear-regression.html#cb89-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Only Exports&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Only Exports&quot;</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="multiple-linear-regression.html#cb91-1" tabindex="-1"></a><span class="fu">summary</span>(outRegExp)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp ~ exp, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0084 -0.6679 -0.1133  0.6581  3.0810 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.32709    0.27818   8.365 1.16e-14 ***
## exp          0.06540    0.02664   2.455    0.015 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.122 on 194 degrees of freedom
## Multiple R-squared:  0.03014,    Adjusted R-squared:  0.02514 
## F-statistic: 6.028 on 1 and 194 DF,  p-value: 0.01496</code></pre>
<p>As we can see, the values for the coefficients can change when doing simple linear
regression and multiple linear regression. If the changes are very dramatic (like change
in the sign of the coefficient) further inspection is necessary for that variable.</p>
</div>
<div id="least-squares-estimation-1" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Least Squares Estimation<a href="multiple-linear-regression.html#least-squares-estimation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For least squares estimation, we need to solve the problem:</p>
<p><span class="math display">\[
\min_\boldsymbol{\beta}Q(\boldsymbol{\beta}) = \sum_{i=1}^n (y_i - \hat{y}(\boldsymbol{\beta}))^2 = (\mathbf{y}- \hat{\mathbf{y}})&#39;(\mathbf{y}- \hat{\mathbf{y}}) = (\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})
\]</span>
The representation in matrix notation of the problem, allows us to use the same
expression to solve this problem as with simple linear regression. The solution
is obtained in the exact same way, and is given by:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = (\mathbf{X}&#39; \mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}
\]</span>
however in this case:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = \left(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2,\ldots,\hat{\beta}_p\right)&#39;
\]</span>
this is the reason, working in matrix form is very useful.</p>
</div>
<div id="properties-of-the-estimates-1" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Properties of the Estimates<a href="multiple-linear-regression.html#properties-of-the-estimates-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As with simple linear regression, we can consider several estimates:</p>
<ul>
<li><span class="math inline">\(\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{\beta}\)</span> the estimates of the observations,</li>
<li><span class="math inline">\(\hat{\mathbf{e}} = \mathbf{y}- \hat{\mathbf{y}} = \mathbf{y}- \mathbf{X}\hat{\boldsymbol{\beta}}\)</span> the estimates of the errors.</li>
</ul>
<p>We also note that:</p>
<p><span class="math display">\[
\hat{\mathbf{y}} = \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{y}= \mathbf{H}y
\]</span>
where <span class="math inline">\(\mathbf{H}\)</span> is called the hat matrix, because it transforms <span class="math inline">\(\mathbf{y}\)</span> into <span class="math inline">\(\hat{\mathbf{y}}\)</span>,
or the projection matrix.</p>
<p>We will see that:</p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is a linear combination of <span class="math inline">\(y\)</span>.</li>
<li>The sum of the estimated errors is equal to zero, <span class="math inline">\(\sum_{i=1}^n \hat{e_i} = 0\)</span>.</li>
<li><span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{x}_j}\)</span> are orthogonal for <span class="math inline">\(j=\{1,\ldots,p\}\)</span>.</li>
<li><span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span> are orthogonal.</li>
<li><span class="math inline">\(\bar{y} = \hat{\bar{y}}\)</span>.</li>
</ul>
<p>To see that <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is a linear combination of <span class="math inline">\(y\)</span>, we need to express <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>
as follows:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = \mathbf{A}\mathbf{y}
\]</span></p>
<p>for some matrix <span class="math inline">\(\mathbf{A}\)</span>. This is very easy to do, we just let <span class="math inline">\(\mathbf{A}= (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\)</span>, so:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}= \mathbf{A}\mathbf{y}
\]</span>
Now to see that the sum of the estimated errors is equal to zero, <span class="math inline">\(\sum_{i=1}^n \hat{e_i} = 0\)</span>, we
notice that we need to show that:</p>
<p><span class="math display">\[
\hat{\mathbf{e}}&#39; \mathbf{1}= 0
\]</span></p>
<p>To do so we notice that:</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}} = (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}
  &amp;\implies (\mathbf{X}&#39;\mathbf{X})\hat{\boldsymbol{\beta}} = \mathbf{X}&#39;\mathbf{y}\\
  &amp;\implies \mathbf{X}&#39;\mathbf{y}- \mathbf{X}&#39;\mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{0}\\
  &amp;\implies \mathbf{X}&#39;\left(\mathbf{y}- \hat{\mathbf{y}}\right) = \mathbf{0}\\
  &amp;\implies \mathbf{X}&#39;\hat{\mathbf{e}} = \mathbf{0}
\end{align*}\]</span></p>
<p>Now focusing on the product <span class="math inline">\(\mathbf{X}&#39;\hat{\mathbf{e}}\)</span> we have that:</p>
<p><span class="math display">\[
\mathbf{X}&#39;\hat{\mathbf{e}} =
  \left[\begin{matrix}
    \mathbf{1}&#39; \\
    \mathbf{x}_1   \\
    \mathbf{x}_2   \\
    \vdots  \\
    \mathbf{x}_p
  \end{matrix}\right] \hat{\mathbf{e}} =
  \left[\begin{matrix}
    \mathbf{1}&#39; \hat{\mathbf{e}} \\
    \mathbf{x}_1   \hat{\mathbf{e}} \\
    \mathbf{x}_2   \hat{\mathbf{e}} \\
    \vdots            \\
    \mathbf{x}_p   \hat{\mathbf{e}}
  \end{matrix}\right]
\]</span>
So we have that:</p>
<p><span class="math display">\[
\left[\begin{matrix}
    \mathbf{1}&#39; \hat{\mathbf{e}} \\
    \mathbf{x}_1   \hat{\mathbf{e}} \\
    \mathbf{x}_2   \hat{\mathbf{e}} \\
    \vdots            \\
    \mathbf{x}_p   \hat{\mathbf{e}}
  \end{matrix}\right] =
  \left[\begin{matrix}
    0 \\
    0 \\
    0 \\
    \vdots            \\
    0
  \end{matrix}\right]
\]</span>
So from the first line of this result, we have that:</p>
<p><span class="math display">\[
\mathbf{1}&#39; \hat{\mathbf{e}} = 0
\]</span>
which is the result we wanted to proof.</p>
<p>Now, to show that <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{x}_j}\)</span> are orthogonal for <span class="math inline">\(j=\{1,\ldots,p\}\)</span>,
we use again on:</p>
<p><span class="math display">\[
\left[\begin{matrix}
    \mathbf{1}&#39; \hat{\mathbf{e}} \\
    \mathbf{x}_1   \hat{\mathbf{e}} \\
    \mathbf{x}_2   \hat{\mathbf{e}} \\
    \vdots            \\
    \mathbf{x}_p   \hat{\mathbf{e}}
  \end{matrix}\right] =
  \left[\begin{matrix}
    0 \\
    0 \\
    0 \\
    \vdots            \\
    0
  \end{matrix}\right]
\]</span></p>
<p>And notice that lines 2 to <span class="math inline">\(p+1\)</span> proof this results, that is</p>
<p><span class="math display">\[
\mathbf{x}_i &#39; \hat{\mathbf{e}} = 0 \quad i=\{1,\ldots,p\}
\]</span>
Now to show that <span class="math inline">\(\hat{\mathbf{e}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span> are orthogonal, we show that:</p>
<p><span class="math display">\[
\hat{\mathbf{e}}&#39;\hat{\mathbf{y}} = 0
\]</span>
Now</p>
<p><span class="math display">\[\begin{align*}
\hat{\mathbf{e}}&#39;\hat{\mathbf{y}}
  &amp;= (\mathbf{y}- \hat{\mathbf{y}})&#39;\hat{\mathbf{y}} \\
  &amp;= \left(\mathbf{y}- \mathbf{X}\hat{\boldsymbol{\beta}}\right)&#39;\mathbf{X}\hat{\boldsymbol{\beta}} \\
  &amp;= \left(\mathbf{y}- \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}\right)&#39;\mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}\\
  &amp;= \mathbf{y}&#39; \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}- \mathbf{y}&#39; \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{y}\\
  &amp;= \mathbf{y}&#39; \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}- \mathbf{y}&#39; \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{y}\\
  &amp;= 0
\end{align*}\]</span></p>
<p>Finally, to show that <span class="math inline">\(\bar{y} = \hat{\bar{y}}\)</span>, we use:</p>
<p><span class="math display">\[
\hat{\mathbf{e}}&#39;\mathbf{1}= (\mathbf{y}- \hat{\mathbf{y}})&#39;\mathbf{1}= \mathbf{y}&#39;\mathbf{1}- \hat{\mathbf{y}}&#39;\mathbf{1}= \sum_{i=1}^ny_i - \sum_{i=1}^n\hat{y}_i = n\bar{y} - n\hat{\bar{y}}
\]</span>
since <span class="math inline">\(\hat{\mathbf{e}}&#39;\mathbf{1}= 0\)</span>, then we have that</p>
<p><span class="math display">\[
n\bar{y} - n\hat{\bar{y}} = 0 \implies n\bar{y} = n\hat{\bar{y}} \implies \bar{y} = \hat{\bar{y}}  
\]</span></p>
</div>
<div id="multiple-r2" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Multiple <span class="math inline">\(R^2\)</span><a href="multiple-linear-regression.html#multiple-r2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As with simple linear regression we can explain the total variability, by decomposing the
variability in two parts, the regression variability and the error variability.</p>
<p>First, we define this concepts:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Total Sum of Squares <span class="math inline">\(SS_{tot}\)</span></strong>:<br />
The total sum of squares measures the total variability in <span class="math inline">\(\mathbf{y}\)</span>:</p>
<p><span class="math display">\[
SS_{tot} = (\mathbf{y} - \bar{y} \mathbf{1})&#39; (\mathbf{y} - \bar{y} \mathbf{1})
\]</span></p></li>
<li><p><strong>Residual Sum of Squares <span class="math inline">\(SS_{res}\)</span></strong>:<br />
The residual sum of squares measures the unexplained variability in the regression model:</p>
<p><span class="math display">\[
SS_{res} = (\mathbf{y} - \hat{\mathbf{y}})&#39; (\mathbf{y} - \hat{\mathbf{y}})
\]</span></p></li>
<li><p><strong>Explained Sum of Squares <span class="math inline">\(SS_{reg}\)</span></strong></p></li>
</ol>
<p>The explained sum of squares measures how much of the total variability is explained by the regression model. It is the difference between the predicted values and the mean of <span class="math inline">\(\mathbf{y}\)</span>:</p>
<p><span class="math display">\[
SS_{reg} = (\hat{\mathbf{y}} - \bar{y} \mathbf{1})&#39; (\hat{\mathbf{y}} - \bar{y} \mathbf{1})
\]</span>
As with simple linear regression, it can be shown that:</p>
<p><span class="math display">\[
SS_{tot} = SS_{reg} + SS_{res}
\]</span>
To see this, we start form <span class="math inline">\(SS_{tot}\)</span>, and do the adding and subtracting trick:</p>
<p><span class="math display">\[\begin{align*}
SS_{tot}
  &amp;= (\mathbf{y} - \bar{y} \mathbf{1})&#39; (\mathbf{y} - \bar{y} \mathbf{1}) \\
  &amp;= (\mathbf{y} - \hat{\mathbf{y}} + \hat{\mathbf{y}} - \bar{y} \mathbf{1})&#39; (\mathbf{y} - \hat{\mathbf{y}} + \hat{\mathbf{y}} - \bar{y} \mathbf{1}) \\
  &amp;= (\mathbf{y} - \hat{\mathbf{y}})&#39; (\mathbf{y} - \hat{\mathbf{y}}) + (\mathbf{y} - \hat{\mathbf{y}})&#39; (\hat{\mathbf{y}} - \bar{y} \mathbf{1}) + (\hat{\mathbf{y}} - \bar{y} \mathbf{1})&#39; (\mathbf{y} - \hat{\mathbf{y}}\mathbf{1}) + (\hat{\mathbf{y}} - \bar{y} \mathbf{1})&#39; (\hat{\mathbf{y}} - \bar{y} \mathbf{1})
\end{align*}\]</span></p>
<p>Now, notice that:</p>
<p><span class="math display">\[
(\mathbf{y} - \hat{\mathbf{y}})&#39; (\hat{\mathbf{y}} - \bar{y} \mathbf{1}) = \hat{\mathbf{e}}&#39; (\hat{\mathbf{y}} - \bar{y} \mathbf{1}) = \hat{\mathbf{e}}&#39;\hat{\mathbf{y}} - \bar{y}\hat{\mathbf{e}}&#39; \mathbf{1} = 0 - \bar{y}0 = 0
\]</span>
And similarly for <span class="math inline">\((\hat{\mathbf{y}} - \bar{y} \mathbf{1})&#39; (\mathbf{y} - \hat{\mathbf{y}}\mathbf{1}) = 0\)</span>, then:</p>
<p><span class="math display">\[
SS_{tot} = (\mathbf{y} - \hat{\mathbf{y}})&#39; (\mathbf{y} - \hat{\mathbf{y}}) + (\hat{\mathbf{y}} - \bar{y} \mathbf{1})&#39; (\hat{\mathbf{y}} - \bar{y} \mathbf{1}) = SS_{reg} + SS_{res}
\]</span>
The multiple <span class="math inline">\(R^2\)</span> is the variability explained by the regression with respect to the total variability
and can be expressed as:</p>
<p><span class="math display">\[
R^2 = \frac{SS_{reg}}{SS_{tot}}
\]</span>
or using the previous expression</p>
<p><span class="math display">\[
1 = \frac{SS_{tot}}{SS_{tot}} = \frac{SS_{reg}}{SS_{tot}} + \frac{SS_{res}}{SS_{tot}} = R^2 + \frac{SS_{res}}{SS_{tot}} \implies R^2 = 1 - \frac{SS_{res}}{SS_{tot}}  
\]</span></p>
<p>Finally, we work on the expressions of <span class="math inline">\(SS_{res}\)</span> and <span class="math inline">\(SS_{tot}\)</span>, to express them
in terms of projection matrices.</p>
<p>First note that:</p>
<p><span class="math display">\[
\mathbf{y}- \hat{\mathbf{y}} = \mathbf{y}- \mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{y}- \mathbf{H}\mathbf{y}= (\mathbf{I}- \mathbf{H})\mathbf{y}
\]</span>
and also notice that <span class="math inline">\((\mathbf{I}- \mathbf{H})\)</span> is symmetric and:</p>
<p><span class="math display">\[
(\mathbf{I}- \mathbf{H})(\mathbf{I}- \mathbf{H}) = \mathbf{I}-\mathbf{H}- \mathbf{H}+ \mathbf{H}\mathbf{H}
\]</span>
and</p>
<p><span class="math display">\[
\mathbf{H}\mathbf{H}= \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; = \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39; = \mathbf{H}
\]</span>
this means <span class="math inline">\(\mathbf{H}\)</span> is idempotent. In fact, all projection matrices are idempotent.</p>
<p>Then, we have that:</p>
<p><span class="math display">\[
(\mathbf{I}- \mathbf{H})(\mathbf{I}- \mathbf{H}) = \mathbf{I}-\mathbf{H}- \mathbf{H}+ \mathbf{H}= \mathbf{I}-\mathbf{H}- \mathbf{H}
\]</span>
which makes <span class="math inline">\(\mathbf{I}- \mathbf{H}\)</span> also idempotent. Therefore:</p>
<p><span class="math display">\[
SS_{res} = (\mathbf{y}- \hat{\mathbf{y}})&#39;(\mathbf{y}- \hat{\mathbf{y}}) = ((\mathbf{I}- \mathbf{H})\mathbf{y})&#39;((\mathbf{I}- \mathbf{H})\mathbf{y}) = \mathbf{y}&#39;(\mathbf{I}- \mathbf{H})&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}= \mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}
\]</span>
And we can do a similar trick for the <span class="math inline">\(SS_{tot}\)</span> by writing <span class="math inline">\(\bar{y} \mathbf{1}\)</span> as a result
of projecting <span class="math inline">\(\mathbf{y}\)</span> with a design matrix <span class="math inline">\(\mathbf{1}\)</span>:</p>
<p><span class="math display">\[
\bar{y} \mathbf{1}= \mathbf{1}\bar{y} = \mathbf{1}\frac{1}{n} \sum_{i=1}^n y_i = \mathbf{1}\frac{1}{n}\mathbf{1}&#39; \mathbf{y}= \mathbf{1}(\mathbf{1}&#39;\mathbf{1})^{-1}\mathbf{1}&#39; \mathbf{y}
\]</span>
where we use the fact that <span class="math inline">\(\mathbf{1}&#39;\mathbf{1}= n\)</span>.</p>
<p>We call <span class="math inline">\(\mathbf{H}_0 = \mathbf{1}(\mathbf{1}&#39;\mathbf{1})^{-1}\mathbf{1}&#39;\)</span>, since <span class="math inline">\(\mathbf{1}(\mathbf{1}&#39;\mathbf{1})^{-1}\mathbf{1}&#39;\)</span>
is a projection matrix. And since it is a projection matrix it is idempotent (it is also not difficult to check this manually)
and <span class="math inline">\(\mathbf{I}- \mathbf{H}_0\)</span> is also idempotent.</p>
<p>So we can do:</p>
<p><span class="math display">\[
\mathbf{y}- \hat{y}\mathbf{1}= \mathbf{y}- \mathbf{H}_0 \mathbf{y}= (\mathbf{I}-\mathbf{H}_0)\mathbf{y}
\]</span></p>
<p><span class="math display">\[
SS_{tot} = (\mathbf{y}- \bar{y}\mathbf{1})&#39;(\mathbf{y}- \bar{y}\mathbf{1}) = ((\mathbf{I}- \mathbf{H}_0)\mathbf{y})&#39;((\mathbf{I}- \mathbf{H}_0)\mathbf{y}) = \mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}= \mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}
\]</span></p>
<p>so the <span class="math inline">\(R^2\)</span> can be expressed as follows:</p>
<p><span class="math display">\[
R^2 = 1 - \frac{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}}{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}}
\]</span>
When written like this, it is easy to see that:</p>
<p><span class="math display">\[
\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}= \min_\boldsymbol{\beta}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})
\]</span>
the solution to this minimization problem, since we are using the optimal value
<span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>. And</p>
<p><span class="math display">\[
\mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}= \min_{\beta_0} (\mathbf{y}- \mathbf{X}_0 \beta_0)&#39;(\mathbf{y}- \mathbf{X}_0 \beta_0)
\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}_0\)</span> is just a matrix with one column <span class="math inline">\(\mathbf{1}\)</span>.</p>
<p>Now, we also have that:</p>
<p><span class="math display">\[
\min_\boldsymbol{\beta}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) \leq \min_{\beta_0} (\mathbf{y}- \mathbf{X}_0 \beta_0)&#39;(\mathbf{y}- \mathbf{X}_0 \beta_0)
\]</span></p>
<p>therefore</p>
<p><span class="math display">\[
\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}\leq \mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}
\]</span>
and since both of them are quadratic forms, we have that:</p>
<p><span class="math inline">\(\mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}, \mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}\geq 0\)</span></p>
<p>then:</p>
<p><span class="math display">\[0 \leq \frac{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H})\mathbf{y}}{\mathbf{y}&#39;(\mathbf{I}- \mathbf{H}_0)\mathbf{y}} \leq 0\]</span>
then:</p>
<p><span class="math display">\[0 \leq R^2 \leq 0\]</span>.</p>
<p>Where we use the fact that all symmetric idempotent matrices are symmetric positive semi-definite.</p>
<p>Another interpretation of <span class="math inline">\(R^2\)</span> is the percentage of the variability explained
by multiple regression of a â€œ<em>poor manâ€™s regression</em>â€ in which you donâ€™t have
independent variables (that is you are independent variable poor). In this way,
we can define</p>
<p><span class="math display">\[
\bar{y} \mathbf{1}= \hat{\mathbf{y}}_0
\]</span>
the â€œ<em>poor manâ€™s prediction</em>â€, of which <span class="math inline">\(\mathbf{H}_0\)</span> is itâ€™s projection matrix (or hat matrix).</p>
</div>
<div id="geometric-interpretation-of-multiple-linear-regression" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Geometric Interpretation of Multiple Linear Regression<a href="multiple-linear-regression.html#geometric-interpretation-of-multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Multiple linear regression can be thought as projecting <span class="math inline">\(\mathbf{y}\)</span> in the column
space of the design matrix <span class="math inline">\(\mathbf{X}\)</span>. The following diagram pictures multiple linear regression.</p>
<p><img src="_main_files/figure-html/geo-int-1.png" width="672" /></p>
<p>Here we can see several components:</p>
<ul>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of observations. Is a vector in <span class="math inline">\(\mathbb{R}^n\)</span>.</li>
<li>The grey hyper-plane is the column space generated by <span class="math inline">\(\mathbf{X}\)</span>, a sub-space of <span class="math inline">\(\mathbb{R}^n\)</span>.</li>
<li>The multiple regression prediction <span class="math inline">\(\hat{\mathbf{y}}\)</span> of <span class="math inline">\(\mathbf{y}\)</span> is the projection of <span class="math inline">\(\mathbf{y}\)</span>
on the space generated by the column of <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>The poor manâ€™s prediction <span class="math inline">\(\hat{\mathbf{y}}_0\)</span>, in the column space of <span class="math inline">\(\mathbf{X}\)</span> (since,
one of the columns is <span class="math inline">\(\mathbf{1}\)</span>), but in most cases it is different to <span class="math inline">\(\hat{\mathbf{y}}\)</span> (the closest vector in
the column space of <span class="math inline">\(\mathbf{X}\)</span> to <span class="math inline">\(\mathbf{y}\)</span>).</li>
<li>We notice that the differences:
<ul>
<li><span class="math inline">\(\mathbf{y}- \hat{\mathbf{y}}\)</span>.</li>
<li><span class="math inline">\(\mathbf{y}- \hat{\mathbf{y}}_0\)</span></li>
<li><span class="math inline">\(\hat{\mathbf{y}} - \hat{\mathbf{y}}_0\)</span></li>
</ul>
form a right triangle, then it must be that:
<span class="math display">\[
||\mathbf{y}- \hat{\mathbf{y}}||^2 = ||\mathbf{y}- \hat{\mathbf{y}}_0||^2 + ||\hat{\mathbf{y}} - \hat{\mathbf{y}}_0||^2
\]</span>
which is the same as
<span class="math display">\[
(\mathbf{y}- \hat{\mathbf{y}})&#39;(\mathbf{y}- \hat{\mathbf{y}}) = (\mathbf{y}- \hat{\mathbf{y}}_0)&#39;(\mathbf{y}- \hat{\mathbf{y}}_0) + (\hat{\mathbf{y}} - \hat{\mathbf{y}}_0)&#39;(\hat{\mathbf{y}} - \hat{\mathbf{y}}_0)
\]</span>
that can be expressed as:
<span class="math display">\[
SS_{tot} = SS_{res} + SS_{reg}
\]</span></li>
</ul>
</div>
<div id="centered-and-standarized-variables" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Centered and Standarized Variables<a href="multiple-linear-regression.html#centered-and-standarized-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="centered-variables" class="section level3 hasAnchor" number="6.7.1">
<h3><span class="header-section-number">6.7.1</span> Centered Variables<a href="multiple-linear-regression.html#centered-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Like with simple linear regression we can center and standardize our variables.</p>
<p>For this section, let us use the notation:</p>
<p><span class="math display">\[\mathbf{X}= \left[\mathbf{x}_1, \mathbf{x}_2, \ldots \mathbf{x}_p \right]\]</span>
a matrix with <span class="math inline">\(p\)</span> variables in which each column is a variable <span class="math inline">\(\mathbf{x}_i\)</span>. In the
context of linear regression you can this is similar to the design matrix except
that it doesnâ€™t have the column of ones. In this way, we will rename the design
matrix as</p>
<p><span class="math display">\[\mathbf{X}_{*} = \left[\mathbf{1}\mathbf{X}\right] \]</span>
we introduce this notation, since we donâ€™t want to center or standardize the
column of ones.</p>
<p>To center matrix <span class="math inline">\(\mathbf{X}\)</span> we need to remove the mean of every column. Notice that the
vector of means is given by:</p>
<p><span class="math display">\[\bar{\mathbf{x}} =
  \left[\begin{matrix}
    \bar{\mathbf{x}}_1 \\
    \bar{\mathbf{x}}_2 \\
    \vdots      \\
    \bar{\mathbf{x}}_p
  \end{matrix}\right] =
  \left[\begin{matrix}
    \frac{1}{n} \mathbf{x}_1&#39;\mathbf{1}\\
    \frac{1}{n} \mathbf{x}_2&#39;\mathbf{1}\\
    \vdots                   \\
    \frac{1}{n} \mathbf{x}_p&#39;\mathbf{1}
  \end{matrix}\right] = \frac{1}{n}
  \left[\begin{matrix}
    \mathbf{x}_1&#39;\mathbf{1}\\
    \mathbf{x}_2&#39;\mathbf{1}\\
    \vdots                   \\
    \mathbf{x}_p&#39;\mathbf{1}
  \end{matrix}\right] = \frac{1}{n} \mathbf{X}&#39; \mathbf{1}\]</span></p>
<p>then the centered data <span class="math inline">\(\mathbf{X}_c\)</span> is given by:</p>
<p><span class="math display">\[\mathbf{X}_c = \mathbf{X}- \left[\begin{matrix}
    \bar{\mathbf{x}}_1 &amp; \bar{\mathbf{x}}_2 &amp; \dots  &amp; \bar{\mathbf{x}}_p\\
    \bar{\mathbf{x}}_1 &amp; \bar{\mathbf{x}}_2 &amp; \dots  &amp; \bar{\mathbf{x}}_p\\
    \vdots      &amp; \vdots      &amp; \ddots &amp; \vdots     \\
    \bar{\mathbf{x}}_1 &amp; \bar{\mathbf{x}}_2 &amp; \dots  &amp; \bar{\mathbf{x}}_p
  \end{matrix}\right] = \mathbf{X}- \mathbf{1}\bar{\mathbf{x}}&#39; = \mathbf{X}- \mathbf{1}\left(\frac{1}{n} \mathbf{X}&#39; \mathbf{1}\right)&#39; = \mathbf{X}- \frac{1}{n}\mathbf{1}\mathbf{1}&#39; \mathbf{X}= \left(\mathbf{I}- \frac{1}{n}\mathbf{1}\mathbf{1}&#39; \right)\mathbf{X}\]</span></p>
<p>We call <span class="math display">\[\mathbf{C}=  \left(\mathbf{I}- \frac{1}{n}\mathbf{1}\mathbf{1}&#39; \right) = \mathbf{I}- \mathbf{H}_0 \]</span> the centering
matrix, since it centers the variables of matrix <span class="math inline">\(\mathbf{X}\)</span>. Note also, that <span class="math inline">\(\mathbf{C}\)</span> also
centers any matrix with <span class="math inline">\(n\)</span> rows, in particular a vector of size <span class="math inline">\(n\)</span> is also centered
by <span class="math inline">\(\mathbf{C}\)</span>. So we can center <span class="math inline">\(y\)</span> the dependent variable, the same way:</p>
<p><span class="math display">\[\mathbf{y}_c = \mathbf{C}\mathbf{y}\]</span></p>
</div>
<div id="sample-covariance" class="section level3 hasAnchor" number="6.7.2">
<h3><span class="header-section-number">6.7.2</span> Sample Covariance<a href="multiple-linear-regression.html#sample-covariance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having defined, the centered matrix <span class="math inline">\(\mathbf{X}_c\)</span> we can define the sample covariance
of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{S}_{XX} \in \mathbb{R}^{p \times p}\)</span> as follows:</p>
<p><span class="math display">\[ \mathbf{S}_{XX} = \frac{1}{n-1} \left(\mathbf{X}- \mathbf{1}\bar{\mathbf{x}}&#39;\right)&#39;\left(\mathbf{X}- \mathbf{1}\bar{\mathbf{x}}&#39;\right) = \frac{1}{n-1} \mathbf{X}_c&#39;\mathbf{X}_c = \frac{1}{n-1} \mathbf{X}&#39;\mathbf{C}&#39; \mathbf{C}\mathbf{X}\]</span>
Now, since <span class="math inline">\(\mathbf{C}= \mathbf{I}- \mathbf{H}_0\)</span> is idempotent and symmetric we have that:</p>
<p><span class="math display">\[ \mathbf{S}_{XX} = \frac{1}{n-1} \mathbf{X}&#39;\mathbf{C}&#39; \mathbf{C}\mathbf{X}= \frac{1}{n-1} \mathbf{X}&#39; \mathbf{C}\mathbf{X}= \frac{1}{n-1} \mathbf{X}_c&#39;\mathbf{X}= \frac{1}{n-1} \mathbf{X}&#39;\mathbf{X}_c \]</span>
So the sample covariance, is the same for the original variables and the centered variables.</p>
<p>We can also define the covariance vector between variables <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \ldots \mathbf{x}_p\)</span> and variable <span class="math inline">\(\mathbf{y}\)</span>,
<span class="math inline">\(\mathbf{S}_{Xy} \in \mathbb{R}^{p \times 1}\)</span>, as follows:</p>
<p><span class="math display">\[ \mathbf{S}_{Xy} = \frac{1}{n-1} \left(\mathbf{X}- \mathbf{1}\bar{\mathbf{x}}&#39;\right)&#39;\left(\mathbf{y}- \mathbf{1}\bar{y}\right) = \frac{1}{n-1} \mathbf{X}_c&#39;\mathbf{y}_c = \frac{1}{n-1} \mathbf{X}&#39;\mathbf{C}&#39; \mathbf{C}\mathbf{y}\]</span></p>
<p>and, in the same way than before, we have that:</p>
<p><span class="math display">\[\mathbf{S}_{Xy} = \frac{1}{n-1} \mathbf{X}&#39; \mathbf{C}&#39; \mathbf{C}\mathbf{y}= \frac{1}{n-1} \mathbf{X}&#39; \mathbf{C}\mathbf{y}= \frac{1}{n-1} \mathbf{X}_c&#39;\mathbf{y}= \frac{1}{n-1} \mathbf{X}&#39;\mathbf{y}_c\]</span></p>
<p>so, you donâ€™t need to center both variables. As long as you center one of them
the result will be the same.</p>
<p>With this measures, we can focus on splitting the vector of estimated coefficients <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, into the estimate for
the intercept and the estimates for the independent variables, as follows:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
  \left[\begin{matrix}
    \hat{\beta}_0 \\
    \hat{\boldsymbol{\beta}}_{-0}
  \end{matrix}\right]\]</span></p>
<p>with <span class="math inline">\(\hat{\beta}_0\)</span> the estimate of the intercept and <span class="math inline">\(\hat{\boldsymbol{\beta}}_{-0}\)</span> the coefficients
for all independent variables. That is:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}}_{-0} =
  \left[\begin{matrix}
    \hat{\beta}_1 \\
    \hat{\beta}_2 \\
    \vdots        \\
    \hat{\beta}_p  
  \end{matrix}\right] \in \mathbb{R}^{p \times 1}\]</span></p>
<p>Under the new notation, <span class="math inline">\(\mathbf{X}_{*}\)</span> for the design matrix, we have that:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\beta}} = \left(\mathbf{X}_{*}&#39;\mathbf{X}_{*}\right)^{-1}\mathbf{X}_{*}&#39;\mathbf{y}\]</span>
so:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\beta}} = \left[\begin{matrix}
    \hat{\beta}_0 \\
    \hat{\boldsymbol{\beta}}_{-0}
  \end{matrix}\right] =
  \left( \left[\mathbf{1}\mathbf{X}\right]&#39; \left[\mathbf{1}\mathbf{X}\right] \right)^{-1} \left[\mathbf{1}\mathbf{X}\right]&#39; \mathbf{y}\]</span></p>
<p>so we need to compute <span class="math inline">\(\left( \left[\mathbf{1}\mathbf{X}\right]&#39; \left[\mathbf{1}\mathbf{X}\right] \right)^{-1}\)</span>.</p>
<p>We start by computing:</p>
<p><span class="math display">\[\left[\mathbf{1}\mathbf{X}\right]&#39; \left[\mathbf{1}\mathbf{X}\right] =
  \left[\begin{matrix}
    \mathbf{1}&#39; \\
    \mathbf{X}&#39;  
  \end{matrix}\right] \left[\mathbf{1}\mathbf{X}\right] =
  \left[\begin{matrix}
    \mathbf{1}&#39; \mathbf{1}&amp; \mathbf{1}&#39; \mathbf{X}\\
    \mathbf{X}&#39;\mathbf{1}&amp; \mathbf{X}&#39; \mathbf{X}&#39;  
  \end{matrix}\right] =
  \left[\begin{matrix}
    n          &amp; n\bar{\mathbf{x}}&#39; \\
    n\bar{\mathbf{x}} &amp; \mathbf{X}&#39; \mathbf{X}&#39;  
  \end{matrix}\right]
\]</span>
Now, we need to invert a 2 by 2 block matrix (luckily there is a formula for this).
The formula is in the prerequisites section, however note that in this case
one of the blocks is of height 1, since the first value we are looking for <span class="math inline">\(\hat{\beta}_0\)</span>
is a scalar. After applying the formula, we have:</p>
<p><span class="math display">\[\begin{align*}
\left( \left[\mathbf{1}\mathbf{X}\right]&#39; \left[\mathbf{1}\mathbf{X}\right] \right)^{-1}
  &amp;= \left[\begin{matrix}
    n^{-1} + n^{-1} n\bar{\mathbf{x}}&#39; \left(\mathbf{X}&#39; \mathbf{X}&#39; -  n\bar{\mathbf{x}} n^{-1} n\bar{\mathbf{x}}&#39; \right)^{-1}n\bar{\mathbf{x}} n^{-1} &amp; -n^{-1} n\bar{\mathbf{x}}&#39; \left(\mathbf{X}&#39; \mathbf{X}&#39; -  n\bar{\mathbf{x}} n^{-1} n\bar{\mathbf{x}}&#39;     \right)^{-1} \\
    -\left(\mathbf{X}&#39; \mathbf{X}&#39; -  n\bar{\mathbf{x}} n^{-1} n\bar{\mathbf{x}}&#39; \right)^{-1}n\bar{\mathbf{x}} n^{-1} &amp; \left(\mathbf{X}&#39; \mathbf{X}&#39; -  n\bar{\mathbf{x}} n^{-1} n\bar{\mathbf{x}}&#39; \right)^{-1}  
  \end{matrix}\right] \\
  &amp;= \left[\begin{matrix}
    n^{-1} + \bar{\mathbf{x}}&#39; \left(\mathbf{X}&#39; \mathbf{X}-  n\bar{\mathbf{x}}\bar{\mathbf{x}}&#39; \right)^{-1}\bar{\mathbf{x}} &amp; \bar{\mathbf{x}}&#39; \left(\mathbf{X}&#39; \mathbf{X}-  n\bar{\mathbf{x}} \bar{\mathbf{x}}&#39; \right)^{-1} \\
    -\left(\mathbf{X}&#39; \mathbf{X}-  n\bar{\mathbf{x}} \bar{\mathbf{x}}&#39; \right)^{-1}\bar{\mathbf{x}}                   &amp; \left(\mathbf{X}&#39; \mathbf{X}-  n\bar{\mathbf{x}} \bar{\mathbf{x}}&#39; \right)^{-1}  
  \end{matrix}\right]  
\end{align*}\]</span></p>
<p>Now, notice that:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{X}&#39; \mathbf{X}-  n\bar{\mathbf{x}} \bar{\mathbf{x}}&#39;
  &amp;= \mathbf{X}&#39; \mathbf{X}-  n\left(\frac{\mathbf{X}&#39;\mathbf{1}}{n}\right)\left(\frac{\mathbf{X}&#39;\mathbf{1}}{n}\right)&#39; \\
  &amp;= \mathbf{X}&#39; \mathbf{X}-  \frac{1}{n}\mathbf{X}&#39;\mathbf{1}\mathbf{1}&#39; \mathbf{X}\\
  &amp;= \mathbf{X}&#39;\left(\mathbf{I}- \frac{1}{n}\mathbf{1}\mathbf{1}&#39;\right) \mathbf{X}\\
  &amp;= \mathbf{X}&#39;\mathbf{C}\mathbf{X}\\
  &amp;= (n-1)\mathbf{S}_{XX}
\end{align*}\]</span></p>
<p>then:</p>
<p><span class="math display">\[\begin{align*}
\left( \left[\mathbf{1}\mathbf{X}\right]&#39; \left[\mathbf{1}\mathbf{X}\right] \right)^{-1}  
  &amp;=
  \left[\begin{matrix}
    n^{-1} + \bar{\mathbf{x}}&#39; \left((n-1)\mathbf{S}_{XX} \right)^{-1}\bar{\mathbf{x}} &amp; -\bar{\mathbf{x}}&#39; \left((n-1)\mathbf{S}_{XX} \right)^{-1} \\
    -\left((n-1)\mathbf{S}_{XX} \right)^{-1}\bar{\mathbf{x}}                    &amp; \left((n-1)\mathbf{S}_{XX} \right)^{-1}   
  \end{matrix}\right] \\
  &amp;=
  \left[\begin{matrix}
    n^{-1} + \frac{1}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1} \bar{\mathbf{x}} &amp; -\frac{1}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1} \\
    -\frac{1}{n-1}\mathbf{S}_{XX}^{-1} \bar{\mathbf{x}}                    &amp; \frac{1}{n-1}\mathbf{S}_{XX}^{-1}   
  \end{matrix}\right]
\end{align*}\]</span></p>
<p>In a similar way we can easily compute:</p>
<p><span class="math display">\[\left[\mathbf{1}\mathbf{X}\right]&#39; \mathbf{y}=
  \left[\begin{matrix}
    \mathbf{1}&#39; \mathbf{y}\\
    \mathbf{X}&#39;\mathbf{y}
  \end{matrix}\right] =
  \left[\begin{matrix}
    n\bar{y} \\
    \mathbf{X}&#39;\mathbf{y}
  \end{matrix}\right]\]</span></p>
<p>Then, we have that:</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}
  &amp;= \left[\begin{matrix} \\
    \hat{\beta}_0 \\
    \hat{\boldsymbol{\beta}}_{-0}
  \end{matrix}\right] \\
  &amp;=
  \left[\begin{matrix}
    n^{-1} + \frac{1}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1} \bar{\mathbf{x}} &amp; -\frac{1}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1} \\
    -\frac{1}{n-1}\mathbf{S}_{XX}^{-1} \bar{\mathbf{x}}                    &amp; \frac{1}{n-1}\mathbf{S}_{XX}^{-1}   
  \end{matrix}\right]
  \left[\begin{matrix}
    n\bar{y} \\
    \mathbf{X}&#39;\mathbf{y}
  \end{matrix}\right] \\
  &amp;=
  \left[\begin{matrix}
    n^{-1}n\bar{y} + \frac{n\bar{y}}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1} \bar{\mathbf{x}} - \frac{1}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1}\mathbf{X}&#39;\mathbf{y}\\
    -\frac{n\bar{y}}{n-1}\mathbf{S}_{XX}^{-1} \bar{\mathbf{x}} + \frac{1}{n-1}\mathbf{S}_{XX}^{-1}\mathbf{X}&#39;\mathbf{y}
  \end{matrix}\right]
\end{align*}\]</span></p>
<p>Then, working first with the second row-block:</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}_{-0}
  &amp;=  -\frac{n\bar{y}}{n-1}\mathbf{S}_{XX}^{-1} \bar{\mathbf{x}} + \frac{1}{n-1}\mathbf{S}_{XX}^{-1}\mathbf{X}&#39;y \\
  &amp;= \frac{1}{n-1}\mathbf{S}_{XX}^{-1}\mathbf{X}&#39;y -\frac{1}{n-1}\mathbf{S}_{XX}^{-1} \bar{\mathbf{x}}n\bar{y}   \\
  &amp;= \frac{1}{n-1}\mathbf{S}_{XX}^{-1}\left(\mathbf{X}&#39;y - n\bar{\mathbf{x}}\bar{y}\right)
\end{align*}\]</span></p>
<p>Now, we note that:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{X}&#39;\mathbf{y}- n\bar{\mathbf{x}}\bar{y}
  &amp;= \mathbf{X}&#39;\mathbf{y}- n\bar{\mathbf{x}}\bar{y}                                                   \\
  &amp;= \mathbf{X}&#39;\mathbf{y}- n\left(\frac{\mathbf{X}&#39;\mathbf{1}}{n}\right)\left(\frac{\mathbf{1}&#39;\mathbf{y}}{n}\right) \\
  &amp;= \mathbf{X}&#39;\mathbf{y}- \frac{1}{n}\mathbf{X}&#39;\mathbf{1}\mathbf{1}&#39;\mathbf{y}\\
  &amp;= \mathbf{X}&#39; \left(\mathbf{I}- \frac{1}{n} \mathbf{1}\mathbf{1}&#39; \right) \mathbf{y}\\
  &amp;= \mathbf{X}&#39; \mathbf{C}\mathbf{y}\\
  &amp;= (n-1)\mathbf{S}_{Xy}
\end{align*}\]</span></p>
<p>Then, we have that:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}}_{-0} = \frac{1}{n-1}\mathbf{S}_{XX}^{-1}(n-1)\mathbf{S}_{Xy} = \mathbf{S}_{XX}^{-1}\mathbf{S}_{Xy}\]</span>
An equivalent result to that of simple linear regression, both using the covariance
matrix of the independent variables and the covariance vector of the independent
variables and the dependent variable.</p>
<p>This way of writing the coefficients shows the influence of each component:</p>
<ul>
<li><span class="math inline">\(\mathbf{S}_{XX}^{-1}\)</span>: The relationship between the independent variables.</li>
<li><span class="math inline">\(\mathbf{S}_{Xy}\)</span>: The relationship between the independent variables and the dependent variables.</li>
</ul>
<p>Also notice that, since centralizing doesnâ€™t change the values of <span class="math inline">\(\mathbf{S}_{X_cX_c}^{-1}\)</span> and <span class="math inline">\(\mathbf{S}_{X_cy_c}\)</span>, centralizing
the independent variables or the dependent variable (or both), doesnâ€™t change the
value of the coefficients of the independent variables.</p>
<p>Now, we can work more easily with the intercept estimate:</p>
<p><span class="math display">\[\begin{align*}
\hat{\beta}_0
  &amp;= n^{-1}n\bar{y} + \frac{n\bar{y}}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1} \bar{\mathbf{x}} - \frac{1}{n-1}\bar{\mathbf{x}}&#39; \mathbf{S}_{XX}^{-1}\mathbf{X}&#39;\mathbf{y}\\
  &amp;= \bar{y} + \frac{1}{n-1}\bar{\mathbf{x}}&#39;\mathbf{S}_{XX}^{-1} n\bar{\mathbf{x}}\bar{y} - \frac{1}{n-1}\bar{\mathbf{x}}&#39;\mathbf{S}_{XX}^{-1}\mathbf{X}&#39;\mathbf{y}\\
  &amp;= \bar{y} + \frac{1}{n-1}\bar{\mathbf{x}}&#39;\mathbf{S}_{XX}^{-1} \left(n\bar{\mathbf{x}}\bar{y}  - \mathbf{X}&#39;\mathbf{y}\right)                              \\
  &amp;= \bar{y} - \frac{1}{n-1}\bar{\mathbf{x}}&#39;\mathbf{S}_{XX}^{-1} \left(\mathbf{X}&#39;\mathbf{y}- n\bar{\mathbf{x}}\bar{y} \right)                               \\
  &amp;= \bar{y} - \frac{1}{n-1}\bar{\mathbf{x}}&#39;\mathbf{S}_{XX}^{-1} (n-1)\mathbf{S}_{Xy}                                                           \\
  &amp;= \bar{y} - \bar{\mathbf{x}}&#39;\mathbf{S}_{XX}^{-1} \mathbf{S}_{Xy}                                                                             \\
  &amp;= \bar{y} - \bar{\mathbf{x}}&#39; \hat{\boldsymbol{\beta}}_{-0}
\end{align*}\]</span></p>
<p>Again, an equivalent result to that of simple linear regression. Like in simple
linear regression, centering the independent variables does affect the intercept
estimate, since <span class="math inline">\(\bar{\mathbf{x}}=\mathbf{0}\)</span>, we have that the coefficient after centering
the independent variables is <span class="math inline">\(\bar{y}\)</span> the mean of the dependent variable. And
if we also center the dependent variable, then <span class="math inline">\(\bar{y}=0\)</span> so the estimate of the
intercept is <span class="math inline">\(0\)</span> also. Therefore, if you are centering all variables, it is not
necessary to add the column of ones in the design matrix, since the estimate of the
intercept is <span class="math inline">\(0\)</span>.</p>
</div>
<div id="satandard-variables" class="section level3 hasAnchor" number="6.7.3">
<h3><span class="header-section-number">6.7.3</span> Satandard Variables<a href="multiple-linear-regression.html#satandard-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the same way we worked with centered variables, we can work with standard variables
to define the sample correlations.</p>
<p>The standardization of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{X}_s\)</span>, is given by:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{X}_s
  &amp;=
  \left[\begin{matrix}
    \frac{x_{11} - \bar{\mathbf{x}}_1}{S_{x_1x_1}^{1/2}} &amp; \frac{x_{12} - \bar{\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \dots  &amp; \frac{x_{1p} - \bar{\mathbf{x}}_p}{S_{x_px_p}^{1/2}} \\
    \frac{x_{21} - \bar{\mathbf{x}}_1}{S_{x_1x_1}^{1/2}} &amp; \frac{x_{22} - \bar{\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \dots  &amp; \frac{x_{2p} - \bar{\mathbf{x}}_p}{S_{x_px_p}^{1/2}} \\
    \vdots                                        &amp; \vdots                                        &amp; \ddots &amp; \vdots                                        \\
    \frac{x_{n1} - \bar{\mathbf{x}}_1}{S_{x_1x_1}^{1/2}} &amp; \frac{x_{n2} - \bar{\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \dots  &amp; \frac{x_{np} - \bar{\mathbf{x}}_p}{S_{x_px_p}^{1/2}}
  \end{matrix}\right] \\
  &amp;=
  \left[\begin{matrix}
    x_{11} - \bar{\mathbf{x}}_1 &amp; x_{12} - \bar{\mathbf{x}}_2 &amp; \dots  &amp; x_{1p} - \bar{\mathbf{x}}_p \\
    x_{21} - \bar{\mathbf{x}}_1 &amp; x_{22} - \bar{\mathbf{x}}_2 &amp; \dots  &amp; x_{2p} - \bar{\mathbf{x}}_p \\
    \vdots               &amp; \vdots               &amp; \ddots &amp; \vdots               \\
    x_{n1} - \bar{\mathbf{x}}_1 &amp; x_{n2} - \bar{\mathbf{x}}_2 &amp; \dots  &amp; x_{np} - \bar{\mathbf{x}}_p
  \end{matrix}\right]
  \left[\begin{matrix}
    \frac{1}{S_{x_1x_1}^{1/2}} &amp; 0                                             &amp; \dots  &amp; 0 \\
    0                          &amp; \frac{x_{22} - \bar{\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \dots  &amp; 0 \\
    \vdots                     &amp; \vdots                                        &amp; \ddots &amp; \vdots                                        \\
    0                          &amp; 0                                             &amp; \dots  &amp; \frac{x_{np} - \bar{\mathbf{x}}_p}{S_{x_px_p}^{1/2}}
  \end{matrix}\right]
  &amp;= \mathbf{X}_c \mathbf{D}_X
  &amp;= \mathbf{C}\mathbf{X}\mathbf{D}_X
\end{align*}\]</span></p>
<p>where</p>
<p><span class="math display">\[ \mathbf{D}_X =  \left[\begin{matrix}
    \frac{1}{S_{x_1x_1}^{1/2}} &amp; 0                                             &amp; \dots  &amp; 0 \\
    0                          &amp; \frac{x_{22} - \bar{\mathbf{x}}_2}{S_{x_2x_2}^{1/2}} &amp; \dots  &amp; 0 \\
    \vdots                     &amp; \vdots                                        &amp; \ddots &amp; \vdots                                        \\
    0                          &amp; 0                                             &amp; \dots  &amp; \frac{x_{np} - \bar{\mathbf{x}}_p}{S_{x_px_p}^{1/2}}
  \end{matrix}\right]\]</span></p>
<p>is the matrix that standardizes <span class="math inline">\(\mathbf{X}\)</span>. Notice that unlike <span class="math inline">\(\mathbf{C}\)</span>, that centers
any matrix with the appropriate number of rows, <span class="math inline">\(\mathbf{D}_X\)</span> only standardizes <span class="math inline">\(\mathbf{X}\)</span>.</p>
</div>
<div id="sample-correlation-matrix" class="section level3 hasAnchor" number="6.7.4">
<h3><span class="header-section-number">6.7.4</span> Sample Correlation Matrix<a href="multiple-linear-regression.html#sample-correlation-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We define the sample correlation matrix as:</p>
<p><span class="math display">\[ r_{XX} = \frac{\mathbf{X}_s&#39;\mathbf{X}_s}{n-1} \]</span>
where we break a little our notation convention of using bold capital letters for
matrices. Entry at row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> of <span class="math inline">\(r_{XX}\)</span> is given by:</p>
<p><span class="math display">\[ \left[r_{XX}\right]_{ij} = \frac{S_{x_i x_j}}{S_{x_ix_i}^{1/2}S_{x_jx_j}^{1/2}} \]</span></p>
<p>We can also define the sample correlation between <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> as follows:</p>
<p><span class="math display">\[r_{Xy} = \frac{\mathbf{X}_s&#39; \mathbf{y}_s}{n-1} \]</span></p>
<p>where <span class="math inline">\(\mathbf{y}_s\)</span> is the standardization of <span class="math inline">\(\mathbf{y}\)</span>.</p>
<p>With this definitions in hand, we can see how the coefficients look like with
standardized variables. Since, standardized variables are also centered, it is not
necessary to include the column of ones in the design matrix, as the intercept
estimate is always 0. Then the estimate of the coefficients of the independent
variables is given by:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}_s} = \left(\mathbf{X}_s&#39; \mathbf{X}_s \right)^{-1} \mathbf{X}_s&#39; \mathbf{y}_s = \frac{1}{n-1}r_{XX}^{-1}(n-1)r_{Xy}=r_{XX}^{-1}r_{Xy}\]</span>
Notice that the coefficients estimates do change for standardized variables, since
in general <span class="math inline">\(r_{XX}\neq\mathbf{S}_{XX}\)</span> and <span class="math inline">\(r_{Xy}\neq\mathbf{S}_{Xy}\)</span>. In the case of standardized
variables the coefficient estimates depend in part from the correlation between
independent variables <span class="math inline">\(r_{XX}\)</span> and the correlations between independent and
dependent variables <span class="math inline">\(r_{Xy}\)</span>.</p>
<p>Working with standardized variables is useful, since standardized variables are unit-less,
so the estimated coefficients magnitudes are comparable.</p>
<p>We can see these results in practice with our GDP data:</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="multiple-linear-regression.html#cb93-1" tabindex="-1"></a><span class="co"># Read Data</span></span>
<span id="cb93-2"><a href="multiple-linear-regression.html#cb93-2" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Gdp data.csv&quot;</span>)</span>
<span id="cb93-3"><a href="multiple-linear-regression.html#cb93-3" tabindex="-1"></a></span>
<span id="cb93-4"><a href="multiple-linear-regression.html#cb93-4" tabindex="-1"></a><span class="co"># Design Matrix Independent Variables</span></span>
<span id="cb93-5"><a href="multiple-linear-regression.html#cb93-5" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(dat[, <span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb93-6"><a href="multiple-linear-regression.html#cb93-6" tabindex="-1"></a><span class="co"># Dependent Variable</span></span>
<span id="cb93-7"><a href="multiple-linear-regression.html#cb93-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> dat<span class="sc">$</span>gdp</span>
<span id="cb93-8"><a href="multiple-linear-regression.html#cb93-8" tabindex="-1"></a><span class="co"># NUmber of Observations</span></span>
<span id="cb93-9"><a href="multiple-linear-regression.html#cb93-9" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb93-10"><a href="multiple-linear-regression.html#cb93-10" tabindex="-1"></a><span class="co"># Design Matrix with the column of Ones</span></span>
<span id="cb93-11"><a href="multiple-linear-regression.html#cb93-11" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, n), X)</span>
<span id="cb93-12"><a href="multiple-linear-regression.html#cb93-12" tabindex="-1"></a></span>
<span id="cb93-13"><a href="multiple-linear-regression.html#cb93-13" tabindex="-1"></a><span class="co"># Centering</span></span>
<span id="cb93-14"><a href="multiple-linear-regression.html#cb93-14" tabindex="-1"></a><span class="co"># Vector of Ones</span></span>
<span id="cb93-15"><a href="multiple-linear-regression.html#cb93-15" tabindex="-1"></a>v1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, n)</span>
<span id="cb93-16"><a href="multiple-linear-regression.html#cb93-16" tabindex="-1"></a><span class="co"># Centering Matrix</span></span>
<span id="cb93-17"><a href="multiple-linear-regression.html#cb93-17" tabindex="-1"></a>C  <span class="ot">&lt;-</span> <span class="fu">diag</span>(n) <span class="sc">-</span> (<span class="dv">1</span><span class="sc">/</span>n) <span class="sc">*</span> v1 <span class="sc">%*%</span> <span class="fu">t</span>(v1)</span>
<span id="cb93-18"><a href="multiple-linear-regression.html#cb93-18" tabindex="-1"></a><span class="co"># Independent Variables Centered</span></span>
<span id="cb93-19"><a href="multiple-linear-regression.html#cb93-19" tabindex="-1"></a>Xc <span class="ot">&lt;-</span> C <span class="sc">%*%</span> X</span>
<span id="cb93-20"><a href="multiple-linear-regression.html#cb93-20" tabindex="-1"></a><span class="co"># Dependent Variable Centered</span></span>
<span id="cb93-21"><a href="multiple-linear-regression.html#cb93-21" tabindex="-1"></a>yc <span class="ot">&lt;-</span> C <span class="sc">%*%</span> y</span>
<span id="cb93-22"><a href="multiple-linear-regression.html#cb93-22" tabindex="-1"></a><span class="co"># Design Matrix with Independent Variables Centered</span></span>
<span id="cb93-23"><a href="multiple-linear-regression.html#cb93-23" tabindex="-1"></a>Zc <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, n), Xc)</span>
<span id="cb93-24"><a href="multiple-linear-regression.html#cb93-24" tabindex="-1"></a></span>
<span id="cb93-25"><a href="multiple-linear-regression.html#cb93-25" tabindex="-1"></a><span class="co"># Checks that the Variables are actually centered</span></span>
<span id="cb93-26"><a href="multiple-linear-regression.html#cb93-26" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">colMeans</span>(Xc), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>## inf une int gov exp 
##   0   0   0   0   0</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="multiple-linear-regression.html#cb95-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">mean</span>(yc), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="multiple-linear-regression.html#cb97-1" tabindex="-1"></a><span class="co"># Shows the Mean of y</span></span>
<span id="cb97-2"><a href="multiple-linear-regression.html#cb97-2" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">mean</span>(y), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>## [1] 2.981131</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="multiple-linear-regression.html#cb99-1" tabindex="-1"></a><span class="co"># Compute the Estimates of the Coefficients with Original Variables</span></span>
<span id="cb99-2"><a href="multiple-linear-regression.html#cb99-2" tabindex="-1"></a>b   <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> Z, <span class="fu">t</span>(Z) <span class="sc">%*%</span> y)</span>
<span id="cb99-3"><a href="multiple-linear-regression.html#cb99-3" tabindex="-1"></a><span class="co"># Compute the Estimates of the Coefficients with Centered Independent Variables Only</span></span>
<span id="cb99-4"><a href="multiple-linear-regression.html#cb99-4" tabindex="-1"></a>bs1 <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Zc) <span class="sc">%*%</span> Zc, <span class="fu">t</span>(Zc) <span class="sc">%*%</span> y)</span>
<span id="cb99-5"><a href="multiple-linear-regression.html#cb99-5" tabindex="-1"></a><span class="co"># Compute the Estimates of the Coefficients with All Variables Centered</span></span>
<span id="cb99-6"><a href="multiple-linear-regression.html#cb99-6" tabindex="-1"></a>bs2 <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Zc) <span class="sc">%*%</span> Zc, <span class="fu">t</span>(Zc) <span class="sc">%*%</span> yc)</span>
<span id="cb99-7"><a href="multiple-linear-regression.html#cb99-7" tabindex="-1"></a><span class="co"># Compute the Estimates of the Coefficients with All variables Centered and no column of ones</span></span>
<span id="cb99-8"><a href="multiple-linear-regression.html#cb99-8" tabindex="-1"></a>bs3 <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Xc) <span class="sc">%*%</span> Xc, <span class="fu">t</span>(Xc) <span class="sc">%*%</span> yc)</span>
<span id="cb99-9"><a href="multiple-linear-regression.html#cb99-9" tabindex="-1"></a></span>
<span id="cb99-10"><a href="multiple-linear-regression.html#cb99-10" tabindex="-1"></a><span class="co"># Shows the Estimated Coefficients Side-by-Side</span></span>
<span id="cb99-11"><a href="multiple-linear-regression.html#cb99-11" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">cbind</span>(b, bs1, bs2, <span class="fu">c</span>(<span class="dv">0</span>, bs3)), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>##            [,1]        [,2]        [,3]        [,4]
##      1.55530096  2.98113131  0.00000000  0.00000000
## inf  0.31221813  0.31221813  0.31221813  0.31221813
## une -0.37733403 -0.37733403 -0.37733403 -0.37733403
## int  0.17782709  0.17782709  0.17782709  0.17782709
## gov -0.00848329 -0.00848329 -0.00848329 -0.00848329
## exp  0.06465692  0.06465692  0.06465692  0.06465692</code></pre>
<p>Here we can appreciate that the estimated coefficients for the independent variables
do not change, however the estimate for the intercept changes depending on if the
independent variables are centered or the dependent variable is centered of both. Also, notice that
this are the that we had using the <code>lm</code> function of <code>R</code>.</p>
<p>We can also check that computing the sample covariance matrix using our formula
results in the same quantities that using the <code>cov</code> function in <code>R</code>.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="multiple-linear-regression.html#cb101-1" tabindex="-1"></a><span class="co"># Sample Covariance of X</span></span>
<span id="cb101-2"><a href="multiple-linear-regression.html#cb101-2" tabindex="-1"></a>SXX <span class="ot">&lt;-</span> <span class="fu">t</span>(Xc) <span class="sc">%*%</span> Xc <span class="sc">/</span> (n<span class="dv">-1</span>)</span>
<span id="cb101-3"><a href="multiple-linear-regression.html#cb101-3" tabindex="-1"></a><span class="co"># Sample Covariance between X and y</span></span>
<span id="cb101-4"><a href="multiple-linear-regression.html#cb101-4" tabindex="-1"></a>SXy <span class="ot">&lt;-</span> <span class="fu">t</span>(Xc) <span class="sc">%*%</span> yc <span class="sc">/</span> (n<span class="dv">-1</span>)</span>
<span id="cb101-5"><a href="multiple-linear-regression.html#cb101-5" tabindex="-1"></a></span>
<span id="cb101-6"><a href="multiple-linear-regression.html#cb101-6" tabindex="-1"></a><span class="co"># Shows the comparison in covariance matrices</span></span>
<span id="cb101-7"><a href="multiple-linear-regression.html#cb101-7" tabindex="-1"></a><span class="fu">print</span>(SXX)</span></code></pre></div>
<pre><code>##             inf         une         int        gov        exp
## inf  4.03991957 -1.28576595  1.52551017  2.4374124 0.03447976
## une -1.28576595  0.66963355 -0.27282160 -0.5319862 0.02605596
## int  1.52551017 -0.27282160  0.83778484  0.7633037 0.04335484
## gov  2.43741235 -0.53198621  0.76330368 15.2006711 0.21732195
## exp  0.03447976  0.02605596  0.04335484  0.2173220 9.10237220</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="multiple-linear-regression.html#cb103-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">cov</span>(X))</span></code></pre></div>
<pre><code>##             inf         une         int        gov        exp
## inf  4.03991957 -1.28576595  1.52551017  2.4374124 0.03447976
## une -1.28576595  0.66963355 -0.27282160 -0.5319862 0.02605596
## int  1.52551017 -0.27282160  0.83778484  0.7633037 0.04335484
## gov  2.43741235 -0.53198621  0.76330368 15.2006711 0.21732195
## exp  0.03447976  0.02605596  0.04335484  0.2173220 9.10237220</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="multiple-linear-regression.html#cb105-1" tabindex="-1"></a><span class="co"># Shows the comparison in covariance vectors</span></span>
<span id="cb105-2"><a href="multiple-linear-regression.html#cb105-2" tabindex="-1"></a><span class="fu">print</span>(SXy)</span></code></pre></div>
<pre><code>##           [,1]
## inf  1.9993285
## une -0.6964324
## int  0.7245455
## gov  0.9825766
## exp  0.5953308</code></pre>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="multiple-linear-regression.html#cb107-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">cov</span>(X, y))</span></code></pre></div>
<pre><code>##           [,1]
## inf  1.9993285
## une -0.6964324
## int  0.7245455
## gov  0.9825766
## exp  0.5953308</code></pre>
<p>Finally, we can test our new formulas for the estimates:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="multiple-linear-regression.html#cb109-1" tabindex="-1"></a><span class="co"># Computes the estimates of the coefficients using the covariance matrices</span></span>
<span id="cb109-2"><a href="multiple-linear-regression.html#cb109-2" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> <span class="fu">solve</span>(SXX, SXy)</span>
<span id="cb109-3"><a href="multiple-linear-regression.html#cb109-3" tabindex="-1"></a>b0 <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) <span class="sc">-</span> <span class="fu">t</span>(<span class="fu">colMeans</span>(X)) <span class="sc">%*%</span> b1</span>
<span id="cb109-4"><a href="multiple-linear-regression.html#cb109-4" tabindex="-1"></a></span>
<span id="cb109-5"><a href="multiple-linear-regression.html#cb109-5" tabindex="-1"></a><span class="co"># Shows the Estimates Side by Side</span></span>
<span id="cb109-6"><a href="multiple-linear-regression.html#cb109-6" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">cbind</span>(b, <span class="fu">c</span>(b0, b1)), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>##            [,1]        [,2]
##      1.55530096  1.55530096
## inf  0.31221813  0.31221813
## une -0.37733403 -0.37733403
## int  0.17782709  0.17782709
## gov -0.00848329 -0.00848329
## exp  0.06465692  0.06465692</code></pre>
<p>In the same way, we can work with the sample correlations</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="multiple-linear-regression.html#cb111-1" tabindex="-1"></a><span class="co"># Standardizing matrix of X</span></span>
<span id="cb111-2"><a href="multiple-linear-regression.html#cb111-2" tabindex="-1"></a>DX <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">diag</span>(SXX)))</span>
<span id="cb111-3"><a href="multiple-linear-regression.html#cb111-3" tabindex="-1"></a><span class="co"># Standardize X</span></span>
<span id="cb111-4"><a href="multiple-linear-regression.html#cb111-4" tabindex="-1"></a>Xs <span class="ot">&lt;-</span> Xc <span class="sc">%*%</span> D</span>
<span id="cb111-5"><a href="multiple-linear-regression.html#cb111-5" tabindex="-1"></a><span class="co"># Shows that Xs is indeed standardize</span></span>
<span id="cb111-6"><a href="multiple-linear-regression.html#cb111-6" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">colMeans</span>(Xs), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>## [1] 0 0 0 0 0</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="multiple-linear-regression.html#cb113-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">apply</span>(<span class="at">X =</span> Xs, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> sd))</span></code></pre></div>
<pre><code>## [1] 1 1 1 1 1</code></pre>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="multiple-linear-regression.html#cb115-1" tabindex="-1"></a><span class="co"># Standardizes y</span></span>
<span id="cb115-2"><a href="multiple-linear-regression.html#cb115-2" tabindex="-1"></a>ys <span class="ot">&lt;-</span> yc <span class="sc">/</span> <span class="fu">sd</span>(y)</span>
<span id="cb115-3"><a href="multiple-linear-regression.html#cb115-3" tabindex="-1"></a><span class="co"># Shows that ys is standardized</span></span>
<span id="cb115-4"><a href="multiple-linear-regression.html#cb115-4" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">mean</span>(ys), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="multiple-linear-regression.html#cb117-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">sd</span>(ys), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="multiple-linear-regression.html#cb119-1" tabindex="-1"></a><span class="co"># Sample Correlation of X</span></span>
<span id="cb119-2"><a href="multiple-linear-regression.html#cb119-2" tabindex="-1"></a>rXX <span class="ot">&lt;-</span> <span class="fu">t</span>(Xs) <span class="sc">%*%</span> Xs <span class="sc">/</span> (n<span class="dv">-1</span>)</span>
<span id="cb119-3"><a href="multiple-linear-regression.html#cb119-3" tabindex="-1"></a><span class="co"># Sample Correlation between X and y</span></span>
<span id="cb119-4"><a href="multiple-linear-regression.html#cb119-4" tabindex="-1"></a>rXy <span class="ot">&lt;-</span> <span class="fu">t</span>(Xs) <span class="sc">%*%</span> ys <span class="sc">/</span> (n<span class="dv">-1</span>)</span>
<span id="cb119-5"><a href="multiple-linear-regression.html#cb119-5" tabindex="-1"></a></span>
<span id="cb119-6"><a href="multiple-linear-regression.html#cb119-6" tabindex="-1"></a><span class="co"># Shows the comparison in correlation matrices</span></span>
<span id="cb119-7"><a href="multiple-linear-regression.html#cb119-7" tabindex="-1"></a><span class="fu">print</span>(rXX)</span></code></pre></div>
<pre><code>##              [,1]        [,2]       [,3]        [,4]        [,5]
## [1,]  1.000000000 -0.78173033  0.8292061  0.31103644 0.005685918
## [2,] -0.781730327  1.00000000 -0.3642453 -0.16674407 0.010553855
## [3,]  0.829206121 -0.36424525  1.0000000  0.21389456 0.015699798
## [4,]  0.311036436 -0.16674407  0.2138946  1.00000000 0.018475446
## [5,]  0.005685918  0.01055386  0.0156998  0.01847545 1.000000000</code></pre>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="multiple-linear-regression.html#cb121-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">cor</span>(X))</span></code></pre></div>
<pre><code>##              inf         une        int         gov         exp
## inf  1.000000000 -0.78173033  0.8292061  0.31103644 0.005685918
## une -0.781730327  1.00000000 -0.3642453 -0.16674407 0.010553855
## int  0.829206121 -0.36424525  1.0000000  0.21389456 0.015699798
## gov  0.311036436 -0.16674407  0.2138946  1.00000000 0.018475446
## exp  0.005685918  0.01055386  0.0156998  0.01847545 1.000000000</code></pre>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="multiple-linear-regression.html#cb123-1" tabindex="-1"></a><span class="co"># Shows the comparison in correlation vectors</span></span>
<span id="cb123-2"><a href="multiple-linear-regression.html#cb123-2" tabindex="-1"></a><span class="fu">print</span>(rXy)</span></code></pre></div>
<pre><code>##            [,1]
## [1,]  0.8751311
## [2,] -0.7487479
## [3,]  0.6964256
## [4,]  0.2217228
## [5,]  0.1736027</code></pre>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="multiple-linear-regression.html#cb125-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">cor</span>(X, y))</span></code></pre></div>
<pre><code>##           [,1]
## inf  0.8751311
## une -0.7487479
## int  0.6964256
## gov  0.2217228
## exp  0.1736027</code></pre>
<p>and can test that:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}_s} = r_{XX}^{-1}r_{Xy}\]</span></p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="multiple-linear-regression.html#cb127-1" tabindex="-1"></a><span class="co"># Computes the estimates of the coefficients using the covariance matrices</span></span>
<span id="cb127-2"><a href="multiple-linear-regression.html#cb127-2" tabindex="-1"></a>bs1 <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Xs) <span class="sc">%*%</span> Xs, <span class="fu">t</span>(Xs) <span class="sc">%*%</span> ys)</span>
<span id="cb127-3"><a href="multiple-linear-regression.html#cb127-3" tabindex="-1"></a>bs2 <span class="ot">&lt;-</span> <span class="fu">solve</span>(rXX, rXy)</span>
<span id="cb127-4"><a href="multiple-linear-regression.html#cb127-4" tabindex="-1"></a></span>
<span id="cb127-5"><a href="multiple-linear-regression.html#cb127-5" tabindex="-1"></a><span class="co"># Shows the Estimates Side by Side</span></span>
<span id="cb127-6"><a href="multiple-linear-regression.html#cb127-6" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">cbind</span>(bs1, bs2), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>##             [,1]        [,2]
## [1,]  0.55210259  0.55210259
## [2,] -0.27165637 -0.27165637
## [3,]  0.14319884  0.14319884
## [4,] -0.02909852 -0.02909852
## [5,]  0.17161988  0.17161988</code></pre>
<p>We can also contrast the effects of the standarization on the coefficients</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="multiple-linear-regression.html#cb129-1" tabindex="-1"></a><span class="co"># Computes the estimates of the coefficients using the covariance matrices</span></span>
<span id="cb129-2"><a href="multiple-linear-regression.html#cb129-2" tabindex="-1"></a>bs1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X)<span class="sc">$</span>coefficients</span>
<span id="cb129-3"><a href="multiple-linear-regression.html#cb129-3" tabindex="-1"></a>bs2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(ys <span class="sc">~</span> Xs)<span class="sc">$</span>coefficients</span>
<span id="cb129-4"><a href="multiple-linear-regression.html#cb129-4" tabindex="-1"></a></span>
<span id="cb129-5"><a href="multiple-linear-regression.html#cb129-5" tabindex="-1"></a><span class="co"># Shows the Estimates Side by Side</span></span>
<span id="cb129-6"><a href="multiple-linear-regression.html#cb129-6" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(<span class="fu">cbind</span>(bs1, bs2), <span class="dv">8</span>))</span></code></pre></div>
<pre><code>##                     bs1         bs2
## (Intercept)  1.55530096  0.00000000
## Xinf         0.31221813  0.55210259
## Xune        -0.37733403 -0.27165637
## Xint         0.17782709  0.14319884
## Xgov        -0.00848329 -0.02909852
## Xexp         0.06465692  0.17161988</code></pre>
<p>First, we can observe the 0 intercept estimate on the standardize values, so it
is not necessary to estimate it, we could have done so by using <code>lm(ys ~ Xs - 1)</code>
instead of <code>lm(ys ~ Xs)</code>. Next we observe, the change in magnitudes for the
estimated coefficients of the independent variables.</p>
</div>
</div>
<div id="variable-cross-effects" class="section level2 hasAnchor" number="6.8">
<h2><span class="header-section-number">6.8</span> Variable Cross-Effects<a href="multiple-linear-regression.html#variable-cross-effects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this sub-section we will work with standardized values so there is no need to
estimate the intercept. Since all variables are standardized, we will not use
the <span class="math inline">\(\mathbf{X}_s\)</span> notation, but instead only <span class="math inline">\(\mathbf{X}\)</span>, to make notation less confusing.</p>
<p>The same goes with <span class="math inline">\(\hat{\boldsymbol{\beta}}_s\)</span>, it will be only <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</p>
<p>The idea is to analyze the estimated coefficients when you divide the independent
variables in to groups <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>. So we can divide the design matrix in two:</p>
<p><span class="math display">\[\mathbf{X}= [\mathbf{X}_1 \mathbf{X}_2]\]</span>
Then, we can compute the coefficient estimates:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
  \left[\begin{matrix}
    \hat{\boldsymbol{\beta}}_1 \\
    \hat{\boldsymbol{\beta}}_2  
  \end{matrix}\right] =
  \left([\mathbf{X}_1 \mathbf{X}_2]&#39;[\mathbf{X}_1 \mathbf{X}_2]\right)^{-1}[\mathbf{X}_1 \mathbf{X}_2]&#39; \mathbf{y}\]</span></p>
<p>So, we can work with these computations in the same way we did before:</p>
<p><span class="math display">\[\begin{align*}
[\mathbf{X}_1 \mathbf{X}_2]&#39;[\mathbf{X}_1 \mathbf{X}_2]
  &amp;=
  \left[\begin{matrix}
    \mathbf{X}_1&#39; \\
    \mathbf{X}_2&#39;  
  \end{matrix}\right] [\mathbf{X}_1 \mathbf{X}_2] \\
  &amp;=
  \left[\begin{matrix}
    \mathbf{X}_1&#39;\mathbf{X}_1 &amp; \mathbf{X}_1&#39;\mathbf{X}_2 \\
    \mathbf{X}_2&#39;\mathbf{X}_1 &amp; \mathbf{X}_2&#39;\mathbf{X}_2  
  \end{matrix}\right] \\
  &amp;= \frac{1}{n-1}
  \left[\begin{matrix}
    \frac{\mathbf{X}_1&#39;\mathbf{X}_1}{n-1} &amp; \frac{\mathbf{X}_1&#39;\mathbf{X}_2}{n-1} \\
    \frac{\mathbf{X}_2&#39;\mathbf{X}_1}{n-1} &amp; \frac{\mathbf{X}_2&#39;\mathbf{X}_2}{n-1}  
  \end{matrix}\right] \\
  &amp;= \frac{1}{n-1}
  \left[\begin{matrix}
    r_{X_1X_1} &amp; r_{X_1X_2} \\
    r_{X_2X_1} &amp; r_{X_2X_2}  
  \end{matrix}\right]
\end{align*}\]</span></p>
<p>In the same way, we have that:</p>
<p><span class="math display">\[\begin{align*}
[\mathbf{X}_1 \mathbf{X}_2]&#39;\mathbf{y}
  &amp;=
  \left[\begin{matrix}
    \mathbf{X}_1&#39; \\
    \mathbf{X}_2&#39;  
  \end{matrix}\right] \mathbf{y}\\
  &amp;=
  \left[\begin{matrix}
    \mathbf{X}_1&#39;\mathbf{y}\\
    \mathbf{X}_2&#39;\mathbf{y}
  \end{matrix}\right] \\
  &amp;= \frac{1}{n-1}
  \left[\begin{matrix}
    \frac{\mathbf{X}_1&#39;\mathbf{y}}{n-1} \\
    \frac{\mathbf{X}_2&#39;\mathbf{y}}{n-1}   
  \end{matrix}\right] \\
  &amp;= \frac{1}{n-1}
  \left[\begin{matrix}
    r_{X_1y} \\
    r_{X_2y}  
  \end{matrix}\right]
\end{align*}\]</span></p>
<p>Then we have that:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} =
  \left[\begin{matrix}
    \hat{\boldsymbol{\beta}}_1 \\
    \hat{\boldsymbol{\beta}}_2  
  \end{matrix}\right] =
  \left(\frac{1}{n-1}
  \left[\begin{matrix}
    r_{X_1X_1} &amp; r_{X_1X_2} \\
    r_{X_2X_1} &amp; r_{X_2X_2}  
  \end{matrix}\right]\right)^{-1} \frac{1}{n-1}
  \left[\begin{matrix}
    r_{Xy,1} \\
    r_{Xy,2}  
  \end{matrix}\right] =
  \left[\begin{matrix}
    r_{X_1X_1} &amp; r_{X_1X_2} \\
    r_{X_2X_1} &amp; r_{X_2X_2}  
  \end{matrix}\right]^{-1}  
  \left[\begin{matrix}
    r_{X_1y} \\
    r_{X_2y}  
  \end{matrix}\right]
\]</span></p>
<p>Now we can compute the inverse of the 2 by 2 block matrix as before with, but
first we define:</p>
<p><span class="math display">\[r_{X_1|X_2} = r_{X_1X_1} - r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2X_1}\]</span>
this is also, the Schur component. This can also be seen as the sample correlation
matrix of <span class="math inline">\(X_1\)</span> after accounting by the relationships with <span class="math inline">\(X_2\)</span>.</p>
<p><span class="math display">\[
\left[\begin{matrix}
    r_{XX,11} &amp; r_{XX,12} \\
    r_{XX,12} &amp; r_{XX,22}  
  \end{matrix}\right]^{-1} =
\left[\begin{matrix}
                              r_{X_1|X_2}^{-1} &amp;                         -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1} \\
    -r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1} &amp; r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}  \end{matrix}\right]
\]</span>
Then, we have that:</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}
  &amp;=
  \left[\begin{matrix}
    \hat{\boldsymbol{\beta}}_1 \\
    \hat{\boldsymbol{\beta}}_2  
  \end{matrix}\right] \\
  &amp;=
  \left[\begin{matrix}
                              r_{X_1|X_2}^{-1} &amp;                         -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1} \\
    -r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1} &amp; r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}
  \end{matrix}\right]
  \left[\begin{matrix}
    r_{X_1y} \\
    r_{X_2y}  
  \end{matrix}\right] \\
  &amp;=
  \left[\begin{matrix}
    r_{X_1|X_2}^{-1}r_{X_1y} -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y} \\
    -r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1y} + r_{X_2X_2}^{-1}r_{X_2X_1}r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y}
  \end{matrix}\right]
\end{align*}\]</span></p>
<p>Since the variables in the groupings 1 and 2 can be switched and have no special
characteristics, we only need to analyze the structure of <span class="math inline">\(\hat{\boldsymbol{\beta}}_1\)</span> in relation
to the variables of group 2, the results would be analogous for <span class="math inline">\(\hat{\boldsymbol{\beta}}_1\)</span>.</p>
<p>Then we have that:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}_1
  = r_{X_1|X_2}^{-1}r_{X_1y} -r_{X_1|X_2}^{-1}r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y}
  = r_{X_1|X_2}^{-1}\left(r_{X_1y} -r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2y}\right)
\]</span>
Now, suppose that we want to fit the following linear models:</p>
<p><span class="math display">\[\mathbf{y}= \mathbf{X}_1 \boldsymbol{\beta}_1 + \mathbf{e}\quad \text{and} \quad \mathbf{y}= \mathbf{X}_2 \boldsymbol{\beta}_2 + \mathbf{e}\]</span>
that is a linear model of <span class="math inline">\(\mathbf{y}\)</span> using only the variables in group 1 for one model
and only variables from the group 2 in the second model. Then,
the coefficient estimates will be:</p>
<p><span class="math display">\[\tilde{\boldsymbol{\beta}}_1 = r_{X_1X_1}^{-1}r_{X_1y} \quad \text{and} \quad \tilde{\boldsymbol{\beta}}_2 = r_{X_2X_2}^{-1}r_{X_2y}\]</span>
Note that, in general, this estimates will be different to the estimates using
all the variables, that is:</p>
<p><span class="math display">\[\tilde{\boldsymbol{\beta}}_1 \neq \hat{\boldsymbol{\beta}}_1 \quad \text{and} \quad \tilde{\boldsymbol{\beta}}_2 \neq \hat{\boldsymbol{\beta}}_2\]</span>
Then, we can re-write, our coefficient estimate for <span class="math inline">\(\boldsymbol{\beta}_1\)</span> as follows:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}_1
  = r_{X_1|X_2}^{-1}\left(r_{X_1y} -r_{X_1X_2}\tilde{\boldsymbol{\beta}}_2\right)
\]</span>
Now, if <span class="math inline">\(r_{X_1X_2} = \mathbf{0}\)</span>, that is the variables in group 1 and group 2 are
uncorrelated, then</p>
<p><span class="math display">\[r_{X_1|X_2} = r_{X_1X_1} - r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2X_1} = r_{X_1X_1}\]</span>
so:
<span class="math display">\[\hat{\boldsymbol{\beta}}_1
  = r_{X_1|X_2}^{-1}\left(r_{X_1y} -r_{X_1X_2}\tilde{\boldsymbol{\beta}}_2\right)
  = r_{X_1X_1}^{-1}r_{X_1y}
  = \tilde{\boldsymbol{\beta}}_1\]</span>
So, when the variables in 1 group are uncorrelated with the other the coefficient
estimates are the same for the full and partial model.</p>
<p>We can also deduce that even when the sample correlation between
<span class="math inline">\(X_1\)</span> and <span class="math inline">\(y\)</span> is <span class="math inline">\(\mathbf{0}\)</span>, that is <span class="math inline">\(r_{X_1y} = \mathbf{0}\)</span>, the estimated coefficients
will not be <span class="math inline">\(\mathbf{0}\)</span> in general. In fact, we have that:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}_1
  = -r_{X_1|X_2}^{-1}r_{X_1X_2}\tilde{\boldsymbol{\beta}}_2
\]</span></p>
<div id="single-variable-cross-effects" class="section level3 hasAnchor" number="6.8.1">
<h3><span class="header-section-number">6.8.1</span> Single Variable Cross-Effects<a href="multiple-linear-regression.html#single-variable-cross-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the special case where group 1 consists of only 1 variable, we have that:</p>
<p><span class="math inline">\(\hat{\beta}_1, \quad \tilde{\beta}_1, \quad r_{X_1X_1} = r_{x_1x_1}=1, \quad r_{X_1|X_2}=r_{x_1|X_2}, \quad r_{X_1y}=r_{x_1y}\)</span></p>
<p>are scalars, while</p>
<p><span class="math display">\[r_{X_1X_2} = r_{x_1X_2}=r_{X_2X_1}&#39; = r_{X_2x_1}&#39;, \quad r_{X_2y}, \quad \tilde{\boldsymbol{\beta}}_2\]</span>
are vectors of size <span class="math inline">\(p-1\)</span>.</p>
<p>Now, consider the following linear model:</p>
<p><span class="math display">\[\mathbf{x}_1 = \mathbf{X}_2 \boldsymbol{\alpha}_2 + \mathbf{e}\]</span></p>
<p>that is, fitting the single variable in group 1, as the dependent variable, and
the variables in group 2, as the independent variables. Then we have that the
estimated coefficients are:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\alpha}}_2 = r_{X_2X_2}^{-1}r_{X_2x_1}\]</span>
then, we notice the following</p>
<p><span class="math display">\[r_{x_1|X_2}
  = r_{X_1X_1} - r_{X_1X_2}r_{X_2X_2}^{-1}r_{X_2X_1}
  = r_{x_1X_1} - r_{x_1X_2}r_{X_2X_2}^{-1}r_{X_2x_1}
  = 1 - r_{x_1X_2}\hat{\boldsymbol{\alpha}}_2
  = 1 - r_{x_1X_2}r_{X_2X_2}^{-1}r_{X_2X_2}\hat{\boldsymbol{\alpha}}_2
  = 1 - \hat{\boldsymbol{\alpha}}_2&#39;r_{X_2X_2}\hat{\boldsymbol{\alpha}}_2
  = 1 - R^2_1
\]</span></p>
<p>where, <span class="math inline">\(R^2_1\)</span> is the multiple coefficient of determination for model:</p>
<p><span class="math display">\[\mathbf{x}_1 = \mathbf{X}_2 \boldsymbol{\alpha}_2 + \mathbf{e}\]</span></p>
<p>that is, how much of <span class="math inline">\(\mathbf{x}_1\)</span> is explained by <span class="math inline">\(\mathbf{X}_2\)</span>.</p>
<p>Then, we have that:</p>
<p><span class="math display">\[\hat{\beta}_1 = r_{1_1|X_2}^{-1}\left(r_{x_1y} -r_{x_1X_2}\tilde{\boldsymbol{\beta}}_2\right) = \frac{1}{1-R^2_1} \left(r_{x_1y} -r_{x_1X_2}\tilde{\boldsymbol{\beta}}_2\right)\]</span>
We name <span class="math inline">\(\frac{1}{1 - R^2_1}\)</span> as:</p>
<p><span class="math display">\[VIF_1 = \frac{1}{1 - R^2_1} \]</span>
the bigger, the more distorted is the value of <span class="math inline">\(\hat{\beta}_1\)</span>. This number increases
as <span class="math inline">\(R^2_1\)</span> in creases.</p>
<p>If <span class="math inline">\(R^2_1 = 0\)</span>, that is <span class="math inline">\(\mathbf{X}_2\)</span> explains nothing of <span class="math inline">\(\mathbf{x}_1\)</span>, then</p>
<p><span class="math display">\[ VIF_1 = 1 \]</span>
that represents no distortion. If <span class="math inline">\(R^2_1 \to 1\)</span>, then:</p>
<p><span class="math display">\[ VIF_1 \to 1 \]</span>
which represents infinite distortion.</p>
<p>The name <span class="math inline">\(VIF\)</span> stands for variance inflation factor. We will see that later, how
it relates to the variance of the estimate <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p>Of course, variable 1, didnâ€™t have any particular property, so we can generalize
this to any variable <span class="math inline">\(k\)</span>.</p>
<p>On the other hand, we also have an effect <span class="math inline">\(r_{x_1X_2}\tilde{\boldsymbol{\beta}}_2\)</span> on the
coefficient</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="polynomial-regression.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
