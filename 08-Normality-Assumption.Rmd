# Normality Assumption

## Introduction

To the mean and covariance assumptions, we can add the normality assumption. This
is a very strong and powerful assumption, that will enable us to obtain the 
distribution of  our data and several of our estimates.

We will assume:

$$\be \sim N(\bzero, \sigma^2 \bI)$$

Also, note that the Normal distribution is completely characterized by its 
mean and variance, so if the distribution of our estimates is normal, we will
already have the information to completely characterize their distribution since
we have computed the mean and variance of the estimates in the previous chapter.

Another thing, that assuming normality allows is to obtain the likelihood of our
data, and in this way obtain Maximum Likelihood Estimates (MLE) of the parameters
we have introduced $\bgb$ and $\sigma^2$.

## Maximum Likelihood Estimation

In order to perform the maximum likelihood estimates of $\bgb$ and $\sigma^2$, we
need the distribution of $\by$. This is very easy to obtain, since:

$$\by = \bX \bgb + \be$$
is a linear combination of $\be$ (in fact is just a translation of $\be$), and
therefore it is normally distributed. Since we have already computed its mean 
and variance in the previous chapter, using those computations, we can conclude
that:

$$\by \sim N(\bX \bgb, \sigma^2 \bI)$$
This means that the likelihood of $\bgb$ and $\sigma^2$ is given by:

$$ \cL(\bgb, \sigma^2 | \by ) = N(\by | \bX \by, \sigma^2 \bI) $$
Then this means that:

\begin{align*}
  \cL(\bgb, \sigma^2 | \by )
    &= (2 \pi)^{-\frac{n}{2}} |\sigma^2 \bI|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2}(\by - \bX \bgb)'(\sigma^2 \bI)^{-1}(\by - \bX \bgb) \right\}    \\
    &= (2 \pi)^{-\frac{n}{2}} (\sigma^2)^{-\frac{n}{2}} |\bI| \exp\left\{ -\frac{1}{2}(\by - \bX \bgb)'\frac{\bI}{\sigma^2}(\by - \bX \bgb) \right\} \\
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\by - \bX \bgb)'(\by - \bX \bgb) \right\}                                   \\
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\by - \bX \bgb)'(\by - \bX \bgb) \right\}                                   \\
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\by - \bX \bgb)'(\by - \bX \bgb) \right\}                                   \\
\end{align*}

Now recalling from:

$$(\by - \dE[\by])'(\by - \dE[\by]) = \hat{\be}'\hat{\be} + (\hat{\by} - \dE[\hat{\by}])'(\hat{\by} - \dE[\hat{\by}])$$
that is:

$$(\by - \bX \bgb)'(\by - \bX \bgb) = \hat{\be}'\hat{\be} + (\bX \hat{\bgb} - \bX \bgb)'(\bX \hat{\bgb} - \bX \bgb) = \hat{\be}'\hat{\be} + (\hat{\bgb} - \bgb)'\bX \bX( \hat{\bgb} - \bgb)$$

then, the likelihood can be written as:

\begin{align*}
  \cL(\bgb, \sigma^2 | \by )
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\by - \bX \bgb)'(\by - \bX \bgb) \right\}                                                      \\
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{\hat{\be}'\hat{\be}}{2 \sigma^2} -\frac{(\hat{\bgb} - \bgb)'\bX \bX( \hat{\bgb} - \bgb)}{2 \sigma^2} \right\} \\
\end{align*}

This is a useful way to write the likelihood, since it is easy to optimize with
respect to $\bgb$. Since, independently of the value of $\sigma^2$, the value of
$\bgb$ that maximizes the likelihood is the OLS estimator $\hat{\bgb}$, since it
makes zero the following term:

$$ -\frac{(\hat{\bgb} - \bgb)'\bX \bX( \hat{\bgb} - \bgb)}{2 \sigma^2} $$
In this way, we only need to maximize the likelihood with respect to $\sigma^2$,
as the following marginal likelihood:

$$ \cL(\sigma^2 | \by, \bgb = \hat{\bgb} ) =  (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{\hat{\be}'\hat{\be}}{2 \sigma^2} \right\}$$
Instead of maximizing the marginal likelihood directly, we will maximize the 
marginal log-likelihood:

$$ \ell(\sigma^2 | \by, \bgb = \hat{\bgb}) = -\frac{n}{2} \log(2 \pi) -\frac{n}{2} \log(\sigma^2) -\frac{\hat{\be}'\hat{\be}}{2 \sigma^2}$$
We can do this maximization, by taking the derivative:

\begin{align*}
  \frac{d \ell}{d \sigma^2} 
    &=        -\frac{n}{2 \sigma^2} + \frac{\hat{\be}'\hat{\be}}{2 (\sigma^2)^2} 
\end{align*}

Then

\begin{align*}
  \frac{d \ell}{d \sigma^2} =0
    &\implies -\frac{n}{2 \sigma^2} + \frac{\hat{\be}'\hat{\be}}{2 (\sigma^2)^2} = 0 \\
    &\implies -n + \frac{\hat{\be}'\hat{\be}}{\sigma^2} = 0                          \\
    &\implies \frac{\hat{\be}'\hat{\be}}{\sigma^2} = n                               \\
    &\implies \sigma^2 = \frac{\hat{\be}'\hat{\be}}{n}                               \\
\end{align*}

So we have, that the Maximum Likelihood Estimate of $\sigma^2$ is given by:

$$\tilde{\sigma}^2 = \frac{\hat{\be}'\hat{\be}}{n}$$

And, taking the second derivative to confirm it is a maximum we have that:

\begin{align*}
  \frac{d^2 \ell}{d (\sigma^2)^2} \bigg|_{\sigma^2 = \tilde{\sigma}^2}
    &= \frac{n}{2 (\tilde{\sigma}^2)^2} - \frac{\hat{\be}'\hat{\be}}{(\tilde{\sigma}^2)^3} \\
    &= \frac{n}{2 (\tilde{\sigma}^2)^2} - \frac{n}{(\tilde{\sigma}^2)^2}                   \\
    &= - \frac{n}{(2 \tilde{\sigma}^2)^2}                                                  \\
    &\leq 0                                                  
\end{align*}

So $\tilde{\sigma}^2$ is indeed maximizing the marginal likelihood.

Note that, unlike with the $\bgb$ parameter, our OLS estimate $\hat{\sigma}^2$
of $\sigma^2$ is different to the MLE estimator $\tilde{\sigma}^2$. In particular
$\tilde{\sigma}^2$ is biased.

This doesn't mean that one estimate is better than the other, they just have 
different properties.

We will work more with $\hat{\sigma}^2$ than $\tilde{\sigma}^2$, since it is more
useful to build certain statistics.

## Distribution of Estimates

In the same way that in the last chapters we computed the mean and the variance 
of our estimates, we can obtain the distribution of the estimates:

### Distribution of $\hat{\bgb}$, $\hat{\by}$ and $\hat{\be}$

Most of our estimates are linear combinations of $\by$, so they are normally 
distributed and we can use the mean and variances computed in the last chapter
to fully characterize their distributions. This is the case for:

$$ \hat{\bgb}, \quad \hat{\by}, \quad \hat{\be} $$
Their distributions are:

$$ \hat{\bgb} \sim N(\bgb, \sigma^2 (\bX'\bX)^{-1}) $$
$$ \hat{\by} \sim N(\bX \bgb, \sigma^2 \bH) $$
$$ \hat{\be} \sim N(\bzero, \sigma^2 (\bI-\bH)) $$

This is not the case for the estimate $\hat{\sigma}^2$, since it is not a linear
transformation of $\by$.

### Distribution of $\hat{\sigma}^2$

Obtaining the distribution of $\hat{\sigma}^2$ is not as straight forward. We
will use 3 steps:

1. Express $\frac{\hat{\be}'\hat{\be}}{\sigma^2}$ as quadratic form of standard
normal with an idempotent matrix.
2. Show that the quadratic form of standard normal with an idempotent matrix is
distributed as a chi squared.
3. Relate the distribution of $\frac{\hat{\be}'\hat{\be}}{\sigma^2}$ to the 
distribution of $\hat{\sigma}^2$.

#### Distribution of $\hat{\sigma}^2$ Step 1

First, note that:

\begin{align*}
  (\bI - \bH) \by
    &= (\bI - \bH) (\bX \bgb + \be)                    && \text{since $\by = \bX \bgb + \be$}  \\
    &= \bI \bX \bgb - \bH \bX \bgb + \bI \be - \bH \be                                         \\
    &= \bX \bgb - \bX \bgb + \be - \bH \be             && \text{since $\bH \bX = \bX$}         \\
    &= \be - \bH \be                                                                           \\
    &= (\bI - \bH) \be                                                                         \\
\end{align*}

Then:

\begin{align*}
  \by' (\bI - \bH) \by
    &= \by' (\bI - \bH) (\bI - \bH) \by  && \text{since $(\bI - \bH)$ is idempotent}         \\
    &= \be' (\bI - \bH) (\bI - \bH) \be  && \text{since $(\bI - \bH) \by = (\bI - \bH) \be$} \\
    &= \be' (\bI - \bH) \be              && \text{since $(\bI - \bH)$ is idempotent}         \\
\end{align*}

Then,

\begin{align*}
  \frac{\hat{\be}'\hat{\be}}{\sigma^2}
    &= \frac{\by' (\bI - \bH) \by}{\sigma^2}  && \text{since $\hat{\be}'\hat{\be} = \by' (\bI - \bH) \by$}  \\
    &= \frac{\be' (\bI - \bH) \be}{\sigma^2}  && \text{since $\be' (\bI - \bH) \be = \by' (\bI - \bH) \by$} \\
    &= \frac{\be}{\sqrt{\sigma^2}}' (\bI - \bH) \frac{\be}{\sqrt{\sigma^2}}                                 \\
\end{align*}

Finally, note that $\frac{\be}{\sqrt{\sigma^2}}$ is a linear function of $\be$ 
that is normal, therefore it is normal also, with mean:

$$ \dE\left[\frac{\be}{\sqrt{\sigma^2}}\right] =  \frac{1}{\sqrt{\sigma^2}} \dE[\be] = \frac{1}{\sqrt{\sigma^2}} \bzero = \bzero$$
$$ \dV\left[\frac{\be}{\sqrt{\sigma^2}}\right] =  \left(\frac{1}{\sqrt{\sigma^2}}\right)^2 \dV[\be] = \frac{1}{\sigma^2} \sigma^2 \bI = \bI$$
So

$$ \frac{\be}{\sqrt{\sigma^2}} \sim N(\bzero, \bI) $$
is a multivariate standard normal. This concludes step 1.

#### Distribution of $\hat{\sigma}^2$ Step 2

Let $\bz \in \dR^{n}$ a multivariate standard normal and $\bM \in \dR^{n \times n}$
and idempotent matrix of rank $m$. Then, we will show that:

$$ \bz' \bM \bz \sim \chi^2_{m} $$

To show this result, we will use the spectral decomposition of $\bM$, that is:

$$ \bM = \bV \bgS \bV' $$

with $\bV$ orthonormal and $\bgS$ diagonal. Since $\bV = [\bv_1,\ldots,\bv_n]$ is
orthonormal, then we have that:

$$ \bv_i'\bv_j = 0 \quad \forall i \neq j \quad \text{and} \quad ||\bv_i||^2_2 = 1 \quad \forall i$$
and, since $\bM$ is idempotent, then $\bgS$ is diagonal with exactly $m$ entries
equal to $1$ and the rest equal to $0$. Without loss of generality, we
can assume that the first $m$ entries of the diagonal are equal to $1$ and the 
next entries equal to $0$.

Then, first note that $\bV' \bz$ is a linear combination of a normal distribution.
We will show, that $\bV' \bz \in \dR^{n}$ is also standard normal.

Note that:

$$ \dE[\bV' \bz] =  \bV' \dE[\bz] = \bV' \bzero = \bzero$$
$$ \dV[\bV' \bz] =  \bV' \dV[\bz] \bV = \bV' \bI \bV = \bV' \bV = \bI$$

Then $\bV' \bz$ is also standard normal. Let's name $\bw = \bV' \bz$, then each of
the components $w_1,\ldots,w_n$ of $\bw$ are independent univariate standard normally
distributed.

Then

\begin{align*}
  \bz' \bM \bz
    &= \bz' (\bV \bgS \bV') \bz       && \text{using the spectral decomposition of $\bM$}         \\
    &= (\bV' \bz)' \bgS (\bV' \bz)                                                                \\
    &= \bw' \bgS \bw                  && \text{since $\bw = \bV' \bz$}                            \\
    &= \sum_{i=1}^n [\bgS]_{ii} w_i^2                                                             \\
    &= \sum_{i=1}^{m} w_i^2           && \text{since only the first $m$ entries are equal to $1$} \\
    &\sim \chi^2_m                    && \text{by definition of the $\chi^2$ distribution}        \\
\end{align*} 

#### Distribution of $\hat{\sigma}^2$ Step 3

Using step 1 and 2, we can conclude that:

$$ \frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \chi^2_{n-p} $$
since the rank of the idempotent matrix $(\bI - \bH)$ is $n-p$. Since:

$$ \hat{\sigma}^2 = \frac{\hat{\be}'\hat{\be}}{n-p} = \frac{\sigma^2}{n-p}\frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \frac{\sigma^2}{n-p}\chi^2_{n-p} $$

### Independence of $\hat{\be}$ and $\hat{\by}$

We have seen that $\hat{\be}$ and $\hat{\by}$ are uncorrelated, that is

$$ \dC[\hat{\be}, \hat{\by}] = \bzero $$
however, this doesn't necessarily mean they are independent. Independence is a 
much more stronger property. For example, if two random variables are independent
then any function of this random variables will be independent. However, if two 
random variables are uncorrelated then not necessarily any function of this 
random variables will be uncorrelated.

Now, after our assumption of normality, then $\hat{\be}$ and $\hat{\by}$ are 
independent, because uncorrelated random variables implies independence for
normally distributed random variables, as is the case of $\hat{\be}$ and $\hat{\by}$.

Then, any variable that is a function of $\hat{\be}$ will be independent of any
random variable that is a function of $\hat{\by}$, even if these new random
variables are not normal themselves.

## Interval Estimation

So far, we have obtained point estimates of different quantities of interest, 
$\bgb$, $\be$ and $\hat{\sigma}^2$, however the fact
we have the distribution of estimates of this quantities will allow us to obtain
interval estimators.

### Confidence Intervals for Coefficients

We know that the OLS estimate for the coefficients, has the following distribution:

$$ \hat{\bgb} \sim N(\bzero, \sigma^2 (\bX'\bX)^{-1}) $$

then, this means that each of the entries of $\hat{\bgb} = (\hat{\beta}_0, \hat{\beta}_1,\ldots,\hat{\beta}_{p-1})$
is normally distributed, as follows:

$$ \hat{\beta}_i \sim N(\beta_i, \sigma^2 [(\bX'\bX)^{-1}]_{ii}) $$
we will call

$$ \sigma^2_{\beta_i} =  \sigma^2 [(\bX'\bX)^{-1}]_{ii} $$
then, we can re-write the distribution as follows:

$$ \hat{\beta}_i \sim N \left(\beta_i, \sigma^2_{\beta_i} \right) $$
unfortunately this distribution depends on $\sigma^2$ which is unknown, so we 
can't use it to build a confidence interval. So we transform the statistic first
by removing the mean $\beta_i$ and by diving by the standard deviation: 

$$ t^0_{\beta_i}=\frac{\hat{\beta}_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}} $$
Then, this new quantity is normally distributed, since it a linear transformation
of a random variable that is normally distributed, and has mean as variance as
follows:

$$ \dE[t^0_{\beta_i}] = \dE\left[\frac{\hat{\beta}_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}}\right]= \frac{\hat{\dE[\beta}_i] - \beta_i}{\sqrt{\sigma^2_{\beta_i}}} = \frac{\beta_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}} = 0  $$
$$ \dV[t^0_{\beta_i}] = \dV\left[\frac{\hat{\beta}_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}}\right]= \left(\frac{1}{\sqrt{\sigma^2_{\beta_i}}}\right)^2 \dV[\hat{\beta}_i - \beta_i] = \frac{1}{\sigma^2_{\beta_i}} \dV[\hat{\beta}_i] = \frac{1}{\sigma^2_{\beta_i}} \sigma^2_{\beta_i} = 1  $$
then:

$$ t^0_{\beta_i} \sim N(0, 1) $$
that is $t^0_{\beta_i}$ is distributed like a standard normal. Now the distribution
doesn't depend on any unknown parameter (on top of $\beta_i$, that is the parameter
of interest), but the quantity itself depends on $\sigma^2$ through $\sigma^2_{\beta_i}$,
so we can't use it to build a confidence interval.

We consider a new quantity

$$ t_{\beta_i} = \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}} $$
where $\hat{\sigma}^2_{\beta_i} = \hat{\sigma}^2 [(\bX'\bX)^{-1}]_{ii}$, so $t_{\beta_i}$
doesn't depend on $\sigma^2$. Let's compute the distribution of this quantity,
first lets re-write the statistic as follows:

$$ t_{\beta_i} = \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}} 
  = \frac{\sqrt{\frac{1}{\sigma^2}}}{\sqrt{\frac{1}{\sigma^2}}}\frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2[(\bX'\bX)^{-1}]_{ii}}}   
  = \frac{\frac{\left(\hat{\beta}_i - \beta_i\right)}{\sqrt{\sigma^2[(\bX'\bX)^{-1}]_{ii}}}}{ \sqrt{\frac{\hat{\sigma}^2}{\sigma^2}}}   
  = \frac{\frac{\left(\hat{\beta}_i - \beta_i\right)}{\sqrt{\sigma^2_{\beta_i}}}}{            \sqrt{\frac{(n-p)\frac{\hat{\sigma}^2}{\sigma^2}}{n-p}}}
  = \frac{t^0_{\beta_i}}{ \sqrt{\frac{\frac{\hat{\be}'\hat{\be}}{\sigma^2}}{n-p}}}$$

Now, we know $t^0_{\beta_i}$ is standard normal distributed, and from 
[the distribution of $\hat{\sigma}^2$](#distribution-of-hatsigma2-step-3) we 
have that:

$$ \frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \chi^2_{n-p} $$
and from 
[the independence of $\hat{\bgb}$ and $\hat{\be}$](#independence-of-hatmathbfe-and-hatmathbfy)
we have that any function of both variables is independent, in particular

$$ t^0_{\beta_i} \quad \text{and} \quad \frac{\hat{\be}'\hat{\be}}{\sigma^2} $$
are independent. Therefore

$$ t_{\beta_i} \sim t_{n-p} $$
Now, let $t \sim t_m$ a random variable with a $t$ distribution with $m$ degrees
of freedom. Then call:

$$ t_m\left(a\right) \quad \text{such that} \quad \dP\left(t\leq t_m\left(a\right) \right) = a$$
for any $a\in[0,1]$

Then, we have that:

\begin{align*}
\dP 
  &\left( -t_{n-p}\left(\frac{\alpha}{2}\right) \leq t_{\beta_i} \leq t_{n-p}\left(\frac{\alpha}{2}\right) \right) = \alpha  
    && \text{since the $t$ distribution is symmetric} \\
  &\implies \dP\left( -t_{n-p}\left(\frac{\alpha}{2}\right) \leq \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}} \leq t_{n-p}\left(\frac{\alpha}{2}\right) \right) = \alpha
    && \text{since the $t_{\beta_i} = \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}}$} \\
  &\implies \dP\left( -t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \leq \hat{\beta}_i - \beta_i \leq t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) = \alpha \\
  &\implies \dP\left( -t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \leq \beta_i - \hat{\beta}_i \leq t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) = \alpha \\
  &\implies \dP\left( \hat{\beta}_i - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \leq \beta_i \leq \hat{\beta}_i + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) = \alpha \\
\end{align*}

So

$$ \left(\hat{\beta}_i - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}}, \hat{\beta}_i + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) $$
is a random interval, that is an interval that is a function of a random variables,
in this case the random variables $\hat{\beta}_i$ and $\hat{\sigma}^2_{\beta_i}$.
This random interval will capture the true parameter $\beta_i$ with probability
$\alpha$. However, when data is observed and the interval is fixed (at the observed
values), the interval either captures the true parameter or not (something we
don't know in general).

### Confidence intervals for the expected mean of a new observation $\bx_{new}$

Note that the expected mean of a new observation $\bx_{new}$ is given by:

$$ \dE[y_{new}] = \bx_{new}' \bgb $$
then we can consider, an estimate of this parameter, as:

$$ \bx_{new}' \hat{\bgb} $$
this estimate is a linear combination of $\hat{\bgb}$, therefore it has a normal
distribution with mean and variance as follows:

$$\dE[\bx_{new} \hat{\bgb}] = \bx_{new} \dE[\hat{\bgb}] = \bx_{new}' \bgb$$
$$\dV[\bx_{new}' \hat{\bgb}] = \bx_{new}' \dV[\hat{\bgb}] \bx_{new} = \bx_{new}' \sigma^2 (\bX' \bX)^{-1} \bx_{new} =  \sigma^2 \bx_{new}' (\bX' \bX)^{-1} \bx_{new} $$
that is:
$$ \bx_{new}' \hat{\bgb} \sim N \left(\bx_{new}' \bgb, \sigma^2 \bx_{new}' (\bX' \bX)^{-1} \bx_{new} \right)  $$

so, similarly, we can consider

$$ t_{\bx_{new}'\bgb} = \frac{\bx_{new}'\hat{\bgb} - \bx_{new}'\bgb}{\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}}}=\frac{\frac{\bx_{new}'\hat{\bgb} - \bx_{new}'\bgb}{\sqrt{\sigma^2_{\bx_{new}'\bgb}}}}{\sqrt{\frac{\frac{\hat{\be}'\hat{\be}}{\sigma^2}}{n-p}}} \sim \chi^2_{n-p} $$
where $\hat{\sigma}^2_{\bx_{new}'\bgb} = \hat{\sigma}^2 \bx_{new}' (\bX' \bX)^{-1} \bx_{new}$.
This quantity is distributed as a $t$ with $n-p$ degrees of freedom since:

$$\frac{\bx_{new}'\hat{\bgb} - \bx_{new}'\bgb}{\sqrt{\sigma^2_{\bx_{new}'\bgb}}} \sim N(0, 1)$$
$$ \frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \chi^2_{n-p}$$
and this random variables are independent since one is a function of $\hat{\bgb}$
and the other a function of $\hat{\be}$.

Then we can conclude that:

\begin{align*}
\dP 
  &\left( -t_{n-p}\left(\frac{\alpha}{2}\right) \leq t_{\bx_{new}'\bgb} \leq t_{n-p}\left(\frac{\alpha}{2}\right) \right) = \alpha  \\
  &\implies \dP\left( \bx_{new}'\hat{\bgb} - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}} \leq \bx_{new}'\bgb \leq \bx_{new}'\hat{\bgb} + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}} \right) = \alpha \\
\end{align*}

so, the random interval is given by:

$$ \left( \bx_{new}'\hat{\bgb} - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}} , \bx_{new}'\hat{\bgb} + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}} \right) $$
is a random interval that captures $\bx_{new}'\bgb$ with probability $\alpha$.

### Confidence intervals for linear combinations of $\bgb$

Note that if we consider the parameter

$$ \ba' \hat{\bgb}$$
then we note that $\beta_i$ and $\bx_{new} \bgb$ are particular cases, where the
value of $\ba$ is a s follows:

$$ \ba = (0,\ldots,0,1,0,\ldots,0) \quad \text{for} \quad \ba \bgb = \beta_i$$
$$ \ba = \bx_{new} \quad \text{for} \quad \ba \bgb = \bx_{new}' \bgb$$
then, performing similar operations as before, we can create random intervals to
estimate $\ba' \bgb$ as follows:

$$ \left( \ba'\hat{\bgb} - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\ba'\bgb}} , \ba'\hat{\bgb} + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\ba'\bgb}} \right) $$
that captures $\ba' \bgb$ with probability $\alpha$. Where 
$\hat{\sigma}^2_{\ba'\bgb} = \hat{\sigma}^2 \ba' (\bX' \bX)^{-1} \ba$.





