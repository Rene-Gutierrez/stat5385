# Normality Assumption

## Introduction

To the mean and covariance assumptions, we can add the normality assumption. This
is a very strong and powerful assumption, that will enable us to obtain the 
distribution of  our data and several of our estimates.

We will assume:

$$\be \sim N(\bzero, \sigma^2 \bI)$$

Also, note that the Normal distribution is completely characterized by its 
mean and variance, so if the distribution of our estimates is normal, we will
already have the information to completely characterize their distribution since
we have computed the mean and variance of the estimates in the previous chapter.

Another thing, that assuming normality allows is to obtain the likelihood of our
data, and in this way obtain Maximum Likelihood Estimates (MLE) of the parameters
we have introduced $\bgb$ and $\sigma^2$.

## Maximum Likelihood Estimation

In order to perform the maximum likelihood estimates of $\bgb$ and $\sigma^2$, we
need the distribution of $\by$. This is very easy to obtain, since:

$$\by = \bX \bgb + \be$$
is a linear combination of $\be$ (in fact is just a translation of $\be$), and
therefore it is normally distributed. Since we have already computed its mean 
and variance in the previous chapter, using those computations, we can conclude
that:

$$\by \sim N(\bX \bgb, \sigma^2 \bI)$$
This means that the likelihood of $\bgb$ and $\sigma^2$ is given by:

$$ \cL(\bgb, \sigma^2 | \by ) = N(\by | \bX \by, \sigma^2 \bI) $$
Then this means that:

\begin{align*}
  \cL(\bgb, \sigma^2 | \by )
    &= (2 \pi)^{-\frac{n}{2}} |\sigma^2 \bI|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2}(\by - \bX \bgb)'(\sigma^2 \bI)^{-1}(\by - \bX \bgb) \right\}    \\
    &= (2 \pi)^{-\frac{n}{2}} (\sigma^2)^{-\frac{n}{2}} |\bI| \exp\left\{ -\frac{1}{2}(\by - \bX \bgb)'\frac{\bI}{\sigma^2}(\by - \bX \bgb) \right\} \\
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\by - \bX \bgb)'(\by - \bX \bgb) \right\}                                   \\
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\by - \bX \bgb)'(\by - \bX \bgb) \right\}                                   \\
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\by - \bX \bgb)'(\by - \bX \bgb) \right\}                                   \\
\end{align*}

Now recalling from:

$$(\by - \dE[\by])'(\by - \dE[\by]) = \hat{\be}'\hat{\be} + (\hat{\by} - \dE[\hat{\by}])'(\hat{\by} - \dE[\hat{\by}])$$
that is:

$$(\by - \bX \bgb)'(\by - \bX \bgb) = \hat{\be}'\hat{\be} + (\bX \hat{\bgb} - \bX \bgb)'(\bX \hat{\bgb} - \bX \bgb) = \hat{\be}'\hat{\be} + (\hat{\bgb} - \bgb)'\bX \bX( \hat{\bgb} - \bgb)$$

then, the likelihood can be written as:

\begin{align*}
  \cL(\bgb, \sigma^2 | \by )
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\by - \bX \bgb)'(\by - \bX \bgb) \right\}                                                      \\
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{\hat{\be}'\hat{\be}}{2 \sigma^2} -\frac{(\hat{\bgb} - \bgb)'\bX \bX( \hat{\bgb} - \bgb)}{2 \sigma^2} \right\} \\
\end{align*}

This is a useful way to write the likelihood, since it is easy to optimize with
respect to $\bgb$. Since, independently of the value of $\sigma^2$, the value of
$\bgb$ that maximizes the likelihood is the OLS estimator $\hat{\bgb}$, since it
makes zero the following term:

$$ -\frac{(\hat{\bgb} - \bgb)'\bX \bX( \hat{\bgb} - \bgb)}{2 \sigma^2} $$
In this way, we only need to maximize the likelihood with respect to $\sigma^2$,
as the following marginal likelihood:

$$ \cL(\sigma^2 | \by, \bgb = \hat{\bgb} ) =  (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{\hat{\be}'\hat{\be}}{2 \sigma^2} \right\}$$
Instead of maximizing the marginal likelihood directly, we will maximize the 
marginal log-likelihood:

$$ \ell(\sigma^2 | \by, \bgb = \hat{\bgb}) = -\frac{n}{2} \log(2 \pi) -\frac{n}{2} \log(\sigma^2) -\frac{\hat{\be}'\hat{\be}}{2 \sigma^2}$$
We can do this maximization, by taking the derivative:

\begin{align*}
  \frac{d \ell}{d \sigma^2} 
    &=        -\frac{n}{2 \sigma^2} + \frac{\hat{\be}'\hat{\be}}{2 (\sigma^2)^2} 
\end{align*}

Then

\begin{align*}
  \frac{d \ell}{d \sigma^2} =0
    &\implies -\frac{n}{2 \sigma^2} + \frac{\hat{\be}'\hat{\be}}{2 (\sigma^2)^2} = 0 \\
    &\implies -n + \frac{\hat{\be}'\hat{\be}}{\sigma^2} = 0                          \\
    &\implies \frac{\hat{\be}'\hat{\be}}{\sigma^2} = n                               \\
    &\implies \sigma^2 = \frac{\hat{\be}'\hat{\be}}{n}                               \\
\end{align*}

So we have, that the Maximum Likelihood Estimate of $\sigma^2$ is given by:

$$\tilde{\sigma}^2 = \frac{\hat{\be}'\hat{\be}}{n}$$

And, taking the second derivative to confirm it is a maximum we have that:

\begin{align*}
  \frac{d^2 \ell}{d (\sigma^2)^2} \bigg|_{\sigma^2 = \tilde{\sigma}^2}
    &= \frac{n}{2 (\tilde{\sigma}^2)^2} - \frac{\hat{\be}'\hat{\be}}{(\tilde{\sigma}^2)^3} \\
    &= \frac{n}{2 (\tilde{\sigma}^2)^2} - \frac{n}{(\tilde{\sigma}^2)^2}                   \\
    &= - \frac{n}{(2 \tilde{\sigma}^2)^2}                                                  \\
    &\leq 0                                                  
\end{align*}

So $\tilde{\sigma}^2$ is indeed maximizing the marginal likelihood.

Note that, unlike with the $\bgb$ parameter, our OLS estimate $\hat{\sigma}^2$
of $\sigma^2$ is different to the MLE estimator $\tilde{\sigma}^2$. In particular
$\tilde{\sigma}^2$ is biased.

This doesn't mean that one estimate is better than the other, they just have 
different properties.

We will work more with $\hat{\sigma}^2$ than $\tilde{\sigma}^2$, since it is more
useful to build certain statistics.

## Distribution of Estimates

In the same way that in the last chapters we computed the mean and the variance 
of our estimates, we can obtain the distribution of the estimates:

### Distribution of $\hat{\bgb}$, $\hat{\by}$ and $\hat{\be}$

Most of our estimates are linear combinations of $\by$, so they are normally 
distributed and we can use the mean and variances computed in the last chapter
to fully characterize their distributions. This is the case for:

$$ \hat{\bgb}, \quad \hat{\by}, \quad \hat{\be} $$
Their distributions are:

$$ \hat{\bgb} \sim N(\bgb, \sigma^2 (\bX'\bX)^{-1}) $$
$$ \hat{\by} \sim N(\bX \bgb, \sigma^2 \bH) $$
$$ \hat{\be} \sim N(\bzero, \sigma^2 (\bI-\bH)) $$

This is not the case for the estimate $\hat{\sigma}^2$, since it is not a linear
transformation of $\by$.

### Distribution of $\hat{\sigma}^2$

Obtaining the distribution of $\hat{\sigma}^2$ is not as straight forward. We
will use 3 steps:

1. Express $\frac{\hat{\be}'\hat{\be}}{\sigma^2}$ as quadratic form of standard
normal with an idempotent matrix.
2. Show that the quadratic form of standard normal with an idempotent matrix is
distributed as a chi squared.
3. Relate the distribution of $\frac{\hat{\be}'\hat{\be}}{\sigma^2}$ to the 
distribution of $\hat{\sigma}^2$.

#### Distribution of $\hat{\sigma}^2$ Step 1

First, note that:

\begin{align*}
  (\bI - \bH) \by
    &= (\bI - \bH) (\bX \bgb + \be)                    && \text{since $\by = \bX \bgb + \be$}  \\
    &= \bI \bX \bgb - \bH \bX \bgb + \bI \be - \bH \be                                         \\
    &= \bX \bgb - \bX \bgb + \be - \bH \be             && \text{since $\bH \bX = \bX$}         \\
    &= \be - \bH \be                                                                           \\
    &= (\bI - \bH) \be                                                                         \\
\end{align*}

Then:

\begin{align*}
  \by' (\bI - \bH) \by
    &= \by' (\bI - \bH) (\bI - \bH) \by  && \text{since $(\bI - \bH)$ is idempotent}         \\
    &= \be' (\bI - \bH) (\bI - \bH) \be  && \text{since $(\bI - \bH) \by = (\bI - \bH) \be$} \\
    &= \be' (\bI - \bH) \be              && \text{since $(\bI - \bH)$ is idempotent}         \\
\end{align*}

Then,

\begin{align*}
  \frac{\hat{\be}'\hat{\be}}{\sigma^2}
    &= \frac{\by' (\bI - \bH) \by}{\sigma^2}  && \text{since $\hat{\be}'\hat{\be} = \by' (\bI - \bH) \by$}  \\
    &= \frac{\be' (\bI - \bH) \be}{\sigma^2}  && \text{since $\be' (\bI - \bH) \be = \by' (\bI - \bH) \by$} \\
    &= \frac{\be}{\sqrt{\sigma^2}}' (\bI - \bH) \frac{\be}{\sqrt{\sigma^2}}                                 \\
\end{align*}

Finally, note that $\frac{\be}{\sqrt{\sigma^2}}$ is a linear function of $\be$ 
that is normal, therefore it is normal also, with mean:

$$ \dE\left[\frac{\be}{\sqrt{\sigma^2}}\right] =  \frac{1}{\sqrt{\sigma^2}} \dE[\be] = \frac{1}{\sqrt{\sigma^2}} \bzero = \bzero$$
$$ \dV\left[\frac{\be}{\sqrt{\sigma^2}}\right] =  \left(\frac{1}{\sqrt{\sigma^2}}\right)^2 \dV[\be] = \frac{1}{\sigma^2} \sigma^2 \bI = \bI$$
So

$$ \frac{\be}{\sqrt{\sigma^2}} \sim N(\bzero, \bI) $$
is a multivariate standard normal. This concludes step 1.

#### Distribution of $\hat{\sigma}^2$ Step 2

Let $\bz \in \dR^{n}$ a multivariate standard normal and $\bM \in \dR^{n \times n}$
and idempotent matrix of rank $m$. Then, we will show that:

$$ \bz' \bM \bz \sim \chi^2_{m} $$

To show this result, we will use the spectral decomposition of $\bM$, that is:

$$ \bM = \bV \bgS \bV' $$

with $\bV$ orthonormal and $\bgS$ diagonal. Since $\bV = [\bv_1,\ldots,\bv_n]$ is
orthonormal, then we have that:

$$ \bv_i'\bv_j = 0 \quad \forall i \neq j \quad \text{and} \quad ||\bv_i||^2_2 = 1 \quad \forall i$$
and, since $\bM$ is idempotent, then $\bgS$ is diagonal with exactly $m$ entries
equal to $1$ and the rest equal to $0$. Without loss of generality, we
can assume that the first $m$ entries of the diagonal are equal to $1$ and the 
next entries equal to $0$.

Then, first note that $\bV' \bz$ is a linear combination of a normal distribution.
We will show, that $\bV' \bz \in \dR^{n}$ is also standard normal.

Note that:

$$ \dE[\bV' \bz] =  \bV' \dE[\bz] = \bV' \bzero = \bzero$$
$$ \dV[\bV' \bz] =  \bV' \dV[\bz] \bV = \bV' \bI \bV = \bV' \bV = \bI$$

Then $\bV' \bz$ is also standard normal. Let's name $\bw = \bV' \bz$, then each of
the components $w_1,\ldots,w_n$ of $\bw$ are independent univariate standard normally
distributed.

Then

\begin{align*}
  \bz' \bM \bz
    &= \bz' (\bV \bgS \bV') \bz       && \text{using the spectral decomposition of $\bM$}         \\
    &= (\bV' \bz)' \bgS (\bV' \bz)                                                                \\
    &= \bw' \bgS \bw                  && \text{since $\bw = \bV' \bz$}                            \\
    &= \sum_{i=1}^n [\bgS]_{ii} w_i^2                                                             \\
    &= \sum_{i=1}^{m} w_i^2           && \text{since only the first $m$ entries are equal to $1$} \\
    &\sim \chi^2_m                    && \text{by definition of the $\chi^2$ distribution}        \\
\end{align*} 

#### Distribution of $\hat{\sigma}^2$ Step 3

Using step 1 and 2, we can conclude that:

$$ \frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \chi^2_{n-p} $$
since the rank of the idempotent matrix $(\bI - \bH)$ is $n-p$. Since:

$$ \hat{\sigma}^2 = \frac{\hat{\be}'\hat{\be}}{n-p} = \frac{\sigma^2}{n-p}\frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \frac{\sigma^2}{n-p}\chi^2_{n-p} $$

### Independence of $\hat{\be}$ and $\hat{\by}$

We have seen that $\hat{\be}$ and $\hat{\by}$ are uncorrelated, that is

$$ \dC[\hat{\be}, \hat{\by}] = \bzero $$
however, this doesn't necessarily mean they are independent. Independence is a 
much more stronger property. For example, if two random variables are independent
then any function of this random variables will be independent. However, if two 
random variables are uncorrelated then not necessarily any function of this 
random variables will be uncorrelated.

Now, after our assumption of normality, then $\hat{\be}$ and $\hat{\by}$ are 
independent, because uncorrelated random variables implies independence for
normally distributed random variables, as is the case of $\hat{\be}$ and $\hat{\by}$.

Then, any variable that is a function of $\hat{\be}$ will be independent of any
random variable that is a function of $\hat{\by}$, even if these new random
variables are not normal themselves.

## Interval Estimation

So far, we have obtained point estimates of different quantities of interest, 
$\bgb$, $\be$ and $\hat{\sigma}^2$, however the fact
we have the distribution of estimates of this quantities will allow us to obtain
interval estimators.

### Confidence Intervals for Coefficients

We know that the OLS estimate for the coefficients, has the following distribution:

$$ \hat{\bgb} \sim N(\bzero, \sigma^2 (\bX'\bX)^{-1}) $$

then, this means that each of the entries of $\hat{\bgb} = (\hat{\beta}_0, \hat{\beta}_1,\ldots,\hat{\beta}_{p-1})$
is normally distributed, as follows:

$$ \hat{\beta}_i \sim N(\beta_i, \sigma^2 [(\bX'\bX)^{-1}]_{ii}) $$
we will call

$$ \sigma^2_{\beta_i} =  \sigma^2 [(\bX'\bX)^{-1}]_{ii} $$
then, we can re-write the distribution as follows:

$$ \hat{\beta}_i \sim N \left(\beta_i, \sigma^2_{\beta_i} \right) $$
unfortunately this distribution depends on $\sigma^2$ which is unknown, so we 
can't use it to build a confidence interval. So we transform the statistic first
by removing the mean $\beta_i$ and by diving by the standard deviation: 

$$ t^0_{\beta_i}=\frac{\hat{\beta}_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}} $$
Then, this new quantity is normally distributed, since it a linear transformation
of a random variable that is normally distributed, and has mean as variance as
follows:

$$ \dE[t^0_{\beta_i}] = \dE\left[\frac{\hat{\beta}_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}}\right]= \frac{\hat{\dE[\beta}_i] - \beta_i}{\sqrt{\sigma^2_{\beta_i}}} = \frac{\beta_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}} = 0  $$
$$ \dV[t^0_{\beta_i}] = \dV\left[\frac{\hat{\beta}_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}}\right]= \left(\frac{1}{\sqrt{\sigma^2_{\beta_i}}}\right)^2 \dV[\hat{\beta}_i - \beta_i] = \frac{1}{\sigma^2_{\beta_i}} \dV[\hat{\beta}_i] = \frac{1}{\sigma^2_{\beta_i}} \sigma^2_{\beta_i} = 1  $$
then:

$$ t^0_{\beta_i} \sim N(0, 1) $$
that is $t^0_{\beta_i}$ is distributed like a standard normal. Now the distribution
doesn't depend on any unknown parameter (on top of $\beta_i$, that is the parameter
of interest), but the quantity itself depends on $\sigma^2$ through $\sigma^2_{\beta_i}$,
so we can't use it to build a confidence interval.

We consider a new quantity

$$ t_{\beta_i} = \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}} $$
where $\hat{\sigma}^2_{\beta_i} = \hat{\sigma}^2 [(\bX'\bX)^{-1}]_{ii}$, so $t_{\beta_i}$
doesn't depend on $\sigma^2$. Let's compute the distribution of this quantity,
first lets re-write the statistic as follows:

$$ t_{\beta_i} = \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}} 
  = \frac{\sqrt{\frac{1}{\sigma^2}}}{\sqrt{\frac{1}{\sigma^2}}}\frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2[(\bX'\bX)^{-1}]_{ii}}}   
  = \frac{\frac{\left(\hat{\beta}_i - \beta_i\right)}{\sqrt{\sigma^2[(\bX'\bX)^{-1}]_{ii}}}}{ \sqrt{\frac{\hat{\sigma}^2}{\sigma^2}}}   
  = \frac{\frac{\left(\hat{\beta}_i - \beta_i\right)}{\sqrt{\sigma^2_{\beta_i}}}}{            \sqrt{\frac{(n-p)\frac{\hat{\sigma}^2}{\sigma^2}}{n-p}}}
  = \frac{t^0_{\beta_i}}{ \sqrt{\frac{\frac{\hat{\be}'\hat{\be}}{\sigma^2}}{n-p}}}$$

Now, we know $t^0_{\beta_i}$ is standard normal distributed, and from 
[the distribution of $\hat{\sigma}^2$](#distribution-of-hatsigma2-step-3) we 
have that:

$$ \frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \chi^2_{n-p} $$
and from 
[the independence of $\hat{\bgb}$ and $\hat{\be}$](#independence-of-hatmathbfe-and-hatmathbfy)
we have that any function of both variables is independent, in particular

$$ t^0_{\beta_i} \quad \text{and} \quad \frac{\hat{\be}'\hat{\be}}{\sigma^2} $$
are independent. Therefore

$$ t_{\beta_i} \sim t_{n-p} $$
Now, let $t \sim t_m$ a random variable with a $t$ distribution with $m$ degrees
of freedom. Then call:

$$ t_m\left(a\right) \quad \text{such that} \quad \dP\left(t\leq t_m\left(a\right) \right) = a$$
for any $a\in[0,1]$

Then, we have that:

\begin{align*}
\dP 
  &\left( -t_{n-p}\left(\frac{\alpha}{2}\right) \leq t_{\beta_i} \leq t_{n-p}\left(\frac{\alpha}{2}\right) \right) = \alpha  
    && \text{since the $t$ distribution is symmetric} \\
  &\implies \dP\left( -t_{n-p}\left(\frac{\alpha}{2}\right) \leq \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}} \leq t_{n-p}\left(\frac{\alpha}{2}\right) \right) = \alpha
    && \text{since the $t_{\beta_i} = \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}}$} \\
  &\implies \dP\left( -t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \leq \hat{\beta}_i - \beta_i \leq t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) = \alpha \\
  &\implies \dP\left( -t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \leq \beta_i - \hat{\beta}_i \leq t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) = \alpha \\
  &\implies \dP\left( \hat{\beta}_i - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \leq \beta_i \leq \hat{\beta}_i + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) = \alpha \\
\end{align*}

So

$$ \left(\hat{\beta}_i - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}}, \hat{\beta}_i + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) $$
is a random interval, that is an interval that is a function of a random variables,
in this case the random variables $\hat{\beta}_i$ and $\hat{\sigma}^2_{\beta_i}$.
This random interval will capture the true parameter $\beta_i$ with probability
$\alpha$. However, when data is observed and the interval is fixed (at the observed
values), the interval either captures the true parameter or not (something we
don't know in general).

### Confidence intervals for the expected mean of a new observation $\bx_{new}$

Note that the expected mean of a new observation $\bx_{new}$ is given by:

$$ \dE[y_{new}] = \bx_{new}' \bgb $$
then we can consider, an estimate of this parameter, as:

$$ \bx_{new}' \hat{\bgb} $$
this estimate is a linear combination of $\hat{\bgb}$, therefore it has a normal
distribution with mean and variance as follows:

$$\dE[\bx_{new} \hat{\bgb}] = \bx_{new} \dE[\hat{\bgb}] = \bx_{new}' \bgb$$
$$\dV[\bx_{new}' \hat{\bgb}] = \bx_{new}' \dV[\hat{\bgb}] \bx_{new} = \bx_{new}' \sigma^2 (\bX' \bX)^{-1} \bx_{new} =  \sigma^2 \bx_{new}' (\bX' \bX)^{-1} \bx_{new} $$
that is:
$$ \bx_{new}' \hat{\bgb} \sim N \left(\bx_{new}' \bgb, \sigma^2 \bx_{new}' (\bX' \bX)^{-1} \bx_{new} \right)  $$

so, similarly, we can consider

$$ t_{\bx_{new}'\bgb} = \frac{\bx_{new}'\hat{\bgb} - \bx_{new}'\bgb}{\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}}}=\frac{\frac{\bx_{new}'\hat{\bgb} - \bx_{new}'\bgb}{\sqrt{\sigma^2_{\bx_{new}'\bgb}}}}{\sqrt{\frac{\frac{\hat{\be}'\hat{\be}}{\sigma^2}}{n-p}}} \sim \chi^2_{n-p} $$
where $\hat{\sigma}^2_{\bx_{new}'\bgb} = \hat{\sigma}^2 \bx_{new}' (\bX' \bX)^{-1} \bx_{new}$.
This quantity is distributed as a $t$ with $n-p$ degrees of freedom since:

$$\frac{\bx_{new}'\hat{\bgb} - \bx_{new}'\bgb}{\sqrt{\sigma^2_{\bx_{new}'\bgb}}} \sim N(0, 1)$$
$$ \frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \chi^2_{n-p}$$
and this random variables are independent since one is a function of $\hat{\bgb}$
and the other a function of $\hat{\be}$.

Then we can conclude that:

\begin{align*}
\dP 
  &\left( -t_{n-p}\left(\frac{\alpha}{2}\right) \leq t_{\bx_{new}'\bgb} \leq t_{n-p}\left(\frac{\alpha}{2}\right) \right) = \alpha  \\
  &\implies \dP\left( \bx_{new}'\hat{\bgb} - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}} \leq \bx_{new}'\bgb \leq \bx_{new}'\hat{\bgb} + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}} \right) = \alpha \\
\end{align*}

so, the random interval is given by:

$$ \left( \bx_{new}'\hat{\bgb} - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}} , \bx_{new}'\hat{\bgb} + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}} \right) $$
is a random interval that captures $\bx_{new}'\bgb$ with probability $\alpha$.

### Confidence intervals for linear combinations of $\bgb$

Note that if we consider the parameter

$$ \ba' \hat{\bgb}$$
then we note that $\beta_i$ and $\bx_{new} \bgb$ are particular cases, where the
value of $\ba$ is a s follows:

$$ \ba = (0,\ldots,0,1,0,\ldots,0) \quad \text{for} \quad \ba \bgb = \beta_i$$
$$ \ba = \bx_{new} \quad \text{for} \quad \ba \bgb = \bx_{new}' \bgb$$
then, performing similar operations as before, we can create random intervals to
estimate $\ba' \bgb$ as follows:

$$ \left( \ba'\hat{\bgb} - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\ba'\bgb}} , \ba'\hat{\bgb} + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\ba'\bgb}} \right) $$
that captures $\ba' \bgb$ with probability $\alpha$. Where 
$\hat{\sigma}^2_{\ba'\bgb} = \hat{\sigma}^2 \ba' (\bX' \bX)^{-1} \ba$.

## Hypothesis Testing

We will approach hypothesis testing using an implausibility framework. 
This involves formulating a null hypothesis, $H_0$, and assuming it to be true.
Next, we calculate a test statistic that follows a specific distribution under 
the null hypothesis. By comparing the observed value of the statistic to this 
distribution, we assess how plausible it is to observe such a value if $H_0$ is true.


### Testing for the Overall Regression

For this hypothesis, we will use the notation of:

$$ \bX^* = [\bones \bX] \quad \text{and} \quad \bgb^* = [\beta_0, \bgb]' \in \dR^{p}$$
that is, the $*$ indicates all the independent variables. With $\bX$ of full rank.

Our first test is to see if the Linear Regression framework is useful at all.
That is, we want to test $\cH_0: \bgb = \bzero$. Before designing our test 
statistic we will show the following auxiliary results:

1. $SS_{reg} = \by'(\bH - \bH_0)\by$.
2. $\bH \bH_0 = \bH_0 \bH = \bH_0$.
3. $(\bH - \bH_0)$ is idempotent.
4. $\by'(\bH - \bH_0)\by$ and $\by'(\bI - \bH)\by$ are independent.
5. Under the null hypothesis $\cH_0: \bgb = \bzero$, $\frac{\by'(\bH - \bH_0)\by}{\sigma^2}$
is distributed like a $\chi^2_{p-1}$.

For auxiliary result 1, we have that:

\begin{align*}
SS_{tot} 
  &= SS_{reg} + SS_{res} \\
  &\implies \by'(\bI - \bH_0)\by = SS_{reg} + \by'(\bI - \bH)\by && \text{since $SS_{tot} = \by'(\bI - \bH)\by$ and $SS_{res} = \by'(\bI - \bH)\by$.} \\
  &\implies SS_{reg} = \by'(\bI - \bH_0)\by - \by'(\bI - \bH)\by &&                                                                                   \\
  &\implies SS_{reg} = \by'(\bI - \bH_0 - \bI + \bH)\by          &&                                                                                   \\
  &\implies SS_{reg} = \by'(\bH - \bH_0)\by                      &&                                                                                   \\
\end{align*}

For auxiliary result 2, we have that:

Both $\bH$ and $\bH0$ are symmetric, then $\bH \bH_0 = \bH_0 \bH$, and:

\begin{align*}
  \bH \bH_0 = \bH \bones (\bones' \bones)^{-1} \bones' &&                                     \\
  \bH \bH_0 = \bones (\bones' \bones)^{-1} \bones'     && \text{since $\bH \bones = \bones$.} \\
  \bH \bH_0 = \bH_0                                    &&                                     \\
\end{align*}

For auxiliary result 3, we have that:

\begin{align*}
  (\bH - \bH_0)(\bH - \bH_0) 
    &= \bH \bH - \bH \bH_0 - \bH_0 \bH + \bH_0 \bH_0 &&                                                \\
    &= \bH - \bH \bH_0 - \bH_0 \bH + \bH_0           && \text{since $\bH_0$ and $\bH$ are idempotent.} \\
    &= \bH - \bH_0 - \bH_0 + \bH_0                   && \text{since $\bH \bH_0 = \bH_0 \bH = \bH_0$.}  \\
    &= \bH - \bH_0                                   &&                                                \\
\end{align*}

so, $(\bH - \bH_0)$ is idempotent.

For auxiliary result 4, first we have that:

\begin{align*}
  \dC[(\bH - \bH_0) \by, (\bI - \bH) \by]
    &= (\bH - \bH_0) \dC[\by,\by] (\bI - \bH)        \\
    &= (\bH - \bH_0) \dV[\by] (\bI - \bH)            \\
    &= \sigma^2 (\bH - \bH_0) (\bI - \bH)            \\
    &= \sigma^2 (\bH - \bH_0  - \bH \bH + \bH_0 \bH) \\
    &= \sigma^2 (\bH - \bH_0  - \bH + \bH_0 \bH)     && \text{since $\bH$ is idempotent.} \\
    &= \sigma^2 (\bH - \bH_0  - \bH + \bH_0)         && \text{since $\bH_0 \bH = \bH_0$.} \\
    &= \sigma^2 \bzero                               &&                                   \\
    &= \bzero                                        &&                                   \\
\end{align*}

This tells us that $(\bH - \bH_0)\by$ and $(\bI - \bH)\by$ are uncorrelated. Now,
since $(\bH - \bH_0)\by$ and $(\bI - \bH)\by$ are normally distributed, then 
zero correlation implies independence. Then, any function of this 2 quantities 
are independent. Note that:

\begin{align*}
  \by'(\bH - \bH_0)\by 
    &= \by'(\bH - \bH_0)(\bH - \bH_0)(\bH - \bH_0)\by && \text{since $(\bH - \bH_0)$ is idempotent.}
\end{align*}

Then, $\by'(\bH - \bH_0)\by$ is a quadratic function of $(\bH - \bH_0)\by$. Similarly,
$\by'(\bH - \bH)\by$ is a quadratic function of $(\bH - \bH)\by$. Therefore, 
$\by'(\bH - \bH_0)\by$ and $\by'(\bH - \bH)\by$ are independent.

For result 5, we have that:

\begin{align*}
  (\bH - \bH_0)\by 
    &= (\bH - \bH_0)(\bX^* \bgb^* + \be) && \text{since $\by = \bX \bgb + \be$.} \\
    &= (\bH - \bH_0)([\bones \bX] [\beta_0, \bgb]' + \be)                && \text{since $\bX^* = [\bones \bX] \quad \text{and} \quad \bgb^* = [\beta_0, \bgb]'$.} \\
    &= (\bH - \bH_0)(\bones\beta_0 + \bX\bgb + \be) &&  \\
    &= (\bH - \bH_0)(\bones\beta_0) + (\bH - \bH_0)(\bX\bgb + \be) &&  \\
    &= (\bH \bones - \bH_0 \bones)\beta_0 + (\bH - \bH_0)(\bX\bgb + \be) &&  \\
    &= (\bones - \bones)\beta_0 + (\bH - \bH_0)(\bX\bgb + \be) &&  \\
    &= (\bH - \bH_0)(\bX\bgb + \be) &&  \\
    &= (\bH - \bH_0)\be                                                  && \text{iff $\cH_0: \bgb = \bzero$ for any full rank $\bX$.}
\end{align*}

That is, for any full rank $\bX$, we have that:

$$ (\bH - \bH_0)\by  = (\bH - \bH_0)\be \iff \cH_0: \bgb = \bzero$$

Then:

\begin{align*}
  \be \sim N(0, \sigma^2 \bI)
    &\implies \be'(\bH - \bH_0)\be \sim \sigma^2 \chi^2_{p-1}                                   && \text{since $(\bH - \bH_0)$ is idempotent of rank $p-1$}. \\
    &\implies \frac{\be'(\bH - \bH_0)\be}{\sigma^2} \sim \chi^2_{p-1}                           &&                                                           \\
    &\implies \frac{\be'(\bH - \bH_0)(\bH - \bH_0)(\bH - \bH_0)\be}{\sigma^2} \sim \chi^2_{p-1} && \text{since $(\bH - \bH_0)$ is idempotent}.               \\
    &\implies \frac{\by'(\bH - \bH_0)(\bH - \bH_0)(\bH - \bH_0)\by}{\sigma^2} \sim \chi^2_{p-1} && \text{iff the null hypothesis holds}.                     \\
    &\implies \frac{\by'(\bH - \bH_0)\by}{\sigma^2} \sim \chi^2_{p-1}                           && \text{since $(\bH - \bH_0)$ is idempotent}.               \\
\end{align*}

That is:

$$ \frac{\by'(\bH - \bH_0)\by}{\sigma^2} \sim \chi^2_{p-1} \iff \cH_0: \bgb = \bzero$$

With this results, we propose the following statistic:

$$ F_{\bgb = 0} = \frac{\frac{SS_{reg}}{p-1}}{\frac{SS_{res}}{n-p}} $$

and we will show that, this statistic is distributed like an $F_{p-1,n-p}$ only
under the null hypothesis.

\begin{align*}
  F_{\bgb = 0} 
    &= \frac{\frac{SS_{reg}}{p-1}}{\frac{SS_{res}}{n-p}}                                                           &&                                                                                                   \\
    &= \frac{\frac{\by'(\bH - \bH_0)\by}{p-1}}{\frac{\by'(\bI - \bH)\by}{n-p}}                                     && \text{since $SS_{reg} = \by'(\bH - \bH_0)\by$ and $SS_{res}=\by'(\bI - \bH)\by$}                  \\
    &= \frac{\frac{\by'(\bH - \bH_0)\by}{\sigma^2}\frac{1}{p-1}}{\frac{\by'(\bI - \bH)\by}{\sigma^2}\frac{1}{n-p}} &&                                                                                                   \\
    &\sim \frac{\frac{\chi^2_{p-1}}{p-1}}{\frac{\chi^2_{n-p}}{n-p}}                                                && \text{since $\frac{\by'(\bH - \bH_0)\by}{\sigma^2} \sim \chi^2_{p-1}$ under the null hypothesis.} \\
    &\sim F_{p-1,n-p}                                                                                              && \text{since $\by'(\bH - \bH_0)\by$ and $\by'(\bH - \bH)\by$ are independent.}
\end{align*}

So, once we observe the value of this statistic, we can contrast it with respect
respect to this distribution. Call $F^*_{\bgb = 0}$ the observed value, and consider
a random variable $F \sim F_{p-1,n-p}$, then we can see what would be the
probability of observing the value of the statistic (or a more extreme value).

$$ \dP(F \geq F^*_{\bgb = 0}) $$
depending on how small or big is this probability, we can reject or not reject the
null hypothesis. This value is a called a p-value.

### Testing if one variable is not relevant

We can test if a particular variable is not relevant for the regression. That is,
$\cH_0: \beta_i = 0$. We will use the same strategy, that is, we will build a 
test statistic that has a certain distribution only under the null hypothesis.

For this hypothesis we propose the following test statistic:

$$ t_{\beta_i = 0} = \frac{\hat{\beta}_i}{\sqrt{\hat{\sigma}^2 [(\bX \bX)^{-1}]_{ii}}} $$
First note that:

\begin{align*}
\hat{\bgb} \sim N \left(\bgb, \sigma^2(\bX \bX)^{-1}\right)
  &\implies \hat{\beta}_i \sim H(\beta_i, \sigma^2 [(\bX \bX)^{-1}]_{ii})                                                                              \\
  &\implies \frac{\hat{\beta}_i}{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}} \sim N \left(\frac{\beta_i}{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}}, 1 \right) \\
  &\implies \frac{\hat{\beta}_i}{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}} \sim N \left( 0, 1 \right) &&  \iff \cH_0: \beta_i = 0                         \\
\end{align*}

Then we have:

$$$$

\begin{align*}
t_{\beta_i = 0} 
  &= \frac{\hat{\beta}_i}{\sqrt{\hat{\sigma}^2 [(\bX \bX)^{-1}]_{ii}}} \\
  &= \frac{\frac{\hat{\beta}_i }{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}}}{\frac{\sqrt{\hat{\sigma}^2 [(\bX \bX)^{-1}]_{ii}}}{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}}} \\
  &= \frac{\frac{\hat{\beta}_i }{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}}}{\sqrt{\frac{\hat{\sigma}^2}{\sigma^2}}}                                                    \\
  &= \frac{\frac{\hat{\beta}_i }{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}}}{\sqrt{\frac{\hat{\be}'\hat{\be}}{\sigma^2}\frac{1}{n-p}}}                                  && \text{since $\hat{\sigma}^2 = \frac{\hat{\be}\hat{\be}}{n-p}$} \\
  &\sim \frac{N \left(\frac{\beta_i}{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}}, 1 \right)}{\sqrt{\frac{\chi^2_{n-p}}{n-p}}}                                            && \text{since $\frac{\hat{\beta}_i}{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}} \sim N \left( 0, 1 \right)$ and $\frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \chi^2_{n-p}$} \\
  &\sim \frac{N \left(0, 1 \right)}{\sqrt{\frac{\chi^2_{n-p}}{n-p}}}                                                                                                && \iff \cH_0: \beta_i = 0  \\
  &\sim t_{n-p}                                                                                                                                                     && \text{since $\hat{\beta}_i$ and $\hat{\sigma}^2$ are independent}.  \\
\end{align*}

Then, under the null hypothesis we have that:

$$ t_{\beta_i = 0} \sim  t_{n-p}$$
So, if we call $t_{\beta_i = 0}^*$ the observed value of $t_{\beta_i = 0}$, and
if we let $t$ be distributed as $t_{n-p}$, we can compute:

$$ \dP(t \geq t_{\beta_i = 0}^*) $$
and depending on the value, we can reject or accept the null hypothesis.

### Testing if a Subgroup of the Variables is Relevant

For this test, we can assume without loss of generality, that the variables we 
want to see if it is relevant are the first $k$. So we can divide the design 
matrix as:

$$ \bX = [\bX_1 \bX_2] $$
where the variables to test are in $\bX_1$ and the rest of the variables are in
$\bX_2$ (including possibly the intercept). And similarly we have $\bgb = [\bgb_1 \bgb_2]'$.

This test is similar to the first test once we express it accordingly. We will 
consider two linear regressions. One including all variables and one excluding 
the variables to be tested indexed by $2$. With this we can build the following 
test statistics:

$$ F_{\bgb_1=\bzero} = \frac{\frac{SS_{res,2} - SS_{res}}{k}}{\frac{SS_{res}}{n-p}} $$
Then note the following:

\begin{align*}
  SS_{res,2} - SS_{res}
    &= \by'(\bI - \bH_2)\by - \by'(\bI - \bH) \by \\
    &= \by'(\bI - \bH_2 - \bI + \bH) \by          \\
    &= \by'(\bH - \bH_2) \by                      \\
\end{align*}

Again, we will see that $(\bH - \bH_2)$ is idempotent and $(\bH - \bH_2)\by = (\bH - \bH_2)\be$
only under the null hypothesis.

First, let us see that $(\bH - \bH_2)$ is idempotent. First note that:

$$ \bH \bH_2 = \bH_2 \bH = \bH_2$$
since $\bH_2$ is the projection matrix of the columns of $\bX_2$ a subspace of the
columns of $\bX$. Then:

\begin{align*}
  (\bH - \bH_2)(\bH - \bH_2)
    &= \bH \bH - \bH_2 \bH - \bH \bH_2 + \bH_2 \bH_2 \\
    &= \bH - \bH_2 \bH - \bH \bH_2 + \bH_2           && \text{since $\bH_2$ and $\bH$ are idempotent}. \\
    &= \bH - \bH_2 - \bH_2 + \bH_2                   && \text{since $\bH \bH_2 = \bH_2 \bH = \bH_2$}.  \\
    &= \bH - \bH_2                                   &&                                                \\
\end{align*}

then $(\bH - \bH_2)$ is idempotent.

Now let us see that $(\bH - \bH_R)\by = (\bH - \bH_R)\be$ under the null hypothesis.
First, let us note that:

$$ \bH \bX_2 = \bX_2 $$
since space generated by $\bX_2$ is a subspace of the space generated by $\bX$,
since $\bX$ contains the columns of $\bX_2$. And we also note that:

$$ \bH_2 \bX_2 = \bX_2 $$
since $\bH_2$ is the projection matrix of the space generated by the columns of
$\bX_2$. We note that this results can be proven algebraically.

Then:

\begin{align*}
  (\bH - \bH_2)\by
    &= (\bH - \bH_2)(\bX \bgb + \be)                                      \\
    &= (\bH - \bH_2)([\bX_1 \bX_2] [\bgb_1' \bgb_2']' + \be)              \\
    &= (\bH - \bH_2)(\bX_1 \bgb_1 \bX_2 \bgb_2 + \be)                     \\
    &= (\bH - \bH_2)(\bX_2 \bgb_2) + (\bH - \bH_1)(\bX_1 \bgb_1 + \be)    \\
    &= (\bH \bX_2 - \bH_2\bX_2)\bgb_2 + (\bH - \bH_1)(\bX_1 \bgb_1 + \be) \\
    &= (\bX_2 - \bX_2)\bgb_2 + (\bH - \bH_1)(\bX_1 \bgb_1 + \be)          && \text{since $\bH \bX_1 = \bX_1$ and $\bH_1 \bX_1 = \bX_1$} \\
    &= (\bH - \bH_R)(\bX_1 \bgb_1 + \be)                                  &&                                                            \\
    &= (\bH - \bH_R)\be                                                   && \iff \cH_0: \bgb_1 = \bzero \\
\end{align*}

So, if $\bX_1$ is full rank, then we have that:

$$ (\bH - \bH_2)\by = (\bH - \bH_R)\be \iff \cH_0: \bgb_1 = \bzero $$

Then we can proceed to see what is the distribution of our test statistic under 
the null hypothesis.

\begin{align*}
  F_{\bgb_1=\bzero} 
    &= \frac{\frac{SS_{res,2} - SS_{res}}{k}}{\frac{SS_{res}}{n-p}}                                                                          &&                                                                                                   \\
    &= \frac{\frac{\by'(\bH - \bH_2)\by}{k}}{\frac{\by'(\bI - \bH)\by}{n-p}}                                                                 && \text{since $SS_{res,2} - SS_{res} = \by'(\bH - \bH_2)\by$ and $SS_{res}=\by'(\bI - \bH)\by$}     \\
    &= \frac{\frac{\by'(\bH - \bH_2)\by}{\sigma^2}\frac{1}{k}}{\frac{\by'(\bI - \bH)\by}{\sigma^2}\frac{1}{n-p}}                             &&                                                                                                   \\
    &= \frac{\frac{\by'(\bH - \bH_2)(\bH - \bH_2)(\bH - \bH_2)\by}{\sigma^2}\frac{1}{k}}{\frac{\by'(\bI - \bH)\by}{\sigma^2}\frac{1}{n-p}}   && \text{snce $(\bH - \bH_2)$ is idempotent}.                                                        \\
    &= \frac{\frac{\be'(\bH - \bH_2)(\bH - \bH_2)(\bH - \bH_2)\be}{\sigma^2}\frac{1}{k}}{\frac{\by'(\bI - \bH)\by}{\sigma^2}\frac{1}{n-p}}   && \iff  \cH_0: \bgb_1 = \bzero                                                                      \\
    &= \frac{\frac{\be'(\bH - \bH_2)\be}{\sigma^2}\frac{1}{k}}{\frac{\by'(\bI - \bH)\by}{\sigma^2}\frac{1}{n-p}}                             && \text{since $(\bH - \bH_2)$ is idempotent}.                                                       \\
    &\sim \frac{\frac{\chi^2_{k}}{k}}{\frac{\chi^2_{n-p}}{n-p}}                                                                              && \text{since $(\bH - \bH_2)$ is idempotent and $\frac{\be}{\sqrt{\sigma^2}} \sim N(0, \bI)$}.      \\
    &\sim F_{k,n-p}                                                                                                                          && 
\end{align*}

So, before we observe the data, $F_{\bgb_1=\bzero}$ has a $F_{k,n-p}$ distribution.
Then, once we observe the data, call $F_{\bgb_1=\bzero}^*$ the observed value of
the statistic, and let $F$ be distributed as an $F_{k,n-p}$, we can compute:

$$ \dP(F \geq F_{\bgb_1=\bzero}^*) $$
and reject the null hypothesis if this probability is small and not reject if 
this probability is small.
















