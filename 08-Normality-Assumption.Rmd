# Normality Assumption

## Introduction

Up to this point, we have treated the linear regression model primarily as an **optimization problem**, where the goal was to find parameter estimates that minimize the sum of squared errors. By making only mild assumptions about the error term—such as a zero mean and constant variance—we were able to establish key results like the **Gauss–Markov theorem**, which guarantees that the Ordinary Least Squares (OLS) estimator is the **Best Linear Unbiased Estimator (BLUE)** under those conditions.

We now take a further step by introducing a **distributional assumption** on the error term. Specifically, we assume that the errors follow a **Normal distribution**. This assumption is much stronger than those introduced before, but it provides powerful analytical advantages. In particular, it allows us to:

* Derive the **sampling distributions** of the estimators,
* Conduct **statistical inference** (e.g., confidence intervals and hypothesis tests),
* Formulate and maximize the **likelihood function**, leading to **Maximum Likelihood Estimates (MLE)** of the parameters.

Formally, we assume:

$$
\mathbf{e} \sim N(\mathbf{0}, \sigma^2 \mathbf{I})
$$

The Normal distribution is completely characterized by its mean and variance. Therefore, once we know the expected value and variance of an estimator, assuming normality allows us to fully determine its distribution. Since in previous chapters we have already computed the mean and variance of the OLS estimators, the normality assumption now enables us to describe their entire probabilistic behavior.

Furthermore, the assumption of normality allows us to derive the **likelihood function** of the observed data, which serves as the foundation for **Maximum Likelihood Estimation**. This framework not only provides an alternative route to parameter estimation but also forms the basis for many modern extensions of regression analysis, including Bayesian regression and generalized linear models.

---

## Maximum Likelihood Estimation

To obtain the maximum likelihood estimates of $\boldsymbol{\beta}$ and $\sigma^2$, we first need the distribution of $\mathbf{y}$. From the regression model

$$
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{e},
$$

and the assumption $\mathbf{e} \sim N(\mathbf{0}, \sigma^2 \mathbf{I})$, we see that $\mathbf{y}$ is simply a **linear transformation** of a multivariate normal random vector. Therefore, $\mathbf{y}$ is also normally distributed, with mean and variance given by:

$$
\mathbf{y} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}).
$$

Consequently, the likelihood function of $\boldsymbol{\beta}$ and $\sigma^2$, given $\mathbf{y}$, is:

$$
\mathcal{L}(\boldsymbol{\beta}, \sigma^2 \mid \mathbf{y}) = N(\mathbf{y} \mid \mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}),
$$

which, when written explicitly, becomes:

\begin{align*}
  \cL(\bgb, \sigma^2 | \by )
    &= (2 \pi)^{-\frac{n}{2}} |\sigma^2 \bI|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2}(\by - \bX \bgb)'(\sigma^2 \bI)^{-1}(\by - \bX \bgb) \right\}    \\
    &= (2 \pi)^{-\frac{n}{2}} (\sigma^2)^{-\frac{n}{2}} |\bI| \exp\left\{ -\frac{1}{2}(\by - \bX \bgb)'\frac{\bI}{\sigma^2}(\by - \bX \bgb) \right\} \\
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\by - \bX \bgb)'(\by - \bX \bgb) \right\}                                   \\
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\by - \bX \bgb)'(\by - \bX \bgb) \right\}                                   \\
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\by - \bX \bgb)'(\by - \bX \bgb) \right\}                                   \\
\end{align*}

---

### Decomposition of the Quadratic Form

Recall that in the previous chapter we showed the following decomposition:

$$
(\mathbf{y} - \mathbb{E}[\mathbf{y}])'(\mathbf{y} - \mathbb{E}[\mathbf{y}])
= \hat{\mathbf{e}}'\hat{\mathbf{e}} + (\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}])'(\hat{\mathbf{y}} - \mathbb{E}[\hat{\mathbf{y}}]),
$$

which implies that

$$
(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
= \hat{\mathbf{e}}'\hat{\mathbf{e}} + (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})' \mathbf{X}'\mathbf{X} (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}).
$$

Substituting this into the likelihood function gives:

\begin{align*}
  \cL(\bgb, \sigma^2 | \by )
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2 \sigma^2}(\by - \bX \bgb)'(\by - \bX \bgb) \right\}                                                      \\
    &= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ -\frac{\hat{\be}'\hat{\be}}{2 \sigma^2} -\frac{(\hat{\bgb} - \bgb)'\bX \bX( \hat{\bgb} - \bgb)}{2 \sigma^2} \right\} \\
\end{align*}

This expression is convenient for optimization. Notice that, for any fixed $\sigma^2$, the likelihood is maximized with respect to $\boldsymbol{\beta}$ when $\boldsymbol{\beta} = \hat{\boldsymbol{\beta}}$, since that value nullifies the second exponential term. Therefore, the MLE for $\boldsymbol{\beta}$ coincides with the OLS estimator.

---

### Estimation of $\sigma^2$

Once we have $\boldsymbol{\beta} = \hat{\boldsymbol{\beta}}$, the likelihood simplifies to a function of $\sigma^2$ only:

$$ \cL(\sigma^2 | \by, \bgb = \hat{\bgb}) = (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{ \frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{2\sigma^2} \right\}  $$

Taking logs gives the **log-likelihood function**:

$$
\ell(\sigma^2 \mid \mathbf{y}, \boldsymbol{\beta} = \hat{\boldsymbol{\beta}})
= -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{2\sigma^2}.
$$

Differentiating with respect to $\sigma^2$ and setting the derivative equal to zero yields:

$$
\tilde{\sigma}^2 = \frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n}.
$$

A second derivative check confirms that this value indeed maximizes the log-likelihood. Thus, $\tilde{\sigma}^2$ is the **maximum likelihood estimator** of $\sigma^2$.

---

### Remarks on Bias and Practical Use

It is important to note that $\tilde{\sigma}^2$ is **biased** as an estimator of $\sigma^2$. In contrast, the usual OLS variance estimator

$$
\hat{\sigma}^2 = \frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n - p}
$$

is unbiased under the Gauss–Markov assumptions. Both estimators are useful: $\tilde{\sigma}^2$ arises naturally in likelihood-based methods and is convenient for theoretical developments, while $\hat{\sigma}^2$ is preferred for unbiased estimation and inferential procedures.

In the following sections, we will build on these results to derive the exact **sampling distributions** of the OLS estimators and the basis for hypothesis testing within the classical linear model framework.

---

## Distribution of the Estimates

In the previous chapters, we derived the mean and variance of the least squares estimators under the assumption of normally distributed errors. We now go further and obtain their **sampling distributions**, which fully characterize the stochastic behavior of these estimators.
This is possible because, under normality, any linear combination of normally distributed random variables is itself normal. Since most of our estimators are linear functions of the observed data, their distributions are readily available.

### Distribution of $\hat{\bgb}$, $\hat{\by}$, and $\hat{\be}$

The least squares estimators $\hat{\bgb}$, $\hat{\by}$, and $\hat{\be}$ are all linear transformations of the response vector $\by$. Therefore, they are normally distributed, and the corresponding mean and variance expressions derived earlier allow us to determine their full distributions. Specifically, we have:

$$
\hat{\bgb}, \quad \hat{\by}, \quad \hat{\be}
$$

and their respective distributions are given by:

$$
\hat{\bgb} \sim N(\bgb, \sigma^2 (\bX'\bX)^{-1})
$$

$$
\hat{\by} \sim N(\bX \bgb, \sigma^2 \bH)
$$

$$
\hat{\be} \sim N(\bzero, \sigma^2 (\bI - \bH))
$$

These results reveal that both fitted values and residuals are multivariate normal, each with a distinct covariance structure determined by the projection matrices $\bH$ and $\bI - \bH$. The matrix $\bH$ projects $\by$ onto the column space of $\bX$, while $(\bI - \bH)$ projects it onto its orthogonal complement.

Note that the estimator of the error variance, $\hat{\sigma}^2$, is **not** a linear function of $\by$, so its distribution must be derived differently.

---

### Distribution of $\hat{\sigma}^2$

The estimator $\hat{\sigma}^2$ measures the variability of the residuals around the fitted model. Its distribution is central to many inferential procedures, such as constructing confidence intervals and hypothesis tests for the regression coefficients.
However, because $\hat{\sigma}^2$ involves the quadratic form $\hat{\be}'\hat{\be}$, its derivation is more involved. We will obtain its distribution in three steps:

1. Express $\frac{\hat{\be}'\hat{\be}}{\sigma^2}$ as a **quadratic form** of a standard normal vector with an idempotent matrix.
2. Show that such a quadratic form follows a **chi-squared distribution** with degrees of freedom equal to the rank of the idempotent matrix.
3. Relate this result to the distribution of $\hat{\sigma}^2$ itself.

---

#### Step 1: Expressing $\frac{\hat{\be}'\hat{\be}}{\sigma^2}$ as a Quadratic Form

We begin by rewriting the residual vector in terms of the error vector $\be$:

\begin{align*}
  (\bI - \bH) \by
    &= (\bI - \bH) (\bX \bgb + \be)                    && \text{since $\by = \bX \bgb + \be$}  \\
    &= \bI \bX \bgb - \bH \bX \bgb + \bI \be - \bH \be                                         \\
    &= \bX \bgb - \bX \bgb + \be - \bH \be             && \text{since $\bH \bX = \bX$}         \\
    &= \be - \bH \be                                                                           \\
    &= (\bI - \bH) \be                                                                         \\
\end{align*}

Then:

\begin{align*}
  \by' (\bI - \bH) \by
    &= \by' (\bI - \bH) (\bI - \bH) \by  && \text{since $(\bI - \bH)$ is idempotent}         \\
    &= \be' (\bI - \bH) (\bI - \bH) \be  && \text{since $(\bI - \bH) \by = (\bI - \bH) \be$} \\
    &= \be' (\bI - \bH) \be              && \text{since $(\bI - \bH)$ is idempotent}         \\
\end{align*}

Thus,

\begin{align*}
  \frac{\hat{\be}'\hat{\be}}{\sigma^2}
    &= \frac{\by' (\bI - \bH) \by}{\sigma^2}  && \text{since $\hat{\be}'\hat{\be} = \by' (\bI - \bH) \by$}  \\
    &= \frac{\be' (\bI - \bH) \be}{\sigma^2}  && \text{since $\be' (\bI - \bH) \be = \by' (\bI - \bH) \by$} \\
    &= \frac{\be}{\sqrt{\sigma^2}}' (\bI - \bH) \frac{\be}{\sqrt{\sigma^2}}                                 \\
\end{align*}

Finally, because $\frac{\be}{\sqrt{\sigma^2}}$ is a scaled version of the normal vector $\be$, it remains normally distributed with zero mean and identity covariance:

$$
\dE\left[\frac{\be}{\sqrt{\sigma^2}}\right] = \bzero, \qquad
\dV\left[\frac{\be}{\sqrt{\sigma^2}}\right] = \bI
$$

Hence,

$$
\frac{\be}{\sqrt{\sigma^2}} \sim N(\bzero, \bI)
$$

This vector is known as a **standard multivariate normal**, completing Step 1.

---

#### Step 2: The Chi-Squared Distribution of a Quadratic Form

Let $\bz \in \mathbb{R}^n$ be a standard multivariate normal vector and $\bM \in \mathbb{R}^{n \times n}$ an idempotent matrix of rank $m$. We now show that:

$$
\bz' \bM \bz \sim \chi^2_m
$$

To prove this, we use the spectral decomposition of $\bM$:

$$
\bM = \bV \bgS \bV'
$$

where $\bV$ is an orthonormal matrix and $\bgS$ is diagonal. Since $\bM$ is idempotent, the diagonal of $\bgS$ consists of $m$ ones and $n-m$ zeros. Without loss of generality, we
can assume that the first $m$ entries of the diagonal are equal to $1$ and the 
next entries equal to $0$.

Then, first note that $\bV' \bz$ is a linear combination of a normal distribution.
We will show, that $\bV' \bz \in \dR^{n}$ is also standard normal.

Note that:

$$ \dE[\bV' \bz] =  \bV' \dE[\bz] = \bV' \bzero = \bzero$$
$$ \dV[\bV' \bz] =  \bV' \dV[\bz] \bV = \bV' \bI \bV = \bV' \bV = \bI$$

Then $\bV' \bz$ is also standard normal. Let's name $\bw = \bV' \bz$, then each of
the components $w_1,\ldots,w_n$ of $\bw$ are independent univariate standard normally
distributed.

Then

\begin{align*}
  \bz' \bM \bz
    &= \bz' (\bV \bgS \bV') \bz       && \text{using the spectral decomposition of $\bM$}         \\
    &= (\bV' \bz)' \bgS (\bV' \bz)                                                                \\
    &= \bw' \bgS \bw                  && \text{since $\bw = \bV' \bz$}                            \\
    &= \sum_{i=1}^n [\bgS]_{ii} w_i^2                                                             \\
    &= \sum_{i=1}^{m} w_i^2           && \text{since only the first $m$ entries are equal to $1$} \\
    &\sim \chi^2_m                    && \text{by definition of the $\chi^2$ distribution}        \\
\end{align*} 

This fundamental result connects linear algebra and probability theory and is essential to statistical inference in linear regression.

---

#### Step 3: Distribution of $\hat{\sigma}^2$

Combining the results from Steps 1 and 2, we conclude that:

$$
\frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \chi^2_{n-p}
$$

since the idempotent matrix $(\bI - \bH)$ has rank $n-p$. Therefore:

$$
\hat{\sigma}^2 = \frac{\hat{\be}'\hat{\be}}{n-p}
= \frac{\sigma^2}{n-p}\frac{\hat{\be}'\hat{\be}}{\sigma^2}
\sim \frac{\sigma^2}{n-p}\chi^2_{n-p}
$$

This result shows that the estimator of $\sigma^2$ is scaled chi-squared distributed, a fact that will be crucial when constructing confidence intervals and hypothesis tests.

---

### Independence of $\hat{\be}$ and $\hat{\by}$

Previously, we showed that $\hat{\be}$ and $\hat{\by}$ are uncorrelated:

$$
\dC[\hat{\be}, \hat{\by}] = \bzero
$$

However, uncorrelatedness does not necessarily imply independence. In general, two random variables can have zero covariance and still exhibit nonlinear dependence.

Under the **normality assumption**, this distinction disappears: if two normally distributed vectors are uncorrelated, they are also independent. Since both $\hat{\be}$ and $\hat{\by}$ are linear combinations of the normal vector $\by$, they are jointly normal. Therefore, the absence of correlation between them implies that they are independent:

* $\hat{\be}$ depends only on $(\bI - \bH)\by$, the projection onto the residual space.
* $\hat{\by}$ depends only on $\bH\by$, the projection onto the column space of $\bX$.

These two subspaces are orthogonal, and under normality, orthogonality implies statistical independence. Consequently, any statistic that is a function of $\hat{\be}$ (such as $\hat{\sigma}^2$) is independent of any statistic that is a function of $\hat{\by}$ (such as $\hat{\bgb}$).

This independence property plays a pivotal role in classical regression inference, as it underlies the derivation of $t$ and $F$ distributions for hypothesis testing.

---

## Interval Estimation

So far, we have derived **point estimates** for several parameters of interest—namely the regression coefficients $\bgb$, the errors $\be$, and the residual variance $\hat{\sigma}^2$.
While point estimates provide single “best guesses” of the true parameters, they do not convey how much uncertainty is associated with these estimates.

However, since we have already obtained the **sampling distributions** of these estimators, we can now use this probabilistic information to construct **interval estimators**, which express a range of plausible values for the unknown parameters, given the observed data.

---

### Confidence Intervals for the Coefficients

Recall that under the classical linear model assumptions, the ordinary least squares estimator satisfies

$$ \hat{\bgb} \sim N(\bgb, \sigma^2 (\bX'\bX)^{-1}) $$

This result implies that each individual coefficient estimator $\hat{\beta}_i$ follows a univariate normal distribution:

$$ \hat{\beta}_i \sim N(\beta_i, \sigma^2 [(\bX'\bX)^{-1}])_{ii}) $$

The quantity

$$ \sigma^2_{\beta_i} =  \sigma^2 [(\bX'\bX)^{-1}]_{ii} $$

represents the **variance** of $\hat{\beta}_i$, which depends both on the noise variance $\sigma^2$ and on the geometry of the design matrix $\bX$.

Thus, we can rewrite the distribution as

$$ \hat{\beta}_i \sim N \left(\beta_i, \sigma^2_{\beta_i} \right) $$

Since $\sigma^2$ is unknown, this distribution cannot be used directly for inference.
To overcome this, we first standardize the estimator by subtracting its mean and dividing by its true standard deviation:

$$ t^0_{\beta_i}=\frac{\hat{\beta}_i - \beta_i}{\sqrt{\sigma^2_{\beta_i}}} $$

This standardized statistic follows the **standard normal distribution**, as confirmed by the derivations of its mean and variance. However, it still involves the unknown $\sigma^2$.

Therefore, we replace $\sigma^2$ with its unbiased estimator $\hat{\sigma}^2$ to form

$$ t_{\beta_i} = \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}} $$

where $\hat{\sigma}^2_{\beta_i} = \hat{\sigma}^2 [(\bX'\bX)^{-1}]_{ii}$, so $t_{\beta_i}$
doesn't depend on $\sigma^2$. Let's compute the distribution of this quantity,
first lets re-write the statistic as follows:

$$ t_{\beta_i} = \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}} 
  = \frac{\sqrt{\frac{1}{\sigma^2}}}{\sqrt{\frac{1}{\sigma^2}}}\frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2[(\bX'\bX)^{-1}]_{ii}}}   
  = \frac{\frac{\left(\hat{\beta}_i - \beta_i\right)}{\sqrt{\sigma^2[(\bX'\bX)^{-1}]_{ii}}}}{ \sqrt{\frac{\hat{\sigma}^2}{\sigma^2}}}   
  = \frac{\frac{\left(\hat{\beta}_i - \beta_i\right)}{\sqrt{\sigma^2_{\beta_i}}}}{            \sqrt{\frac{(n-p)\frac{\hat{\sigma}^2}{\sigma^2}}{n-p}}}
  = \frac{t^0_{\beta_i}}{ \sqrt{\frac{\frac{\hat{\be}'\hat{\be}}{\sigma^2}}{n-p}}}$$

Now, we know $t^0_{\beta_i}$ is standard normal distributed, and from 
[the distribution of $\hat{\sigma}^2$](#distribution-of-hatsigma2-step-3) we 
have that:

$$ \frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \chi^2_{n-p} $$
and from 
[the independence of $\hat{\bgb}$ and $\hat{\be}$](#independence-of-hatmathbfe-and-hatmathbfy)
we have that any function of both variables is independent, in particular

$$ t^0_{\beta_i} \quad \text{and} \quad \frac{\hat{\be}'\hat{\be}}{\sigma^2} $$
are independent. Therefore the ratio $t_{\beta_i}$ follows a **Student’s $t$ distribution** with $n - p$ degrees of freedom:

$$ t_{\beta_i} \sim t_{n-p} $$

This fundamental result enables us to construct confidence intervals for each regression coefficient.
Now, let $t \sim t_m$ a random variable with a $t$ distribution with $m$ degrees
of freedom. Then call:

$$ t_m\left(a\right) \quad \text{such that} \quad \dP\left(t\leq t_m\left(a\right) \right) = a$$
for any $a\in[0,1]$

Then, we have that:

\begin{align*}
\dP 
  &\left( -t_{n-p}\left(\frac{\alpha}{2}\right) \leq t_{\beta_i} \leq t_{n-p}\left(\frac{\alpha}{2}\right) \right) = \alpha  
    && \text{since the $t$ distribution is symmetric} \\
  &\implies \dP\left( -t_{n-p}\left(\frac{\alpha}{2}\right) \leq \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}} \leq t_{n-p}\left(\frac{\alpha}{2}\right) \right) = \alpha
    && \text{since the $t_{\beta_i} = \frac{\hat{\beta}_i - \beta_i}{\sqrt{\hat{\sigma}^2_{\beta_i}}}$} \\
  &\implies \dP\left( -t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \leq \hat{\beta}_i - \beta_i \leq t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) = \alpha \\
  &\implies \dP\left( -t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \leq \beta_i - \hat{\beta}_i \leq t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) = \alpha \\
  &\implies \dP\left( \hat{\beta}_i - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \leq \beta_i \leq \hat{\beta}_i + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) = \alpha \\
\end{align*}

So

$$ \left(\hat{\beta}_i - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}}, \hat{\beta}_i + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\beta_i}} \right) $$

This **random interval** depends on the sample through $\hat{\beta}_i$ and $\hat{\sigma}^2_{\beta_i}$ and contains the true parameter $\beta_i$ with probability $\alpha$.
Once data are observed, the interval becomes fixed, and while it either includes or excludes $\beta_i$, the confidence level reflects the long-run frequency with which such intervals contain the true value under repeated sampling.

---

### Confidence Intervals for the Expected Mean of a New Observation $\bx_{new}$

In many applications, the researcher is not only interested in the regression coefficients themselves but also in the **expected value of the response** for a new vector of predictors $\bx_{new}$.

The expected response is given by

$$ \dE[y_{new}] = \bx_{new}' \bgb $$

and its natural estimator is

$$ \bx_{new}' \hat{\bgb} $$

Since $\hat{\bgb}$ is normally distributed, this linear combination is also normal with

$$ \bx_{new}' \hat{\bgb} \sim N \left(\bx_{new}' \bgb, \sigma^2 \bx_{new}' (\bX' \bX)^{-1} \bx_{new} \right)  $$

Replacing $\sigma^2$ with $\hat{\sigma}^2$ and applying the same reasoning as before, we obtain the statistic

$$ t_{\bx_{new}'\bgb} = \frac{\bx_{new}'\hat{\bgb} - \bx_{new}'\bgb}{\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}}}=\frac{\frac{\bx_{new}'\hat{\bgb} - \bx_{new}'\bgb}{\sqrt{\sigma^2_{\bx_{new}'\bgb}}}}{\sqrt{\frac{\frac{\hat{\be}'\hat{\be}}{\sigma^2}}{n-p}}} \sim \chi^2_{n-p} $$
where $\hat{\sigma}^2_{\bx_{new}'\bgb} = \hat{\sigma}^2 \bx_{new}' (\bX' \bX)^{-1} \bx_{new}$.
This quantity is distributed as a $t$ with $n-p$ degrees of freedom since:

$$\frac{\bx_{new}'\hat{\bgb} - \bx_{new}'\bgb}{\sqrt{\sigma^2_{\bx_{new}'\bgb}}} \sim N(0, 1)$$
$$ \frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \chi^2_{n-p}$$
and this random variables are independent since one is a function of $\hat{\bgb}$
and the other a function of $\hat{\be}$.

Then we can conclude that:

\begin{align*}
\dP 
  &\left( -t_{n-p}\left(\frac{\alpha}{2}\right) \leq t_{\bx_{new}'\bgb} \leq t_{n-p}\left(\frac{\alpha}{2}\right) \right) = \alpha  \\
  &\implies \dP\left( \bx_{new}'\hat{\bgb} - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}} \leq \bx_{new}'\bgb \leq \bx_{new}'\hat{\bgb} + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}} \right) = \alpha \\
\end{align*}

so, the random interval is given by:

$$ \left( \bx_{new}'\hat{\bgb} - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}} , \bx_{new}'\hat{\bgb} + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\bx_{new}'\bgb}} \right) $$
is a random interval that captures $\bx_{new}'\bgb$ with probability $\alpha$.


This interval quantifies uncertainty about the **mean response** at $\bx_{new}$, not about an individual observation, which would require adding the variance of the random error term.

---

### Confidence Intervals for Linear Combinations of $\bgb$

A particularly elegant aspect of the linear model is that it allows inference on **any linear combination** of the coefficients.
Consider a vector $\ba \in \mathbb{R}^p$ and the parameter of interest $\ba' \bgb$.

This formulation encompasses several important special cases:

* $\ba = (0, \ldots, 0, 1, 0, \ldots, 0)$ yields $\ba' \bgb = \beta_i$, the $i$-th coefficient;
* $\ba = \bx_{new}$ yields $\ba' \bgb = \bx_{new}' \bgb$, the expected mean at a new data point.

Since $\ba' \hat{\bgb}$ is a linear combination of normally distributed estimators, it follows that

$$ \ba' \hat{\bgb} \sim N(\ba' \bgb, \sigma^2 \ba' (\bX'\bX)^{-1} \ba) $$

and replacing $\sigma^2$ by its estimator leads to a $t_{n-p}$ distribution for the corresponding standardized statistic.
Therefore, the general form of a $(1 - \alpha)$ confidence interval for any linear combination is

$$ \left( \ba'\hat{\bgb} - t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\ba'\bgb}} , \ba'\hat{\bgb} + t_{n-p}\left(\frac{\alpha}{2}\right)\sqrt{\hat{\sigma}^2_{\ba'\bgb}} \right) $$

where $\hat{\sigma}^2_{\ba'\bgb} = \hat{\sigma}^2 \ba' (\bX' \bX)^{-1} \ba$.

This result unifies the previous confidence intervals into a single, compact framework and highlights the power and flexibility of the linear model for statistical inference.

---


## Hypothesis Testing

We will approach hypothesis testing using an implausibility framework. 
This involves formulating a null hypothesis, $H_0$, and assuming it to be true.
Next, we calculate a test statistic that follows a specific distribution under 
the null hypothesis. By comparing the observed value of the statistic to this 
distribution, we assess how plausible it is to observe such a value if $H_0$ is true.


### Testing for the Overall Regression

For this hypothesis, we will use the notation of:

$$ \bX^* = [\bones \bX] \quad \text{and} \quad \bgb^* = [\beta_0, \bgb]' \in \dR^{p}$$
that is, the $*$ indicates all the independent variables. With $\bX$ of full rank.

Our first test is to see if the Linear Regression framework is useful at all.
That is, we want to test $\cH_0: \bgb = \bzero$. Before designing our test 
statistic we will show the following auxiliary results:

1. $SS_{reg} = \by'(\bH - \bH_0)\by$.
2. $\bH \bH_0 = \bH_0 \bH = \bH_0$.
3. $(\bH - \bH_0)$ is idempotent.
4. $\by'(\bH - \bH_0)\by$ and $\by'(\bI - \bH)\by$ are independent.
5. Under the null hypothesis $\cH_0: \bgb = \bzero$, $\frac{\by'(\bH - \bH_0)\by}{\sigma^2}$
is distributed like a $\chi^2_{p-1}$.

For auxiliary result 1, we have that:

\begin{align*}
SS_{tot} 
  &= SS_{reg} + SS_{res} \\
  &\implies \by'(\bI - \bH_0)\by = SS_{reg} + \by'(\bI - \bH)\by && \text{since $SS_{tot} = \by'(\bI - \bH)\by$ and $SS_{res} = \by'(\bI - \bH)\by$.} \\
  &\implies SS_{reg} = \by'(\bI - \bH_0)\by - \by'(\bI - \bH)\by &&                                                                                   \\
  &\implies SS_{reg} = \by'(\bI - \bH_0 - \bI + \bH)\by          &&                                                                                   \\
  &\implies SS_{reg} = \by'(\bH - \bH_0)\by                      &&                                                                                   \\
\end{align*}

For auxiliary result 2, we have that:

Both $\bH$ and $\bH0$ are symmetric, then $\bH \bH_0 = \bH_0 \bH$, and:

\begin{align*}
  \bH \bH_0 = \bH \bones (\bones' \bones)^{-1} \bones' &&                                     \\
  \bH \bH_0 = \bones (\bones' \bones)^{-1} \bones'     && \text{since $\bH \bones = \bones$.} \\
  \bH \bH_0 = \bH_0                                    &&                                     \\
\end{align*}

For auxiliary result 3, we have that:

\begin{align*}
  (\bH - \bH_0)(\bH - \bH_0) 
    &= \bH \bH - \bH \bH_0 - \bH_0 \bH + \bH_0 \bH_0 &&                                                \\
    &= \bH - \bH \bH_0 - \bH_0 \bH + \bH_0           && \text{since $\bH_0$ and $\bH$ are idempotent.} \\
    &= \bH - \bH_0 - \bH_0 + \bH_0                   && \text{since $\bH \bH_0 = \bH_0 \bH = \bH_0$.}  \\
    &= \bH - \bH_0                                   &&                                                \\
\end{align*}

so, $(\bH - \bH_0)$ is idempotent.

For auxiliary result 4, first we have that:

\begin{align*}
  \dC[(\bH - \bH_0) \by, (\bI - \bH) \by]
    &= (\bH - \bH_0) \dC[\by,\by] (\bI - \bH)        \\
    &= (\bH - \bH_0) \dV[\by] (\bI - \bH)            \\
    &= \sigma^2 (\bH - \bH_0) (\bI - \bH)            \\
    &= \sigma^2 (\bH - \bH_0  - \bH \bH + \bH_0 \bH) \\
    &= \sigma^2 (\bH - \bH_0  - \bH + \bH_0 \bH)     && \text{since $\bH$ is idempotent.} \\
    &= \sigma^2 (\bH - \bH_0  - \bH + \bH_0)         && \text{since $\bH_0 \bH = \bH_0$.} \\
    &= \sigma^2 \bzero                               &&                                   \\
    &= \bzero                                        &&                                   \\
\end{align*}

This tells us that $(\bH - \bH_0)\by$ and $(\bI - \bH)\by$ are uncorrelated. Now,
since $(\bH - \bH_0)\by$ and $(\bI - \bH)\by$ are normally distributed, then 
zero correlation implies independence. Then, any function of this 2 quantities 
are independent. Note that:

\begin{align*}
  \by'(\bH - \bH_0)\by 
    &= \by'(\bH - \bH_0)(\bH - \bH_0)(\bH - \bH_0)\by && \text{since $(\bH - \bH_0)$ is idempotent.}
\end{align*}

Then, $\by'(\bH - \bH_0)\by$ is a quadratic function of $(\bH - \bH_0)\by$. Similarly,
$\by'(\bH - \bH)\by$ is a quadratic function of $(\bH - \bH)\by$. Therefore, 
$\by'(\bH - \bH_0)\by$ and $\by'(\bH - \bH)\by$ are independent.

For result 5, we have that:

\begin{align*}
  (\bH - \bH_0)\by 
    &= (\bH - \bH_0)(\bX^* \bgb^* + \be) && \text{since $\by = \bX \bgb + \be$.} \\
    &= (\bH - \bH_0)([\bones \bX] [\beta_0, \bgb]' + \be)                && \text{since $\bX^* = [\bones \bX] \quad \text{and} \quad \bgb^* = [\beta_0, \bgb]'$.} \\
    &= (\bH - \bH_0)(\bones\beta_0 + \bX\bgb + \be) &&  \\
    &= (\bH - \bH_0)(\bones\beta_0) + (\bH - \bH_0)(\bX\bgb + \be) &&  \\
    &= (\bH \bones - \bH_0 \bones)\beta_0 + (\bH - \bH_0)(\bX\bgb + \be) &&  \\
    &= (\bones - \bones)\beta_0 + (\bH - \bH_0)(\bX\bgb + \be) &&  \\
    &= (\bH - \bH_0)(\bX\bgb + \be) &&  \\
    &= (\bH - \bH_0)\be                                                  && \text{iff $\cH_0: \bgb = \bzero$ for any full rank $\bX$.}
\end{align*}

That is, for any full rank $\bX$, we have that:

$$ (\bH - \bH_0)\by  = (\bH - \bH_0)\be \iff \cH_0: \bgb = \bzero$$

Then:

\begin{align*}
  \be \sim N(0, \sigma^2 \bI)
    &\implies \be'(\bH - \bH_0)\be \sim \sigma^2 \chi^2_{p-1}                                   && \text{since $(\bH - \bH_0)$ is idempotent of rank $p-1$}. \\
    &\implies \frac{\be'(\bH - \bH_0)\be}{\sigma^2} \sim \chi^2_{p-1}                           &&                                                           \\
    &\implies \frac{\be'(\bH - \bH_0)(\bH - \bH_0)(\bH - \bH_0)\be}{\sigma^2} \sim \chi^2_{p-1} && \text{since $(\bH - \bH_0)$ is idempotent}.               \\
    &\implies \frac{\by'(\bH - \bH_0)(\bH - \bH_0)(\bH - \bH_0)\by}{\sigma^2} \sim \chi^2_{p-1} && \text{iff the null hypothesis holds}.                     \\
    &\implies \frac{\by'(\bH - \bH_0)\by}{\sigma^2} \sim \chi^2_{p-1}                           && \text{since $(\bH - \bH_0)$ is idempotent}.               \\
\end{align*}

That is:

$$ \frac{\by'(\bH - \bH_0)\by}{\sigma^2} \sim \chi^2_{p-1} \iff \cH_0: \bgb = \bzero$$

With this results, we propose the following statistic:

$$ F_{\bgb = 0} = \frac{\frac{SS_{reg}}{p-1}}{\frac{SS_{res}}{n-p}} $$

and we will show that, this statistic is distributed like an $F_{p-1,n-p}$ only
under the null hypothesis.

\begin{align*}
  F_{\bgb = 0} 
    &= \frac{\frac{SS_{reg}}{p-1}}{\frac{SS_{res}}{n-p}}                                                           &&                                                                                                   \\
    &= \frac{\frac{\by'(\bH - \bH_0)\by}{p-1}}{\frac{\by'(\bI - \bH)\by}{n-p}}                                     && \text{since $SS_{reg} = \by'(\bH - \bH_0)\by$ and $SS_{res}=\by'(\bI - \bH)\by$}                  \\
    &= \frac{\frac{\by'(\bH - \bH_0)\by}{\sigma^2}\frac{1}{p-1}}{\frac{\by'(\bI - \bH)\by}{\sigma^2}\frac{1}{n-p}} &&                                                                                                   \\
    &\sim \frac{\frac{\chi^2_{p-1}}{p-1}}{\frac{\chi^2_{n-p}}{n-p}}                                                && \text{since $\frac{\by'(\bH - \bH_0)\by}{\sigma^2} \sim \chi^2_{p-1}$ under the null hypothesis.} \\
    &\sim F_{p-1,n-p}                                                                                              && \text{since $\by'(\bH - \bH_0)\by$ and $\by'(\bH - \bH)\by$ are independent.}
\end{align*}

So, once we observe the value of this statistic, we can contrast it with respect
respect to this distribution. Call $F^*_{\bgb = 0}$ the observed value, and consider
a random variable $F \sim F_{p-1,n-p}$, then we can see what would be the
probability of observing the value of the statistic (or a more extreme value).

$$ \dP(F \geq F^*_{\bgb = 0}) $$
depending on how small or big is this probability, we can reject or not reject the
null hypothesis. This value is a called a p-value.

### Testing if one variable is not relevant

We can test if a particular variable is not relevant for the regression. That is,
$\cH_0: \beta_i = 0$. We will use the same strategy, that is, we will build a 
test statistic that has a certain distribution only under the null hypothesis.

For this hypothesis we propose the following test statistic:

$$ t_{\beta_i = 0} = \frac{\hat{\beta}_i}{\sqrt{\hat{\sigma}^2 [(\bX \bX)^{-1}]_{ii}}} $$
First note that:

\begin{align*}
\hat{\bgb} \sim N \left(\bgb, \sigma^2(\bX \bX)^{-1}\right)
  &\implies \hat{\beta}_i \sim H(\beta_i, \sigma^2 [(\bX \bX)^{-1}]_{ii})                                                                              \\
  &\implies \frac{\hat{\beta}_i}{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}} \sim N \left(\frac{\beta_i}{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}}, 1 \right) \\
  &\implies \frac{\hat{\beta}_i}{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}} \sim N \left( 0, 1 \right) &&  \iff \cH_0: \beta_i = 0                         \\
\end{align*}

Then we have:

$$$$

\begin{align*}
t_{\beta_i = 0} 
  &= \frac{\hat{\beta}_i}{\sqrt{\hat{\sigma}^2 [(\bX \bX)^{-1}]_{ii}}} \\
  &= \frac{\frac{\hat{\beta}_i }{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}}}{\frac{\sqrt{\hat{\sigma}^2 [(\bX \bX)^{-1}]_{ii}}}{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}}} \\
  &= \frac{\frac{\hat{\beta}_i }{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}}}{\sqrt{\frac{\hat{\sigma}^2}{\sigma^2}}}                                                    \\
  &= \frac{\frac{\hat{\beta}_i }{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}}}{\sqrt{\frac{\hat{\be}'\hat{\be}}{\sigma^2}\frac{1}{n-p}}}                                  && \text{since $\hat{\sigma}^2 = \frac{\hat{\be}\hat{\be}}{n-p}$} \\
  &\sim \frac{N \left(\frac{\beta_i}{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}}, 1 \right)}{\sqrt{\frac{\chi^2_{n-p}}{n-p}}}                                            && \text{since $\frac{\hat{\beta}_i}{\sqrt{\sigma^2 [(\bX \bX)^{-1}]_{ii}}} \sim N \left( 0, 1 \right)$ and $\frac{\hat{\be}'\hat{\be}}{\sigma^2} \sim \chi^2_{n-p}$} \\
  &\sim \frac{N \left(0, 1 \right)}{\sqrt{\frac{\chi^2_{n-p}}{n-p}}}                                                                                                && \iff \cH_0: \beta_i = 0  \\
  &\sim t_{n-p}                                                                                                                                                     && \text{since $\hat{\beta}_i$ and $\hat{\sigma}^2$ are independent}.  \\
\end{align*}

Then, under the null hypothesis we have that:

$$ t_{\beta_i = 0} \sim  t_{n-p}$$
So, if we call $t_{\beta_i = 0}^*$ the observed value of $t_{\beta_i = 0}$, and
if we let $t$ be distributed as $t_{n-p}$, we can compute:

$$ \dP(t \geq t_{\beta_i = 0}^*) $$
and depending on the value, we can reject or accept the null hypothesis.

### Testing if a Subgroup of the Variables is Relevant

For this test, we can assume without loss of generality, that the variables we 
want to see if it is relevant are the first $k$. So we can divide the design 
matrix as:

$$ \bX = [\bX_1 \bX_2] $$
where the variables to test are in $\bX_1$ and the rest of the variables are in
$\bX_2$ (including possibly the intercept). And similarly we have $\bgb = [\bgb_1 \bgb_2]'$.

This test is similar to the first test once we express it accordingly. We will 
consider two linear regressions. One including all variables and one excluding 
the variables to be tested indexed by $2$. With this we can build the following 
test statistics:

$$ F_{\bgb_1=\bzero} = \frac{\frac{SS_{res,2} - SS_{res}}{k}}{\frac{SS_{res}}{n-p}} $$
Then note the following:

\begin{align*}
  SS_{res,2} - SS_{res}
    &= \by'(\bI - \bH_2)\by - \by'(\bI - \bH) \by \\
    &= \by'(\bI - \bH_2 - \bI + \bH) \by          \\
    &= \by'(\bH - \bH_2) \by                      \\
\end{align*}

Again, we will see that $(\bH - \bH_2)$ is idempotent and $(\bH - \bH_2)\by = (\bH - \bH_2)\be$
only under the null hypothesis.

First, let us see that $(\bH - \bH_2)$ is idempotent. First note that:

$$ \bH \bH_2 = \bH_2 \bH = \bH_2$$
since $\bH_2$ is the projection matrix of the columns of $\bX_2$ a subspace of the
columns of $\bX$. Then:

\begin{align*}
  (\bH - \bH_2)(\bH - \bH_2)
    &= \bH \bH - \bH_2 \bH - \bH \bH_2 + \bH_2 \bH_2 \\
    &= \bH - \bH_2 \bH - \bH \bH_2 + \bH_2           && \text{since $\bH_2$ and $\bH$ are idempotent}. \\
    &= \bH - \bH_2 - \bH_2 + \bH_2                   && \text{since $\bH \bH_2 = \bH_2 \bH = \bH_2$}.  \\
    &= \bH - \bH_2                                   &&                                                \\
\end{align*}

then $(\bH - \bH_2)$ is idempotent.

Now let us see that $(\bH - \bH_R)\by = (\bH - \bH_R)\be$ under the null hypothesis.
First, let us note that:

$$ \bH \bX_2 = \bX_2 $$
since space generated by $\bX_2$ is a subspace of the space generated by $\bX$,
since $\bX$ contains the columns of $\bX_2$. And we also note that:

$$ \bH_2 \bX_2 = \bX_2 $$
since $\bH_2$ is the projection matrix of the space generated by the columns of
$\bX_2$. We note that this results can be proven algebraically.

Then:

\begin{align*}
  (\bH - \bH_2)\by
    &= (\bH - \bH_2)(\bX \bgb + \be)                                      \\
    &= (\bH - \bH_2)([\bX_1 \bX_2] [\bgb_1' \bgb_2']' + \be)              \\
    &= (\bH - \bH_2)(\bX_1 \bgb_1 \bX_2 \bgb_2 + \be)                     \\
    &= (\bH - \bH_2)(\bX_2 \bgb_2) + (\bH - \bH_1)(\bX_1 \bgb_1 + \be)    \\
    &= (\bH \bX_2 - \bH_2\bX_2)\bgb_2 + (\bH - \bH_1)(\bX_1 \bgb_1 + \be) \\
    &= (\bX_2 - \bX_2)\bgb_2 + (\bH - \bH_1)(\bX_1 \bgb_1 + \be)          && \text{since $\bH \bX_1 = \bX_1$ and $\bH_1 \bX_1 = \bX_1$} \\
    &= (\bH - \bH_R)(\bX_1 \bgb_1 + \be)                                  &&                                                            \\
    &= (\bH - \bH_R)\be                                                   && \iff \cH_0: \bgb_1 = \bzero \\
\end{align*}

So, if $\bX_1$ is full rank, then we have that:

$$ (\bH - \bH_2)\by = (\bH - \bH_R)\be \iff \cH_0: \bgb_1 = \bzero $$

Then we can proceed to see what is the distribution of our test statistic under 
the null hypothesis.

\begin{align*}
  F_{\bgb_1=\bzero} 
    &= \frac{\frac{SS_{res,2} - SS_{res}}{k}}{\frac{SS_{res}}{n-p}}                                                                          &&                                                                                                   \\
    &= \frac{\frac{\by'(\bH - \bH_2)\by}{k}}{\frac{\by'(\bI - \bH)\by}{n-p}}                                                                 && \text{since $SS_{res,2} - SS_{res} = \by'(\bH - \bH_2)\by$ and $SS_{res}=\by'(\bI - \bH)\by$}     \\
    &= \frac{\frac{\by'(\bH - \bH_2)\by}{\sigma^2}\frac{1}{k}}{\frac{\by'(\bI - \bH)\by}{\sigma^2}\frac{1}{n-p}}                             &&                                                                                                   \\
    &= \frac{\frac{\by'(\bH - \bH_2)(\bH - \bH_2)(\bH - \bH_2)\by}{\sigma^2}\frac{1}{k}}{\frac{\by'(\bI - \bH)\by}{\sigma^2}\frac{1}{n-p}}   && \text{snce $(\bH - \bH_2)$ is idempotent}.                                                        \\
    &= \frac{\frac{\be'(\bH - \bH_2)(\bH - \bH_2)(\bH - \bH_2)\be}{\sigma^2}\frac{1}{k}}{\frac{\by'(\bI - \bH)\by}{\sigma^2}\frac{1}{n-p}}   && \iff  \cH_0: \bgb_1 = \bzero                                                                      \\
    &= \frac{\frac{\be'(\bH - \bH_2)\be}{\sigma^2}\frac{1}{k}}{\frac{\by'(\bI - \bH)\by}{\sigma^2}\frac{1}{n-p}}                             && \text{since $(\bH - \bH_2)$ is idempotent}.                                                       \\
    &\sim \frac{\frac{\chi^2_{k}}{k}}{\frac{\chi^2_{n-p}}{n-p}}                                                                              && \text{since $(\bH - \bH_2)$ is idempotent and $\frac{\be}{\sqrt{\sigma^2}} \sim N(0, \bI)$}.      \\
    &\sim F_{k,n-p}                                                                                                                          && 
\end{align*}

So, before we observe the data, $F_{\bgb_1=\bzero}$ has a $F_{k,n-p}$ distribution.
Then, once we observe the data, call $F_{\bgb_1=\bzero}^*$ the observed value of
the statistic, and let $F$ be distributed as an $F_{k,n-p}$, we can compute:

$$ \dP(F \geq F_{\bgb_1=\bzero}^*) $$
and reject the null hypothesis if this probability is small and not reject if 
this probability is small.
















