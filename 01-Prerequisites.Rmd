\newcommand{\ba}{\mathbf{a}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bgb}{\boldsymbol{\beta}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bones}{\mathbf{1}}
\newcommand{\hy}{\hat{y}}
\newcommand{\he}{\hat{e}}
\newcommand{\hgb}{\hat{\beta}}
\newcommand{\hby}{\hat{\mathbf{y}}}
\newcommand{\hbe}{\hat{\mathbf{e}}}
\newcommand{\sumin}{\sum_{i=1}^n}
\newcommand{\sumjn}{\sum_{j=1}^n}

# Prerequisites

Before diving into the course, itâ€™s important to have a solid understanding of the following foundational concepts. These are categorized into five key topics:

* [General Math](#general-math)
* [Linear Algebra](#linear-algebra)
* [Probability](#probability)
* [Statistics](#statistics)
* [Calculus](#calculus)

You can check some of the requirements on Chapter 1 of the textbook.

## General Math

You should be familiar with the **summation operator** $\sum$. This operator is defined as follows:

$$\sum_{i=1}^n x_i = x_1 + x_2 + \ldots + x_n $$

Key properties of the summation operator include:

* **Linearity**:
  $$\sum_{i=1}^N (a + b x_i) = aN + b \sum_{i=1}^N x_i$$

* **Additivity**:
  $$\sum_{i=1}^N (x_i + y_i) = \sum_{i=1}^N x_i + \sum_{i=1}^N y_i$$

## Linear Algebra

You should be familiar with the following linear algebra concepts:

* [Linear Independence](#linear-independence)
* [Column Space of a Matrix](#column-space-of-a-matrix)
* [Rank of a Matrix](#rank-of-a-matrix)
* [Full Rank Matrix](#full-rank-matrix)
* [Inverse Matrix](#inverse-matrix)
* [Positive Definite Matrix](#positive-definite-matrix)
* Determinants
* Eigenvalues and Eigenvectors
* Diagonalization
* Vector Spaces
* Linear Transformations

### Linear Independence

**Linear independence** is a fundamental concept in linear algebra that describes a set of vectors where no vector can be written as a linear combination of the others. In other words, the vectors are not "redundant," meaning none of the vectors depends on any other in the set.

#### Definition:
A set of vectors $\{ \mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n \}$ in a vector space is **linearly independent** if the only solution to the equation:

$$
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n = \mathbf{0}
$$

is when all the scalar coefficients $c_1, c_2, \ldots, c_n$ are zero, i.e., $c_1 = c_2 = \cdots = c_n = 0$.

If any of the coefficients can be non-zero while still satisfying this equation, then the vectors are **linearly dependent**.

#### Example:

Consider two vectors in $\mathbb{R}^2$:

$$
\mathbf{v}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad \mathbf{v}_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}
$$

These vectors are **linearly independent** because there is no way to express one as a multiple of the other. The only solution to:

$$
c_1 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + c_2 \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
$$

is $c_1 = 0$ and $c_2 = 0$.

In contrast, if:

$$
\mathbf{v}_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad \mathbf{v}_2 = \begin{bmatrix} 2 \\ 4 \end{bmatrix}
$$

These vectors are **linearly dependent**, because $\mathbf{v}_2 = 2 \mathbf{v}_1$. Therefore, you can express $mathbf{v}_2$ as a linear combination of $\mathbf{v}_1$.

#### Key Points:
- **Linearly independent** vectors carry distinct information and cannot be derived from each other.
- **Linearly dependent** vectors are redundant because one or more can be expressed as a combination of others.
- In a set of linearly independent vectors, removing any vector would reduce the span of the vector space they cover.

#### Importance:
- Linear independence is crucial in determining the **rank** of a matrix.
- In systems of equations, linear independence of the rows or columns determines if the system has a unique solution.
- In vector spaces, the **dimension** of the space is the maximum number of linearly independent vectors.

### Column Space of a Matrix

The **column space** of a matrix is the set of all possible linear combinations of its columns. If you have a matrix $\mathbf{A}$ with $n$ rows and $p$ columns, the column space of $\mathbf{A}$, denoted as **Col($\mathbf{A}$)**, consists of all vectors in $\mathbb{R}^n$ that can be expressed as a linear combination of the columns of $\mathbf{A}$. 

#### Definition:
Given a matrix $\mathbf{A}$ with columns $\mathbf{a}_1, \mathbf{a}_2, \dots, \mathbf{a}_p$, the column space of $\mathbf{A}$ is defined as:

$$
\text{Col}(\mathbf{A}) = \left\{ \mathbf{y} \in \mathbb{R}^n \mid \mathbf{y} = \mathbf{A} \mathbf{c} \text{ for some } \mathbf{c} \in \mathbb{R}^p \right\}
$$

This means the column space is the span of the columns of $\mathbf{A}$, or equivalently, all vectors that can be written as $\mathbf{y} = c_1 \mathbf{a}_1 + c_2 \mathbf{a}_2 + \dots + c_p \mathbf{a}_p$, where $c_1, c_2, \dots, c_p$ are scalars.

#### Properties:
- The column space of $\mathbf{A}$ is a **subspace** of $\mathbb{R}^n$.
- The **dimension** of the column space of $\mathbf{A}$ is called the **rank** of the matrix and corresponds to the number of linearly independent columns in $\mathbf{A}$.
- The column space provides valuable information about the linear independence and span of the columns of a matrix.

#### Geometric Interpretation:
In geometric terms, the column space represents the set of all possible vectors that can be "reached" by linearly combining the columns of the matrix. For example:
- For a matrix with 2 columns in $\mathbb{R}^3$, the column space will be a plane in $\mathbb{R}^3$ if the columns are linearly independent.
- For a matrix with 3 columns in $\mathbb{R}^2$, the column space will span all of $\mathbb{R}^2$ (if the columns are linearly independent) or a line (if they are dependent).

### Rank of a Matrix

The **rank** of a matrix is the dimension of its column space, which is the number of linearly independent columns in the matrix. Alternatively, it is also the dimension of the row space, which is the number of linearly independent rows.

#### Definition:
For a matrix $\mathbf{A}$, the rank is defined as:

$$
\text{rank}(\mathbf{A}) = \dim(\text{Col}(\mathbf{A})) = \dim(\text{Row}(\mathbf{A}))
$$

This is the maximum number of linearly independent rows or columns in the matrix. In other words, it tells you how many of the matrix's columns (or rows) are not redundant and cannot be written as a linear combination of the others.

#### Key Points:
- The rank of a matrix $\mathbf{A}$ is denoted as **rank($\mathbf{A}$)**.
- It measures the number of independent directions in the column space or row space.
- **Full rank**: A matrix is said to have full rank if its rank is equal to the smaller of the number of rows or columns. For an $m \times n$ matrix:
  - If $\text{rank}(\mathbf{A}) = m$ (number of rows), it has full row rank.
  - If $\text{rank}(\mathbf{A}) = n$ (number of columns), it has full column rank.
- **Rank-deficient**: If the rank of the matrix is less than the smaller of the number of rows or columns, the matrix is called rank-deficient, meaning that some of its rows or columns are linearly dependent.

#### Example:
Consider the matrix:
$$
\mathbf{A} = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}
$$

The rank of $\mathbf{A}$ is 2 because two of the rows (or columns) are linearly independent, but the third row (or column) is a linear combination of the others.

#### Properties:
- The rank of a matrix is always less than or equal to the minimum of the number of rows and columns:
  $$ \text{rank}(\mathbf{A}) \leq \min(m, n) $$
- The rank of a matrix is equal to the number of non-zero singular values in its singular value decomposition (SVD).
- In square matrices, the rank gives insight into whether the matrix is **invertible**. A square matrix is invertible if and only if it has full rank.

### Full Rank Matrix

A **full rank matrix** is a matrix in which the rank is equal to the largest possible value for that matrix, meaning:

- For an $m \times n$ matrix $A$, the rank is the maximum number of linearly independent rows or columns.
  - If the rank is equal to $m$ (the number of rows), the matrix has **full row rank**.
  - If the rank is equal to $n$ (the number of columns), the matrix has **full column rank**.

#### For a square matrix ($m = n$):
- A square matrix is **full rank** if its rank is equal to its dimension, i.e., if the matrix is invertible.
- In this case, $\text{rank}(\bA) = n$, meaning all rows and columns are linearly independent, and the matrix has an inverse.

#### For a rectangular matrix ($m \neq n$):
- A matrix is **full rank** if the rank equals the smaller of the number of rows or columns. For an $m \times n$ matrix, the rank is at most $\min(m, n)$.
  - If the matrix has full row rank, all rows are linearly independent.
  - If the matrix has full column rank, all columns are linearly independent.

#### Example:

Consider the matrix:

$$
\bA = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}
$$

This is a $2 \times 3$ matrix. Since its two rows are linearly independent, it has **full row rank**, with rank = 2 (the number of rows). However, it does not have full column rank because it has only two independent rows for three columns.

#### Key Properties:
- A full rank matrix has **no redundant rows or columns** (no row or column can be written as a linear combination of others).
- A square matrix with full rank is **invertible** (non-singular).
- For a rectangular matrix, full rank implies the matrix has **maximal independent information** in terms of its rows or columns.

#### Importance:
- Full rank matrices are crucial in solving systems of linear equations. A system $\bA\mathbf{x} = \mathbf{b}$ has a unique solution if $\bA$ is a square, full rank matrix.
- In linear algebra and machine learning, the rank provides insight into the dimensionality and the independence of the data or transformation matrix.

### Inverse Matrix

An **inverse matrix** of a square matrix $\bA$, denoted as $\bA^{-1}$, is a matrix that, when multiplied by $\bA$, results in the identity matrix $I$. This relationship is expressed as:

$$
\bA \bA^{-1} = \bA^{-1} \bA = \bI
$$

where $\bI$ is the identity matrix, and its diagonal elements are 1, with all off-diagonal elements being 0.

#### Conditions for a Matrix to Have an Inverse:
- The matrix $\bA$ must be **square**, meaning it has the same number of rows and columns.
- The matrix $\bA$ must be **non-singular**, meaning its **determinant** is non-zero ($|\bA| \neq 0$).

##### Properties of the Inverse Matrix:
1. **Uniqueness:** If a matrix has an inverse, it is unique.
2. **Inverse of a Product:** The inverse of the product of two matrices $\bA$ and $\bB$ is given by $(\bA\bB)^{-1} = \bB^{-1} \bA^{-1}$.
3. **Inverse of the Inverse:** $(\bA^{-1})^{-1} = \bA$.
4. **Transpose of the Inverse:** $(\bA^{-1})' = (\bA')^{-1}$.

#### Special Case:

For a $2 \times 2$ matrix:

$$
\bA = \begin{bmatrix} a & b \\ c & d \end{bmatrix}
$$

The inverse of $\bA$ (if $|\bA|=\det(\bA) \neq 0$) is:

$$
A^{-1} = \frac{1}{ad - bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}
$$

where $ad - bc$ is the **determinant** of the matrix $\bA$.

### Positive Definite Matrix

A **positive definite matrix** is a symmetric matrix $\bA$ where, for any non-zero vector $\mathbf{x}$, the following condition holds:

$$
\mathbf{x}' \bA \mathbf{x} > 0
$$

#### Key Properties:
1. **Symmetry:** The matrix $\bA$ must be symmetric, meaning $\bA = \bA'$.
2. **Positive quadratic form:** For any non-zero vector $\mathbf{x}$, the quadratic form $\mathbf{x}' \bA \mathbf{x}$ must yield a positive value.

#### Characteristics of a Positive Definite Matrix:
- All the **eigenvalues** of a positive definite matrix are **positive**.
- The **determinants** of the leading principal minors (submatrices) of the matrix are positive.
- The **diagonal elements** of a positive definite matrix are positive.

#### Example:

The matrix:
$$
\bA = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}
$$
is positive definite, because for any non-zero vector $\mathbf{x}$, $\mathbf{x}' \bA \mathbf{x} > 0$. For instance, if $\mathbf{x} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$, then:

$$
\mathbf{x}' \bA \mathbf{x} = \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = 6 > 0
$$
### Singular Value Decomposition

**Singular Value Decomposition (SVD)** is a fundamental matrix factorization technique used in linear algebra to break down a matrix into three distinct components. It provides valuable insight into the structure of a matrix and is widely used in applications like data compression, signal processing, and dimensionality reduction.

#### Definition:
For any real (or complex) matrix $\mathbf{A}$ of size $m \times n$, the Singular Value Decomposition is given by:

$$
\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}'
$$

where:
- $\mathbf{U}$ is an $m \times m$ orthogonal matrix, whose columns are called the **left singular vectors**.
- $\mathbf{\Sigma}$ is an $m \times n$ diagonal matrix, where the diagonal entries are the **singular values** of $\mathbf{A}$. The singular values are always non-negative and arranged in decreasing order.
- $\mathbf{V}$ is an $n \times n$ orthogonal matrix, whose columns are called the **right singular vectors**.

#### Interpretation of the Components:
- $\mathbf{U}$ represents the orthonormal basis for the **column space** of $\mathbf{A}$.
- $\mathbf{V}$ represents the orthonormal basis for the **row space** of $\mathbf{A}$.
- $\mathbf{\Sigma}$ contains the singular values, which provide information about the importance or magnitude of the corresponding singular vectors. Large singular values indicate directions with significant data spread, while small or zero singular values correspond to directions with little or no data variation.

#### Geometric Interpretation:
SVD can be viewed geometrically as a transformation where:
1. $\mathbf{V}$ applies a rotation or reflection in the input space.
2. $\mathbf{\Sigma}$ stretches or compresses the data along certain axes.
3. $\mathbf{U}$ applies a final rotation or reflection in the output space.

#### Key Points:
- **Rank**: The number of non-zero singular values in $\mathbf{\Sigma}$ equals the rank of the matrix $\mathbf{A}$.
- **Dimensionality Reduction**: By truncating small singular values in $\mathbf{\Sigma}$, we can approximate $\mathbf{A}$ with a lower-rank matrix, which is useful in compressing data while retaining most of its structure.
- **Condition Number**: The ratio of the largest to the smallest non-zero singular value gives the condition number of the matrix, which indicates how sensitive a matrix is to numerical errors or perturbations.

#### Example:
For a matrix $\mathbf{A}$ of size $3 \times 2$, the SVD would look like:

$$
\mathbf{A} = \mathbf{U}
\begin{bmatrix}
\sigma_1 & 0 \\
0 & \sigma_2 \\
0 & 0
\end{bmatrix}
\mathbf{V}'
$$

where $\sigma_1$ and $\sigma_2$ are the singular values, and $\mathbf{U}$ and $\mathbf{V}$ are orthogonal matrices.

#### Applications of SVD:
- **Dimensionality Reduction**: SVD is widely used in Principal Component Analysis (PCA) for reducing the dimensionality of large datasets.
- **Low-Rank Approximations**: In data compression, SVD helps to approximate matrices with fewer dimensions while maintaining the core structure.
- **Solving Linear Systems**: In cases where a matrix is close to singular, SVD can be used to solve linear systems more stably.
- **Latent Semantic Analysis (LSA)**: In natural language processing, SVD is used to reduce the dimensionality of word-document matrices to capture latent relationships between words and documents.

### Eigendecomposition

**Eigendecomposition** is a matrix factorization technique used in linear algebra, where a square matrix is decomposed into its eigenvalues and eigenvectors. It is applicable to square matrices and provides deep insight into the matrix's structure, particularly in understanding transformations, systems of linear equations, and differential equations.

#### Definition:
Given a square matrix $\mathbf{A}$ of size $n \times n$, eigendecomposition is a factorization of the matrix into the following form:

$$
\mathbf{A} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^{-1}
$$

where:
- $\mathbf{V}$ is the matrix of **eigenvectors** of $\mathbf{A}$, and each column of $\mathbf{V}$ is an eigenvector.
- $\mathbf{\Lambda}$ is a diagonal matrix of **eigenvalues** of $\mathbf{A}$, with each diagonal element corresponding to an eigenvalue of $\mathbf{A}$.
- $\mathbf{V}^{-1}$ is the inverse of the matrix of eigenvectors.

#### Eigenvalues and Eigenvectors:
- **Eigenvalue** ($\lambda$): A scalar $\lambda$ is an eigenvalue of $\mathbf{A}$ if there exists a non-zero vector $\mathbf{v}$ such that:
  
  $$
  \mathbf{A} \mathbf{v} = \lambda \mathbf{v}
  $$

  In this case, $\mathbf{v}$ is called the eigenvector corresponding to the eigenvalue $\lambda$.

- **Eigenvector**: A non-zero vector $\mathbf{v}$ that remains parallel to itself (i.e., only scaled) when multiplied by $\mathbf{A}$ is called an eigenvector.

#### Conditions for Eigendecomposition:
- A matrix $\mathbf{A}$ is **diagonalizable** (i.e., it can be factored into $\mathbf{A} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^{-1}$) if and only if it has $n$ linearly independent eigenvectors.
- Not all matrices are diagonalizable. However, if $\mathbf{A}$ has $n$ distinct eigenvalues, then it is guaranteed to be diagonalizable.
- Symmetric matrices are always diagonalizable.

### Geometric Interpretation:
Eigendecomposition reveals the directions (eigenvectors) along which the matrix transformation $\mathbf{A}$ acts as a simple scaling by the eigenvalues. Geometrically:
- Eigenvectors point in directions that remain invariant under the transformation by $\mathbf{A}$.
- The corresponding eigenvalues tell us how much the matrix stretches or compresses vectors in the direction of those eigenvectors.

#### Example:
For a matrix $\mathbf{A}$:
$$
\mathbf{A} = \begin{bmatrix} 4 & 1 \\ 2 & 3 \end{bmatrix}
$$
The eigenvalues $\lambda_1 = 5$ and $\lambda_2 = 2$ can be found by solving the characteristic equation $\det(\mathbf{A} - \lambda \mathbf{I}) = 0$. Corresponding eigenvectors can then be computed, allowing the matrix to be diagonalized as:

$$
\mathbf{A} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^{-1}
$$

where $\mathbf{\Lambda} = \text{diag}(5, 2)$ and $\mathbf{V}$ is the matrix of eigenvectors.

#### Applications of Eigendecomposition:
- **Diagonalization**: Eigendecomposition allows matrices to be diagonalized, simplifying many computations (such as raising matrices to powers).
- **Principal Component Analysis (PCA)**: In data science, eigendecomposition is used in PCA to find directions of maximum variance in data.
- **Solving Differential Equations**: Eigenvalues and eigenvectors are useful in solving systems of linear differential equations.
- **Quantum Mechanics**: In physics, eigenvalues and eigenvectors describe the measurable properties (like energy levels) of systems.

In summary, eigendecomposition is a powerful tool in linear algebra that provides insight into how a matrix transforms space, offering valuable properties through its eigenvalues and eigenvectors.

### Idempotent Matrix

An **idempotent matrix** is a matrix that, when multiplied by itself, yields the same matrix. In other words, a matrix $\mathbf{M}$ is idempotent if it satisfies the condition:

$$
\mathbf{M}^2 = \mathbf{M}
$$

### Key Properties of Idempotent Matrices:

1. **Eigenvalues**: The eigenvalues of an idempotent matrix are either 0 or 1. This is because for an eigenvector $\mathbf{v}$ with eigenvalue $\lambda$, the equation $\mathbf{M}^2 \mathbf{v} = \mathbf{M} \mathbf{v}$ simplifies to $\lambda^2 \mathbf{v} = \lambda \mathbf{v}$, meaning $\lambda(\lambda - 1) = 0$, so $\lambda = 0$ or $\lambda = 1$.

2. **Rank**: The rank of an idempotent matrix $\mathbf{M}$ is equal to the trace of the matrix (the sum of the diagonal elements), which is also the number of eigenvalues equal to 1.

3. **Projection Interpretation**: Idempotent matrices often represent projection matrices in linear algebra. A projection matrix projects vectors onto a subspace, and applying the projection multiple times doesn't change the result beyond the first application, which is why it satisfies $\mathbf{M}^2 = \mathbf{M}$.

### Examples:

1. **Identity Matrix**: The identity matrix $\mathbf{I}$ is idempotent because:
   $$
   \mathbf{I}^2 = \mathbf{I}
   $$

2. **Zero Matrix**: The zero matrix $\mathbf{0}$ is also idempotent because:
   $$
   \mathbf{0}^2 = \mathbf{0}
   $$

3. **Projection Matrix**: Consider a projection matrix onto the x-axis in 2D:
   $$
   \mathbf{P} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}
   $$
   This matrix is idempotent since:
   $$
   \mathbf{P}^2 = \mathbf{P}
   $$

### Use in Statistics:
Idempotent matrices are commonly used in statistics, particularly in the context of regression analysis. For example, the **hat matrix** $\mathbf{H}$ in linear regression, which transforms the observed values into the predicted values, is idempotent:

$$
\mathbf{H} = \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top
$$
where $\mathbf{X}$ is the design matrix.

In summary, idempotent matrices have unique properties and are frequently encountered in linear algebra, projections, and statistical applications.


## Probability

Key probability concepts to understand include:

* Expected Value
* Variance
* Covariance
* Correlation
* Joint, Marginal, and Conditional Distributions
* Independence
* Central Limit Theorem
* Distributions:
  * Normal
  * Chi-Squared ($\chi^2$)
  * t-distribution
  * F-distribution

## Statistics

Essential statistical concepts include:

* **Point Estimation**:
  * Maximum Likelihood
  * Least Squares Estimation

* **Properties of Point Estimators**:
  * Unbiased
  * Consistent
  * Minimum Variance

* **Interval Estimation**
* **Hypothesis Testing**

## Calculus

Key calculus topics include:

* [Gradient](#gradient)
* [Hessian](#hessian)
* [Matrix Calculus](#matrix-calculus)
* Optimization

### Gradient

The **gradient** of a function is a vector that contains the partial derivatives of the function with respect to each of its variables. It points in the direction of the steepest ascent of the function, and its magnitude indicates the rate of change in that direction.

For a scalar function $f(x_1, x_2, \ldots, x_n)$, where $x_1, x_2, \ldots, x_n$ are the variables, the gradient is defined as:

$$
\nabla f = \frac{d}{d \mathbf{x}} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}
$$

#### Key Points:
- **Direction:** The gradient points in the direction of the greatest increase of the function.
- **Magnitude:** The magnitude of the gradient represents how fast the function increases in that direction.
- **Zero Gradient:** If $\nabla f = 0$, it indicates that the function has a critical point, which could be a local minimum, maximum, or saddle point.

#### Example:

For a function $f(x, y) = x^2 + y^2$, the gradient is:

$$
\nabla f = \begin{bmatrix} \frac{\partial}{\partial x} (x^2 + y^2) \\ \frac{\partial}{\partial y} (x^2 + y^2) \end{bmatrix} = \begin{bmatrix} 2x \\ 2y \end{bmatrix}
$$

This shows that the gradient points outward from the origin, and its magnitude increases as $x$ and $y$ increase.

#### Applications:
- In **optimization**, the gradient is used to find the minimum or maximum of a function (e.g., in gradient descent, a common optimization algorithm).
- In **vector calculus**, the gradient is used to describe the slope or rate of change of scalar fields (such as temperature, pressure, or altitude in physical applications).

### Hessian Matrix

The **Hessian matrix** is a square matrix of second-order partial derivatives of a scalar-valued function. It describes the local curvature of a multivariable function and is used to assess the nature of critical points (i.e., whether they are minima, maxima, or saddle points).

For a scalar function $f(x_1, x_2, \ldots, x_n)$, the Hessian matrix $\bH$ is defined as:

$$
\bH(f) = \frac{d}{d \mathbf{x} d\bx'} f(\mathbf{x}) =\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

#### Key Properties:
- The Hessian is **symmetric** if the second-order partial derivatives are continuous (by Clairaut's theorem, also called Schwarz's theorem).
- It provides important information about the local behavior of the function, particularly around critical points where the gradient is zero.
- **Eigenvalues** of the Hessian matrix determine the type of critical points:
  - If all eigenvalues are positive, the function has a **local minimum**.
  - If all eigenvalues are negative, the function has a **local maximum**.
  - If some eigenvalues are positive and others are negative, the function has a **saddle point**.

#### Example:

For a function $f(x, y) = x^2 + xy + y^2$, the Hessian matrix is:

$$
\bH(f) = \begin{bmatrix} 
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\ 
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2} 
\end{bmatrix} 
= \begin{bmatrix} 
2 & 1 \\ 
1 & 2 
\end{bmatrix}
$$

### Applications:
- In **optimization**, the Hessian is used to assess the convexity or concavity of a function, which helps in identifying the nature of critical points.
- In **machine learning**, it is used to optimize loss functions and can be part of second-order optimization methods like Newton's method.
- In **economics** and **engineering**, the Hessian helps in analyzing systems involving multiple variables and understanding how they interact with each other.

### Matrix Calculus

You need to know the following matrix calculus operations:

$$
\frac{d}{d \mathbf{x}} \left(\bc'\bx \right)
$$
$$
\frac{d}{d \mathbf{x}} \left(\bx'\bA \bx \right)
$$
$$
\frac{d}{d \mathbf{x} d\bx'} \left(\bx'\bA \bx \right)
$$

Let $\mathbf{c}$ be a constant vector and $\mathbf{x}$ be a variable vector, both of size \( n \times 1 \). We want to compute the derivative of the product:

$$
f(\mathbf{x}) = \mathbf{c}' \mathbf{x}
$$
Where:
$$
\mathbf{c}' \mathbf{x} = \sum_{i=1}^{n} c_i x_i
$$

To differentiate $f(\mathbf{x}) = \mathbf{c}' \mathbf{x}$ with respect to the variable vector $\mathbf{x}$, we take the derivative of each component separately:

$$
\nabla f = \frac{d}{d \mathbf{x}} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix} = \begin{bmatrix} \frac{\partial }{\partial x_1} (\mathbf{c}' \mathbf{x})  \\ \frac{\partial }{\partial x_2} (\mathbf{c}' \mathbf{x}) \\ \vdots \\ \frac{\partial }{\partial x_n} (\mathbf{c}' \mathbf{x}) \end{bmatrix} = \begin{bmatrix} \frac{\partial }{\partial x_1} \left(\sum_{i=1}^{n} c_i x_i\right)  \\ \frac{\partial }{\partial x_2} \left(\sum_{i=1}^{n} c_i x_i\right) \\ \vdots \\ \frac{\partial }{\partial x_n} \left(\sum_{i=1}^{n} c_i x_i\right) \end{bmatrix}
$$

Since $\mathbf{c}$ is a constant vector, the derivative of each term $c_i x_i$ is simply $c_i$, that is:

$$
\frac{d}{d x_j} \left(\sum_{i=1}^{n} c_i x_i\right) = c_j
$$

Thus, the derivative of the entire sum is the vector:

$$
\frac{d}{d \mathbf{x}} \left( \mathbf{c}' \mathbf{x} \right) = \begin{bmatrix} c_1  \\ c_2 \\ \vdots \\ c_n \end{bmatrix} = \mathbf{c}
$$

Now, let's go through the derivative of the quadratic form $f(\mathbf{x}) = \mathbf{x}' \bA \mathbf{x}$, where:

- $\mathbf{x}$ is a variable vector of size $n \times 1$,
- $\bA$ is a constant, symmetric matrix of size $n \times n$.

$$
f(\mathbf{x}) = \mathbf{x}' \bA \mathbf{x}
$$
First, expand the quadratic form:

$$
f(\mathbf{x}) = \sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j
$$
Then

$$
\nabla f = \frac{d}{d \mathbf{x}} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix} = \begin{bmatrix} \frac{\partial }{\partial x_1} (\mathbf{x}' \bA \mathbf{x})  \\ \frac{\partial }{\partial x_2} (\mathbf{x}' \bA \mathbf{x}) \\ \vdots \\ \frac{\partial }{\partial x_n} (\mathbf{x}' \bA \mathbf{x}) \end{bmatrix} = \begin{bmatrix} \frac{\partial }{\partial x_1} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right)  \\ \frac{\partial }{\partial x_2} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right) \\ \vdots \\ \frac{\partial }{\partial x_n} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right) \end{bmatrix}
$$
For each component $x_k$ in the vector $\mathbf{x}$, the derivative of $f(\mathbf{x})$ is:

$$
\frac{\partial}{\partial x_k} f(\mathbf{x}) = \frac{\partial}{\partial x_k} \left( \sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j \right) = \sum_{i=1}^{n} \sum_{j=1}^{n} \frac{\partial}{\partial x_k}  x_i a_{ij} x_j
$$

Each term $x_i a_{ij} x_j$ has two components that depend on $\bx$:

- If $i = j = k$, the derivative with respect to $x_k$ is:

$$
\frac{\partial}{\partial x_k} (x_i a_{ij} x_j) = 2 a_{kk} x_k
$$

- If $i \neq j$ and $i = k$, the derivative with respect to $x_k$ is:

$$
\frac{\partial}{\partial x_k} (x_i a_{ij} x_j) = a_{kj} x_j
$$
- Similarly, if $i \neq j$ and $j = k$, the derivative with respect to $x_k$ is:

$$
\frac{\partial}{\partial x_k} (x_i a_{ij} x_j) = a_{ik} x_i
$$
- Finally, if $i \neq k$ and $j \neq k$, then:

$$
\frac{\partial}{\partial x_k} (x_i a_{ij} x_j) = 0
$$
Then

$$
\frac{\partial}{\partial x_k} f(\mathbf{x}) = 2 a_{kk} x_k + \sum_{i \neq k} a_{ik} x_i + \sum_{j \neq k} a_{kj} x_j 
$$

Now since $\bA$ is symmetric ($a_{ij} = a_{ji}$), then:

\begin{align*}
\frac{\partial}{\partial x_k} f(\mathbf{x}) 
  &= 2 a_{kk} x_k + \sum_{i \neq k} a_{ik} x_i + \sum_{i \neq k} a_{ik} x_i \\
  &= 2 a_{kk} x_k + 2\sum_{i \neq k} a_{ik} x_i \\
  &= 2 \left(\sum_{i \neq k} a_{ik} x_i + a_{kk}x_k \right) \\
  &= 2 \left(\sum_{i = 1}^n a_{ki} x_i\right)   
\end{align*}

Then:

$$
\nabla f = \begin{bmatrix} \frac{\partial }{\partial x_1} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right)  \\ \frac{\partial }{\partial x_2} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right) \\ \vdots \\ \frac{\partial }{\partial x_n} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} x_i a_{ij} x_j\right) \end{bmatrix} = \begin{bmatrix} 2 \sum_{i = 1}^n a_{1i} x_i  \\ 2 \sum_{i = 1}^n a_{2i} x_i \\ \vdots \\ 2 \sum_{i = 1}^n a_{ni} x_i \end{bmatrix} = 2 \bA \bx
$$

Finally for the second derivative we have that:

In general, the Hessian matrix of a scalar function \( f(\mathbf{x}) \), where \( \mathbf{x} \in \mathbb{R}^n \) is a vector of variables, is a matrix that contains all the second-order partial derivatives of the function. It is defined as:

$$
\bH(f) = \frac{d^2 f}{d\bx d\bx'} =  \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix} 
= \begin{bmatrix}
\frac{\partial }{\partial x_1}\left(\frac{d f}{d\bx}\right)' \\
\frac{\partial }{\partial x_2}\left(\frac{d f}{d\bx}\right)' \\
\vdots  \\
\frac{\partial }{\partial x_n}\left(\frac{d f}{d\bx}\right)'
\end{bmatrix} 
= \begin{bmatrix}
\frac{\partial }{\partial x_1}\left(2\bA\bx\right)' \\
\frac{\partial }{\partial x_2}\left(2\bA\bx\right)' \\
\vdots  \\
\frac{\partial }{\partial x_n}\left(2\bA\bx\right)'
\end{bmatrix} 
$$

Now

$$
\frac{\partial }{\partial x_k}\left(2\bA\bx\right)
= 2\frac{\partial }{\partial x_k}\begin{bmatrix}
\sum_{i = 1}^n a_{1i} x_i \\
\sum_{i = 1}^n a_{2i} x_i \\
\vdots  \\
\sum_{i = 1}^n a_{ni} x_i
\end{bmatrix}
= 2 \begin{bmatrix}
\frac{\partial }{\partial x_k} \left(\sum_{i = 1}^n a_{1i} x_i \right)\\
\frac{\partial }{\partial x_k} \left(\sum_{i = 1}^n a_{2i} x_i \right)\\
\vdots  \\
\frac{\partial }{\partial x_k} \left(\sum_{i = 1}^n a_{ni} x_i \right)
\end{bmatrix}
= 2 \begin{bmatrix}
a_{k1} \\
a_{k2} \\
\vdots \\
a_{kn}
\end{bmatrix}
$$

Then

$$
\bH(f) = \frac{d^2 f}{d\bx d\bx'} 
= \begin{bmatrix}
2\begin{bmatrix}
a_{11} \\
a_{12} \\
\vdots \\
a_{1n}
\end{bmatrix}' \\
2\begin{bmatrix}
a_{21} \\
a_{22} \\
\vdots \\
a_{2n}
\end{bmatrix}' \\
\vdots \\
2\begin{bmatrix}
a_{n1} \\
a_{n2} \\
\vdots \\
a_{nn}
\end{bmatrix}'
\end{bmatrix}
=  2\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{bmatrix}
= 2 \bA
$$
